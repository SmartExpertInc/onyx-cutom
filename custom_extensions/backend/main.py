# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4
from cryptography.fernet import Fernet

# NEW: PDF manipulation imports
try:
    from PyPDF2 import PdfMerger
except ImportError:
    PdfMerger = None

# Feature management models
from app.models.feature_models import (
    FeatureDefinition, UserFeature, UserFeatureWithDetails,
    BulkFeatureToggleRequest, FeatureToggleRequest, UserTypeAssignmentRequest, UserTypeAssignmentRequest
)
from app.models.feature_models import UserTypeAssignmentRequest

# Workspace management models and services
from app.models.workspace_models import (
    Workspace, WorkspaceCreate, WorkspaceUpdate,
    WorkspaceWithMembers, WorkspaceRole, WorkspaceRoleCreate, WorkspaceRoleUpdate,
    WorkspaceMember, WorkspaceMemberCreate, WorkspaceMemberUpdate,
    ProductAccess, ProductAccessCreate
)
from app.services.workspace_service import WorkspaceService
from app.services.role_service import RoleService
from app.services.product_access_service import ProductAccessService

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_VIDEO_LESSON_PRESENTATION = "VideoLessonPresentationDisplay"  # New component for video lesson presentations
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"
COMPONENT_NAME_LESSON_PLAN = "LessonPlanDisplay"  # New component for lesson plans

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

SERPAPI_KEY = "ef10e9f3a1c8f0c2cd5d9379e39c597b58b6d0628f465c3030cace4d70494df7"

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

def parse_id_list(id_string: str, context_name: str) -> List[int]:
    """
    Parse a comma-separated string of IDs, handling negative integers (like -1 for special cases).
    
    Args:
        id_string: Comma-separated string of IDs (e.g., "1,2,3" or "-1" or "42")
        context_name: Context name for logging (e.g., "folder" or "file")
    
    Returns:
        List of parsed integer IDs
    """
    if not id_string:
        return []
    
    id_list = []
    try:
        for id_part in id_string.split(','):
            id_stripped = id_part.strip()
            if id_stripped.lstrip('-').isdigit():  # Allow negative numbers
                id_list.append(int(id_stripped))
            elif id_stripped:  # Log non-empty invalid parts
                logger.warning(f"[ID_PARSING] Skipping invalid {context_name} ID: '{id_stripped}'")
        
        logger.debug(f"[ID_PARSING] Parsed {context_name} IDs from '{id_string}': {id_list}")
        return id_list
    except Exception as e:
        logger.error(f"[ID_PARSING] Failed to parse {context_name} IDs from '{id_string}': {e}")
        return []

def should_use_hybrid_approach(payload) -> bool:
    """
    Determine if we should use the hybrid approach (Onyx for context extraction + OpenAI for generation).
    Returns True when file context is present or when connector-based filtering is requested.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this also uses hybrid approach)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Check if Knowledge Base search is requested
    has_knowledge_base = (
        hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
    )
    
    # Check if connector-based filtering is requested (including SmartDrive files)
    logger.info(f"🔍 [HYBRID_CHECK] Checking connector filtering:")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'fromConnectors'): {hasattr(payload, 'fromConnectors')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.fromConnectors: {getattr(payload, 'fromConnectors', None)}")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'connectorSources'): {hasattr(payload, 'connectorSources')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.connectorSources: {getattr(payload, 'connectorSources', None)}")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'selectedFiles'): {hasattr(payload, 'selectedFiles')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.selectedFiles: {getattr(payload, 'selectedFiles', None)}")
    
    has_connector_filtering = (
        hasattr(payload, 'fromConnectors') and payload.fromConnectors and
        (
            (hasattr(payload, 'connectorSources') and payload.connectorSources) or
            (hasattr(payload, 'selectedFiles') and payload.selectedFiles)
        )
    )
    
    logger.info(f"🔍 [HYBRID_CHECK] Final has_connector_filtering: {has_connector_filtering}")
    
    # Use hybrid approach when there's file context, text context, Knowledge Base search, or connector filtering
    use_hybrid = has_files or has_text_context or has_knowledge_base or has_connector_filtering
    
    logger.info(f"[HYBRID_SELECTION] has_files={has_files}, has_text_context={has_text_context}, has_knowledge_base={has_knowledge_base}, has_connector_filtering={has_connector_filtering}, use_hybrid={use_hybrid}")
    
    # EXTENSIVE DEBUG: Show why connector filtering failed
    if hasattr(payload, 'fromConnectors') and payload.fromConnectors:
        logger.info(f"🔍 [HYBRID_DEBUG] fromConnectors=True but connector_filtering={has_connector_filtering}")
        logger.info(f"🔍 [HYBRID_DEBUG] connectorSources check: hasattr={hasattr(payload, 'connectorSources')}, value={getattr(payload, 'connectorSources', None)}, truthy={bool(getattr(payload, 'connectorSources', None))}")
        logger.info(f"🔍 [HYBRID_DEBUG] selectedFiles check: hasattr={hasattr(payload, 'selectedFiles')}, value={getattr(payload, 'selectedFiles', None)}, truthy={bool(getattr(payload, 'selectedFiles', None))}")
    return use_hybrid

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))

# Optionally include Nextcloud origins (when iframe points to direct Nextcloud domain rather than proxied /smartdrive)
try:
    from urllib.parse import urlparse
    def _to_origin(url_value: Optional[str]) -> Optional[str]:
        if not url_value:
            return None
        u = url_value.strip()
        if not u:
            return None
        try:
            p = urlparse(u)
            if p.scheme and p.netloc:
                return f"{p.scheme}://{p.netloc}"
            if u.startswith("http://") or u.startswith("https://"):
                return u.rstrip('/')
            return None
        except Exception:
            return None

    nc_candidates = [
        os.environ.get("NEXTCLOUD_BASE_URL"),
        os.environ.get("NEXTCLOUD_PUBLIC_SHARE_DOMAIN"),
        os.environ.get("NEXTCLOUD_ALLOWED_ORIGIN"),
    ]
    nc_origins = [o for o in map(_to_origin, nc_candidates) if o]
    if nc_origins:
        effective_origins = list(set(effective_origins + nc_origins))
except Exception:
    pass

if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

class ImageBlock(BaseContentBlock):
    type: str = "image"
    src: str
    alt: Optional[str] = None
    caption: Optional[str] = None
    width: Optional[Union[int, str]] = None
    height: Optional[Union[int, str]] = None
    alignment: Optional[str] = "center"
    borderRadius: Optional[str] = "8px"
    maxWidth: Optional[str] = "100%"
    # Layout mode fields for positioning
    layoutMode: Optional[str] = None  # 'standalone', 'inline-left', 'inline-right'
    layoutPartnerIndex: Optional[int] = None  # Index of the content block to pair with for side-by-side layouts
    layoutProportion: Optional[str] = None  # '50-50', '60-40', '40-60', '70-30', '30-70'
    float: Optional[str] = None  # Legacy field for backward compatibility: 'left', 'right'

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# Lesson Plan Generation Models
class LessonPlanGenerationRequest(BaseModel):
    outlineProjectId: int
    lessonTitle: str
    moduleName: str
    lessonNumber: int
    recommendedProducts: List[str]

# Content Development Specifications Models for Lesson Plans
class ContentTextBlock(BaseModel):
    type: Literal["text"] = "text"
    block_title: str
    block_content: str  # Can contain plain text, bullet lists (with -), or numbered lists (with 1.)

class ContentProductBlock(BaseModel):
    type: Literal["product"] = "product"
    product_name: str  # e.g., "video-lesson", "presentation", "quiz", "one-pager"
    product_description: str

ContentBlock = Union[ContentTextBlock, ContentProductBlock]

class LessonPlanData(BaseModel):
    lessonTitle: str
    lessonObjectives: List[str]
    shortDescription: str
    contentDevelopmentSpecifications: List[ContentBlock]  # New flowing structure
    materials: List[str]
    suggestedPrompts: List[str]

class LessonPlanResponse(BaseModel):
    success: bool
    project_id: int
    lesson_plan_data: LessonPlanData
    message: str

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    },
    {
      "slideId": "slide_16_table_dark",
      "slideNumber": 16,
      "slideTitle": "Technology Comparison",
      "templateId": "table-dark",
      "props": {
        "title": "Technology Comparison",
        "tableData": {
          "headers": ["Technology", "Performance", "Security", "Cost"],
          "rows": [
            ["React", "High", "Good", "Free"],
            ["Vue.js", "Medium", "Excellent", "Free"],
            ["Angular", "High", "Excellent", "Free"]
          ]
        }
      }
    },
    {
      "slideId": "slide_17_table_light",
      "slideNumber": 17,
      "slideTitle": "Product Features",
      "templateId": "table-light",
      "props": {
        "title": "Product Features Comparison",
        "tableData": {
          "headers": ["Feature", "Basic Plan", "Pro Plan", "Enterprise"],
          "rows": [
            ["Storage", "10GB", "100GB", "Unlimited"],
            ["Users", "5", "25", "Unlimited"],
            ["Support", "Email", "Priority", "24/7"]
          ]
        }
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example Video Lesson with Voiceover",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "big-image-left",
      "voiceoverText": "Welcome to this comprehensive lesson. Today we'll explore the fundamentals of our topic, breaking down complex concepts into easy-to-understand segments. This introduction sets the stage for what you're about to learn.",
      "props": {
          "title": "Welcome to the Lesson",
          "subtitle": "This slide introduces the main topic.",
          "imageUrl": "https://via.placeholder.com/600x400?text=Your+Image",
          "imageAlt": "Descriptive alt text",
          "imagePrompt": "A high-quality illustration that visually represents the lesson introduction",
          "imageSize": "large"
      }
    },
    {
      "slideId": "slide_2_main",
      "slideNumber": 2,
      "slideTitle": "Main Concepts",
      "templateId": "content-slide",
      "voiceoverText": "Now let's dive into the core concepts. These fundamental ideas form the foundation of our understanding. We'll explore each concept in detail, ensuring you have a solid grasp before moving forward.",
      "props": {
        "title": "Core Ideas",
        "content": "These concepts form the foundation of understanding.\n\n• First important concept\n• Second important concept\n• Third important concept",
        "alignment": "left"
      }
    },
    {
      "slideId": "slide_3_bullets",
      "slideNumber": 3,
      "slideTitle": "Key Points",
      "templateId": "bullet-points",
      "voiceoverText": "Here are the key takeaways from our lesson. Each of these points represents a critical insight that you should remember. Let me walk you through each one to ensure you understand their significance.",
      "props": {
        "title": "Key Points",
        "bullets": [
          "First important point",
          "Second key insight",
          "Third critical element"
        ],
        "maxColumns": 2,
        "bulletStyle": "dot",
        "imagePrompt": "A relevant illustration for the bullet points, e.g. 'Checklist, modern flat style, purple and yellow accents'",
        "imageAlt": "Illustration for bullet points"
      }
    },
    {
      "slideId": "slide_4_two_column",
      "slideNumber": 4,
      "slideTitle": "Comparison Analysis",
      "templateId": "two-column",
      "voiceoverText": "Let's examine this topic from two different perspectives. On the left, we have one approach, and on the right, we have another. Both perspectives are valuable and complement each other to give you a complete understanding.",
      "props": {
        "title": "Two Column Layout",
        "leftTitle": "Left Column Title",
        "leftContent": "Content for the left side with detailed explanations",
        "rightTitle": "Right Column Title",
        "rightContent": "Content for the right side with detailed information",
        "columnRatio": "50-50"
      }
    },
    {
      "slideId": "slide_5_four_box",
      "slideNumber": 5,
      "slideTitle": "Four Key Areas",
      "templateId": "four-box-grid",
      "voiceoverText": "Now we'll explore four essential areas that are crucial to understanding this topic. Each box represents a different aspect, and together they provide a comprehensive overview of the subject matter.",
      "props": {
        "title": "Main Title for Four Boxes",
        "boxes": [
          { "heading": "Box 1 Heading", "text": "Detailed description for the first box" },
          { "heading": "Box 2 Heading", "text": "Comprehensive explanation for the second box" },
          { "heading": "Box 3 Heading", "text": "Thorough description for the third box" },
          { "heading": "Box 4 Heading", "text": "In-depth explanation for the fourth box" }
        ]
      }
    },
    {
      "slideId": "slide_6_challenges",
      "slideNumber": 6,
      "slideTitle": "Problem Solving",
      "templateId": "challenges-solutions",
      "voiceoverText": "Every field has its challenges, but for every challenge, there's a solution. Let's examine the common obstacles you might face and the proven strategies to overcome them effectively.",
      "props": {
        "title": "Challenges and Solutions",
        "challengesTitle": "Common Challenges",
        "solutionsTitle": "Effective Solutions",
        "challenges": [
          "Challenge 1 with detailed explanation of the problem",
          "Challenge 2 with comprehensive analysis of the issue"
        ],
        "solutions": [
          "Solution 1 with detailed approach and implementation strategy",
          "Solution 2 with comprehensive methodology and practical steps"
        ]
      }
    },
    {
      "slideId": "slide_7_process",
      "slideNumber": 7,
      "slideTitle": "Step-by-Step Process",
      "templateId": "process-steps",
      "voiceoverText": "Finally, let's look at the practical implementation. This step-by-step process shows you exactly how to apply what you've learned. Follow along carefully as we go through each step together.",
      "props": {
        "title": "Implementation Steps",
        "steps": [
          "Analyze the requirements carefully",
          "Design the solution architecture",
          "Implement core functionality",
          "Test and validate results"
        ]
      }
    },
    {
      "slideId": "slide_5_table_dark",
      "slideNumber": 5,
      "slideTitle": "Technology Comparison",
      "templateId": "table-dark",
      "voiceoverText": "Let's examine the technology comparison table. This table shows us the key differences between various technologies in terms of performance, security, and cost. Understanding these comparisons helps us make informed decisions.",
      "props": {
        "title": "Technology Comparison",
        "tableData": {
          "headers": ["Technology", "Performance", "Security", "Cost"],
          "rows": [
            ["React", "High", "Good", "Free"],
            ["Vue.js", "Medium", "Excellent", "Free"],
            ["Angular", "High", "Excellent", "Free"]
          ]
        }
      }
    },
    {
      "slideId": "slide_6_table_light",
      "slideNumber": 6,
      "slideTitle": "Product Features",
      "templateId": "table-light",
      "voiceoverText": "Now let's look at the product features comparison. This table clearly shows the differences between our various subscription plans, helping you understand what each tier offers.",
      "props": {
        "title": "Product Features Comparison",
        "tableData": {
          "headers": ["Feature", "Basic Plan", "Pro Plan", "Enterprise"],
          "rows": [
            ["Storage", "10GB", "100GB", "Unlimited"],
            ["Users", "5", "25", "Unlimited"],
            ["Support", "Email", "Priority", "24/7"]
          ]
        }
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en",
  "hasVoiceover": true
}
"""

def normalize_slide_props(slides: List[Dict], component_name: str = None) -> List[Dict]:
    """
    Normalize slide props to match frontend template schemas.
    
    This function fixes common prop mismatches between AI-generated JSON
    and the expected frontend template schemas. Invalid slides are automatically
    removed to prevent rendering errors.
    
    Args:
        slides: List of slide dictionaries to normalize
        component_name: Component type (e.g., COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
                       Used to determine if voiceoverText should be preserved
    """
    if not slides:
        return slides
        
    normalized_slides = []
    
    for slide_index, slide in enumerate(slides):
        if not isinstance(slide, dict) or 'templateId' not in slide or 'props' not in slide:
            normalized_slides.append(slide)
            continue
            
        template_id = slide.get('templateId')
        props = slide.get('props', {})
        
        # Create a copy to avoid modifying the original
        normalized_slide = slide.copy()
        normalized_props = props.copy()
        
        try:
            # ENHANCED LOGGING: Log raw AI-parsed content for big-numbers and event-list templates
            if template_id in ['big-numbers', 'event-dates', 'event-list']:
                logger.info(f"=== RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                logger.info(f"Template ID: {template_id}")
                logger.info(f"Slide Title: {normalized_props.get('title', 'NO TITLE')}")
                logger.info(f"Raw Props Keys: {list(normalized_props.keys())}")
                logger.info(f"Raw Props Content: {normalized_props}")
                
                # Log specific content for big-numbers
                if template_id == 'big-numbers':
                    items = normalized_props.get('items', [])
                    numbers = normalized_props.get('numbers', [])
                    steps = normalized_props.get('steps', [])
                    logger.info(f"Big-Numbers Raw Items: {items}")
                    logger.info(f"Big-Numbers Raw Numbers: {numbers}")
                    logger.info(f"Big-Numbers Raw Steps: {steps}")
                
                # Log specific content for event-list/event-dates
                if template_id in ['event-dates', 'event-list']:
                    events = normalized_props.get('events', [])
                    items = normalized_props.get('items', [])
                    logger.info(f"Event-List Raw Events: {events}")
                    logger.info(f"Event-List Raw Items: {items}")
                
                logger.info(f"=== END RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
            
            # Fix template ID mappings first
            if template_id == 'event-dates':
                # Map event-dates (AI instruction) to event-list (frontend registry)
                template_id = 'event-list'
                normalized_slide['templateId'] = template_id
                
            # Ensure critical props are preserved for all templates
            # Fix missing imagePrompt and other content issues
            if 'imagePrompt' not in normalized_props and 'imageAlt' in normalized_props:
                # If we have imageAlt but no imagePrompt, use imageAlt as the prompt
                normalized_props['imagePrompt'] = normalized_props['imageAlt']
            
            # Generate missing imagePrompts for templates that require them
            if template_id in ['bullet-points', 'bullet-points-right'] and not normalized_props.get('imagePrompt'):
                title = normalized_props.get('title', 'concept')
                bullets = normalized_props.get('bullets', [])
                
                # Create a contextual prompt based on title and content keywords
                title_lower = title.lower()
                content_sample = ' '.join(bullets[:2]) if bullets else ''
                content_lower = content_sample.lower()
                
                if 'tool' in title_lower or 'software' in content_lower or 'application' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a developer using programming tools at a clean workstation. The scene features a young Asian woman sitting at a modern desk with a single laptop displaying simple code interface elements (no readable text) and one external monitor showing basic geometric development tool mockups. A wireless keyboard and mouse are positioned on the desk alongside a coffee cup. The laptop screen and coding interface are [COLOR1], the external monitor and keyboard are [COLOR2], and the desk and accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'trend' in title_lower or 'future' in title_lower or 'innovation' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of futuristic technology concepts in a clean tech environment. The scene features a Hispanic male scientist in a white lab coat standing next to a single large holographic display showing simple geometric patterns and flowing data visualizations (no readable text). A modern desk with a tablet displaying basic technology interface elements sits nearby. The holographic display and data flows are [COLOR1], the scientist's lab coat and tablet are [COLOR2], and the desk and lab environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'learn' in title_lower or 'education' in title_lower or 'skill' in content_lower or 'training' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of modern learning in a clean educational environment. The scene features a young Black female student sitting at a modern desk using a tablet displaying simple educational interface elements and geometric learning modules (no readable text). A single interactive whiteboard in the background shows basic diagrams with simple shapes and connecting lines. Educational materials like a notebook and digital stylus are positioned on the desk. The tablet interface and learning modules are [COLOR1], the interactive whiteboard and educational tools are [COLOR2], and the desk and educational environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'business' in title_lower or 'strategy' in content_lower or 'management' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional business strategy and management in a modern corporate environment. The scene features a contemporary conference room with three business professionals engaged in strategic planning. A confident Latina businesswoman in a navy blazer stands at the left presenting to a large wall display showing simple business charts, process flows, and strategic diagrams with geometric shapes (no readable text). In the center, a Black male executive sits at a glass conference table reviewing documents and tablets displaying abstract business analytics as simple bar charts and pie segments. On the right, a Caucasian female manager takes notes while sitting in an ergonomic chair, with a laptop showing business interface mockups with geometric layouts. Business materials like documents, tablets, coffee cups, and strategic planning boards are arranged throughout the professional space. The presentation displays and business interfaces are [COLOR1], conference furniture and professional devices are [COLOR2], documents and planning materials are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                else:
                    # General professional/educational fallback
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional collaboration and knowledge sharing in a modern educational environment. The scene features three diverse professionals working together in a bright, contemporary workspace. A young Hispanic woman in business attire sits at a modern desk on the left, using a laptop displaying simple interface elements and geometric data visualizations (no readable text). In the center, a Black male professional stands presenting to a wall-mounted display showing abstract concepts as interconnected nodes, flowcharts, and simple diagrams with geometric shapes. On the right, a Caucasian female colleague sits in a comfortable chair reviewing materials on a tablet, with documents and notebooks arranged on a side table. Professional tools like laptops, tablets, notebooks, coffee cups, and presentation materials are positioned throughout the collaborative workspace. The digital displays and interface elements are [COLOR1], professional devices and presentation tools are [COLOR2], furniture and workspace accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                normalized_props['imageAlt'] = f"Professional illustration for {title}"
            
            # Ensure subtitle/content exists for templates that need it
            if template_id in ['big-image-left', 'big-image-top']:
                if 'subtitle' not in normalized_props and 'content' in normalized_props:
                    normalized_props['subtitle'] = normalized_props['content']
                # Ensure subtitle is different from title
                if (normalized_props.get('subtitle') == normalized_props.get('title') and 
                    len(normalized_props.get('subtitle', '')) > 50):
                    # If subtitle equals title and is long, use it as subtitle and create shorter title
                    full_text = normalized_props['subtitle']
                    # Extract first sentence as title
                    sentences = full_text.split('. ')
                    if len(sentences) > 1:
                        normalized_props['title'] = sentences[0]
                        normalized_props['subtitle'] = '. '.join(sentences[1:])
                        
            # Fix template selection for analytics/evaluation content
            if (template_id == 'metrics-analytics' and 
                'metrics' in normalized_props and 
                isinstance(normalized_props['metrics'], list) and 
                len(normalized_props['metrics']) <= 3):
                # If metrics-analytics has only bullet points, convert to bullet-points template
                logger.info(f"Converting slide {slide_index + 1} from metrics-analytics to bullet-points (better fit)")
                normalized_slide['templateId'] = 'bullet-points'
                template_id = 'bullet-points'
                normalized_props['bullets'] = normalized_props.pop('metrics')
                # Add image prompt for bullet-points
                if not normalized_props.get('imagePrompt'):
                    title = normalized_props.get('title', 'concepts')
                    title_lower = title.lower()
                    
                    # Generate contextual, detailed image prompts for metrics/analytics content
                    if 'metric' in title_lower or 'analytic' in title_lower or 'performance' in title_lower:
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern data analytics workspace. The scene features a professional data analyst sitting at a clean desk with a laptop displaying simple geometric charts and performance dashboards (no readable text). A large monitor shows flowing data visualizations with abstract patterns and trends. The workspace includes notebooks, a coffee cup, and modern office accessories. Natural light streams through large windows. The laptop charts and data visualizations are [COLOR1], the monitor and office equipment are [COLOR2], and the workspace environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'tracking' in title_lower or 'monitoring' in title_lower:
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern monitoring and tracking center. The scene features a professional analyst standing next to a large wall display showing flowing geometric patterns representing tracking systems and monitoring data. A clean workstation with a tablet displaying simple interface elements sits nearby. The environment is bright and contemporary with floor-to-ceiling windows. The wall display and tracking patterns are [COLOR1], the analyst's attire and tablet are [COLOR2], and the monitoring center environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # General professional data/analytics fallback
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern professional workspace focused on {title.lower()}. The scene features a diverse professional in business attire working at a contemporary desk with a laptop displaying simple data interface elements and geometric visualizations (no readable text). Professional tools like a tablet, notebooks, and a coffee cup are positioned around the clean workspace. Large windows provide natural light to the modern office environment. The laptop interface and data displays are [COLOR1], the professional's attire and desk accessories are [COLOR2], and the office environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    
                    normalized_props['imageAlt'] = f"Professional illustration for {title}"
                    
            # This big-numbers conversion logic is moved to after big-numbers normalization below
                
            # ENHANCED LOGGING: Log raw AI-parsed content for big-numbers and event-list templates
            if template_id in ['big-numbers', 'event-dates', 'event-list']:
                logger.info(f"=== RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                logger.info(f"Template ID: {template_id}")
                logger.info(f"Slide Title: {normalized_props.get('title', 'NO TITLE')}")
                logger.info(f"Raw Props Keys: {list(normalized_props.keys())}")
                logger.info(f"Raw Props Content: {normalized_props}")
                
                # Log specific content for big-numbers
                if template_id == 'big-numbers':
                    items = normalized_props.get('items', [])
                    numbers = normalized_props.get('numbers', [])
                    steps = normalized_props.get('steps', [])
                    logger.info(f"Big-Numbers Raw Items: {items}")
                    logger.info(f"Big-Numbers Raw Numbers: {numbers}")
                    logger.info(f"Big-Numbers Raw Steps: {steps}")
                
                # Log specific content for event-list/event-dates
                if template_id in ['event-dates', 'event-list']:
                    events = normalized_props.get('events', [])
                    items = normalized_props.get('items', [])
                    logger.info(f"Event-List Raw Events: {events}")
                    logger.info(f"Event-List Raw Items: {items}")
                
                logger.info(f"=== END RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                
            # Fix big-numbers template props
            if template_id == 'big-numbers':
                # FIXED: Accept 'items', 'numbers', or 'steps' as the source array
                source_list = normalized_props.get('items')
                source_type = 'items'
                
                if not (isinstance(source_list, list) and source_list):
                    alt_list = normalized_props.get('numbers')
                    if isinstance(alt_list, list) and alt_list:
                        logger.info(f"Normalizing 'big-numbers' slide {slide_index + 1} from 'numbers' → 'items'")
                        source_list = alt_list
                        source_type = 'numbers'
                    else:
                        # FIXED: Also check for 'steps' which AI commonly generates
                        steps_list = normalized_props.get('steps')
                        if isinstance(steps_list, list) and steps_list:
                            logger.info(f"Normalizing 'big-numbers' slide {slide_index + 1} from 'steps' → 'items'")
                            source_list = steps_list
                            source_type = 'steps'
                        else:
                            source_list = []

                # Validate and coerce each item
                fixed_items = []
                for item in source_list:
                    if isinstance(item, dict):
                        fixed_item = {
                            'value': str(item.get('value') or item.get('number') or '').strip(),
                            'label': str(item.get('label') or item.get('title') or '').strip(),
                            'description': str(item.get('description') or item.get('desc') or item.get('text') or '').strip()
                        }
                        if fixed_item['value'] and fixed_item['label']:
                            fixed_items.append(fixed_item)

                # Pad/trim to exactly 3 items to preserve slide instead of skipping
                if len(fixed_items) != 3:
                    logger.warning(f"Coercing slide {slide_index + 1} with template 'big-numbers': Expected 3 items, got {len(fixed_items)}")
                    while len(fixed_items) < 3:
                        idx = len(fixed_items) + 1
                        fixed_items.append({'value': '0', 'label': f'Item {idx}', 'description': 'No description available'})
                    if len(fixed_items) > 3:
                        fixed_items = fixed_items[:3]

                # Frontend expects 'steps' not 'items' for big-numbers template
                normalized_props['steps'] = fixed_items
                # Drop legacy keys to unify shape
                if 'numbers' in normalized_props:
                    normalized_props.pop('numbers', None)
                if 'items' in normalized_props:
                    normalized_props.pop('items', None)
                
                # FIXED: Don't convert big-numbers to bullet-points to prevent mixed language content
                # Check if we generated placeholder content but preserve big-numbers template
                has_placeholder_content = all(
                    step.get('value', '').strip() in ['0', ''] and 
                    step.get('label', '').startswith('Item ') and
                    step.get('description', '') == 'No description available'
                    for step in fixed_items
                )
                
                # FIXED: Completely disable problematic conversion to bullet-points
                # This prevents mixed language content by preserving AI-generated big-numbers slides
                if has_placeholder_content:
                    logger.info(f"FIXED: Detected placeholder content in big-numbers slide {slide_index + 1}")
                    logger.info(f"Preserving big-numbers template instead of converting to bullet-points to prevent mixed language issues")
                
                # Always preserve big-numbers template and add image prompt if needed
                    if not normalized_props.get('imagePrompt'):
                        slide_title = normalized_props.get('title', 'concepts')
                    # Generate language-neutral image prompt for big-numbers
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern data analytics environment. The scene features a professional analyst working at a clean desk with multiple monitors displaying charts, graphs, and numerical data visualizations (no readable text or numbers). The workspace includes a laptop, tablet, and organized documents. Large windows provide natural light to the modern office space. The data visualizations and analytics displays are [COLOR1], the analyst's attire and equipment are [COLOR2], and the office environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    normalized_props['imageAlt'] = f"Professional data visualization illustration for {slide_title}"
                
                # Log final processed big-numbers content
                logger.info(f"=== FINAL PROCESSED BIG-NUMBERS for slide {slide_index + 1} ===")
                logger.info(f"Final Steps: {normalized_props.get('steps', [])}")
                logger.info(f"Final Props: {normalized_props}")
                logger.info(f"=== END FINAL PROCESSED BIG-NUMBERS for slide {slide_index + 1} ===")
                    
            # Fix four-box-grid template props
            elif template_id == 'four-box-grid':
                boxes = normalized_props.get('boxes', [])
                if boxes and isinstance(boxes, list):
                    fixed_boxes = []
                    for box in boxes:
                        if isinstance(box, dict):
                            # Convert title/content to heading/text
                            fixed_box = {
                                'heading': box.get('heading') or box.get('title', ''),
                                'text': box.get('text') or box.get('content', '')
                            }
                            if fixed_box['heading']:  # Only add if heading exists
                                fixed_boxes.append(fixed_box)
                    normalized_props['boxes'] = fixed_boxes
                    
            # Fix two-column template props
            elif template_id == 'two-column':
                # Handle missing right column content by splitting existing content if it's substantial
                if (not normalized_props.get('rightContent') or normalized_props.get('rightContent') == '') and normalized_props.get('leftContent'):
                    left_content = normalized_props.get('leftContent', '')
                    
                    # If left content is substantial and right is empty, try to split content intelligently
                    if len(left_content) > 100:  # Only split if there's substantial content
                        lines = left_content.split('\n')
                        if len(lines) >= 2:
                            # Split content roughly in half
                            mid_point = len(lines) // 2
                            left_part = '\n'.join(lines[:mid_point]).strip()
                            right_part = '\n'.join(lines[mid_point:]).strip()
                            
                            if left_part and right_part:
                                normalized_props['leftContent'] = left_part
                                normalized_props['rightContent'] = right_part
                                logger.info(f"Split two-column content for slide {slide_index + 1}")
                
                # Generate appropriate titles if missing
                if not normalized_props.get('rightTitle') or normalized_props.get('rightTitle') == '':
                    title = normalized_props.get('title', '')
                    left_title = normalized_props.get('leftTitle', '')
                    
                    # Generate contextual right title based on slide content
                    if 'advantages' in left_title.lower() or 'benefits' in left_title.lower():
                        normalized_props['rightTitle'] = 'Disadvantages' if 'advantages' in left_title.lower() else 'Challenges'
                    elif 'pros' in left_title.lower():
                        normalized_props['rightTitle'] = 'Cons'
                    elif 'before' in left_title.lower():
                        normalized_props['rightTitle'] = 'After'
                    elif 'strategies' in title.lower() or 'methods' in title.lower():
                        normalized_props['rightTitle'] = 'Implementation' if left_title else 'Best Practices'
                    else:
                        # Generic fallback
                        normalized_props['rightTitle'] = 'Additional Details'
                
                # Ensure leftTitle exists
                if not normalized_props.get('leftTitle'):
                    normalized_props['leftTitle'] = 'Key Points'
                
                # Generate missing image prompts for two-column templates (check each side independently)
                title = normalized_props.get('title', 'comparison')
                left_title = normalized_props.get('leftTitle', 'concept')
                right_title = normalized_props.get('rightTitle', 'concept')
                
                # Generate left image prompt if missing using detailed format
                if not normalized_props.get('leftImagePrompt') and normalized_props.get('leftContent'):
                    left_content_sample = normalized_props.get('leftContent', '')[:100].lower()
                    if 'network' in left_content_sample or 'event' in left_content_sample or 'meeting' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing people networking and collaborating. The scene features a group of diverse professionals in business attire engaging in conversations, exchanging ideas, and building connections in a modern corporate environment. People are positioned throughout the scene in small groups, with some standing and others sitting around tables. Professional networking activities are highlighted with [COLOR1], conversation indicators in [COLOR2], and background elements in [COLOR3]. NO text, labels, or readable content on any surfaces or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in left_content_sample or 'digital' in left_content_sample or 'system' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} featuring modern technology and digital systems. The scene shows interconnected technological components, digital interfaces, and system architectures with detailed visual elements. Main technology components are [COLOR1], connecting elements are [COLOR2], and supporting details are [COLOR3]. All screens and displays show abstract geometric patterns with NO readable text or labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'process' in left_content_sample or 'step' in left_content_sample or 'method' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing a detailed process workflow. The scene features sequential steps connected by arrows, with clear visual indicators for each stage of the process. Process elements are rendered in [COLOR1], connecting arrows in [COLOR2], and step indicators in [COLOR3]. Use symbols and geometric shapes instead of text labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        left_content_lower = normalized_props.get('leftContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in left_content_lower or 'education' in left_content_lower or 'student' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of students learning in a modern classroom setting. The scene features diverse students sitting at desks with laptops, a teacher presenting at the front, and educational materials around the room. Students are engaged and taking notes, with some raising hands to ask questions. Student laptops and materials are [COLOR1], the teacher and presentation board are [COLOR2], and classroom furniture is [COLOR3]. No readable text on any surfaces. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in left_content_lower or 'meeting' in left_content_lower or 'team' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of professionals collaborating in a modern office meeting room. The scene features a diverse group of business people sitting around a conference table, engaged in discussion, with one person presenting ideas. Laptops and documents are on the table, and a presentation screen shows charts. Conference table and laptops are [COLOR1], people and presentation screen are [COLOR2], and office furniture is [COLOR3]. No readable text anywhere. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in left_content_lower or 'analysis' in left_content_lower or 'research' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of a data analyst working with information in a modern office. The scene features a focused professional at a desk with multiple monitors displaying charts and graphs, surrounded by organized workspace elements. The person is analyzing data patterns on screen while taking notes. Computer monitors and data visualizations are [COLOR1], the analyst and desk setup are [COLOR2], and office environment is [COLOR3]. All screens show abstract geometric patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of people working together in a professional environment related to {left_title.lower()}. The scene features diverse professionals engaged in relevant activities, using modern tools and technology. The setting shows a clean, organized workspace with people collaborating effectively. Primary work elements are [COLOR1], people and main activities are [COLOR2], and environmental details are [COLOR3]. No readable text on any surfaces or screens. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Generate right image prompt if missing using detailed format
                if not normalized_props.get('rightImagePrompt') and normalized_props.get('rightContent'):
                    right_content_sample = normalized_props.get('rightContent', '')[:100].lower()
                    if 'association' in right_content_sample or 'professional' in right_content_sample or 'group' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing professional associations and industry connections. The scene features business professionals in formal attire attending conferences, participating in panel discussions, and engaging in professional development activities. A large conference hall setting with speakers, audience members, and networking areas. Association activities are highlighted in [COLOR1], professional interactions in [COLOR2], and venue elements in [COLOR3]. NO visible text on presentations, banners, or displays - use abstract symbols instead. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in right_content_sample or 'digital' in right_content_sample or 'system' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} featuring advanced technology and digital innovation. The scene shows cutting-edge technological solutions, modern interfaces, and innovative systems with comprehensive visual details. Technology components are [COLOR1], interface elements are [COLOR2], and innovation indicators are [COLOR3]. All digital displays show abstract geometric patterns with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'strategy' in right_content_sample or 'approach' in right_content_sample or 'solution' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing strategic planning and solution implementation. The scene features strategic diagrams, planning documents, and implementation frameworks with detailed visual representations. Strategic elements are [COLOR1], planning components are [COLOR2], and implementation details are [COLOR3]. Documents and charts show abstract shapes and symbols with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        right_content_lower = normalized_props.get('rightContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in right_content_lower or 'education' in right_content_lower or 'student' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of students using technology for learning. The scene features diverse students in a modern computer lab, working on individual projects with tablets and laptops. Some students are collaborating on assignments while others focus independently. A teacher walks among them providing guidance. Student devices and work materials are [COLOR1], students and teacher are [COLOR2], and lab environment is [COLOR3]. No readable text on any screens or materials. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in right_content_lower or 'meeting' in right_content_lower or 'team' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of business professionals in a client presentation meeting. The scene features a confident presenter explaining concepts to seated clients, with visual aids displayed on a large screen. The atmosphere is professional and engaging, with participants actively listening and taking notes. Presentation equipment and materials are [COLOR1], presenter and clients are [COLOR2], and meeting room elements are [COLOR3]. No readable text on presentations or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in right_content_lower or 'analysis' in right_content_lower or 'research' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of researchers collaborating on data analysis in a modern lab setting. The scene features scientists and analysts working together around computer workstations, discussing findings and sharing insights. Multiple screens display data visualizations while team members point to specific patterns. Computer equipment and data displays are [COLOR1], researchers and team members are [COLOR2], and lab environment is [COLOR3]. All screens show abstract patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of professionals implementing solutions in a modern workplace related to {right_title.lower()}. The scene features a diverse team actively working on practical applications, using contemporary tools and methods. The environment shows organized, efficient operations with people engaged in meaningful work. Implementation tools and equipment are [COLOR1], team members and activities are [COLOR2], and workplace environment is [COLOR3]. No readable text on any surfaces or equipment. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Handle case where AI used leftContent/rightContent but missing titles
                if normalized_props.get('leftContent') and not normalized_props.get('leftTitle'):
                    # Try to extract title from content
                    left_content = normalized_props.get('leftContent', '')
                    if '\n' in left_content:
                        lines = left_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['leftTitle'] = lines[0].strip()
                            normalized_props['leftContent'] = '\n'.join(lines[1:]).strip()
                
                if normalized_props.get('rightContent') and not normalized_props.get('rightTitle'):
                    # Try to extract title from content
                    right_content = normalized_props.get('rightContent', '')
                    if '\n' in right_content:
                        lines = right_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['rightTitle'] = lines[0].strip()
                            normalized_props['rightContent'] = '\n'.join(lines[1:]).strip()
                            
            # Fix challenges-solutions template props
            elif template_id == 'challenges-solutions':
                # Convert leftContent/rightContent to challenges/solutions arrays
                if 'leftContent' in normalized_props and 'challenges' not in normalized_props:
                    left_content = normalized_props.get('leftContent', '')
                    # Parse content into challenges array
                    challenges = []
                    for line in left_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('common challenges'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                challenges.append(clean_line)
                    
                    if challenges:
                        normalized_props['challenges'] = challenges
                        del normalized_props['leftContent']
                        
                if 'rightContent' in normalized_props and 'solutions' not in normalized_props:
                    right_content = normalized_props.get('rightContent', '')
                    # Parse content into solutions array
                    solutions = []
                    for line in right_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('recommended resources') and not line.lower().startswith('solutions'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                solutions.append(clean_line)
                    
                    if solutions:
                        normalized_props['solutions'] = solutions
                        del normalized_props['rightContent']
                
                # Ensure required titles exist
                if 'challengesTitle' not in normalized_props:
                    normalized_props['challengesTitle'] = 'Challenges'
                if 'solutionsTitle' not in normalized_props:
                    normalized_props['solutionsTitle'] = 'Solutions'
            
            # Fix timeline template props
            elif template_id == 'timeline':
                # Convert "events" to "steps" and restructure data
                events = normalized_props.get('events', [])
                if events and isinstance(events, list):
                    steps = []
                    for event in events:
                        if isinstance(event, dict):
                            # Convert event structure to step structure
                            heading = (event.get('title') or event.get('heading') or event.get('date') or '').strip()
                            description = (event.get('description') or '').strip()
                            if heading or description:
                                steps.append({
                                    'heading': heading or 'Timeline Step',
                                    'description': description or 'No description available'
                                })
                    normalized_props['steps'] = steps
                    normalized_props.pop('events', None)  # Remove the old structure
                elif 'steps' not in normalized_props:
                    # Create default steps if no events or steps exist
                    normalized_props['steps'] = [
                        {'heading': 'Step 1', 'description': 'First milestone'},
                        {'heading': 'Step 2', 'description': 'Second milestone'},
                        {'heading': 'Step 3', 'description': 'Third milestone'},
                        {'heading': 'Step 4', 'description': 'Final milestone'}
                    ]
            
            # Fix pyramid template props
            elif template_id == 'pyramid':
                # Ensure 'steps' array exists by parsing from common inputs
                steps = normalized_props.get('steps', []) or normalized_props.get('items', [])
                levels = normalized_props.get('levels', [])
                
                # If we have levels data, use it to create proper steps
                if levels and isinstance(levels, list) and len(levels) >= 3:
                    parsed_items = []
                    for i, level in enumerate(levels[:3], start=1):
                        if isinstance(level, dict):
                            heading = level.get('text', f'Level {i}')
                            description = level.get('description', '')
                            parsed_items.append({'heading': heading, 'description': description})
                        else:
                            parsed_items.append({'heading': f'Level {i}', 'description': str(level) if level else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                
                # If no levels but we have steps, validate and fix them
                elif steps and isinstance(steps, list):
                    fixed_steps = []
                    for i, step in enumerate(steps[:3], start=1):
                        if isinstance(step, dict):
                            heading = step.get('heading', f'Level {i}')
                            description = step.get('description', '')
                            fixed_steps.append({'heading': heading, 'description': description})
                        else:
                            fixed_steps.append({'heading': f'Level {i}', 'description': str(step) if step else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(fixed_steps) < 3:
                        idx = len(fixed_steps) + 1
                        fixed_steps.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = fixed_steps[:3]
                
                # If no structured data, try to parse from content
                else:
                    import re
                    text = (normalized_props.get('content') or '').strip()
                    parsed_items = []
                    if text:
                        # Try to extract segments like **Heading**: description ... up to next **Heading**
                        pattern = re.compile(r"\*\*([^*]+)\*\*\s*:?[\s\-–—]*([^*]+?)(?=\s*\*\*[^*]+\*\*|$)", re.S)
                        for m in pattern.finditer(text):
                            heading = m.group(1).strip()
                            desc = m.group(2).strip().replace('\n', ' ')
                            if heading and desc:
                                parsed_items.append({'heading': heading, 'description': desc})
                    
                    if not parsed_items and text:
                        # Fallback: split into up to 3 sentence-like chunks
                        # First try double newlines, then periods.
                        chunks = [c.strip() for c in re.split(r"\n\n+", text) if c.strip()]
                        if not chunks:
                            chunks = [c.strip() for c in re.split(r"(?<=[.!?])\s+", text) if c.strip()]
                        for i, ch in enumerate(chunks[:3], start=1):
                            parsed_items.append({'heading': f'Level {i}', 'description': ch})
                    
                    # Ensure we have exactly 3 levels, but don't add "No description available"
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                # Clean up: pyramid does not use a long 'content' blob when steps are present
                if normalized_props.get('steps') and 'content' in normalized_props:
                    pass  # keep content for now as optional; frontend ignores it
                    
            # Fix event-list template props
            elif template_id == 'event-list':
                # Ensure events array exists
                events = normalized_props.get('events', [])
                if not (isinstance(events, list) and events):
                    # Create default events if none exist
                    normalized_props['events'] = [
                        {'date': 'Event 1', 'description': 'Event description'},
                        {'date': 'Event 2', 'description': 'Event description'},
                        {'date': 'Event 3', 'description': 'Event description'}
                    ]
                else:
                    # Ensure each event has required fields
                    fixed_events = []
                    for event in events:
                        if isinstance(event, dict):
                            # FIXED: Use 'title' as description if 'description' is empty
                            description = event.get('description') or event.get('desc') or ''
                            if not description.strip():
                                # If description is empty, use title as description
                                description = event.get('title') or 'Event description'
                            
                            fixed_event = {
                                'date': str(event.get('date') or 'Event Date'),
                                'description': str(description)
                            }
                            fixed_events.append(fixed_event)
                    if fixed_events:
                        normalized_props['events'] = fixed_events
                
                # Log final processed event-list content
                logger.info(f"=== FINAL PROCESSED EVENT-LIST for slide {slide_index + 1} ===")
                logger.info(f"Final Events: {normalized_props.get('events', [])}")
                logger.info(f"Final Props: {normalized_props}")
                logger.info(f"=== END FINAL PROCESSED EVENT-LIST for slide {slide_index + 1} ===")
        
            # Fix bullet-points template props
            elif template_id in ['bullet-points', 'bullet-points-right']:
                bullets = normalized_props.get('bullets', [])
                if bullets and isinstance(bullets, list):
                    # Ensure bullets are strings and not empty
                    fixed_bullets = [str(bullet).strip() for bullet in bullets if str(bullet).strip()]
                    if fixed_bullets:
                        normalized_props['bullets'] = fixed_bullets
                    else:
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': No valid bullet points, adding placeholder")
                        normalized_props['bullets'] = ['No content available']
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid or missing bullets array, adding placeholder")
                    normalized_props['bullets'] = ['No content available']
            
            # Fix comparison-slide template props
            elif template_id == 'comparison-slide':
                table_data = normalized_props.get('tableData', {})
                if isinstance(table_data, dict):
                    # Ensure headers exist
                    if 'headers' not in table_data or not isinstance(table_data['headers'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid headers")
                        table_data['headers'] = ['Feature', 'Option A', 'Option B']
                    
                    # Ensure rows exist
                    if 'rows' not in table_data or not isinstance(table_data['rows'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid rows")
                        table_data['rows'] = [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    
                    # Ensure all rows have the correct number of columns
                    expected_cols = len(table_data['headers'])
                    fixed_rows = []
                    for row_idx, row in enumerate(table_data['rows']):
                        if isinstance(row, list):
                            # Pad or trim row to match header count
                            if len(row) < expected_cols:
                                # Pad with empty strings
                                row = row + [''] * (expected_cols - len(row))
                            elif len(row) > expected_cols:
                                # Trim to match headers
                                row = row[:expected_cols]
                            fixed_rows.append([str(cell) for cell in row])
                        else:
                            logger.warning(f"Coercing slide {slide_index + 1}: Invalid row format at index {row_idx}")
                            # Create a placeholder row
                            fixed_rows.append([f'Row {row_idx + 1} Col {i + 1}' for i in range(expected_cols)])
                    
                    table_data['rows'] = fixed_rows
                    normalized_props['tableData'] = table_data
                    
                    logger.info(f"Fixed comparison table data for slide {slide_index + 1}: {len(table_data['headers'])} columns, {len(table_data['rows'])} rows")
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid tableData, adding default comparison")
                    normalized_props['tableData'] = {
                        'headers': ['Feature', 'Option A', 'Option B'],
                        'rows': [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    }
        
            normalized_slide['props'] = normalized_props
            
            # Remove voiceoverText for non-video presentations
            if (component_name == COMPONENT_NAME_SLIDE_DECK and 
                'voiceoverText' in normalized_slide):
                logger.info(f"Removing voiceoverText from slide {slide_index + 1} for regular slide deck")
                normalized_slide.pop('voiceoverText', None)
            
            normalized_slides.append(normalized_slide)
            
        except Exception as e:
            logger.error(f"Error normalizing slide {slide_index + 1} with template '{template_id}': {e}")
            logger.warning(f"Removing problematic slide {slide_index + 1}")
            continue  # Skip this slide
    
    logger.info(f"Slide normalization complete: {len(slides)} -> {len(normalized_slides)} slides (removed {len(slides) - len(normalized_slides)} invalid slides)")
    return normalized_slides

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

BING_API_KEY = os.getenv("BING_API_KEY")
BING_API_URL = "https://api.bing.microsoft.com/v7.0/search"

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}

# In-memory job status store (for demo; use Redis for production)
AI_AUDIT_PROGRESS = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AiAuditQuestionnaireRequest(BaseModel):
    companyName: str
    companyDesc: str
    companyWebsite: str
    employees: str
    franchise: str
    onboardingProblems: str
    documents: list[str]
    documentsOther: str = ""
    priorities: list[str]
    priorityOther: str = ""

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
        {
          "type": "numbered_list",
          "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

@app.on_event("startup")
async def startup_event():
    global DB_POOL
    logger.info("Custom Backend starting...")
    if not CUSTOM_PROJECTS_DATABASE_URL:
        logger.critical("CRITICAL: CUSTOM_PROJECTS_DATABASE_URL env var not set.")
        return
    try:
        DB_POOL = await asyncpg.create_pool(dsn=CUSTOM_PROJECTS_DATABASE_URL, min_size=1, max_size=10,
                                            init=lambda conn: conn.set_type_codec(
                                                'jsonb',
                                                encoder=lambda value: json.dumps(value) if value is not None else None,
                                                decoder=lambda value: json.loads(value) if value is not None else None,
                                                schema='pg_catalog',
                                                format='text'
                                            ))
        async with DB_POOL.acquire() as connection:
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS design_templates (
                    id SERIAL PRIMARY KEY,
                    template_name TEXT NOT NULL UNIQUE,
                    template_structuring_prompt TEXT NOT NULL,
                    design_image_path TEXT,
                    microproduct_type TEXT,
                    component_name TEXT NOT NULL,
                    date_created TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    project_name TEXT NOT NULL,
                    product_type TEXT,
                    microproduct_type TEXT,
                    microproduct_name TEXT,
                    microproduct_content JSONB,
                    design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_name TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS product_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_onyx_user_id ON projects(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_design_template_id ON projects(design_template_id);")
            logger.info("'projects' table ensured and updated.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS microproduct_pipelines (
                    id SERIAL PRIMARY KEY,
                    pipeline_name TEXT NOT NULL,
                    pipeline_description TEXT,
                    is_prompts_data_collection BOOLEAN DEFAULT FALSE,
                    is_prompts_data_formating BOOLEAN DEFAULT FALSE,
                    prompts_data_collection JSONB,
                    prompts_data_formating JSONB,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_pipelines_name ON microproduct_pipelines(pipeline_name);")
            logger.info("'microproduct_pipelines' table ensured.")

            col_type_row = await connection.fetchrow(
                "SELECT data_type FROM information_schema.columns "
                "WHERE table_name = 'projects' AND column_name = 'microproduct_content';"
            )
            if col_type_row and col_type_row['data_type'] != 'jsonb':
                logger.info("Attempting to alter 'microproduct_content' column type to JSONB...")
                await connection.execute("ALTER TABLE projects ALTER COLUMN microproduct_content TYPE JSONB USING microproduct_content::text::jsonb;")
                logger.info("Successfully altered 'microproduct_content' to JSONB.")

            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            logger.info("'projects' table ensured and updated with 'source_chat_session_id'.")

            # --- Add source context tracking columns ---
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_context_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_context_data JSONB;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_source_context_type ON projects(source_context_type);")
            logger.info("'projects' table updated with source context tracking columns.")

            # --- Add lesson plan specific columns ---
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS lesson_plan_data JSONB;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS parent_outline_id INTEGER REFERENCES projects(id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_parent_outline_id ON projects(parent_outline_id);")
            logger.info("'projects' table updated with lesson plan columns.")

            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_name ON design_templates(template_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_mptype ON design_templates(microproduct_type);")
            logger.info("'design_templates' table ensured.")

            # --- Initialize workspace database tables ---
            try:
                from app.core.database import init_database
                await init_database()
                logger.info("Workspace database tables initialized successfully")
            except Exception as db_init_error:
                logger.warning(f"Failed to initialize workspace database tables: {db_init_error}")

            # --- Ensure a soft-delete trash table for projects ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS trashed_projects (LIKE projects INCLUDING ALL);
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_trashed_projects_user ON trashed_projects(onyx_user_id);")
            logger.info("'trashed_projects' table ensured (soft-delete).")

            # --- Ensure user credits table ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_credits (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL UNIQUE,
                    name TEXT NOT NULL,
                    credits_balance INTEGER NOT NULL DEFAULT 0,
                    total_credits_used INTEGER NOT NULL DEFAULT 0,
                    credits_purchased INTEGER NOT NULL DEFAULT 0,
                    last_purchase_date TIMESTAMP WITH TIME ZONE,
                    subscription_tier TEXT DEFAULT 'basic',
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_credits_onyx_user_id ON user_credits(onyx_user_id);")
            logger.info("'user_credits' table ensured.")

            # NEW: Ensure credit transactions table for analytics/timeline
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS credit_transactions (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    user_credits_id INTEGER REFERENCES user_credits(id) ON DELETE CASCADE,
                    type TEXT NOT NULL CHECK (type IN ('purchase','product_generation','admin_removal')),
                    title TEXT,
                    product_type TEXT,
                    credits INTEGER NOT NULL CHECK (credits >= 0),
                    delta INTEGER NOT NULL,
                    reason TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_user ON credit_transactions(onyx_user_id, created_at DESC);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_type ON credit_transactions(type);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_product ON credit_transactions(product_type);")
            
            # Migration: Update credit_transactions table to support admin_removal type
            try:
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    DROP CONSTRAINT IF EXISTS credit_transactions_type_check;
                """)
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    ADD CONSTRAINT credit_transactions_type_check 
                    CHECK (type IN ('purchase','product_generation','admin_removal'));
                """)
                logger.info("Updated credit_transactions table to support admin_removal type")
            except Exception as e:
                logger.warning(f"Could not update credit_transactions constraint: {e}")
            
            logger.info("'credit_transactions' table ensured.")

            # Migration: Populate user_credits table with existing Onyx users
            try:
                migrated_count = await migrate_onyx_users_to_credits_table()
                logger.info(f"Populated user_credits table with {migrated_count} existing Onyx users (100 credits each).")
            except Exception as e:
                logger.error(f"Failed to migrate Onyx users to credits table: {e}")
                logger.info("Migration will be available manually via admin interface.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS project_folders (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    "order" INTEGER DEFAULT 0,
                    parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_onyx_user_id ON project_folders(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_folder_id ON projects(folder_id);")
            
            # Add parent_id column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for parent_id column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_parent_id ON project_folders(parent_id);")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add order column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for order column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_order ON project_folders(\"order\");")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add quality_tier column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS quality_tier TEXT DEFAULT 'medium';")
                logger.info("Added quality_tier column to project_folders table")
                
                # Update existing folders to have 'medium' tier if they don't have one
                await connection.execute("UPDATE project_folders SET quality_tier = 'medium' WHERE quality_tier IS NULL;")
                logger.info("Updated existing folders with default 'medium' tier")
                
                # Create index for quality_tier column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_quality_tier ON project_folders(quality_tier);")
                logger.info("Created index for quality_tier column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding quality_tier column: {e}")
                    raise e
            
            # Add custom_rate column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS custom_rate INTEGER DEFAULT 200;")
                logger.info("Added custom_rate column to project_folders table")
                
                # Update existing folders to have default custom_rate if they don't have one
                await connection.execute("UPDATE project_folders SET custom_rate = 200 WHERE custom_rate IS NULL;")
                logger.info("Updated existing folders with default custom_rate")
                
                # Create index for custom_rate column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_custom_rate ON project_folders(custom_rate);")
                logger.info("Created index for custom_rate column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding custom_rate column: {e}")
                    raise e
            
            # Add order column for project sorting
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_order ON projects(\"order\");")

            # Add completionTime column to projects table (for the new completion time feature)
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level custom rate and quality tier columns
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to projects table")
                
                # Create indexes for project-level custom rate and quality tier
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_custom_rate ON projects(custom_rate);")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_quality_tier ON projects(quality_tier);")
                logger.info("Created indexes for project-level custom_rate and quality_tier columns")
                
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level custom_rate/quality_tier columns: {e}")
                    raise e
            
            # Add is_advanced and advanced_rates to project_folders for advanced per-product rates
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS is_advanced BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS advanced_rates JSONB;")
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS completion_times JSONB;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_is_advanced ON project_folders(is_advanced);")
                logger.info("Ensured is_advanced, advanced_rates, and completion_times on project_folders")
            except Exception as e:
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding is_advanced/advanced_rates to project_folders: {e}")
                    raise e

            # Add is_advanced and advanced_rates to projects for advanced per-product rates
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_advanced BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS advanced_rates JSONB;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS completion_times JSONB;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_advanced ON projects(is_advanced);")
                logger.info("Ensured is_advanced, advanced_rates, and completion_times on projects")
            except Exception as e:
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding is_advanced/advanced_rates to projects: {e}")
                    raise e
            
            # Add completionTime column to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add other missing columns to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level columns to trashed_projects table
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to trashed_projects table")
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level columns to trashed_projects: {e}")
                    raise e

            # CRITICAL FIX: Ensure order and completion_time columns are TEXT type to prevent casting errors
            try:
                logger.info("Applying critical fix: Ensuring order and completion_time columns are TEXT type")
                
                # Fix projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set projects.order and projects.completion_time to TEXT type")
                
                # Fix trashed_projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE trashed_projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set trashed_projects.order and trashed_projects.completion_time to TEXT type")
                
                # Set default values for empty strings
                await connection.execute("""
                    UPDATE projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                logger.info("Successfully set default values for empty order and completion_time fields")
                
            except Exception as e:
                logger.error(f"Error applying critical TEXT type fix: {e}")

            # Final verification - ensure all required columns exist with correct types
            try:
                # Verify projects table schema
                projects_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Projects table schema verification:")
                for row in projects_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
                # Verify trashed_projects table schema
                trashed_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'trashed_projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Trashed_projects table schema verification:")
                for row in trashed_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
            except Exception as e:
                logger.error(f"Error during schema verification: {e}")

            # Create request analytics table
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS request_analytics (
                    id TEXT PRIMARY KEY,
                    endpoint TEXT NOT NULL,
                    method TEXT NOT NULL,
                    user_id TEXT,
                    status_code INTEGER NOT NULL,
                    response_time_ms INTEGER NOT NULL,
                    request_size_bytes INTEGER,
                    response_size_bytes INTEGER,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_created_at ON request_analytics(created_at);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_endpoint ON request_analytics(endpoint);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_user_id ON request_analytics(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_status_code ON request_analytics(status_code);")
            logger.info("'request_analytics' table ensured.")

            # Add AI parser tracking columns to request_analytics table
            try:
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS is_ai_parser_request BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_tokens INTEGER;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_model TEXT;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_project_name TEXT;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_ai_parser ON request_analytics(is_ai_parser_request);")
                logger.info("AI parser tracking columns added to request_analytics table.")
            except Exception as e:
                logger.warning(f"Error adding AI parser columns (may already exist): {e}")

            # Add is_standalone field to projects table to track standalone vs outline-based products
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_standalone ON projects(is_standalone);")
                logger.info("Added is_standalone column to projects table.")
                
                # Add same field to trashed_projects table to match schema
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                logger.info("Added is_standalone column to trashed_projects table.")
                
                # For legacy support: Set is_standalone = NULL for all existing products
                # This allows the frontend filtering logic to handle legacy products gracefully
                # New products will have this field explicitly set during creation
                logger.info("Legacy support: is_standalone field defaults to NULL for existing products.")
                
            except Exception as e:
                logger.warning(f"Error adding is_standalone column (may already exist): {e}")

            # ============================
            # SMART DRIVE DATABASE MIGRATIONS
            # ============================
            
            # SmartDrive Accounts: Per-user SmartDrive linkage with individual Nextcloud credentials
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS smartdrive_accounts (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL UNIQUE,
                    nextcloud_username VARCHAR(255),
                    nextcloud_password_encrypted TEXT,
                    nextcloud_base_url VARCHAR(512) DEFAULT 'http://nc1.contentbuilder.ai:8080',
                    sync_cursor JSONB DEFAULT '{}',
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    CONSTRAINT idx_smartdrive_accounts_onyx_user UNIQUE (onyx_user_id)
                );
            """)
            
            # Add new columns to existing tables (migration-safe)
            try:
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_username VARCHAR(255);")
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_password_encrypted TEXT;")
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_base_url VARCHAR(512) DEFAULT 'http://nc1.contentbuilder.ai:8080';")
            except Exception as e:
                logger.info(f"Columns may already exist: {e}")
                pass
            # Add encryption helper functions - provide placeholder for old nextcloud_user_id column
            await connection.execute("INSERT INTO smartdrive_accounts (onyx_user_id, nextcloud_username, nextcloud_password_encrypted) VALUES ('system_encryption_key', '__encryption_key__', '__placeholder__') ON CONFLICT (onyx_user_id) DO NOTHING;")
            logger.info("'smartdrive_accounts' table ensured.")

            # SmartDrive Imports: Maps SmartDrive files to Onyx files with etags/checksums
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS smartdrive_imports (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    smartdrive_path VARCHAR(1000) NOT NULL,
                    onyx_file_id VARCHAR(255) NOT NULL,
                    etag VARCHAR(255),
                    checksum VARCHAR(255),
                    file_size BIGINT,
                    mime_type VARCHAR(255),
                    imported_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP WITH TIME ZONE,
                    CONSTRAINT idx_smartdrive_imports_user_path UNIQUE (onyx_user_id, smartdrive_path)
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_onyx_user_id ON smartdrive_imports(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_onyx_file_id ON smartdrive_imports(onyx_file_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_imported_at ON smartdrive_imports(imported_at);")
            logger.info("'smartdrive_imports' table ensured.")

            # User Connectors: Per-user connector configs and encrypted tokens
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_connectors (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    source VARCHAR(100) NOT NULL,
                    config JSONB DEFAULT '{}',
                    credentials_encrypted TEXT,
                    status VARCHAR(50) DEFAULT 'active',
                    last_sync_at TIMESTAMP WITH TIME ZONE,
                    last_error TEXT,
                    total_docs_indexed INTEGER DEFAULT 0,
                    onyx_connector_id INTEGER,
                    onyx_credential_id INTEGER,
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_onyx_user_id ON user_connectors(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_source ON user_connectors(source);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_status ON user_connectors(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_created_at ON user_connectors(created_at);")
            logger.info("'user_connectors' table ensured.")

            # --- Ensure offers table ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS offers (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    company_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE,
                    offer_name TEXT NOT NULL,
                    created_on TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    manager TEXT NOT NULL,
                    status TEXT NOT NULL CHECK (status IN (
                        'Draft',
                        'Internal Review', 
                        'Approved',
                        'Sent to Client',
                        'Viewed by Client',
                        'Negotiation',
                        'Accepted',
                        'Rejected',
                        'Archived'
                    )),
                    total_hours INTEGER DEFAULT 0,
                    link TEXT,
                    share_token TEXT UNIQUE,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("ALTER TABLE offers ADD COLUMN IF NOT EXISTS share_token TEXT;")
            await connection.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_offers_share_token ON offers(share_token) WHERE share_token IS NOT NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_onyx_user_id ON offers(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_company_id ON offers(company_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_status ON offers(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_created_on ON offers(created_on);")
            logger.info("'offers' table ensured.")

            logger.info("Smart Drive database migrations completed successfully.")
            # --- Feature Management Tables ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS feature_definitions (
                    id SERIAL PRIMARY KEY,
                    feature_name VARCHAR(100) UNIQUE NOT NULL,
                    display_name VARCHAR(200) NOT NULL,
                    description TEXT,
                    category VARCHAR(100),
                    is_active BOOLEAN DEFAULT true,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_feature_definitions_name ON feature_definitions(feature_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_feature_definitions_active ON feature_definitions(is_active);")
            logger.info("'feature_definitions' table ensured.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_features (
                    id SERIAL PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    feature_name VARCHAR(100) NOT NULL,
                    is_enabled BOOLEAN DEFAULT false,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    UNIQUE(user_id, feature_name)
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_user_id ON user_features(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_feature_name ON user_features(feature_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_enabled ON user_features(is_enabled);")
            
            # Migration: Ensure user_id column is TEXT type (not UUID)
            try:
                await connection.execute("""
                    ALTER TABLE user_features 
                    ALTER COLUMN user_id TYPE TEXT USING user_id::TEXT
                """)
                logger.info("Migrated user_features.user_id column to TEXT type.")
            except Exception as e:
                # This might fail if column is already TEXT or if there are no records
                logger.info(f"user_features.user_id column migration skipped (likely already TEXT): {e}")
            
            logger.info("'user_features' table ensured.")

            # Seed initial feature definitions
            try:
                initial_features = [
                    ('ai_audit_templates', 'AI Audit Templates', 'Access to AI-powered audit template generation', 'Templates'),
                    ('deloitte_banner', 'Deloitte Banner', 'Show Deloitte banner on Projects page', 'Branding'),
                    ('offers_tab', 'Offers Tab', 'Access to Offers tab in Projects', 'Navigation'),
                    ('workspace_tab', 'Workspace Tab', 'Access to Workspace tab in Projects', 'Navigation'),
                    ('video_lesson', 'Video Lesson', 'Allow creating Video Lessons in Generate page', 'Creation'),
                    ('lesson_draft', 'Lesson Draft', 'Allow creating and viewing Lesson Drafts', 'Creation'),
                    ('col_assessment_type', 'Column: Assessment Type', 'Shows the Assessment Type column', 'Columns'),
                    ('col_content_volume', 'Column: Content Volume', 'Shows the Content Volume column', 'Columns'),
                    ('col_source', 'Column: Source', 'Shows the Source column', 'Columns'),
                    ('col_est_creation_time', 'Column: Est. Creation Time', 'Shows the Est. Creation Time column', 'Columns'),
                    ('col_est_completion_time', 'Column: Est. Completion Time', 'Shows the Est. Completion Time column', 'Columns'),
                    ('col_quality_tier', 'Column: Quality Tier', 'Shows the Quality Tier column', 'Columns'),
                    ('col_quiz', 'Column: Quiz', 'Shows the Quiz column', 'Columns'),
                    ('col_one_pager', 'Column: One-Pager', 'Shows the One-Pager column', 'Columns'),
                    ('col_video_presentation', 'Column: Video Lesson', 'Shows the Video Lesson column', 'Columns'),
                    ('col_lesson_presentation', 'Column: Presentation', 'Shows the Presentation column', 'Columns'),
                ]

                for feature_name, display_name, description, category in initial_features:
                    await connection.execute("""
                        INSERT INTO feature_definitions (feature_name, display_name, description, category)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (feature_name) DO UPDATE SET
                            display_name = EXCLUDED.display_name,
                            description = EXCLUDED.description,
                            category = EXCLUDED.category,
                            is_active = true
                    """, feature_name, display_name, description, category)
                
                logger.info(f"Seeded {len(initial_features)} feature definitions.")
                
                # Deactivate unused features that are not wired
                unused_features = [
                    'advanced_analytics', 'bulk_operations', 'premium_support', 
                    'beta_features', 'api_access', 'custom_themes', 'advanced_export'
                ]
                
                for feature_name in unused_features:
                    await connection.execute("""
                        UPDATE feature_definitions 
                        SET is_active = false 
                        WHERE feature_name = $1
                    """, feature_name)
                
                logger.info(f"Deactivated {len(unused_features)} unused feature definitions.")
            except Exception as e:
                logger.warning(f"Error seeding feature definitions (may already exist): {e}")

            # Create feature entries for existing users (all disabled by default)
            try:
                users = await connection.fetch("SELECT onyx_user_id FROM user_credits")
                
                if users:
                    # Get all active feature names
                    feature_names = await connection.fetch("SELECT feature_name FROM feature_definitions WHERE is_active = true")
                    
                    for user in users:
                        user_id = user['onyx_user_id']
                        for feature_row in feature_names:
                            feature_name = feature_row['feature_name']
                            await connection.execute("""
                                INSERT INTO user_features (user_id, feature_name, is_enabled)
                                VALUES ($1, $2, false)
                                ON CONFLICT (user_id, feature_name) DO NOTHING
                            """, user_id, feature_name)
                    
                    logger.info(f"Created feature entries for {len(users)} existing users.")
            except Exception as e:
                logger.warning(f"Error creating user feature entries (may already exist): {e}")

            logger.info("Database schema migration completed successfully.")
    except Exception as e:
        logger.critical(f"Failed to initialize custom DB pool or ensure tables: {e}", exc_info=not IS_PRODUCTION)
        DB_POOL = None

@app.on_event("shutdown")
async def shutdown_event():
    if DB_POOL:
        await DB_POOL.close()
        logger.info("Custom projects DB pool closed.")

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

# Offers Models
class OfferBase(BaseModel):
    company_id: int
    offer_name: str
    manager: str
    status: str
    total_hours: int = 0
    # link is auto-generated, not provided in create requests

class OfferCreate(OfferBase):
    pass

class OfferUpdate(BaseModel):
    company_id: Optional[int] = None
    offer_name: Optional[str] = None
    manager: Optional[str] = None
    status: Optional[str] = None
    total_hours: Optional[int] = None
    created_on: Optional[datetime] = None
    # link is auto-generated and not editable

class OfferResponse(OfferBase):
    id: int
    onyx_user_id: str
    created_on: datetime
    created_at: datetime
    updated_at: datetime
    company_name: str  # Joined from project_folders

class OfferListResponse(BaseModel):
    offers: List[OfferResponse]
    total_count: int

# NEW: Analytics/timeline models
class ProductUsage(BaseModel):
    product_type: str
    credits_used: int

class CreditUsageAnalyticsResponse(BaseModel):
    usage_by_product: List[ProductUsage]
    total_credits_used: int

class TimelineActivity(BaseModel):
    id: str
    type: Literal['purchase', 'product_generation', 'admin_removal']
    title: str
    credits: int
    timestamp: datetime
    product_type: Optional[str] = None

class UserTransactionHistoryResponse(BaseModel):
    user_id: int
    user_email: str
    user_name: str
    transactions: List[TimelineActivity]

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]

class DesignTemplateBase(BaseModel):
    template_name: str
    template_structuring_prompt: str
    microproduct_type: str
    component_name: str
    model_config = {"from_attributes": True}

class DesignTemplateCreate(DesignTemplateBase):
    design_image_path: Optional[str] = None

class DesignTemplateUpdate(BaseModel):
    template_name: Optional[str] = None
    template_structuring_prompt: Optional[str] = None
    microproduct_type: Optional[str] = None
    component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class DesignTemplateResponse(DesignTemplateBase):
    id: int
    design_image_path: Optional[str] = None
    date_created: datetime

class ProjectCreateRequest(BaseModel):
    projectName: str
    design_template_id: int
    microProductName: Optional[str] = None
    aiResponse: str
    chatSessionId: Optional[uuid.UUID] = None
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    folder_id: Optional[int] = None  # Add folder_id for automatic folder assignment
    theme: Optional[str] = None      # Selected theme for presentations
    # Source context tracking
    source_context_type: Optional[str] = None  # 'files', 'connectors', 'knowledge_base', 'text', 'prompt'
    source_context_data: Optional[dict] = None  # JSON data about the source
    model_config = {"from_attributes": True}

class ProjectDB(BaseModel):
    id: int
    onyx_user_id: str
    project_name: str
    product_type: Optional[str] = None
    microproduct_type: Optional[str] = None
    microproduct_name: Optional[str] = None
    microproduct_content: Optional[MicroProductContentType] = None
    design_template_id: Optional[int] = None
    created_at: datetime
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None
    model_config = {"from_attributes": True}

class MicroProductApiResponse(BaseModel):
    name: str
    slug: str
    project_id: int
    design_template_id: int
    component_name: str
    parentProjectName: Optional[str] = None
    webLinkPath: Optional[str] = None
    pdfLinkPath: Optional[str] = None
    details: Optional[MicroProductContentType] = None
    sourceChatSessionId: Optional[uuid.UUID] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    lesson_plan_data: Optional[Dict[str, Any]] = None  # Add lesson plan data field
    model_config = {"from_attributes": True}

class ProjectApiResponse(BaseModel):
    id: int
    projectName: str
    projectSlug: str
    microproduct_name: Optional[str] = None
    design_template_name: Optional[str] = None
    design_microproduct_type: Optional[str] = None
    created_at: datetime
    design_template_id: Optional[int] = None
    folder_id: Optional[int] = None
    order: Optional[int] = None
    source_chat_session_id: Optional[str] = None
    is_standalone: Optional[bool] = None  # Track whether this is standalone or part of an outline
    model_config = {"from_attributes": True}

class ProjectDetailForEditResponse(BaseModel):
    id: int
    projectName: str
    microProductName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductContent: Optional[MicroProductContentType] = None
    createdAt: Optional[datetime] = None
    design_template_name: Optional[str] = None
    design_component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectUpdateRequest(BaseModel):
    projectName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductName: Optional[str] = None
    microProductContent: Optional[Dict[str, Any]] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

class ErrorDetail(BaseModel):
    detail: str

class RequestAnalytics(BaseModel):
    id: str
    endpoint: str
    method: str
    user_id: Optional[str] = None
    status_code: int
    response_time_ms: int
    request_size_bytes: Optional[int] = None
    response_size_bytes: Optional[int] = None
    error_message: Optional[str] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class ProjectsDeleteRequest(BaseModel):
    project_ids: List[int]
    scope: Optional[str] = 'self'

class MicroproductPipelineBase(BaseModel):
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool = Field(False, alias="is_prompts_data_collection")
    is_structuring_prompts: bool = Field(False, alias="is_prompts_data_formating")
    discovery_prompts_list: Optional[List[str]] = Field(default_factory=list)
    structuring_prompts_list: Optional[List[str]] = Field(default_factory=list)
    model_config = {"from_attributes": True, "populate_by_name": True}

class MicroproductPipelineCreateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineUpdateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineDBRaw(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_prompts_data_collection: bool
    is_prompts_data_formating: bool
    prompts_data_collection: Optional[Dict[str, str]] = None
    prompts_data_formating: Optional[Dict[str, str]] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class MicroproductPipelineGetResponse(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool
    is_structuring_prompts: bool
    discovery_prompts_list: List[str] = Field(default_factory=list)
    structuring_prompts_list: List[str] = Field(default_factory=list)
    created_at: datetime
    model_config = {"from_attributes": True}

class DuplicatedProductInfo(BaseModel):
    original_id: int
    new_id: int
    type: str
    name: str

class ProjectDuplicationResponse(BaseModel):
    id: int
    name: str
    type: str
    connected_products: Optional[List[DuplicatedProductInfo]] = None
    total_products_duplicated: int
    model_config = {"from_attributes": True}

    @classmethod
    def from_db_model(cls, db_model: MicroproductPipelineDBRaw) -> "MicroproductPipelineGetResponse":
        discovery_list = [db_model.prompts_data_collection[key] for key in sorted(db_model.prompts_data_collection.keys(), key=int)] if db_model.prompts_data_collection else []
        structuring_list = [db_model.prompts_data_formating[key] for key in sorted(db_model.prompts_data_formating.keys(), key=int)] if db_model.prompts_data_formating else []
        return cls(
            id=db_model.id,
            pipeline_name=db_model.pipeline_name,
            pipeline_description=db_model.pipeline_description,
            is_discovery_prompts=db_model.is_prompts_data_collection,
            is_structuring_prompts=db_model.is_prompts_data_formating,
            discovery_prompts_list=discovery_list,
            structuring_prompts_list=structuring_list,
            created_at=db_model.created_at
        )

# --- Authentication and Utility Functions ---
# async def bing_company_research(company_name: str, company_desc: str) -> str:
#     if not BING_API_KEY:
#         return "(Bing API key not configured)"
#     query = f"{company_name} {company_desc} официальный сайт отзывы новости"
#     headers = {"Ocp-Apim-Subscription-Key": BING_API_KEY}
#     params = {"q": query, "mkt": "ru-RU"}
#     async with httpx.AsyncClient(timeout=15.0) as client:
#         resp = await client.get(BING_API_URL, headers=headers, params=params)
#         resp.raise_for_status()
#         data = resp.json()
#     # Extract summary: snippet, knowledge panel, news, etc.
#     snippets = []
#     if "webPages" in data:
#         for item in data["webPages"].get("value", [])[:3]:
#             snippets.append(item.get("snippet", ""))
#     if "entities" in data and data["entities"].get("value"):
#         for ent in data["entities"]["value"]:
#             if ent.get("description"):
#                 snippets.append(ent["description"])
#     if "news" in data and data["news"].get("value"):
#         for news in data["news"]["value"][:2]:
#             snippets.append(f"Новость: {news.get('name', '')} — {news.get('description', '')}")
#     return "\n".join(snippets)

async def serpapi_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    """
    Uses SerpAPI to gather:
    - General company info (snippets, knowledge panel, about, etc.)
    - Website-specific info (site: queries)
    - Open job listings (site:company_website jobs/careers, and generic queries)
    Returns a structured string with all findings.
    """
    url = "https://serpapi.com/search.json"
    async with httpx.AsyncClient(timeout=20.0) as client:
        # 1. General company info
        params_general = {
            "q": f"{company_name} {company_desc}",
            "engine": "google",
            "api_key": SERPAPI_KEY,
            "hl": "ru"
        }
        resp = await client.get(url, params=params_general)
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if "organic_results" in data:
            for item in data["organic_results"][:3]:
                if "snippet" in item:
                    general_snippets.append(item["snippet"])
        if "knowledge_graph" in data:
            kg = data["knowledge_graph"]
            if "description" in kg:
                general_snippets.append(kg["description"])
            if "title" in kg:
                general_snippets.append(f"Название: {kg['title']}")
            if "type" in kg:
                general_snippets.append(f"Тип: {kg['type']}")
            if "website" in kg:
                general_snippets.append(f"Сайт: {kg['website']}")
            if "address" in kg:
                general_snippets.append(f"Адрес: {kg['address']}")
            if "phone" in kg:
                general_snippets.append(f"Телефон: {kg['phone']}")
        general_info = "\n".join(general_snippets) or "(Нет релевантных данных SerpAPI)"

        # 2. Website-specific info
        website_info = ""
        if company_website:
            params_site = {
                "q": f"site:{company_website} о компании информация контакты",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp2 = await client.get(url, params=params_site)
            resp2.raise_for_status()
            data2 = resp2.json()
            site_snippets = []
            if "organic_results" in data2:
                for item in data2["organic_results"][:3]:
                    if "snippet" in item:
                        site_snippets.append(item["snippet"])
            website_info = "\n".join(site_snippets) or "(Нет информации по сайту)"

        # 3. Open job listings (site:company_website + jobs/careers)
        jobs_info = ""
        jobs_snippets = []
        if company_website:
            # Try site:company_website jobs/careers
            params_jobs_site = {
                "q": f"site:{company_website} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp3 = await client.get(url, params=params_jobs_site)
            resp3.raise_for_status()
            data3 = resp3.json()
            if "organic_results" in data3:
                for item in data3["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        # If not enough, try generic company_name + jobs
        if len(jobs_snippets) < 2:
            params_jobs_generic = {
                "q": f"{company_name} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp4 = await client.get(url, params=params_jobs_generic)
            resp4.raise_for_status()
            data4 = resp4.json()
            if "organic_results" in data4:
                for item in data4["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        jobs_info = "\n\n".join(jobs_snippets) or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[SerpAPI General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def duckduckgo_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    # Step 1: General info
    general_query = f"{company_name} {company_desc} официальный сайт отзывы новости"
    url = "https://api.duckduckgo.com/"
    params = {
        "format": "json",
        "no_redirect": 1,
        "no_html": 1,
        "skip_disambig": 1,
    }
    async with httpx.AsyncClient(timeout=10.0) as client:
        # General info
        resp = await client.get(url, params={**params, "q": general_query})
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if data.get("AbstractText"):
            general_snippets.append(data["AbstractText"])
        if data.get("RelatedTopics"):
            for topic in data["RelatedTopics"]:
                if isinstance(topic, dict) and topic.get("Text"):
                    general_snippets.append(topic["Text"])
        if data.get("Answer"):
            general_snippets.append(data["Answer"])
        if data.get("Definition"):
            general_snippets.append(data["Definition"])
        general_info = "\n".join([s for s in general_snippets if s]).strip() or "(Нет релевантных данных DuckDuckGo)"

        # Step 2: Info about the company website
        website_info = ""
        if company_website:
            website_query = f"site:{company_website} о компании информация контакты"
            resp2 = await client.get(url, params={**params, "q": website_query})
            resp2.raise_for_status()
            data2 = resp2.json()
            website_snippets = []
            if data2.get("AbstractText"):
                website_snippets.append(data2["AbstractText"])
            if data2.get("RelatedTopics"):
                for topic in data2["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        website_snippets.append(topic["Text"])
            if data2.get("Answer"):
                website_snippets.append(data2["Answer"])
            if data2.get("Definition"):
                website_snippets.append(data2["Definition"])
            website_info = "\n".join([s for s in website_snippets if s]).strip() or "(Нет информации по сайту)"

        # Step 3: Open positions
        jobs_info = ""
        if company_website:
            jobs_query = f"site:{company_website} вакансии OR careers OR jobs"
            resp3 = await client.get(url, params={**params, "q": jobs_query})
            resp3.raise_for_status()
            data3 = resp3.json()
            jobs_snippets = []
            if data3.get("AbstractText"):
                jobs_snippets.append(data3["AbstractText"])
            if data3.get("RelatedTopics"):
                for topic in data3["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        jobs_snippets.append(topic["Text"])
            if data3.get("Answer"):
                jobs_snippets.append(data3["Answer"])
            if data3.get("Definition"):
                jobs_snippets.append(data3["Definition"])
            jobs_info = "\n".join([s for s in jobs_snippets if s]).strip() or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[DuckDuckGo General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def get_current_onyx_user_id(request: Request) -> str:
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: return dev_user_id
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id)
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_id: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def get_current_onyx_user_with_email(request: Request) -> tuple[str, str]:
    """Get both user ID and email from Onyx"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id), user_email
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_with_email: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def verify_admin_user(request: Request) -> tuple[str, str]:
    """Verify that the current user is an admin using Onyx's built-in role system"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id and not IS_PRODUCTION: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            user_role = user_data.get("role", "")
            
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
            # Check if user has admin role in Onyx
            if user_role != "admin":
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN, 
                    detail="Access denied. Admin privileges required."
                )
            
            return str(onyx_user_id), user_email
            
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in verify_admin_user: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

def create_slug(text: Optional[str]) -> str:
    if not text: return "default-slug"
    text_processed = str(text).lower()
    text_processed = re.sub(r'\s+', '-', text_processed)
    text_processed = re.sub(r'[^\wа-яёa-z0-9\-]+', '', text_processed, flags=re.UNICODE | re.IGNORECASE)
    return text_processed or "generated-slug"

def get_tier_ratio(tier: str) -> int:
    """Get the creation hours ratio for a given tier (legacy support)"""
    ratios = {
        'starter': 120,
        'medium': 200,
        'advanced': 320,
        'professional': 450,
        'basic': 100,
        'interactive': 200,
        'immersive': 700
    }
    return ratios.get(tier, 200)  # Default to medium (200) if tier not found

def calculate_creation_hours(completion_time_minutes: int, custom_rate: int) -> int:
    """Calculate creation hours based on completion time and custom rate, rounded to nearest integer"""
    if completion_time_minutes <= 0:
        return 0
    
    # Convert completion time from minutes to hours, then multiply by custom rate
    completion_hours = completion_time_minutes / 60.0
    creation_hours = completion_hours * custom_rate
    return round(creation_hours)


def analyze_lesson_content_recommendations(lesson_title: str, quality_tier: Optional[str], existing_content: Optional[Dict[str, bool]] = None) -> Dict[str, Any]:
    """Smart, robust combo recommendations per tier.
    Returns a "primary" list of product types composing the chosen combo.
    Types: 'one-pager' | 'presentation' | 'quiz' | 'video-lesson'
    """
    import hashlib

    if existing_content is None:
        existing_content = {}

    title = (lesson_title or "").strip().lower()
    tier = (quality_tier or "interactive").strip().lower()

    # Keyword signals
    kw_one_pager = ["introduction", "overview", "basics", "summary", "quick", "reference", "primer", "cheatsheet"]
    kw_presentation = ["tutorial", "step-by-step", "process", "method", "workflow", "guide", "how to", "how-to", "walkthrough"]
    kw_video = ["demo", "walkthrough", "show", "demonstrate", "visual", "hands-on", "practical", "screencast", "recording"]
    kw_quiz = ["test", "check", "verify", "practice", "exercise", "assessment", "evaluation", "quiz"]

    def score_for(keys: list[str]) -> float:
        hits = sum(1 for k in keys if k in title)
        return min(1.0, hits / 3.0)  # saturate after 3 hits

    s_one = score_for(kw_one_pager)
    s_pres = score_for(kw_presentation)
    s_vid = score_for(kw_video)
    s_quiz = score_for(kw_quiz)

    # Deterministic variety seed per lesson
    seed_val = int(hashlib.sha1(f"{title}|{tier}".encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF

    # Define candidate combos per tier
    # combos are arrays of product types constituting the recommendation
    if tier == "basic":
        combos = [
            ["one-pager"],
            ["presentation"],
        ]
        # weights prefer brevity/overview to one-pager, procedural to presentation
        weights = [
            0.55 + 0.35 * s_one - 0.10 * s_pres,
            0.45 + 0.35 * s_pres - 0.10 * s_one,
        ]
    elif tier == "interactive":
        combos = [
            ["presentation", "quiz"],
            ["presentation"],
            ["one-pager", "quiz"],
        ]
        weights = [
            0.40 + 0.30 * s_pres + 0.30 * s_quiz,  # pres+quiz
            0.30 + 0.50 * s_pres - 0.10 * s_quiz,  # pres
            0.30 + 0.40 * s_one + 0.30 * s_quiz,   # one+quiz
        ]
    elif tier == "advanced":
        combos = [
            ["presentation", "quiz"],
            ["video-lesson", "quiz"],
        ]
        weights = [
            0.50 + 0.30 * s_pres + 0.20 * s_quiz,
            0.50 + 0.40 * s_vid + 0.20 * s_quiz,
        ]
    else:  # immersive
        combos = [
            ["video-lesson", "quiz"],
            ["video-lesson"],
        ]
        weights = [
            0.60 + 0.25 * s_vid + 0.15 * s_quiz,
            0.40 + 0.60 * s_vid - 0.10 * s_quiz,
        ]

    # Normalize weights, add small hash-based jitter for deterministic variety
    eps = 1e-6
    jitter = [(i + 1) * 0.0005 * seed_val for i in range(len(weights))]
    norm_weights = [max(eps, w + jitter[i]) for i, w in enumerate(weights)]

    # Sort combos by weight desc, break ties deterministically
    ranked = sorted(range(len(combos)), key=lambda i: (-norm_weights[i], i))

    # Choose the best combo that doesn’t fully collide with existing content
    chosen: list[str] | None = None
    for idx in ranked:
        c = combos[idx]
        # If combo has two items and one exists, we still propose the remaining one; if all exist, skip.
        missing = [t for t in c if not existing_content.get(t, False)]
        if missing:
            chosen = missing
            break

    # Fallback to the top combo if everything existed (rare)
    if not chosen:
        chosen = combos[ranked[0]]

    return {
        "primary": chosen,
        "reasoning": (
            f"tier={tier}; signals(one={s_one:.2f}, pres={s_pres:.2f}, video={s_vid:.2f}, quiz={s_quiz:.2f}); "
            f"seed={seed_val:.3f}; combos={combos}"
        ),
        "last_updated": datetime.utcnow().isoformat(),
        "quality_tier_used": tier,
    }

# --- Completion time from recommendations ---
PRODUCT_COMPLETION_RANGES = {
    "one-pager": (2, 3),
    "presentation": (5, 10),
    "quiz": (5, 7),
    "video-lesson": (2, 5),
}

def compute_completion_time_from_recommendations(primary_types: list[str]) -> str:
    total = 0
    for p in primary_types:
        r = PRODUCT_COMPLETION_RANGES.get(p)
        if not r:
            continue
        total += random.randint(r[0], r[1])
    if total <= 0:
        total = 5
    return f"{total}m"

def sanitize_training_plan_for_parse(content: Dict[str, Any]) -> Dict[str, Any]:
    try:
        sections = content.get('sections') or []
        for section in sections:
            lessons = section.get('lessons') or []
            for lesson in lessons:
                if isinstance(lesson, dict):
                    # keep recommended_content_types for persistence
                    pass
    except Exception:
        pass
    return content


def round_hours_in_content(content: Any) -> Any:
    """Recursively round all hours fields to integers in content structure"""
    if isinstance(content, dict):
        result = {}
        for key, value in content.items():
            if key == 'hours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif key == 'totalHours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif isinstance(value, (dict, list)):
                result[key] = round_hours_in_content(value)
            else:
                result[key] = value
        return result
    elif isinstance(content, list):
        return [round_hours_in_content(item) for item in content]
    else:
        return content

async def get_folder_tier(folder_id: int, pool: asyncpg.Pool) -> str:
    """Get the tier of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its tier
        folder = await conn.fetchrow(
            "SELECT quality_tier, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 'interactive'  # Default tier
        
        # If folder has its own tier, use it
        if folder['quality_tier']:
            return folder['quality_tier']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_tier(folder['parent_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

async def get_folder_custom_rate(folder_id: int, pool: asyncpg.Pool) -> int:
    """Get the custom rate of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its custom rate
        folder = await conn.fetchrow(
            "SELECT custom_rate, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 200  # Default custom rate
        
        # If folder has its own custom rate, use it
        if folder['custom_rate']:
            return folder['custom_rate']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_custom_rate(folder['parent_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_custom_rate(project_id: int, pool: asyncpg.Pool) -> int:
    """Get the effective custom rate for a project, falling back to folder rate if not set"""
    async with pool.acquire() as conn:
        # Get the project's custom rate and folder_id
        project = await conn.fetchrow(
            "SELECT custom_rate, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 200  # Default custom rate
        
        # If project has its own custom rate, use it
        if project['custom_rate']:
            return project['custom_rate']
        
        # Otherwise, get the folder's custom rate
        if project['folder_id']:
            return await get_folder_custom_rate(project['folder_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_quality_tier(project_id: int, pool: asyncpg.Pool) -> str:
    """Get the effective quality tier for a project, falling back to folder tier if not set"""
    async with pool.acquire() as conn:
        # Get the project's quality tier and folder_id
        project = await conn.fetchrow(
            "SELECT quality_tier, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 'interactive'  # Default tier
        
        # If project has its own quality tier, use it
        if project['quality_tier']:
            return project['quality_tier']
        
        # Otherwise, get the folder's quality tier
        if project['folder_id']:
            return await get_folder_tier(project['folder_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

def get_lesson_effective_custom_rate(lesson: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a lesson, falling back to project rate if not set"""
    if lesson.get('custom_rate'):
        return lesson['custom_rate']
    return project_custom_rate

def get_lesson_effective_quality_tier(lesson: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a lesson, falling back to project tier if not set"""
    if lesson.get('quality_tier'):
        return lesson['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours(lesson: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a specific lesson using its tier settings"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        effective_custom_rate = get_lesson_effective_custom_rate(lesson, project_custom_rate)
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

def get_module_effective_custom_rate(section: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a module, falling back to project rate if not set"""
    if section.get('custom_rate'):
        return section['custom_rate']
    return project_custom_rate

def get_module_effective_quality_tier(section: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a module, falling back to project tier if not set"""
    if section.get('quality_tier'):
        return section['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours_with_module_fallback(lesson: dict, section: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a lesson with module-level fallback"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        
        # Check lesson-level tier first, then module-level, then project-level
        if lesson.get('custom_rate'):
            effective_custom_rate = lesson['custom_rate']
        elif section.get('custom_rate'):
            effective_custom_rate = section['custom_rate']
        else:
            effective_custom_rate = project_custom_rate
            
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

# Define user types and their associated features
USER_TYPES = {
    "normal_hr": {
        "display_name": "Normal (HR)",
        "features": [
            "col_one_pager",
            "col_lesson_presentation", 
            "col_quiz"
        ]
    },
    "enterprise": {
        "display_name": "Enterprise",
        "features": [
            "col_one_pager",
            "col_lesson_presentation",
            "col_quiz",
            "deloitte_banner",
            "col_est_completion_time",
            "col_est_creation_time",
            "col_content_volume",
            "col_quality_tier",
            "lesson_draft",
            "offers_tab"
        ]
    },
    "beta": {
        "display_name": "Beta",
        "features": [
            "ai_audit_templates",
            "deloitte_banner",
            "offers_tab",
            "workspace_tab",
            "video_lesson",
            "lesson_draft",
            "col_assessment_type",
            "col_content_volume",
            "col_source",
            "col_est_creation_time",
            "col_est_completion_time",
            "col_quality_tier",
            "col_quiz",
            "col_one_pager",
            "col_video_presentation",
            "col_lesson_presentation",
            "quality_tier"
        ]
    }
}

async def assign_default_user_type(user_id: str, conn: asyncpg.Connection):
    """Assign default 'Normal (HR)' user type to a new user"""
    try:
        default_user_type = "normal_hr"
        if default_user_type not in USER_TYPES:
            logger.warning(f"Default user type {default_user_type} not found in USER_TYPES")
            return
        
        user_type_info = USER_TYPES[default_user_type]
        features_to_enable = user_type_info["features"]
        
        # Enable features for the default user type
        features_assigned = 0
        for feature_name in features_to_enable:
            # Check if feature exists before trying to assign it
            feature_exists = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_name
            )
            
            if feature_exists:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, created_at, updated_at)
                    VALUES ($1, $2, true, NOW(), NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = true,
                        updated_at = NOW()
                """, user_id, feature_name)
                features_assigned += 1
            else:
                logger.warning(f"Feature {feature_name} not found or inactive for new user {user_id}")
        
        logger.info(f"Assigned default user type '{user_type_info['display_name']}' to new user {user_id} ({features_assigned} features enabled)")
        
    except Exception as e:
        logger.error(f"Error assigning default user type to new user {user_id}: {e}")
        # Don't raise exception to avoid blocking user creation

async def get_or_create_user_credits(onyx_user_id: str, user_name: str, pool: asyncpg.Pool) -> UserCredits:
    """Get user credits or create if doesn't exist"""
    async with pool.acquire() as conn:
        # Try to get existing credits
        credits_row = await conn.fetchrow(
            "SELECT * FROM user_credits WHERE onyx_user_id = $1",
            onyx_user_id
        )
        
        if credits_row:
            return UserCredits(**dict(credits_row))
        
        # Create new user credits entry with default values
        new_credits_row = await conn.fetchrow("""
            INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
            VALUES ($1, $2, $3, $3)
            RETURNING *
        """, onyx_user_id, user_name, 100, 100)  # Default 100 credits for new users
        
        # Assign default "Normal (HR)" user type to new users
        await assign_default_user_type(onyx_user_id, conn)
        
        logger.info(f"Auto-migrated new user {onyx_user_id} ({user_name}) with 100 credits and Normal (HR) user type")
        return UserCredits(**dict(new_credits_row))

def calculate_product_credits(product_type: str, content_data: dict = None) -> int:
    """Calculate credit cost for product creation"""
    if product_type == "course_outline":
        return 5  # Course outline finalization costs 5 credits
    elif product_type == "lesson_presentation":
        # Calculate based on slide count
        if content_data and isinstance(content_data, dict):
            slides = content_data.get("slides", [])
            slide_count = len(slides) if slides else 0
            
            if slide_count <= 5:
                return 3
            elif slide_count <= 10:
                return 5
            else:
                return 10
        return 5  # Default if we can't determine slide count
    elif product_type in ["quiz", "one_pager"]:
        return 5  # Quiz and one-pager both cost 5 credits
    else:
        return 0  # Unknown product type, no cost

# Helper: reason -> product type fallback
def infer_product_type_from_reason(reason: str) -> str:
    r = (reason or "").lower()
    if "course outline" in r:
        return "Course Outline"
    if "lesson presentation" in r or "text presentation" in r or "presentation" in r:
        return "Presentation"
    if "quiz" in r:
        return "Quiz"
    if "one-pager" in r or "one pager" in r:
        return "One-Pager"
    if "video" in r:
        return "Video Lesson"
    return "Unknown"

# Helper: write a credit transaction row
async def log_credit_transaction(
    conn,
    *,
    onyx_user_id: str,
    user_credits_id: Optional[int],
    type_: Literal['purchase','product_generation','admin_removal'],
    amount: int,
    delta: int,
    title: Optional[str] = None,
    product_type: Optional[str] = None,
    reason: Optional[str] = None,
) -> None:
    tx_id = str(uuid.uuid4())
    await conn.execute(
        """
        INSERT INTO credit_transactions
            (id, onyx_user_id, user_credits_id, type, title, product_type, credits, delta, reason, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
        """,
        tx_id, onyx_user_id, user_credits_id, type_, title, product_type, abs(amount), delta, reason
    )

async def deduct_credits(
    onyx_user_id: str,
    amount: int,
    pool: asyncpg.Pool,
    reason: str = "Product creation",
    product_type: Optional[str] = None,
    title: Optional[str] = None,
) -> UserCredits:
    """Deduct credits from user balance with transaction safety"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Check current balance
            current_balance = await conn.fetchval(
                "SELECT credits_balance FROM user_credits WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if current_balance is None:
                raise HTTPException(status_code=404, detail="User credits not found")
            
            if current_balance < amount:
                raise HTTPException(
                    status_code=402, 
                    detail=f"Insufficient credits. Current balance: {current_balance}, Required: {amount}"
                )
            
            # Deduct credits and update usage
            updated_row = await conn.fetchrow("""
                UPDATE user_credits 
                SET credits_balance = credits_balance - $1,
                    total_credits_used = total_credits_used + $1,
                    updated_at = NOW()
                WHERE onyx_user_id = $2
                RETURNING *
            """, amount, onyx_user_id)

            # Log product_generation transaction
            user_row = await conn.fetchrow("SELECT id FROM user_credits WHERE onyx_user_id = $1", onyx_user_id)
            await log_credit_transaction(
                conn,
                onyx_user_id=onyx_user_id,
                user_credits_id=int(user_row["id"]) if user_row else None,
                type_='product_generation',
                amount=amount,
                delta=-abs(amount),
                title=title or reason,
                product_type=product_type or infer_product_type_from_reason(reason),
                reason=reason,
            )
            
            return UserCredits(**dict(updated_row))

async def modify_user_credits_by_email(user_email: str, amount: int, action: str, pool: asyncpg.Pool, reason: str = "Admin adjustment") -> UserCredits:
    """Modify user credits by email (for admin use)"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            if action == "add":
                # Add credits (can be new user or existing)
                # Use email as onyx_user_id for simplicity in admin interface
                credits_row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) 
                    DO UPDATE SET 
                        credits_balance = user_credits.credits_balance + $3,
                        credits_purchased = user_credits.credits_purchased + $3,
                        last_purchase_date = NOW(),
                        updated_at = NOW()
                    RETURNING *
                """, user_email, user_email.split('@')[0], amount, amount)

                # Log purchase
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='purchase',
                    amount=amount,
                    delta=abs(amount),
                    title="Credits purchase",
                    product_type=None,
                    reason=reason or "Credits purchase",
                )
                
            elif action == "remove":
                # Remove credits (must exist)
                credits_row = await conn.fetchrow("""
                    UPDATE user_credits 
                    SET credits_balance = GREATEST(0, credits_balance - $1),
                        updated_at = NOW()
                    WHERE onyx_user_id = $2
                    RETURNING *
                """, amount, user_email)
                
                if not credits_row:
                    raise HTTPException(status_code=404, detail="User not found")

                # Log admin removal as 'admin_removal'
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='admin_removal',
                    amount=amount,
                    delta=-abs(amount),
                    title="Admin adjustment",
                    product_type=None,
                    reason=reason or "Admin adjustment",
                )
            
            else:
                raise HTTPException(status_code=400, detail="Invalid action. Must be 'add' or 'remove'")
            
            return UserCredits(**dict(credits_row))

async def migrate_onyx_users_to_credits_table() -> int:
    """Migrate users from Onyx database to credits table"""
    if not ONYX_DATABASE_URL:
        raise Exception("ONYX_DATABASE_URL not configured")
    
    logger.info(f"Attempting to connect to Onyx database: {ONYX_DATABASE_URL}")
    
    # Try multiple possible database names
    possible_db_urls = [
        ONYX_DATABASE_URL,
        ONYX_DATABASE_URL.replace('/onyx_db', '/postgres'),  # Try postgres DB name
        ONYX_DATABASE_URL.replace('/postgres', '/onyx_db')   # Try onyx_db name
    ]
    
    # Remove duplicates while preserving order
    db_urls_to_try = []
    for url in possible_db_urls:
        if url not in db_urls_to_try:
            db_urls_to_try.append(url)
    
    onyx_pool = None
    last_error = None
    
    for db_url in db_urls_to_try:
        try:
            logger.info(f"Trying to connect to: {db_url}")
            # Connect to Onyx database
            onyx_pool = await asyncpg.create_pool(dsn=db_url, min_size=1, max_size=5)
        
            async with onyx_pool.acquire() as onyx_conn:
                # Get users from Onyx database
                onyx_users = await onyx_conn.fetch("""
                    SELECT 
                        id::text as onyx_user_id,
                        COALESCE(email, 'Unknown User') as name
                    FROM "user" 
                    WHERE is_active = true
                    AND role != 'ext_perm_user'
                    AND role != 'slack_user'
                    AND email NOT LIKE '%@danswer-api-key.invalid'
                """)
            
            if not onyx_users:
                logger.info("No Onyx users found to migrate")
                return 0
            
            logger.info(f"Found {len(onyx_users)} Onyx users to migrate")
            
            # Insert into custom database
            async with DB_POOL.acquire() as custom_conn:
                migrated_count = 0
                for user in onyx_users:
                    try:
                        await custom_conn.execute("""
                            INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                            VALUES ($1, $2, 100)
                            ON CONFLICT (onyx_user_id) DO NOTHING
                        """, user['onyx_user_id'], user['name'])
                        migrated_count += 1
                    except Exception as e:
                        logger.warning(f"Failed to migrate user {user['onyx_user_id']}: {e}")
                
                return migrated_count
                
        except Exception as e:
            last_error = e
            logger.warning(f"Failed to connect to {db_url}: {e}")
            if onyx_pool:
                await onyx_pool.close()
                onyx_pool = None
            continue  # Try next database URL
        
        # If we got here, the connection worked, so break out of the loop
        break
    
    # If we tried all URLs and none worked, raise the last error
    if onyx_pool is None:
        raise Exception(f"Could not connect to any Onyx database. Last error: {last_error}")
    
    # This should never be reached, but just in case
    return 0

VIDEO_SCRIPT_LANG_STRINGS = {
    'ru': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Видео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Отображаемый текст:',
        'DISPLAYED_IMAGE_LABEL': 'Отображаемая картинка:',
        'DISPLAYED_VIDEO_LABEL': 'Отображаемое видео:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучки:',
        'NO_SLIDES_TEXT': 'Нет слайдов для отображения.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Название теста',
            'question': 'Вопрос',
            'correctAnswer': 'Правильный ответ',
            'correctAnswers': 'Правильные ответы',
            'acceptableAnswers': 'Допустимые ответы',
            'prompts': 'Элементы',
            'options': 'Варианты',
            'correctMatches': 'Правильные соответствия',
            'itemsToSort': 'Элементы для сортировки',
            'explanation': 'Объяснение',
            'multipleChoice': 'Один правильный ответ',
            'multiSelect': 'Несколько правильных ответов',
            'matching': 'Соответствие',
            'sorting': 'Сортировка',
            'openAnswer': 'Свободный ответ',
            'answerKey': 'Ключ ответов',
            'correctOrder': 'Правильный порядок',
            'emptyContent': '...',
        }
    },
    'en': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Video Lesson Script',
        'SLIDE_NUMBER_PREFIX': 'SLIDE №',
        'DISPLAYED_TEXT_LABEL': 'Displayed Text:',
        'DISPLAYED_IMAGE_LABEL': 'Displayed Image:',
        'DISPLAYED_VIDEO_LABEL': 'Displayed Video:',
        'VOICEOVER_TEXT_LABEL': 'Voiceover Text:',
        'NO_SLIDES_TEXT': 'No slides to display.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'COURSE',
        'lessonLabel': 'LESSON',
        'quiz': {
            'quizTitle': 'Quiz Title',
            'question': 'Question',
            'correctAnswer': 'Correct Answer',
            'correctAnswers': 'Correct Answers',
            'acceptableAnswers': 'Acceptable Answers',
            'prompts': 'Items',
            'options': 'Options',
            'correctMatches': 'Correct Matches',
            'itemsToSort': 'Items to Sort',
            'explanation': 'Explanation',
            'multipleChoice': 'Multiple Choice',
            'multiSelect': 'Multi-Select',
            'matching': 'Matching',
            'sorting': 'Sorting',
            'openAnswer': 'Open Answer',
            'answerKey': 'Answer Key',
            'correctOrder': 'Correct Order',
            'emptyContent': '...',
        }
    },
    'uk': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Відео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Текст, що відображається:',
        'DISPLAYED_IMAGE_LABEL': 'Зображення, що відображається:',
        'DISPLAYED_VIDEO_LABEL': 'Відео, що відображається:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучення:',
        'NO_SLIDES_TEXT': 'Немає слайдів для відображення.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Назва тесту',
            'question': 'Питання',
            'correctAnswer': 'Правильна відповідь',
            'correctAnswers': 'Правильні відповіді',
            'acceptableAnswers': 'Допустимі відповіді',
            'prompts': 'Елементи',
            'options': 'Варіанти',
            'correctMatches': 'Правильні відповідності',
            'itemsToSort': 'Елементи для сортування',
            'explanation': 'Пояснення',
            'multipleChoice': 'Одна правильна відповідь',
            'multiSelect': 'Декілька правильних відповідей',
            'matching': 'Відповідність',
            'sorting': 'Сортування',
            'openAnswer': 'Вільна відповідь',
            'answerKey': 'Ключ відповідей',
            'correctOrder': 'Правильний порядок',
            'emptyContent': '...',
        }
    },
    'es': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Guión de la lección en video',
        'SLIDE_NUMBER_PREFIX': 'DIAPOSITIVA №',
        'DISPLAYED_TEXT_LABEL': 'Texto mostrado:',
        'DISPLAYED_IMAGE_LABEL': 'Imagen mostrada:',
        'DISPLAYED_VIDEO_LABEL': 'Video mostrado:',
        'VOICEOVER_TEXT_LABEL': 'Texto de voz en off:',
        'NO_SLIDES_TEXT': 'No hay diapositivas para mostrar.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'CURSO',
        'lessonLabel': 'LECCIÓN',
        'quiz': {
            'quizTitle': 'Título del cuestionario',
            'question': 'Pregunta',
            'correctAnswer': 'Respuesta correcta',
            'correctAnswers': 'Respuestas correctas',
            'acceptableAnswers': 'Respuestas aceptables',
            'prompts': 'Elementos',
            'options': 'Opciones',
            'correctMatches': 'Correspondencias correctas',
            'itemsToSort': 'Elementos para ordenar',
            'explanation': 'Explicación',
            'multipleChoice': 'Opción múltiple',
            'multiSelect': 'Selección múltiple',
            'matching': 'Correspondencia',
            'sorting': 'Ordenamiento',
            'openAnswer': 'Respuesta abierta',
            'answerKey': 'Clave de respuestas',
            'correctOrder': 'Orden correcto',
            'emptyContent': '...',
        }
    }
}

def detect_language(text: str, configs: Dict[str, Dict[str, str]] = LANG_CONFIG) -> str:
    en_score = 0; ru_score = 0; uk_score = 0
    en_config = configs.get('en', {})
    ru_config = configs.get('ru', {})
    uk_config = configs.get('uk', {})

    if en_config.get('MODULE_KEYWORD') and en_config.get('LESSONS_HEADER_KEYWORD') and en_config.get('TOTAL_TIME_KEYWORD'):
        if en_config['MODULE_KEYWORD'] in text and \
           en_config['LESSONS_HEADER_KEYWORD'] in text and \
           en_config['TOTAL_TIME_KEYWORD'] in text:
            en_score += 3
    if ru_config.get('MODULE_KEYWORD') and ru_config.get('LESSONS_HEADER_KEYWORD') and ru_config.get('TOTAL_TIME_KEYWORD'):
        if ru_config['MODULE_KEYWORD'] in text and \
           ru_config['LESSONS_HEADER_KEYWORD'] in text and \
           ru_config['TOTAL_TIME_KEYWORD'] in text:
            ru_score += 3
    if uk_config.get('MODULE_KEYWORD') and uk_config.get('LESSONS_HEADER_KEYWORD') and uk_config.get('TOTAL_TIME_KEYWORD'):
        if uk_config['MODULE_KEYWORD'] in text and \
           uk_config['LESSONS_HEADER_KEYWORD'] in text and \
           uk_config['TOTAL_TIME_KEYWORD'] in text:
            uk_score += 3
    if en_score == 0 and ru_score == 0 and uk_score == 0:
        if en_config.get('MODULE_KEYWORD') and en_config['MODULE_KEYWORD'] in text: en_score +=1
        if ru_config.get('MODULE_KEYWORD') and ru_config['MODULE_KEYWORD'] in text: ru_score +=1
        if uk_config.get('MODULE_KEYWORD') and uk_config['MODULE_KEYWORD'] in text: uk_score +=1
        if en_config.get('TIME_KEYWORD') and en_config['TIME_KEYWORD'] in text: en_score +=1
        if ru_config.get('TIME_KEYWORD') and ru_config['TIME_KEYWORD'] in text: ru_score +=1
        if uk_config.get('TIME_KEYWORD') and uk_config['TIME_KEYWORD'] in text: uk_score +=1
        if en_score == 0 and ru_score == 0 and uk_score == 0:
            en_chars = sum(1 for char_ in text if 'a' <= char_.lower() <= 'z')
            cyrillic_chars = sum(1 for char_ in text if 'а' <= char_.lower() <= 'я' or char_.lower() in ['і', 'ї', 'є', 'ґ'])
            if en_chars > cyrillic_chars and en_chars > 10 :
                 en_score += 0.1
            elif cyrillic_chars > en_chars and cyrillic_chars > 10:
                if uk_score == 0: uk_score += 0.05
                if ru_score == 0: ru_score += 0.05
                ukrainian_specific_chars = sum(1 for char_ in text if char_.lower() in ['і', 'ї', 'є', 'ґ'])
                if ukrainian_specific_chars > 0:
                    uk_score += 0.05 * ukrainian_specific_chars
    if en_score > ru_score and en_score > uk_score: return 'en'
    if ru_score > en_score and ru_score > uk_score: return 'ru'
    if uk_score > en_score and uk_score > ru_score: return 'uk'
    if uk_score > 0 and uk_score >= ru_score and uk_score >= en_score: return 'uk'
    if ru_score > 0 and ru_score >= en_score : return 'ru'
    if en_score > 0 : return 'en'
    logger.warning("detect_language could not reliably determine language. Defaulting to 'en'.")
    return 'en'

def parse_training_plan_from_string(original_content_str: str, main_table_title: str) -> Optional[TrainingPlanDetails]:
    logger.warning("Old 'parse_training_plan_from_string' called. Ensure this is intended for legacy data.")
    return TrainingPlanDetails(mainTitle=f"Content for {main_table_title} (Old Parser)", sections=[], detectedLanguage='ru')

async def parse_ai_response_with_llm(
    ai_response: str,
    project_name: str,
    target_model: Type[BaseModel],
    default_error_model_instance: BaseModel,
    dynamic_instructions: str,
    target_json_example: str
) -> BaseModel:
    # Start timing for analytics
    start_time = time.time()
    
    # DEBUG: Log that the function was called
    logger.info(f"=== AI PARSER FUNCTION CALLED ===")
    logger.info(f"Project: {project_name}")
    logger.info(f"Target model: {target_model.__name__}")
    logger.info(f"AI response length: {len(ai_response)}")
    logger.info(f"DB_POOL available: {DB_POOL is not None}")
    logger.info(f"Call stack: {len(inspect.stack())} frames")
    logger.info(f"=== END FUNCTION CALL DEBUG ===")
    
    # Create a list of API keys to try, filtering out any that are not set
    api_keys_to_try = [key for key in [LLM_API_KEY, LLM_API_KEY_FALLBACK] if key]

    if not api_keys_to_try:
        logger.error(f"LLM_API_KEY not configured for {project_name}. Cannot parse AI response with LLM.")
        return default_error_model_instance

    prompt_message = f"""
You are a highly accurate text-to-JSON parsing assistant. Your task is to convert the *entirety* of the following unstructured text into a single, structured JSON object.
Ensure *all* relevant information from the "Raw text to parse" is included in your JSON output.
Pay close attention to data types: strings should be quoted, numerical values should be numbers, and lists should be arrays. Null values are not permitted for string fields; use an empty string "" instead if text is absent but the field is required according to the example structure.
Maintain the original language of the input text for all textual content in the JSON.

🚨 SPECIAL INSTRUCTION FOR VIDEO LESSONS: If the target model is SlideDeckDetails and the JSON example contains "voiceoverText" fields, you MUST generate voiceover text for every slide object. Look at the example JSON structure and ensure your output matches it exactly, including all voiceoverText fields and hasVoiceover flag.

Specific Instructions for this Content Type ({target_model.__name__}):
---
{dynamic_instructions}
---

The desired JSON output format is exemplified below. This example is CRUCIAL and your output MUST strictly follow this JSON format and structure.
---
{target_json_example}
---

Raw text to parse:
---
{ai_response}
---

Return ONLY the JSON object corresponding to the parsed text. Do not include any other explanatory text or markdown formatting (like ```json ... ```) around the JSON.
The entire output must be a single, valid JSON object and must include all relevant data found in the input, with textual content in the original language.
    """
    # OpenAI Chat API expects a list of chat messages
    system_msg = {"role": "system", "content": "You are a JSON parsing expert. You must output ONLY valid JSON in the exact format specified. Do not include any explanations, markdown formatting, or additional text. Your response must be a single, complete JSON object. CRITICAL: If the example JSON contains voiceoverText fields, your output MUST include them for every slide. Match the example structure exactly."}
    user_msg = {"role": "user", "content": prompt_message}
    base_payload: Dict[str, Any] = {"model": LLM_DEFAULT_MODEL, "messages": [system_msg, user_msg], "temperature": 0.1}
    # Ask the model to output pure JSON
    base_payload_with_rf = {**base_payload, "response_format": {"type": "json_object"}}
    detected_lang_by_rules = detect_language(ai_response)
    last_exception = None

    for i, api_key in enumerate(api_keys_to_try):
        attempt_number = i + 1
        logger.info(f"Attempting LLM call for '{project_name}' using API key #{attempt_number}.")
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        try:
            # Try with response_format first, then without if Cohere rejects it
            for pf_idx, payload_variant in enumerate([base_payload_with_rf, base_payload]):
                try:
                    # Remove per-request timeout so long parses are not cut off (backend will rely on upstream timeouts)
                    async with httpx.AsyncClient(timeout=None) as client:
                        response = await client.post(LLM_API_URL, headers=headers, json=payload_variant)
                        response.raise_for_status()
                    break  # success
                except httpx.HTTPStatusError as he:
                    # OpenAI returns 400 when response_format isn't supported for a given model
                    if he.response.status_code in (400, 422) and pf_idx == 0:
                        logger.info("LLM rejected response_format – retrying without it.")
                        continue
                    raise

            llm_api_response_data = response.json()

            # --- Process the Response ---
            json_text_output = None
            if "text" in llm_api_response_data: json_text_output = llm_api_response_data["text"]
            elif "chatHistory" in llm_api_response_data and llm_api_response_data["chatHistory"]:
                last_message = next((msg for msg in reversed(llm_api_response_data["chatHistory"]) if msg.get("role") == "CHATBOT"), None)
                if last_message and "message" in last_message: json_text_output = last_message["message"]
            elif llm_api_response_data.get("generations") and isinstance(llm_api_response_data["generations"], list) and llm_api_response_data["generations"][0].get("text"):
                json_text_output = llm_api_response_data["generations"][0]["text"]
            elif "choices" in llm_api_response_data and isinstance(llm_api_response_data["choices"], list) and llm_api_response_data["choices"]:
                # Support for OpenAI Chat Completions format: choices[0].message.content (or .delta.content in streaming)
                first_choice = llm_api_response_data["choices"][0]
                if isinstance(first_choice, dict):
                    # Standard chat completion response
                    if "message" in first_choice and isinstance(first_choice["message"], dict):
                        json_text_output = first_choice["message"].get("content")
                    # If using the delta format (streaming-style response aggregated by OpenAI)
                    if not json_text_output and "delta" in first_choice and isinstance(first_choice["delta"], dict):
                        json_text_output = first_choice["delta"].get("content")
                    # Fallback to any direct text field
                    if not json_text_output:
                        json_text_output = first_choice.get("text")

            if json_text_output is None:
                # Log the raw keys of the response for easier debugging
                logger.warning(
                    "No 'content' field found in LLM response. Raw keys: %s" % list(llm_api_response_data.keys())
                )
                logger.debug("Full LLM response: %s" % json.dumps(llm_api_response_data)[:1000])
                raise ValueError("LLM response did not contain an expected text field.")

            json_text_output = re.sub(r"^```json\s*|\s*```$", "", json_text_output.strip(), flags=re.MULTILINE)

            cleaned_json_str = json_text_output.strip()
            cleaned_json_str = re.sub(r"^```(?:json)?\s*|\s*```$", "", cleaned_json_str, flags=re.IGNORECASE | re.MULTILINE).strip()
            first_brace = cleaned_json_str.find('{')
            last_brace = cleaned_json_str.rfind('}')
            if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
                cleaned_json_str = cleaned_json_str[first_brace:last_brace + 1]
            if not cleaned_json_str.startswith('{'):
                cleaned_json_str = json_text_output.strip()
            json_text_output = cleaned_json_str

            try:
                parsed_json_data = json.loads(json_text_output)
            except json.JSONDecodeError:
                fixed_str = _clean_loose_json(json_text_output)
                try:
                    parsed_json_data = json.loads(fixed_str)
                except json.JSONDecodeError:
                    import ast
                    try:
                        parsed_json_data = ast.literal_eval(fixed_str)
                    except (ValueError, SyntaxError):
                        # Last-ditch heuristic: convert single quotes to double and
                        # Python constants to JSON equivalents, then try json.loads again.
                        brute = fixed_str
                        brute = re.sub(r"'([^']*)'", lambda m: '"' + m.group(1).replace('"', '\\"') + '"', brute)
                        brute = brute.replace("True", "true").replace("False", "false").replace("None", "null")
                        parsed_json_data = json.loads(brute)

            logger.debug(f'Cohere response: {parsed_json_data}')

            if 'detectedLanguage' not in parsed_json_data or not parsed_json_data['detectedLanguage']:
                parsed_json_data['detectedLanguage'] = detected_lang_by_rules

            if target_model == TrainingPlanDetails and ('mainTitle' not in parsed_json_data or not parsed_json_data['mainTitle']):
                parsed_json_data['mainTitle'] = project_name
            elif target_model == PdfLessonDetails and ('lessonTitle' not in parsed_json_data or not parsed_json_data['lessonTitle']):
                parsed_json_data['lessonTitle'] = project_name
            
            # Round hours to integers before validation to prevent float validation errors
            if target_model == TrainingPlanDetails:
                parsed_json_data = round_hours_in_content(parsed_json_data)
            
            validated_model = target_model.model_validate(parsed_json_data)
            logger.info(f"LLM parsing for '{project_name}' succeeded on attempt #{attempt_number}.")

            # Log AI parser usage
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    response_tokens = len(encoding.encode(json.dumps(parsed_json_data)))
                    total_tokens = prompt_tokens + response_tokens
                    
                    logger.info(f"=== AI PARSER LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Response tokens: {response_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"=== END AI PARSER LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 200, int((time.time() - start_time) * 1000), len(ai_response), len(json.dumps(parsed_json_data)), None, True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Database insert result: {result}")
                            logger.info(f"Successfully logged AI parser usage for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Database insert failed: {db_error}")
                            logger.error(f"Insert parameters: endpoint='/ai/parse', method='POST', status_code=200, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as e:
                    logger.warning(f"Failed to log AI parser usage: {e}")
                    logger.error(f"AI Parser logging error details: {str(e)}")

            return validated_model

        except Exception as e:
            last_exception = e
            logger.warning(
                f"LLM parsing attempt #{attempt_number} for '{project_name}' failed with {type(e).__name__}. "
                f"Details: {str(e)[:250]}. Trying next key if available."
            )
            
            # Log failed AI parser attempt
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    total_tokens = prompt_tokens  # No response tokens for failed attempts
                    
                    logger.info(f"=== AI PARSER FAILED LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"Error: {str(e)[:200]}")
                    logger.info(f"=== END AI PARSER FAILED LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert failed AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 500, int((time.time() - start_time) * 1000), len(ai_response), 0, str(e)[:500], True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Failed attempt database insert result: {result}")
                            logger.info(f"Successfully logged failed AI parser attempt for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Failed attempt database insert failed: {db_error}")
                            logger.error(f"Failed attempt insert parameters: endpoint='/ai/parse', method='POST', status_code=500, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as log_error:
                    logger.warning(f"Failed to log AI parser error: {log_error}")
                    logger.error(f"AI Parser failed logging error details: {str(log_error)}")
            
            continue

    # --- Handle Final Failure ---
    # This block is reached only if the loop completes without a successful return.
    logger.error(f"All LLM API call attempts failed for '{project_name}'. Last error: {last_exception}")
    if hasattr(default_error_model_instance, 'detectedLanguage'):
        default_error_model_instance.detectedLanguage = detected_lang_by_rules
    return default_error_model_instance

def _clean_loose_json(text: str) -> str:
    """Attempt to fix common minor JSON issues produced by LLMs: trailing commas, smart quotes, etc."""
    # remove common markdown code fences again (defensive)
    text = re.sub(r"^```(?:json)?\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE | re.MULTILINE)
    # replace smart quotes with standard quotes
    for sq in ('\u201c', '\u201d', '\u2018', '\u2019'):
        text = text.replace(sq, '"')
    # strip trailing commas before object/array close
    text = re.sub(r",\s*(\}|\])", r"\1", text)
    # remove escaped newlines that are unnecessary
    text = text.replace('\\n', '\n')
    return text

# --- API Endpoints ---
@app.post("/api/custom/pipelines/add", response_model=MicroproductPipelineDBRaw, status_code=status.HTTP_201_CREATED)
async def add_pipeline(pipeline_data: MicroproductPipelineCreateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        INSERT INTO microproduct_pipelines (pipeline_name, pipeline_description, {db_is_discovery}, {db_is_structuring}, prompts_data_collection, prompts_data_formating, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create pipeline.")
        return MicroproductPipelineDBRaw(**dict(row))
    except Exception as e:
        logger.error(f"Error inserting pipeline: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding the pipeline." if IS_PRODUCTION else f"DB error on pipeline insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines", response_model=List[MicroproductPipelineGetResponse])
async def get_pipelines(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines ORDER BY created_at DESC;"
    try:
        async with pool.acquire() as conn: rows = await conn.fetch(query)
        pipelines_list = [MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row))) for row in rows]
        return pipelines_list
    except Exception as e:
        logger.error(f"Error fetching pipelines: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching pipelines." if IS_PRODUCTION else f"DB error fetching pipelines: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines/{pipeline_id}", response_model=MicroproductPipelineGetResponse)
async def get_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines WHERE id = $1;"
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row)))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching the pipeline." if IS_PRODUCTION else f"DB error fetching pipeline: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/pipelines/update/{pipeline_id}", response_model=MicroproductPipelineDBRaw)
async def update_pipeline(pipeline_id: int, pipeline_data: MicroproductPipelineUpdateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        UPDATE microproduct_pipelines SET pipeline_name = $1, pipeline_description = $2, {db_is_discovery} = $3, {db_is_structuring} = $4, prompts_data_collection = $5, prompts_data_formating = $6
        WHERE id = $7 RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found or update failed.")
        return MicroproductPipelineDBRaw(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating the pipeline." if IS_PRODUCTION else f"DB error on pipeline update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/pipelines/delete/{pipeline_id}", status_code=status.HTTP_200_OK)
async def delete_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "DELETE FROM microproduct_pipelines WHERE id = $1 RETURNING id;"
    try:
        async with pool.acquire() as conn: deleted_id = await conn.fetchval(query, pipeline_id)
        if deleted_id is None:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return {"detail": f"Successfully deleted pipeline with ID {pipeline_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the pipeline." if IS_PRODUCTION else f"DB error on pipeline deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_design_template_image(file: UploadFile = File(...)):
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 5 * 1024 * 1024
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving design image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/onepager/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_onepager_image(file: UploadFile = File(...)):
    """Upload an image for use in one-pagers"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for one-pager images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"onepager_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving one-pager image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/presentation/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_presentation_image(file: UploadFile = File(...)):
    """Upload an image for use in presentations"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for presentation images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"presentation_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving presentation image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

# NEW: AI Image Generation Endpoint
class AIImageGenerationRequest(BaseModel):
    prompt: str = Field(..., description="Text prompt for image generation")
    width: int = Field(..., description="Image width in pixels", ge=256, le=1792)
    height: int = Field(..., description="Image height in pixels", ge=256, le=1792)
    quality: str = Field(default="standard", description="Image quality: standard or hd")
    style: str = Field(default="vivid", description="Image style: vivid or natural")
    model: str = Field(default="dall-e-3", description="DALL-E model to use")

@app.post("/api/custom/presentation/generate_image", responses={
    200: {"description": "Image generated successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/ai_generated_image.png"}}}},
    400: {"description": "Invalid request parameters", "model": ErrorDetail},
    500: {"description": "AI generation failed", "model": ErrorDetail}
})
async def generate_ai_image(request: AIImageGenerationRequest):
    """Generate an image using DALL-E AI"""
    try:
        logger.info(f"[AI_IMAGE_GENERATION] Starting generation with prompt: '{request.prompt[:50]}...'")
        logger.info(f"[AI_IMAGE_GENERATION] Dimensions: {request.width}x{request.height}, Quality: {request.quality}, Style: {request.style}")
        
        # Validate dimensions (DALL-E 3 requirements)
        valid_sizes = [(1024, 1024), (1792, 1024), (1024, 1792)]
        current_size = (request.width, request.height)
        
        if current_size not in valid_sizes:
            # Find the closest valid size based on aspect ratio
            aspect_ratio = request.width / request.height
            
            if aspect_ratio > 1.5:  # Landscape
                request.width, request.height = 1792, 1024
            elif aspect_ratio < 0.7:  # Portrait
                request.width, request.height = 1024, 1792
            else:  # Square-ish
                request.width, request.height = 1024, 1024
                
            logger.info(f"[AI_IMAGE_GENERATION] Adjusted dimensions from {current_size} to {request.width}x{request.height}")
        
        # Get OpenAI client
        client = get_openai_client()
        
        # Generate image using DALL-E
        response = await client.images.generate(
            model=request.model,
            prompt=request.prompt,
            size=f"{request.width}x{request.height}",
            quality=request.quality,
            style=request.style,
            n=1
        )
        
        if not response.data or len(response.data) == 0:
            raise Exception("No image data received from DALL-E")
        
        # Get the generated image URL
        image_url = response.data[0].url
        if not image_url:
            raise Exception("No image URL received from DALL-E")
        
        logger.info(f"[AI_IMAGE_GENERATION] Image generated successfully, downloading from: {image_url[:50]}...")
        
        # Download the image
        async with httpx.AsyncClient() as http_client:
            image_response = await http_client.get(image_url)
            image_response.raise_for_status()
            image_data = image_response.content
        
        # Save the image to disk
        safe_filename_base = str(uuid.uuid4())
        unique_filename = f"ai_generated_{safe_filename_base}.png"
        file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
        
        try:
            with open(file_path_on_disk, "wb") as buffer:
                buffer.write(image_data)
            
            web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
            
            logger.info(f"[AI_IMAGE_GENERATION] Image saved successfully: {web_accessible_path}")
            
            return {
                "file_path": web_accessible_path,
                "prompt": request.prompt,
                "dimensions": {"width": request.width, "height": request.height},
                "quality": request.quality,
                "style": request.style
            }
            
        except Exception as e:
            logger.error(f"[AI_IMAGE_GENERATION] Error saving image to disk: {e}", exc_info=not IS_PRODUCTION)
            detail_msg = "Could not save generated image." if IS_PRODUCTION else f"Could not save generated image: {str(e)}"
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[AI_IMAGE_GENERATION] Error generating image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "AI image generation failed." if IS_PRODUCTION else f"AI image generation failed: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/add", response_model=DesignTemplateResponse, status_code=status.HTTP_201_CREATED)
async def add_design_template(template_data: DesignTemplateCreate, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "INSERT INTO design_templates (template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created) VALUES ($1, $2, $3, $4, $5, $6) RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, template_data.template_name, template_data.template_structuring_prompt, template_data.design_image_path, template_data.microproduct_type, template_data.component_name, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Design template with this name already exists." if IS_PRODUCTION else f"Design template with name '{template_data.template_name}' already exists."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except Exception as e:
        logger.error(f"Error inserting design template: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding design template." if IS_PRODUCTION else f"DB error on design template insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates", response_model=List[DesignTemplateResponse])
async def get_design_templates_list(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates ORDER BY date_created DESC;"
    try:
        async with pool.acquire() as conn: rows = await conn.fetch(query)
        return [DesignTemplateResponse(**dict(row)) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching design templates: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design templates." if IS_PRODUCTION else f"DB error fetching design templates: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates/{template_id}", response_model=DesignTemplateResponse)
async def get_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates WHERE id = $1;"
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, template_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")
        return DesignTemplateResponse(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design template." if IS_PRODUCTION else f"DB error fetching design template: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/design_templates/update/{template_id}", response_model=DesignTemplateResponse)
async def update_design_template(template_id: int, template_data: DesignTemplateUpdate, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            existing_template_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", template_id)
            if not existing_template_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")

        update_fields = template_data.model_dump(exclude_unset=True)
        if not update_fields:
            return DesignTemplateResponse(**dict(existing_template_row))

        set_clauses = []; update_values = []; i = 1
        for key, value in update_fields.items(): set_clauses.append(f"{key} = ${i}"); update_values.append(value); i += 1
        update_values.append(template_id)
        query = f"UPDATE design_templates SET {', '.join(set_clauses)} WHERE id = ${i} RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"

        async with pool.acquire() as conn: row = await conn.fetchrow(query, *update_values)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Update would violate a unique constraint (e.g., template name)."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating design template." if IS_PRODUCTION else f"DB error on design template update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/design_templates/delete/{template_id}", status_code=status.HTTP_200_OK)
async def delete_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            template_to_delete = await conn.fetchrow("SELECT design_image_path FROM design_templates WHERE id = $1", template_id)
            if not template_to_delete:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found.")

            if template_to_delete["design_image_path"]:
                filename_only = os.path.basename(template_to_delete["design_image_path"])
                full_image_path = os.path.join(STATIC_DESIGN_IMAGES_DIR, filename_only)
                if os.path.exists(full_image_path):
                    try:
                        os.remove(full_image_path)
                        logger.info(f"Successfully deleted image file: {full_image_path}")
                    except OSError as e_img:
                        logger.warning(f"Error deleting image file {full_image_path}: {e_img}. Continuing with DB record deletion.", exc_info=not IS_PRODUCTION)
                else:
                    logger.warning(f"Image file not found for deletion: {full_image_path}")
            deleted_count_status = await conn.execute("DELETE FROM design_templates WHERE id = $1", template_id)
        if deleted_count_status == "DELETE 0":
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found during delete, or already deleted.")
        return {"detail": f"Successfully initiated deletion for design template with ID {template_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during design template deletion." if IS_PRODUCTION else f"DB error on design template deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS = [
    "Training Plan", "PDF Lesson", "Slide Deck", "Text Presentation"
]

# Constants for text size thresholds - Aggressive thresholds to prevent AI memory issues
TEXT_SIZE_THRESHOLD = 1500  # Characters - switch to compression for texts larger than this
LARGE_TEXT_THRESHOLD = 3000  # Characters - use virtual file system to prevent AI memory issues
VIRTUAL_TEXT_FILE_PREFIX = "paste_text_"

# Cache for virtual text files to prevent duplicate uploads
VIRTUAL_TEXT_FILE_CACHE: Dict[str, int] = {}

def compress_text(text_content: str) -> str:
    """
    Compress large text content using gzip and encode as base64.
    This reduces the payload size for large texts.
    """
    try:
        # Compress the text
        text_bytes = text_content.encode('utf-8')
        compressed = gzip.compress(text_bytes)
        # Encode as base64 for JSON transmission
        compressed_b64 = base64.b64encode(compressed).decode('utf-8')
        logger.info(f"Compressed text from {len(text_content)} chars to {len(compressed_b64)} chars (reduction: {(1 - len(compressed_b64)/len(text_content))*100:.1f}%)")
        return compressed_b64
    except Exception as e:
        logger.error(f"Error compressing text: {e}")
        # Return original text if compression fails
        return text_content

def decompress_text(compressed_b64: str) -> str:
    """
    Decompress base64-encoded gzipped text content.
    """
    try:
        # Decode from base64
        compressed = base64.b64decode(compressed_b64.encode('utf-8'))
        # Decompress
        text_bytes = gzip.decompress(compressed)
        return text_bytes.decode('utf-8')
    except Exception as e:
        logger.error(f"Error decompressing text: {e}")
        # Return original if decompression fails (assume it wasn't compressed)
        return compressed_b64

def chunk_text(text_content: str, max_chunk_size: int = 2000) -> List[str]:
    """
    Split large text into manageable chunks while preserving sentence boundaries.
    """
    if len(text_content) <= max_chunk_size:
        return [text_content]
    
    chunks = []
    current_chunk = ""
    
    # Split by sentences first, then by words if needed
    sentences = text_content.replace('\n', ' ').split('. ')
    
    for sentence in sentences:
        # Add period back if it's not the last sentence
        if not sentence.endswith('.') and sentence != sentences[-1]:
            sentence += '.'
        
        # If adding this sentence would exceed chunk size
        if len(current_chunk) + len(sentence) + 1 > max_chunk_size:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                # Single sentence is too long, split by words
                words = sentence.split()
                for word in words:
                    if len(current_chunk) + len(word) + 1 > max_chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            current_chunk = word
                        else:
                            # Single word is too long, truncate
                            chunks.append(word[:max_chunk_size])
                            current_chunk = ""
                    else:
                        current_chunk += " " + word if current_chunk else word
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    logger.info(f"Split text into {len(chunks)} chunks (original: {len(text_content)} chars)")
    return chunks



async def create_virtual_text_file(text_content: str, cookies: Dict[str, str]) -> int:
    """
    Create a virtual text file for large text content and return the file ID.
    Uses caching to prevent duplicate uploads of the same text.
    """
    try:
        # Create a hash of the text content for caching
        import hashlib
        text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
        
        # Check if we already have this text cached
        if text_hash in VIRTUAL_TEXT_FILE_CACHE:
            cached_file_id = VIRTUAL_TEXT_FILE_CACHE[text_hash]
            logger.info(f"Using cached virtual file for text hash {text_hash[:8]}... -> file ID: {cached_file_id}")
            return cached_file_id
        
        # Create a temporary file-like object with the text content
        text_bytes = text_content.encode('utf-8')
        text_file = io.BytesIO(text_bytes)
        
        # Create a filename with timestamp for uniqueness
        timestamp = int(asyncio.get_event_loop().time())
        filename = f"{VIRTUAL_TEXT_FILE_PREFIX}{timestamp}.txt"
        
        # Create FormData for file upload
        files = {
            'files': (filename, text_file, 'text/plain')
        }
        
        # Upload file to Onyx file system
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files
            logger.info(f"Uploading virtual text file: {filename} ({len(text_content)} chars)")
            
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/user/file/upload",
                files=files,
                cookies=cookies
            )
            response.raise_for_status()
            
            # Parse response to get file ID
            upload_result = response.json()
            if not upload_result or len(upload_result) == 0:
                raise HTTPException(status_code=500, detail="No file ID returned from upload response")
            
            file_id = upload_result[0].get('id')
            if not file_id:
                raise HTTPException(status_code=500, detail="Invalid file ID in upload response")
            
            logger.info(f"File uploaded successfully with ID: {file_id}")
            
            # Cache the file ID for this text content
            VIRTUAL_TEXT_FILE_CACHE[text_hash] = file_id
            
            # For text files, we don't need to wait for processing - they're immediately available
            # The 405 error suggests the status endpoint doesn't exist for simple text files
            logger.info(f"Virtual text file ready for use: {file_id}")
            return file_id
                    
    except Exception as e:
        logger.error(f"Error creating virtual text file: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail=f"Failed to create virtual text file: {str(e)}")

# --- Enhanced Hybrid Approach Functions ---

# Cache for file contexts to avoid repeated extraction
FILE_CONTEXT_CACHE: Dict[str, Dict[str, Any]] = {}
FILE_CONTEXT_CACHE_TTL = 3600  # 1 hour cache

async def extract_file_context_from_onyx(file_ids: List[int], folder_ids: List[int], cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract relevant context from files and folders using Onyx's capabilities.
    Returns structured context that can be used with OpenAI.
    """
    try:
        # Create cache key
        cache_key = f"{hash(tuple(sorted(file_ids)))}_{hash(tuple(sorted(folder_ids)))}"
        
        # Check cache first
        if cache_key in FILE_CONTEXT_CACHE:
            cached_data = FILE_CONTEXT_CACHE[cache_key]
            if time.time() - cached_data["timestamp"] < FILE_CONTEXT_CACHE_TTL:
                logger.info(f"[FILE_CONTEXT] Using cached context for key: {cache_key[:16]}...")
                return cached_data["context"]
        
        logger.info(f"[FILE_CONTEXT] Extracting context from {len(file_ids)} files and {len(folder_ids)} folders")
        
        extracted_context = {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {
                "total_files": len(file_ids),
                "total_folders": len(folder_ids),
                "extraction_time": time.time()
            }
        }
        
        # Extract file contexts with enhanced retry mechanism
        successful_extractions = 0
        for file_id in file_ids:
            file_context = None
            for retry_attempt in range(3):  # Up to 3 attempts per file
                try:
                    file_context = await extract_single_file_context(file_id, cookies)
                    if file_context and (file_context.get("summary") or file_context.get("content")):
                        # Check if this was a successful extraction (not a generic response or error)
                        content = file_context.get("content", "")
                        if any(phrase in content.lower() for phrase in ["file access issue", "not indexed", "could not access", "file_access_error"]):
                            logger.warning(f"[FILE_CONTEXT] File {file_id} has access issues (attempt {retry_attempt + 1})")
                            if retry_attempt < 2:  # Don't sleep on the last attempt
                                await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                                continue
                        
                        # Success - add to context
                        extracted_context["file_summaries"].append(file_context["summary"])
                        extracted_context["file_contents"].append(file_context["content"])
                        extracted_context["key_topics"].extend(file_context.get("topics", []))
                        successful_extractions += 1
                        logger.info(f"[FILE_CONTEXT] Successfully extracted context from file {file_id} (attempt {retry_attempt + 1})")
                        break  # Success, no need for more retries
                    else:
                        logger.warning(f"[FILE_CONTEXT] No valid context extracted from file {file_id} (attempt {retry_attempt + 1})")
                        if retry_attempt < 2:  # Don't sleep on the last attempt
                            await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                except Exception as e:
                    logger.warning(f"[FILE_CONTEXT] Failed to extract context from file {file_id} (attempt {retry_attempt + 1}): {e}")
                    if retry_attempt < 2:  # Don't sleep on the last attempt
                        await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
            
            if not file_context or not (file_context.get("summary") or file_context.get("content")):
                logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
        
        # Extract folder contexts
        for folder_id in folder_ids:
            try:
                folder_context = await extract_folder_context(folder_id, cookies)
                if folder_context and folder_context.get("summary"):
                    extracted_context["folder_contexts"].append(folder_context)
                    extracted_context["key_topics"].extend(folder_context.get("topics", []))
                    successful_extractions += 1
                    logger.info(f"[FILE_CONTEXT] Successfully extracted context from folder {folder_id}")
                else:
                    logger.warning(f"[FILE_CONTEXT] No valid context extracted from folder {folder_id}")
            except Exception as e:
                logger.warning(f"[FILE_CONTEXT] Failed to extract context from folder {folder_id}: {e}")
        
        # If no context was extracted successfully, provide a fallback
        if successful_extractions == 0:
            logger.warning(f"[FILE_CONTEXT] No context extracted successfully, providing fallback context")
            extracted_context["file_summaries"] = [f"File(s) provided for content creation (IDs: {file_ids + folder_ids})"]
            extracted_context["key_topics"] = ["content creation", "educational materials"]
            extracted_context["metadata"]["fallback_used"] = True
        
        # Remove duplicate topics
        extracted_context["key_topics"] = list(set(extracted_context["key_topics"]))
        
        # Cache the result
        FILE_CONTEXT_CACHE[cache_key] = {
            "context": extracted_context,
            "timestamp": time.time()
        }
        
        logger.info(f"[FILE_CONTEXT] Successfully extracted context: {len(extracted_context['file_summaries'])} file summaries, {len(extracted_context['key_topics'])} key topics")
        
        return extracted_context
        
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting file context: {e}", exc_info=True)
        return {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {"error": str(e)}
        }

async def extract_connector_context_from_onyx(connector_sources: str, prompt: str, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from specific connectors using the Search persona with connector filtering.
    This function performs a comprehensive search within selected connectors only.
    Uses the same approach as Knowledge Base search but with connector source filtering.
    """
    try:
        logger.info(f"[CONNECTOR_CONTEXT] Starting connector search for sources: {connector_sources}")
        
        # Parse connector sources
        connector_list = [source.strip() for source in connector_sources.split(',') if source.strip()]
        logger.info(f"[CONNECTOR_CONTEXT] Parsed connector sources: {connector_list}")
        
        # Create a temporary chat session with the Search persona (ID 0)
        search_persona_id = 0
        temp_chat_id = await create_onyx_chat_session(search_persona_id, cookies)
        logger.info(f"[CONNECTOR_CONTEXT] Created search chat session: {temp_chat_id}")
        
        # Create a comprehensive search prompt (similar to Knowledge Base approach)
        search_prompt = f"""
        Please search within the following connector sources for information relevant to this topic: "{prompt}"
        
        Search only within these specific sources: {', '.join(connector_list)}
        
        I need you to:
        1. Search within the specified connector sources only
        2. Find the most relevant information related to this topic
        3. Provide a comprehensive summary of what you find
        4. Extract key topics, concepts, and important details
        5. Identify any specific examples, case studies, or practical applications
        
        Please format your response as:
        SUMMARY: [comprehensive summary of relevant information found]
        KEY_TOPICS: [comma-separated list of key topics and concepts]
        IMPORTANT_DETAILS: [specific details, examples, or practical information]
        RELEVANT_SOURCES: [mention of any specific documents or sources that were particularly relevant]
        
        Focus only on content from these connector sources: {', '.join(connector_list)}
        Be thorough and comprehensive in your search and analysis.
        """
        
        # Use the Search persona to perform the connector-filtered search
        logger.info(f"[CONNECTOR_CONTEXT] Sending search request to Search persona with connector filters")
        search_result = await enhanced_stream_chat_message_with_filters(temp_chat_id, search_prompt, cookies, connector_list)
        logger.info(f"[CONNECTOR_CONTEXT] Received search result ({len(search_result)} chars)")
        
        # Log the full response for debugging
        logger.info(f"[CONNECTOR_CONTEXT] Full search response: {search_result}")
        
        if len(search_result) == 0:
            logger.warning(f"[CONNECTOR_CONTEXT] Search result is empty! This might indicate no documents in connectors or search failed")
        
        # Parse the search result - handle Onyx response format (same as Knowledge Base)
        summary = ""
        key_topics = []
        important_details = ""
        relevant_sources = ""
        
        # Extract content flexibly using string searching
        logger.info(f"[CONNECTOR_CONTEXT] Starting content extraction from search result")
        
        if "SUMMARY:" in search_result:
            summary_start = search_result.find("SUMMARY:") + 8
            summary_end = search_result.find("KEY_TOPICS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("IMPORTANT_DETAILS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("RELEVANT_SOURCES:", summary_start)
            if summary_end == -1:
                summary_end = len(search_result)
            summary = search_result[summary_start:summary_end].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted summary: {len(summary)} chars")
        
        if "KEY_TOPICS:" in search_result:
            topics_start = search_result.find("KEY_TOPICS:") + 11
            topics_end = search_result.find("IMPORTANT_DETAILS:", topics_start)
            if topics_end == -1:
                topics_end = search_result.find("RELEVANT_SOURCES:", topics_start)
            if topics_end == -1:
                # Look for next section marker or end of text
                next_section = search_result.find("\n\n", topics_start)
                topics_end = next_section if next_section != -1 else len(search_result)
            topics_text = search_result[topics_start:topics_end].strip()
            key_topics = [t.strip() for t in topics_text.split(',') if t.strip()]
            logger.info(f"[CONNECTOR_CONTEXT] Extracted {len(key_topics)} key topics")
        
        if "IMPORTANT_DETAILS:" in search_result:
            details_start = search_result.find("IMPORTANT_DETAILS:") + 18
            details_end = search_result.find("RELEVANT_SOURCES:", details_start)
            if details_end == -1:
                details_end = len(search_result)
            important_details = search_result[details_start:details_end].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted important details: {len(important_details)} chars")
        
        if "RELEVANT_SOURCES:" in search_result:
            sources_start = search_result.find("RELEVANT_SOURCES:") + 17
            relevant_sources = search_result[sources_start:].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted relevant sources: {len(relevant_sources)} chars")
        
        # Final fallback if still no content
        if not summary and not key_topics:
            summary = search_result[:1000] + "..." if len(search_result) > 1000 else search_result
            key_topics = ["connector search"]
            logger.info(f"[CONNECTOR_CONTEXT] Using fallback summary from raw response")
        
        # Log the extracted information
        logger.info(f"[CONNECTOR_CONTEXT] Extracted summary: {summary[:200]}...")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted key topics: {key_topics}")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted important details: {important_details[:200]}...")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted relevant sources: {relevant_sources[:200]}...")
        
        # Return context in the same format as knowledge base context
        return {
            "connector_search": True,
            "topic": prompt,
            "connector_sources": connector_list,
            "summary": summary,
            "key_topics": key_topics,
            "important_details": important_details,
            "relevant_sources": relevant_sources,
            "full_search_result": search_result,
            "file_summaries": [{
                "file_id": "connector_search",
                "name": f"Connector Search: {', '.join(connector_list)}",
                "summary": summary,
                "topics": key_topics,
                "key_info": important_details
            }]
        }
        
    except Exception as e:
        logger.error(f"[CONNECTOR_CONTEXT] Error extracting connector context: {e}", exc_info=True)
        # Return fallback context
        return {
            "connector_search": True,
            "topic": prompt,
            "connector_sources": connector_sources.split(','),
            "summary": f"Connector search failed for sources: {connector_sources}",
            "key_topics": ["search error"],
            "important_details": "Unable to search connectors",
            "relevant_sources": "",
            "full_search_result": "",
            "file_summaries": [{
                "file_id": "connector_search_error",
                "name": f"Connector Search Error: {connector_sources}",
                "summary": "Search failed",
                "topics": ["error"],
                "key_info": str(e)
            }]
        }

def _save_section_content(section_name: str, content_lines: list, local_vars: dict):
    """Helper function to save accumulated section content"""
    content = " ".join(content_lines).strip()
    if section_name == "summary":
        local_vars["summary"] = content
    elif section_name == "important_details":
        local_vars["important_details"] = content
    elif section_name == "relevant_sources":
        local_vars["relevant_sources"] = content

async def enhanced_stream_chat_message(chat_session_id: str, message: str, cookies: Dict[str, str]) -> str:
    """Enhanced version of stream_chat_message specifically for Knowledge Base searches with better streaming handling."""
    logger.info(f"[enhanced_stream_chat_message] Starting Knowledge Base search - chat_id={chat_session_id} len(message)={len(message)}")

    async with httpx.AsyncClient(timeout=600.0) as client:  # Longer timeout for KB searches
        retrieval_options = {
            "run_search": "always",  # Always search for Knowledge Base
            "real_time": False,
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": True,  # Force streaming for better control
        }
        
        logger.info(f"[enhanced_stream_chat_message] Sending request to {ONYX_API_SERVER_URL}/chat/send-message")
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/send-message",
            json=payload,
            cookies=cookies,
        )
        
        logger.info(f"[enhanced_stream_chat_message] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        
        # Handle the response
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[enhanced_stream_chat_message] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            last_log_length = 0
            import time
            start_time = time.time()
            last_activity_time = start_time
            max_idle_time = 120.0  # Wait up to 2 minutes without new content
            max_total_time = 600.0  # Maximum 10 minutes total
            
            logger.info(f"[enhanced_stream_chat_message] Starting to read lines from stream...")
            async for line in resp.aiter_lines():
                line_count += 1
                current_time = time.time()
                elapsed_time = current_time - start_time
                idle_time = current_time - last_activity_time
                
                # Log progress every 25 lines to track what's happening
                if line_count % 25 == 0:
                    logger.info(f"[enhanced_stream_chat_message] Progress: Line {line_count}, Elapsed: {elapsed_time:.1f}s, Idle: {idle_time:.1f}s, Chars: {len(full_answer)}")
                
                # Check for timeouts - but be more patient
                if elapsed_time > max_total_time:
                    logger.warning(f"[enhanced_stream_chat_message] Maximum total time ({max_total_time}s) exceeded after {line_count} lines, {len(full_answer)} chars")
                    break
                    
                # Only timeout on idle if we have NO content after significant time
                if idle_time > max_idle_time and len(full_answer) == 0 and elapsed_time > 60.0:
                    logger.warning(f"[enhanced_stream_chat_message] Maximum idle time ({max_idle_time}s) exceeded since last content, still no answer content after {line_count} lines, elapsed: {elapsed_time:.1f}s")
                    break
                
                if not line:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count}: Empty line")
                    continue
                    
                # Onyx doesn't use "data: " prefix - each line is a direct JSON object  
                # Skip empty lines but process all non-empty lines as JSON
                payload_text = line.strip()
                if not payload_text:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count}: Empty line")
                    continue
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[enhanced_stream_chat_message] Failed to parse JSON line {line_count}: {str(e)} | Line: {payload_text[:100]}")
                    continue
                
                # For the first 10 packets, log full content to understand structure
                if line_count <= 10:
                    packet_str = str(packet)[:500] if packet else "empty"
                    logger.info(f"[enhanced_stream_chat_message] Packet {line_count} content: {packet_str}")
                
                # Log packet structure for debugging (every 50 lines to avoid spam)
                if line_count % 50 == 0:
                    packet_keys = list(packet.keys()) if isinstance(packet, dict) else "not-dict"
                    logger.info(f"[enhanced_stream_chat_message] Line {line_count} packet keys: {packet_keys}")
                
                # Handle different Onyx packet types
                answer_content = None
                
                # Check for OnyxAnswerPiece
                if "answer_piece" in packet:
                    answer_piece = packet["answer_piece"]
                    if answer_piece is None:
                        # OnyxAnswerPiece with None signals end of answer
                        logger.info(f"[enhanced_stream_chat_message] Received answer termination signal (answer_piece=None) after {line_count} lines")
                        done_received = True
                        break
                    elif answer_piece:
                        answer_content = answer_piece
                        
                # Check for AgentAnswerPiece (agent search responses)
                elif packet.get("answer_type") and packet.get("answer_piece"):
                    answer_content = packet["answer_piece"]
                    logger.info(f"[enhanced_stream_chat_message] Received agent answer piece: {packet.get('answer_type')}")
                    
                # Check for QADocsResponse (search results)
                elif packet.get("top_documents") or packet.get("rephrased_query"):
                    logger.info(f"[enhanced_stream_chat_message] Received search results packet")
                    last_activity_time = current_time  # Reset timer for search activity
                    
                # Check for StreamStopInfo
                elif packet.get("stop_reason"):
                    if packet["stop_reason"] == "finished":
                        logger.info(f"[enhanced_stream_chat_message] Received stream stop signal: finished")
                        done_received = True
                        break
                    
                if answer_content:
                    full_answer += answer_content
                    last_activity_time = current_time  # Reset activity timer on content
                    
                    # Log progress every 200 chars to track streaming
                    if len(full_answer) - last_log_length >= 200:
                        logger.info(f"[enhanced_stream_chat_message] Accumulated {len(full_answer)} chars so far...")
                        last_log_length = len(full_answer)
                else:
                    # Log what we're getting for non-answer packets
                    if line_count <= 10 or line_count % 100 == 0:  # Log first 10 and every 100th
                        packet_preview = str(packet)[:200] if packet else "empty"
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count} - non-answer packet: {packet_preview}")
            
            # Stream ended - determine why
            logger.info(f"[enhanced_stream_chat_message] Stream reading loop ended naturally")
            final_elapsed = time.time() - start_time
            logger.info(f"[enhanced_stream_chat_message] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}, Elapsed: {final_elapsed:.1f}s")
            
            # If we got no content and stream ended quickly, something went wrong
            if len(full_answer) == 0 and final_elapsed < 60.0 and not done_received:
                logger.error(f"[enhanced_stream_chat_message] Stream ended prematurely! Only {final_elapsed:.1f}s elapsed, {line_count} lines processed, no content received")
                logger.error(f"[enhanced_stream_chat_message] This suggests an issue with the Onyx search or streaming connection")
                
            if not done_received and len(full_answer) == 0:
                logger.warning(f"[enhanced_stream_chat_message] Stream ended without [DONE] signal and no content - may be incomplete")
            elif not done_received:
                logger.warning(f"[enhanced_stream_chat_message] Stream ended without [DONE] signal but got {len(full_answer)} chars")
                
            # Ensure we have some minimum content or waited minimum time
            if len(full_answer) == 0 and final_elapsed < 60.0:
                logger.warning(f"[enhanced_stream_chat_message] No content received and insufficient wait time ({final_elapsed:.1f}s < 60s)")
                # Wait a bit more to see if content comes
                logger.info(f"[enhanced_stream_chat_message] Attempting extended wait for delayed response...")
                import asyncio
                await asyncio.sleep(5.0)  # Wait 5 more seconds
                
            return full_answer
        else:
            # Non-streaming response
            logger.info(f"[enhanced_stream_chat_message] Processing non-streaming response")
            try:
                data = resp.json()
                result = data.get("answer") or data.get("answer_citationless") or ""
                logger.info(f"[enhanced_stream_chat_message] Non-streaming result: {len(result)} chars")
                return result
            except Exception as e:
                logger.error(f"[enhanced_stream_chat_message] Failed to parse non-streaming response: {e}")
                return resp.text.strip()

async def enhanced_stream_chat_message_with_filters(chat_session_id: str, message: str, cookies: Dict[str, str], connector_sources: list) -> str:
    """Enhanced version of stream_chat_message for connector searches with source filtering."""
    logger.info(f"[enhanced_stream_chat_message_with_filters] Starting connector search - chat_id={chat_session_id} sources={connector_sources} len(message)={len(message)}")

    async with httpx.AsyncClient(timeout=600.0) as client:  # Longer timeout for searches
        retrieval_options = {
            "run_search": "always",  # Always search for connectors
            "real_time": False,
            "filters": {
                "connectorSources": connector_sources  # Filter by specific connector sources
            }
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": True,  # Force streaming for better control
        }
        
        logger.info(f"[enhanced_stream_chat_message_with_filters] Sending request to {ONYX_API_SERVER_URL}/chat/send-message with connector filters: {connector_sources}")
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/send-message",
            json=payload,
            cookies=cookies,
        )
        
        logger.info(f"[enhanced_stream_chat_message_with_filters] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        
        # Handle the response (EXACT same logic as enhanced_stream_chat_message for Knowledge Base)
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[enhanced_stream_chat_message_with_filters] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            last_log_length = 0
            import time
            start_time = time.time()
            last_activity_time = start_time
            max_idle_time = 120.0  # Wait up to 2 minutes without new content
            max_total_time = 600.0  # Maximum 10 minutes total
            
            logger.info(f"[enhanced_stream_chat_message_with_filters] Starting to read lines from stream...")
            async for line in resp.aiter_lines():
                line_count += 1
                current_time = time.time()
                elapsed_time = current_time - start_time
                idle_time = current_time - last_activity_time
                
                # Log progress every 25 lines to track what's happening
                if line_count % 25 == 0:
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Progress: Line {line_count}, Elapsed: {elapsed_time:.1f}s, Idle: {idle_time:.1f}s, Chars: {len(full_answer)}")
                
                # Check for timeouts - but be more patient
                if elapsed_time > max_total_time:
                    logger.warning(f"[enhanced_stream_chat_message_with_filters] Maximum total time ({max_total_time}s) exceeded after {line_count} lines, {len(full_answer)} chars")
                    break
                    
                # Only timeout on idle if we have NO content after significant time
                if idle_time > max_idle_time and len(full_answer) == 0 and elapsed_time > 60.0:
                    logger.warning(f"[enhanced_stream_chat_message_with_filters] Maximum idle time ({max_idle_time}s) exceeded since last content, still no answer content after {line_count} lines, elapsed: {elapsed_time:.1f}s")
                    break

                if not line:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count}: Empty line")
                    continue
                    
                # Onyx doesn't use "data: " prefix - each line is a direct JSON object  
                # Skip empty lines but process all non-empty lines as JSON
                payload_text = line.strip()
                if not payload_text:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count}: Empty line")
                    continue
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[enhanced_stream_chat_message_with_filters] Failed to parse JSON line {line_count}: {str(e)} | Line: {payload_text[:100]}")
                    continue

                # For the first 10 packets, log full content to understand structure
                if line_count <= 10:
                    packet_str = str(packet)[:500] if packet else "empty"
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Packet {line_count} content: {packet_str}")

                # Log packet structure for debugging (every 50 lines to avoid spam)
                if line_count % 50 == 0:
                    packet_keys = list(packet.keys()) if isinstance(packet, dict) else "not-dict"
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Line {line_count} packet keys: {packet_keys}")

                # Handle different Onyx packet types (EXACT same as Knowledge Base)
                answer_content = None
                
                # Check for OnyxAnswerPiece
                if "answer_piece" in packet:
                    answer_piece = packet["answer_piece"]
                    if answer_piece is None:
                        # OnyxAnswerPiece with None signals end of answer
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Received answer termination signal (answer_piece=None) after {line_count} lines")
                        done_received = True
                        break
                    elif answer_piece:
                        answer_content = answer_piece
                        
                # Check for AgentAnswerPiece (agent search responses)
                elif packet.get("answer_type") and packet.get("answer_piece"):
                    answer_content = packet["answer_piece"]
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Received agent answer piece: {packet.get('answer_type')}")
                    
                # Check for QADocsResponse (search results)
                elif packet.get("top_documents") or packet.get("rephrased_query"):
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Received search results packet")
                    last_activity_time = current_time  # Reset timer for search activity
                    
                # Check for StreamStopInfo
                elif packet.get("stop_reason"):
                    if packet["stop_reason"] == "finished":
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Received stream stop signal: finished")
                        done_received = True
                        break
                    
                if answer_content:
                    full_answer += answer_content
                    last_activity_time = current_time  # Reset activity timer on content
                    
                    # Log progress every 200 chars to track streaming
                    if len(full_answer) - last_log_length >= 200:
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Accumulated {len(full_answer)} chars so far...")
                        last_log_length = len(full_answer)
                else:
                    # Log what we're getting for non-answer packets
                    if line_count <= 10 or line_count % 100 == 0:  # Log first 10 and every 100th
                        packet_preview = str(packet)[:200] if packet else "empty"
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count} - non-answer packet: {packet_preview}")
            
            # Stream ended - determine why
            logger.info(f"[enhanced_stream_chat_message_with_filters] Stream reading loop ended naturally")
            final_elapsed = time.time() - start_time
            logger.info(f"[enhanced_stream_chat_message_with_filters] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}, Elapsed: {final_elapsed:.1f}s")
            
            # Log full raw response for debugging
            logger.info(f"[enhanced_stream_chat_message_with_filters] Full raw response: {full_answer}")
            
            # If we got no content and stream ended quickly, something went wrong
            if len(full_answer) == 0 and final_elapsed < 60.0 and not done_received:
                logger.error(f"[enhanced_stream_chat_message_with_filters] Stream ended prematurely! Only {final_elapsed:.1f}s elapsed, {line_count} lines processed, no content received")
                logger.error(f"[enhanced_stream_chat_message_with_filters] This suggests an issue with the Onyx search or streaming connection")
            
            return full_answer.strip()
            
        else:
            # Non-streaming response
            logger.info(f"[enhanced_stream_chat_message_with_filters] Processing non-streaming response")
            try:
                data = resp.json()
                result = data.get("answer") or data.get("answer_citationless") or ""
                logger.info(f"[enhanced_stream_chat_message_with_filters] Non-streaming result: {len(result)} chars")
                return result
            except Exception as e:
                logger.error(f"[enhanced_stream_chat_message_with_filters] Failed to parse non-streaming response: {e}")
                return resp.text.strip()

async def extract_knowledge_base_context(topic: str, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from the entire Knowledge Base using the Search persona.
    This function performs a comprehensive search across all documents in the Knowledge Base.
    """
    try:
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Starting Knowledge Base search for topic: {topic}")
        
        # Create a temporary chat session with the Search persona (ID 0)
        search_persona_id = 0
        temp_chat_id = await create_onyx_chat_session(search_persona_id, cookies)
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Created search chat session: {temp_chat_id}")
        
        # Create a comprehensive search prompt
        search_prompt = f"""
        Please search your entire Knowledge Base for information relevant to this topic: "{topic}"
        
        I need you to:
        1. Search across all available documents and knowledge sources
        2. Find the most relevant information related to this topic
        3. Provide a comprehensive summary of what you find
        4. Extract key topics, concepts, and important details
        5. Identify any specific examples, case studies, or practical applications
        
        Please format your response as:
        SUMMARY: [comprehensive summary of relevant information found]
        KEY_TOPICS: [comma-separated list of key topics and concepts]
        IMPORTANT_DETAILS: [specific details, examples, or practical information]
        RELEVANT_SOURCES: [mention of any specific documents or sources that were particularly relevant]
        
        Be thorough and comprehensive in your search and analysis.
        """
        
        # Use the Search persona to perform the Knowledge Base search
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Sending search request to Search persona")
        search_result = await enhanced_stream_chat_message(temp_chat_id, search_prompt, cookies)
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Received search result ({len(search_result)} chars)")
        
        # Log the full response for debugging
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Full search response: {search_result}")
        
        if len(search_result) == 0:
            logger.warning(f"[KNOWLEDGE_BASE_CONTEXT] Search result is empty! This might indicate no documents in Knowledge Base or search failed")
        
        # Parse the search result - handle Onyx response format  
        summary = ""
        key_topics = []
        important_details = ""
        relevant_sources = ""
        
        # Extract content flexibly using string searching
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Starting content extraction from search result")
        
        if "SUMMARY:" in search_result:
            summary_start = search_result.find("SUMMARY:") + 8
            summary_end = search_result.find("KEY_TOPICS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("IMPORTANT_DETAILS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("RELEVANT_SOURCES:", summary_start)
            if summary_end == -1:
                summary_end = len(search_result)
            summary = search_result[summary_start:summary_end].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted summary: {len(summary)} chars")
        
        if "KEY_TOPICS:" in search_result:
            topics_start = search_result.find("KEY_TOPICS:") + 11
            topics_end = search_result.find("IMPORTANT_DETAILS:", topics_start)
            if topics_end == -1:
                topics_end = search_result.find("RELEVANT_SOURCES:", topics_start)
            if topics_end == -1:
                # Look for next section marker or end of text
                next_section = search_result.find("\n\n", topics_start)
                topics_end = next_section if next_section != -1 else len(search_result)
            topics_text = search_result[topics_start:topics_end].strip()
            key_topics = [t.strip() for t in topics_text.split(',') if t.strip()]
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted {len(key_topics)} key topics")
        
        if "IMPORTANT_DETAILS:" in search_result:
            details_start = search_result.find("IMPORTANT_DETAILS:") + 18
            details_end = search_result.find("RELEVANT_SOURCES:", details_start)
            if details_end == -1:
                details_end = len(search_result)
            important_details = search_result[details_start:details_end].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted important details: {len(important_details)} chars")
        
        if "RELEVANT_SOURCES:" in search_result:
            sources_start = search_result.find("RELEVANT_SOURCES:") + 17
            relevant_sources = search_result[sources_start:].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted relevant sources: {len(relevant_sources)} chars")
        
        # Final fallback if still no content
        if not summary and not key_topics:
            summary = search_result[:1000] + "..." if len(search_result) > 1000 else search_result
            key_topics = ["knowledge base search"]
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Using fallback summary from raw response")
        
        # Log the extracted information
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted summary: {summary[:200]}...")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted key topics: {key_topics}")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted important details: {important_details[:200]}...")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted relevant sources: {relevant_sources[:200]}...")
        
        # Return context in the same format as file context
        return {
            "knowledge_base_search": True,
            "topic": topic,
            "summary": summary,
            "key_topics": key_topics,
            "important_details": important_details,
            "relevant_sources": relevant_sources,
            "full_search_result": search_result,
            "file_summaries": [{
                "file_id": "knowledge_base",
                "name": f"Knowledge Base Search: {topic}",
                "summary": summary,
                "topics": key_topics,
                "key_info": important_details
            }]
        }
        
    except Exception as e:
        logger.error(f"[KNOWLEDGE_BASE_CONTEXT] Error extracting Knowledge Base context: {e}", exc_info=True)
        # Return fallback context
        return {
            "knowledge_base_search": True,
            "topic": topic,
            "summary": f"Knowledge Base search failed for topic: {topic}",
            "key_topics": ["search error"],
            "important_details": "Unable to search Knowledge Base",
            "relevant_sources": "",
            "full_search_result": f"Error: {str(e)}",
            "file_summaries": []
        }

async def extract_single_file_context(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a single file using Onyx's chat API with 100% file attachment guarantee.
    """
    try:
        # Step 1: Verify file exists and is accessible
        file_info = await verify_file_accessibility(file_id, cookies)
        if not file_info:
            return {
                "file_id": file_id,
                "summary": f"File {file_id} is not accessible or does not exist",
                "topics": ["file access error"],
                "key_info": "File may need to be re-uploaded",
                "content": f"File {file_id} access verification failed"
            }
        
        # Step 2: Create a temporary chat session with forced file attachment
        persona_id = await get_contentbuilder_persona_id(cookies)
        temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
        
        # Step 3: Flexible analysis prompt that works with both text files and images
        analysis_prompt = f"""
        I have attached a file (ID: {file_id}) to this message. Please help me understand what this file contains.
        
        For images: Tell me what you see in this image, what it shows, and what it might be about.
        For documents: Provide a summary of the main content and key topics.
        For any file type: Focus on information that would be useful for creating educational content.
        
        Please describe:
        1. What is this file? (image, document, etc.)
        2. What does it contain or show? (max 200 words)
        3. What are the main topics, concepts, or subjects?
        4. What information would be most relevant for lesson planning or content creation?
        
        If you can see/access the file, please proceed with the description.
        If you cannot access it, simply say "FILE_ACCESS_ERROR".
        
        Format your response as:
        SUMMARY: [what this file contains/shows]
        TOPICS: [main topics or subjects, comma-separated]  
        KEY_INFO: [most educational/relevant information]
        """
        
        # Step 4: Multiple retry attempts with different strategies
        for attempt in range(3):
            try:
                result = await attempt_file_analysis_with_retry(
                    temp_chat_id, file_id, analysis_prompt, cookies, attempt
                )
                if result and not is_generic_response(result):
                    return parse_analysis_result(file_id, result)
                elif attempt < 2:
                    logger.warning(f"[FILE_CONTEXT] Attempt {attempt + 1} failed for file {file_id}, retrying...")
                    await asyncio.sleep(1)  # Brief delay before retry
                else:
                    logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
                    break
            except Exception as e:
                logger.error(f"[FILE_CONTEXT] Attempt {attempt + 1} error for file {file_id}: {e}")
                if attempt < 2:
                    await asyncio.sleep(1)
                else:
                    raise
        
        # Step 5: Fallback response if all attempts fail
        return {
            "file_id": file_id,
            "summary": f"File analysis failed after multiple attempts (ID: {file_id})",
            "topics": ["analysis error", "file processing"],
            "key_info": "File may need manual review or re-upload",
            "content": f"Analysis failed for file {file_id} ({file_info.get('name', 'Unknown')})"
        }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting single file context for file {file_id}: {e}")
        return {
            "file_id": file_id,
            "summary": f"Error processing file {file_id}: {str(e)}",
            "topics": ["processing error"],
            "key_info": "File processing encountered an error",
            "content": f"Error: {str(e)}"
        }

async def verify_file_accessibility(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Verify that a file exists and is accessible before attempting analysis.
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Check file indexing status using the correct endpoint
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/file/indexing-status?file_ids={file_id}", 
                cookies=cookies
            )
            response.raise_for_status()
            status_data = response.json()
            
            is_indexed = status_data.get(str(file_id), False)
            file_status = "INDEXED" if is_indexed else "NOT_INDEXED"
            
            logger.info(f"[FILE_CONTEXT] File {file_id} indexing status: {file_status}")
            
            # For now, assume the file is accessible if we can check its status
            # The actual file content will be verified during the analysis phase
            return {
                "id": file_id,
                "name": f"file_{file_id}",  # We'll get the real name during analysis
                "status": file_status,
                "accessible": True  # Assume accessible, let the analysis phase handle actual access
            }
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] File accessibility check failed for {file_id}: {e}")
        # Return a basic info structure even if check fails
        return {
            "id": file_id,
            "name": f"file_{file_id}",
            "status": "UNKNOWN",
            "accessible": True  # Let the analysis phase determine actual accessibility
        }

async def attempt_file_analysis_with_retry(
    chat_id: str, 
    file_id: int, 
    prompt: str, 
    cookies: Dict[str, str], 
    attempt: int
) -> str:
    """
    Attempt file analysis with different strategies based on attempt number.
    """
    # Different strategies for each attempt
    strategies = [
        # Attempt 1: Standard approach with user_file_ids
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "never", "real_time": False},
            "force_direct_attachment": True
        },
        # Attempt 2: Force search tool with file-specific query
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Analyze the content of file ID {file_id}"
        },
        # Attempt 3: Use file_descriptors as fallback with search
        {
            "file_descriptors": [{"id": str(file_id), "type": "USER_KNOWLEDGE", "name": f"file_{file_id}"}],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Find and analyze the content of file {file_id}"
        }
    ]
    
    strategy = strategies[attempt]
    
    async with httpx.AsyncClient(timeout=180.0) as client:
        payload = {
            "chat_session_id": chat_id,
            "message": prompt,
            "parent_message_id": None,
            "file_descriptors": strategy.get("file_descriptors", []),
            "user_file_ids": strategy.get("user_file_ids", []),
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": strategy["retrieval_options"],
            "stream_response": True,
            "query_override": strategy.get("query_override")
        }
        
        logger.info(f"[FILE_CONTEXT] Attempt {attempt + 1} for file {file_id} with strategy: {list(strategy.keys())}")
        
        try:
            # Try simple API first
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                json=payload,
                cookies=cookies
            )
            response.raise_for_status()
            result = response.json()
            return result.get("answer", "")
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                # Fallback to streaming endpoint
                return await stream_file_analysis(client, payload, cookies, file_id)
            else:
                raise

async def stream_file_analysis(
    client: httpx.AsyncClient, 
    payload: Dict[str, Any], 
    cookies: Dict[str, str], 
    file_id: int
) -> str:
    """
    Stream file analysis response with enhanced error handling.
    """
    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
        resp.raise_for_status()
        analysis_text = ""
        line_count = 0
        file_mentioned = False
        
        async for raw_line in resp.aiter_lines():
            line_count += 1
            if not raw_line:
                continue
                
            line = raw_line.strip()
            if line.startswith("data:"):
                line = line.split("data:", 1)[1].strip()
                
            if line == "[DONE]":
                logger.info(f"[FILE_CONTEXT] Stream completed for file {file_id} after {line_count} lines")
                break
                
            try:
                pkt = json.loads(line)
                if "answer_piece" in pkt:
                    piece = pkt["answer_piece"].replace("\\n", "\n")
                    analysis_text += piece
                    
                    # Check if file is mentioned in the response
                    if str(file_id) in piece or "file" in piece.lower():
                        file_mentioned = True
                        
            except json.JSONDecodeError:
                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                continue
        
        logger.info(f"[FILE_CONTEXT] Stream processing completed for file {file_id}, "
                   f"total text length: {len(analysis_text)}, file mentioned: {file_mentioned}")
        
        return analysis_text

def is_generic_response(text: str) -> bool:
    """
    Check if the AI response is generic (indicating file access issues).
    """
    # Updated to be less strict for image descriptions and more specific
    generic_phrases = [
        "could you please share the file",
        "please share the file", 
        "paste its content",
        "upload the file",
        "provide the file",
        "share the document", 
        "i don't see any file attached",
        "no file was provided to me",
        "file_access_error",
        "please provide the content",
        "try a different file",
        "proceed based on a general topic",
        "using my knowledge"
    ]
    
    # Additional check: if response is very short and contains access issue, it's likely generic
    # But allow longer responses that might contain some useful info even if they mention access issues
    text_lower = text.lower()
    if len(text) < 150 and any(phrase in text_lower for phrase in [
        "cannot access", "unable to access", "don't have access", 
        "wasn't able to access", "access the file"
    ]):
        return True
    
    text_lower = text.lower()
    return any(phrase in text_lower for phrase in generic_phrases)

def parse_analysis_result(file_id: int, analysis_text: str) -> Dict[str, Any]:
    """
    Parse the analysis result and extract structured information.
    """
    summary = ""
    topics = []
    key_info = ""
    
    # Log the raw response for debugging
    logger.info(f"[FILE_CONTEXT] Raw analysis response for file {file_id} (length: {len(analysis_text)}): "
               f"{analysis_text[:500]}{'...' if len(analysis_text) > 500 else ''}")
    
    lines = analysis_text.split('\n')
    for line in lines:
        if line.startswith("SUMMARY:"):
            summary = line.replace("SUMMARY:", "").strip()
        elif line.startswith("TOPICS:"):
            topics_text = line.replace("TOPICS:", "").strip()
            topics = [t.strip() for t in topics_text.split(',') if t.strip()]
        elif line.startswith("KEY_INFO:"):
            key_info = line.replace("KEY_INFO:", "").strip()
    
    # If no structured response, try to extract meaningful content
    if not summary and analysis_text.strip():
        # Take first 200 characters as summary if no structured response
        summary = analysis_text.strip()[:200]
        if len(analysis_text) > 200:
            summary += "..."
        logger.info(f"[FILE_CONTEXT] No structured SUMMARY found, using first 200 chars as summary for file {file_id}")
    
    # If still no summary, use a fallback
    if not summary:
        summary = f"File content analyzed successfully (ID: {file_id})"
        logger.warning(f"[FILE_CONTEXT] No summary could be extracted for file {file_id}, using fallback")
    
    return {
        "file_id": file_id,
        "summary": summary,
        "topics": topics,
        "key_info": key_info,
        "content": analysis_text
    }

async def extract_folder_context(folder_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a folder by analyzing its files.
    """
    try:
        # Get folder files
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files like 200-page PDFs
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/folder/{folder_id}",
                cookies=cookies
            )
            response.raise_for_status()
            
            folder_data = response.json()
            files = folder_data.get("files", [])
            
            if not files:
                return {"folder_id": folder_id, "summary": "Empty folder", "topics": []}
            
            # Create a temporary chat session to analyze folder content
            persona_id = await get_contentbuilder_persona_id(cookies)
            temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
            
            # Analyze folder content
            analysis_prompt = f"""
            This folder contains {len(files)} files. Please analyze the overall theme and provide:
            1. A summary of what this folder is about (max 150 words)
            2. Key topics that are covered across all files
            3. The main purpose or theme of this collection
            
            Format your response as:
            SUMMARY: [summary here]
            TOPICS: [comma-separated topics]
            THEME: [main theme or purpose]
            """
            
            file_ids = [f["id"] for f in files if f.get("status") == "INDEXED"]
            
            if not file_ids:
                return {"folder_id": folder_id, "summary": "No indexed files in folder", "topics": []}
            
            payload = {
                "chat_session_id": temp_chat_id,
                "message": analysis_prompt,
                "parent_message_id": None,
                "file_descriptors": [],
                "user_file_ids": file_ids,
                "user_folder_ids": [],
                "prompt_id": None,
                "search_doc_ids": None,
                "retrieval_options": {"run_search": "never", "real_time": False},
                "stream_response": True,
            }
            
            # Try the simple API first, fallback to regular streaming endpoint
            try:
                response = await client.post(
                    f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                    json=payload,
                    cookies=cookies
                )
                response.raise_for_status()
                result = response.json()
                analysis_text = result.get("answer", "")
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404:
                    logger.info(f"[FILE_CONTEXT] Simple API not available, using streaming endpoint for folder {folder_id}")
                    # Fallback to streaming endpoint
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
                        resp.raise_for_status()
                        analysis_text = ""
                        line_count = 0
                        async for raw_line in resp.aiter_lines():
                            line_count += 1
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info(f"[FILE_CONTEXT] Stream completed for folder {folder_id} after {line_count} lines")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    analysis_text += pkt["answer_piece"].replace("\\n", "\n")
                            except json.JSONDecodeError:
                                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                                continue
                        logger.info(f"[FILE_CONTEXT] Stream processing completed for folder {folder_id}, total text length: {len(analysis_text)}")
                else:
                    raise
            
            # Parse the analysis
            summary = ""
            topics = []
            theme = ""
            
            lines = analysis_text.split('\n')
            for line in lines:
                if line.startswith("SUMMARY:"):
                    summary = line.replace("SUMMARY:", "").strip()
                elif line.startswith("TOPICS:"):
                    topics_text = line.replace("TOPICS:", "").strip()
                    topics = [t.strip() for t in topics_text.split(',') if t.strip()]
                elif line.startswith("THEME:"):
                    theme = line.replace("THEME:", "").strip()
            
            return {
                "folder_id": folder_id,
                "folder_name": folder_data.get("name", ""),
                "summary": summary,
                "topics": topics,
                "theme": theme,
                "file_count": len(files)
            }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting folder context for folder {folder_id}: {e}")
        return None

def build_enhanced_prompt_with_context(original_prompt: str, file_context: Union[Dict[str, Any], str], product_type: str) -> str:
    """
    Build an enhanced prompt that includes the extracted file context for OpenAI.
    Handles both dict (structured context) and str (fallback context) cases.
    """
    enhanced_prompt = f"""
{original_prompt}

--- CONTEXT FROM UPLOADED FILES ---

"""
    
    # Handle string file_context (fallback case)
    if isinstance(file_context, str):
        enhanced_prompt += file_context
        enhanced_prompt += "\n\nPlease create the content based on the information above.\n"
        return enhanced_prompt
    
    # Handle dict file_context (normal case)
    # Handle string file_context (fallback case)
    if isinstance(file_context, str):
        enhanced_prompt += file_context
        enhanced_prompt += "\n\nPlease create the content based on the information above.\n"
        return enhanced_prompt
    
    # Check if fallback was used (dict case)
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += "NOTE: File context extraction was limited, but files were provided for content creation.\n\n"
    
    # Add file summaries
    if file_context.get("file_summaries"):
        enhanced_prompt += "FILE SUMMARIES:\n"
        for i, summary in enumerate(file_context["file_summaries"], 1):
            enhanced_prompt += f"{i}. {summary}\n"
        enhanced_prompt += "\n"
    
    # Add folder contexts
    if file_context.get("folder_contexts"):
        enhanced_prompt += "FOLDER CONTEXTS:\n"
        for folder_ctx in file_context["folder_contexts"]:
            enhanced_prompt += f"• {folder_ctx.get('folder_name', 'Unknown')}: {folder_ctx.get('summary', '')}\n"
        enhanced_prompt += "\n"
    
    # Add key topics
    if file_context.get("key_topics"):
        enhanced_prompt += f"KEY TOPICS COVERED: {', '.join(file_context['key_topics'])}\n\n"
    
    # Add specific instructions for the product type with enhanced formatting guidance
    if product_type == "Course Outline":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR COURSE OUTLINE:
1. Use exactly this structure: ## Module [Number]: [Module Title]
2. Each module must be a separate H2 header starting with ##
3. Lessons must be numbered list items (1. 2. 3.) under each module

ENSURE: Create the requested number of modules, not a single module with all lessons.
"""
    elif product_type == "Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Lesson Presentation** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly.
"""
    elif product_type == "Video Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]**
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly for proper video lesson processing.
"""
    
    # Add specific instructions for the product type
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += f"""
IMPORTANT: Files were provided for content creation. Create a {product_type} that is relevant to the uploaded materials.
If specific content details are not available, focus on creating high-quality educational content that would be appropriate for the file types provided.
"""
    else:
        enhanced_prompt += f"""
IMPORTANT: Use the context from the uploaded files to create a {product_type} that is relevant and accurate to the source materials. 
Ensure that the content aligns with the topics and information provided in the file summaries and folder contexts.
"""
    
    return enhanced_prompt

async def stream_hybrid_response(prompt: str, file_context: Union[Dict[str, Any], str], product_type: str, model: str = None):
    """
    Stream response using OpenAI with enhanced context from Onyx file extraction.
    """
    try:
        # Build enhanced prompt with file context
        enhanced_prompt = build_enhanced_prompt_with_context(prompt, file_context, product_type)
        
        logger.info(f"[HYBRID_STREAM] Starting hybrid streaming with enhanced context")
        logger.info(f"[HYBRID_STREAM] Original prompt length: {len(prompt)} chars")
        logger.info(f"[HYBRID_STREAM] Enhanced prompt length: {len(enhanced_prompt)} chars")
        logger.info(f"[HYBRID_STREAM] File context: {len(file_context.get('file_summaries', []))} summaries, {len(file_context.get('key_topics', []))} topics")
        
        # Use OpenAI with enhanced prompt
        async for chunk_data in stream_openai_response(enhanced_prompt, model):
            yield chunk_data
            
    except Exception as e:
        logger.error(f"[HYBRID_STREAM] Error in hybrid streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"Hybrid streaming error: {str(e)}"}

@app.get("/api/custom/microproduct_types", response_model=List[str])
async def get_allowed_microproduct_types_list_for_design_templates():
    return ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS

# --- Project and MicroProduct Endpoints ---
@app.post("/api/custom/projects/add", response_model=ProjectDB, status_code=status.HTTP_201_CREATED)
def build_source_context(payload) -> tuple[Optional[str], Optional[dict]]:
    """
    Build source context type and data from a finalize payload.
    Returns (context_type, context_data) tuple.
    """
    context_type = None
    context_data = {}
    
    # Check for connector context
    if hasattr(payload, 'fromConnectors') and payload.fromConnectors:
        context_type = 'connectors'
        context_data = {
            'connector_ids': payload.connectorIds.split(',') if payload.connectorIds else [],
            'connector_sources': payload.connectorSources.split(',') if payload.connectorSources else []
        }
    # Check for Knowledge Base context
    elif hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase:
        context_type = 'knowledge_base'
        context_data = {'search_query': payload.prompt if hasattr(payload, 'prompt') else None}
    # Check for file context
    elif hasattr(payload, 'fromFiles') and payload.fromFiles:
        context_type = 'files'
        context_data = {
            'folder_ids': payload.folderIds.split(',') if payload.folderIds else [],
            'file_ids': payload.fileIds.split(',') if payload.fileIds else []
        }
    # Check for text context
    elif hasattr(payload, 'fromText') and payload.fromText:
        context_type = 'text'
        context_data = {
            'text_mode': payload.textMode if hasattr(payload, 'textMode') else None,
            'user_text': payload.userText if hasattr(payload, 'userText') and payload.userText else None,
            'user_text_length': len(payload.userText) if hasattr(payload, 'userText') and payload.userText else 0
        }
    # Default to prompt-based
    else:
        context_type = 'prompt'
        context_data = {
            'prompt': payload.prompt if hasattr(payload, 'prompt') else None,
            'prompt_length': len(payload.prompt) if hasattr(payload, 'prompt') and payload.prompt else 0
        }
    
    return context_type, context_data

async def add_project_to_custom_db(project_data: ProjectCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # ---- Guard against duplicate concurrent submissions (same user+project name) ----
    lock_key = f"{onyx_user_id}:{project_data.projectName.strip().lower()}"
    if lock_key in ACTIVE_PROJECT_CREATE_KEYS:
        raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="Project creation already in progress.")
    
    ACTIVE_PROJECT_CREATE_KEYS.add(lock_key)
    
    # Auto-cleanup lock after maximum processing time to prevent deadlocks
    async def cleanup_lock_after_timeout():
        await asyncio.sleep(300)  # 5 minutes max processing time
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)
        logger.warning(f"Auto-cleaned stuck project creation lock: {lock_key}")
    
    # Start cleanup task in background
    asyncio.create_task(cleanup_lock_after_timeout())
    try:
        selected_design_template: Optional[DesignTemplateResponse] = None
        async with pool.acquire() as conn:
            design_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", project_data.design_template_id)
            if not design_row:
                detail_msg = "Design template not found." if IS_PRODUCTION else f"Design Template with ID {project_data.design_template_id} not found."
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=detail_msg)
            selected_design_template = DesignTemplateResponse(**dict(design_row))

        db_microproduct_name_to_store = project_data.microProductName if project_data.microProductName and project_data.microProductName.strip() else selected_design_template.template_name

        target_content_model: Type[BaseModel]
        default_error_instance: BaseModel
        llm_json_example: str
        component_specific_instructions: str

        # Using the long specific instructions from the original user prompt
        if selected_design_template.component_name == COMPONENT_NAME_PDF_LESSON:
            target_content_model = PdfLessonDetails
            default_error_instance = PdfLessonDetails(lessonTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'PDF Lesson' content.
    Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

    **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Capture all information and hierarchical relationships. Maintain original language.

    **Global Fields:**
    1.  `lessonTitle` (string): Main lesson title for the document.
       - Look for patterns like "**Course Name** : **Lesson** : **Lesson Title**" or "**Lesson** : **Lesson Title**"
       - Extract ONLY the lesson title part (the last part after the last "**")
       - For example: "**Code Optimization Course** : **Lesson** : **Introduction to Optimization**" → extract "Introduction to Optimization"
       - For example: "**Lesson** : **JavaScript Basics**" → extract "JavaScript Basics"
       - Do NOT include the course name or "Lesson" label in the title
       - If no clear pattern is found, use the first meaningful title or heading
    2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
    3.  `detectedLanguage` (string): e.g., "en", "ru".

    **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

    1.  **`type: "headline"`**
        * `level` (integer):
            * `1`: Reserved for the main title of a document, usually handled by `lessonTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
            * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
            * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
            * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
        * `text` (string): Headline text.
        * `iconName` (string, optional): Based on level and context as described above.
        * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.


    2.  **`type: "paragraph"`**
        * `text` (string): Full paragraph text.
        * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thoght in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендация', 'Рекомендація' — or their localized equivalents, and isn't a part of the buller list.

    3.  **`type: "bullet_list"`**
        * `items` (array of `ListItem`): Can be strings or other nested content blocks.
        * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

    4.  **`type: "numbered_list"`**
        * `items` (array of `ListItem`):
            * Can be simple strings for basic numbered points.
            * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                * The `items` of this *inner* `bullet_list` should then be:
                    1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                    2. A `paragraph` block (for the main descriptive text).
                    3. Optionally, another `paragraph` block with `isRecommendation: true`.
            * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

    **General Parsing Rules & Icon Names:**
    * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
    * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
    * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
    * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
    * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
    * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

    Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
      • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
      • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
      • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

    Return ONLY the JSON object. 
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            target_content_model = TextPresentationDetails
            default_error_instance = TextPresentationDetails(textTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM # Can reuse this example structure
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string, optional): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):**

            1.  **`type: "headline"`**
                * `level` (integer): `2`, `3`, or `4`.
                * `text` (string): Headline text.
                * `iconName` (string, optional): If the raw text includes an icon name like `{iconName}`, extract it. Permissible Icon Names: `info`, `goal`, `star`, `apple`, `award`, `boxes`, `calendar`, `chart`, `clock`, `globe`.
                * `isImportant` (boolean, optional): If the raw text includes `{isImportant}`, set this to `true`. If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): Set to `true` if this paragraph should be styled as a recommendation (e.g., with a side border).

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be simple strings. Nested lists are supported; you can place a `bullet_list` or `numbered_list` inside another list's `items` array.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`): Can be simple strings or other blocks, including a `bullet_list` for nested content.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.

            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **Key Parsing Rules:**
            *   Parse `{isImportant}` on headlines to the `isImportant` boolean field.
            *   Parse `{iconName}` on headlines to the `iconName` string field.
            *   After extracting `iconName` and `isImportant` values, you MUST remove their corresponding `{...}` tags from the final headline `text` field. The user should not see these tags in the output text.
            *   If a paragraph starts with `**Recommendation:**` (or a localized translation like `**Рекомендация:**`, `**Рекомендація:**`), you MUST set the `isRecommendation` field on that paragraph block to `true` and remove the keyword itself from the final `text` field.
            *   Do NOT remove the `**` from the text for any other purpose; treat it as part of the text. It is critical that you preserve the double-asterisk (`**`) markdown for bold text within all `text` fields.
            *   You are encouraged to use a diverse range of the available `iconName` values to make the presentation visually engaging.
            *   If the raw text starts with `# Title`, this becomes the `textTitle`. The `contentBlocks` should not include this Level 1 headline. All other headlines (`##`, `###`, `####`) are content blocks.
            *   **If the source text contains a Markdown table or tabular data, and the 'tables' style is selected, you MUST output a `table` block as described above. Do NOT output Markdown tables or represent tables as lists or paragraphs.**

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TRAINING_PLAN:
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Parsing Error for {project_data.projectName}", sections=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): Sequential number formatted as "№1", "№2", … Always use this exact format; never "Module 1".
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
+                - IMPORTANT: When the raw text explicitly names the assessment (for example just "Test"), copy that word *exactly*—do not expand it to phrases such as "Knowledge Test", "Proficiency Test", or similar, and do not spell-correct it.
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}. DO NOT use "Content missing" or "Content Coverage:" or similar phrases in the text.

            **Parsing Rules & Constraints:**
+            • Except where explicit transformations are required by these instructions, reproduce every extracted text fragment verbatim — preserving spelling, punctuation, capitalisation, and line breaks. Absolutely do NOT paraphrase, translate, or autocorrect the source text.
            • Detect modules and lessons from headings, tables, or enumerations in the source text. Preserve their original order.
            • Always use dot as decimal separator for `hours` (e.g., 2.5).
            • If `hours` is written as "2 h 30 min", convert to 2.5.
            • Do not create empty arrays; if a module has no lessons, set `lessons: []`.
            • Never output null values for required string fields; use an empty string instead.
            • Ensure that every lesson belongs to a module; do not leave stray lessons.
            • Preserve bold (`**`) or italic (`*`) markdown that exists inside titles or texts.
            • Auxiliary keywords like "Goal", "Outcome", "Assessment" must be translated to the language of the content using the same localization rules described earlier.

            **Validation Checklist BEFORE returning JSON:**
            □ Each module id follows the "№X" pattern.
            □ No lesson titles start with "Lesson X.Y" or similar numbering patterns.
            □ Sum of `hours` in lessons equals `totalHours` if `autoCalculateHours` is false.
            □ Every `check.type` other than "none" has non-empty `text`.
            □ `detectedLanguage` is filled with a 2-letter code.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM  # Force use of new template format
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has 2-3 numerical metrics/statistics with clear values → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - If slide has event dates → use "event-list"
            - If slide has 6 numbered ideas → use "six-ideas-list"
            - If slide has challenges vs solutions → use "challenges-solutions"
            - If slide has analytics metrics in bullet points → use "metrics-analytics"
            - For standard content → use "content-slide"
            
            **CRITICAL TEMPLATE SELECTION RULES:**
            - NEVER use "big-numbers" unless content has exactly 2-3 clear numerical metrics with values, labels, and descriptions
            - NEVER use "metrics-analytics" unless content specifically mentions analytics/performance metrics  
            - If content has bullet points about concepts (not metrics), use "bullet-points" NOT "metrics-analytics"
            - If content mentions "evaluation", "analysis", or has bullet points about tracking/measuring, consider "bullet-points" first
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "steps" array with value/label/description
            - For timeline: parse chronological content into "steps" array
            - For pyramid: parse hierarchical content into "steps" array
            
            **CRITICAL IMAGE PROMPT EXTRACTION - PRESENTATION ILLUSTRATIONS:**
            - ALWAYS extract image prompts from [IMAGE_PLACEHOLDER] sections
            - Format: [IMAGE_PLACEHOLDER: SIZE | POSITION | DESCRIPTION]
            - Map DESCRIPTION to "imagePrompt" and "imageAlt" fields
            - **CRITICAL: Generate extremely detailed, descriptive prompts with specific visual elements**
            - **DETAILED PROMPT FORMAT REQUIREMENTS:**
              - Start with "Minimalist flat design illustration of [detailed subject/scene description]"
              - Include SPECIFIC visual elements: exact objects, people, layouts, arrangements
              - Describe COMPOSITION: positioning, spatial relationships, perspective
              - Detail CHARACTER descriptions: gender, age, clothing, poses, actions
              - Specify OBJECT details: shapes, sizes, orientations, interactions
              - Include ENVIRONMENTAL elements: setting, context, atmosphere
              - Use color placeholders: [COLOR1], [COLOR2], [COLOR3], [BACKGROUND]
              - End with style and background specifications
              - NO separate color descriptions or presentation context
            - **VISUAL ELEMENT REQUIREMENTS:**
              - **People**: Describe gender, ethnicity, age range, specific clothing, poses, facial expressions, interactions
              - **Objects**: Detail size, shape, orientation, material appearance, positioning relative to other elements
              - **Technology**: Specify device types, screen content, interface elements, connection indicators
              - **Architecture**: Describe building styles, structural elements, spatial relationships, interior/exterior details
              - **Data/Charts**: Detail chart types, data representation methods, axis labels, trend indicators
              - **Nature/Abstract**: Specify shapes, patterns, flow directions, organic vs geometric elements
            - **COMPOSITION REQUIREMENTS:**
              - Describe exact positioning: "person sitting on the left side", "laptop positioned at center-right"
              - Include spatial relationships: "behind", "in front of", "surrounding", "connected by"
              - Specify viewing angles: "front view", "three-quarter perspective", "top-down view"
              - Detail background/foreground layering: "foreground elements", "middle ground", "background context"
            - **COLOR PLACEHOLDER USAGE:**
              - [COLOR1] or [PRIMARY]: Main accent color for primary focal elements
              - [COLOR2] or [SECONDARY]: Secondary color for supporting elements, borders, text
              - [COLOR3] or [TERTIARY]: Accent color for details, highlights, subtle elements
              - [BACKGROUND]: Background color for the entire illustration
            - **ENHANCED PROMPT STRUCTURE:**
              - "Minimalist flat design illustration of [comprehensive scene description with 3-5 specific visual elements]. The scene features [detailed character/object descriptions with exact positioning]. [Additional environmental and compositional details]. [Specific color assignments for each visual element using placeholders]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
            - **MANDATORY SCENE STRUCTURING:**
              - ALWAYS describe WHO is in the scene (specific people with demographics, clothing, poses)
              - ALWAYS describe WHERE they are positioned (left, center, right, foreground, background)
              - ALWAYS describe WHAT they are doing (specific actions, interactions, activities)
              - ALWAYS describe the SETTING details (furniture, equipment, environment specifics)
              - ALWAYS describe the LAYOUT (how elements are arranged spatially)
              - NEVER use vague terms like "featuring visual representations" or "playful design"
              - REPLACE abstract descriptions with concrete, observable elements
            - **SIMPLICITY REQUIREMENTS:**
              - LIMIT to 1-2 people maximum per illustration (never 3+ people)
              - SHOW 1-3 main visual elements only (avoid complex multi-panel setups)
              - FOCUS on clean, uncluttered compositions with plenty of white space
              - AVOID crowded scenes with multiple monitors, workstations, or complex layouts
              - PREFER single focal points rather than busy multi-element arrangements
            - **VISUAL ILLUSTRATION REQUIREMENTS:**
              - CREATE scenic illustrations NOT infographics or charts
              - AVOID "featuring icons representing" or "with clear labels" language
              - GENERATE actual scenes with objects, environments, and atmospheres
              - PREFER realistic scenarios over abstract concept representations
              - FOCUS on visual storytelling rather than information display
              - REPLACE "infographic" prompts with "illustration of [actual scene/environment]"
            - **DETAILED SCENE EXAMPLES (SIMPLE COMPOSITIONS):**
              - TEAM COLLABORATION: "two professionals at a clean desk: a Black woman in a blue blazer presenting to an Asian man in glasses who is taking notes on a single laptop, with one simple whiteboard showing basic geometric shapes in the background"
              - TECHNOLOGY SETUP: "a single modern workstation with one large monitor displaying simple geometric charts, a wireless keyboard, and minimal desk accessories"
              - DATA FLOW: "a simple network diagram with three circular nodes connected by clean arrow lines, showing data flow between connected points"
              - EDUCATIONAL SCENE: "one Hispanic female teacher standing next to a single wall chart with simple pictographic elements, facing two students sitting at clean desks"
              - LANGUAGE LEARNING: "one student at a modern desk using a tablet for language learning, with simple educational materials nearby"
            - **TEXT AND LABELING RESTRICTIONS:**
              - MINIMIZE text elements in illustrations - use symbols, icons, and visual indicators instead
              - AVOID readable text, labels, signs, or written content on screens, documents, or displays
              - USE abstract geometric shapes, simple icons, and visual patterns instead of text
              - REPLACE charts with text labels with simple bar charts, pie segments, or geometric data representations
              - AVOID books, documents, or papers with visible text - use blank documents or simple geometric patterns
              - USE color coding and visual hierarchy instead of text labels for differentiation
            - **MANDATORY REQUIREMENTS:**
              - NEVER include "presentation slide" or "for presentations" in the prompt
              - NEVER add separate color descriptions after the main scene description
              - ALWAYS describe at least 3-5 specific visual elements in detail
              - ALWAYS specify exact positioning and spatial relationships
              - ALWAYS include character demographics and specific object details
              - ALWAYS assign colors to specific elements using placeholders
              - MINIMIZE or eliminate text elements - focus on visual symbols and icons
              - NEVER leave imagePrompt fields empty - generate comprehensive, detailed prompts
            - **FORBIDDEN VAGUE LANGUAGE:**
              - NEVER use "featuring visual representations" - describe specific objects instead
              - NEVER use "playful design" - describe specific arrangement and visual elements
              - NEVER use "colorful illustration" - specify who, what, where, and how they're positioned
              - NEVER use "depicting [concept]" - describe the actual scene with people and objects
              - REPLACE abstract concepts with concrete scenes showing people engaged in specific activities
              - REPLACE "showing [topic]" with "scene features [specific people] doing [specific actions] in [specific setting]"
            - **FORBIDDEN INFOGRAPHIC LANGUAGE:**
              - NEVER use "infographic illustrating" - create actual scenic illustrations instead
              - NEVER use "featuring icons representing" - describe real objects and environments
              - NEVER use "with clear labels" or "arranged in a layout" - focus on natural scenes
              - NEVER use "icons for [concepts]" - create realistic environments where concepts occur
              - REPLACE "infographic of [topic]" with "illustration of [people doing topic-related activities in specific environment]"
              - REPLACE "featuring icons" with "scene showing [specific objects, people, and activities]"
            
            **TEMPLATE-SPECIFIC PROPS REQUIREMENTS:**
            
            For "big-image-left" and "big-image-top":
            - "title": Main slide heading
            - "subtitle": Descriptive content (NOT same as title)  
            - "imagePrompt": Detailed description for AI image generation
            - "imageAlt": Alt text for the image
            
            For "bullet-points" and "bullet-points-right":
            - "title": Main heading
            - "bullets": Array of bullet point strings
            - "imagePrompt": Description for supporting image (REQUIRED) - MUST be scenic illustration showing people in real environments, NOT infographics or icons. NEVER leave this field empty or the slide will use generic fallback prompts.
            - "imageAlt": Alt text for image
            
            For "two-column":
            - "title": Main slide title
            - "leftTitle": Left column heading  
            - "rightTitle": Right column heading (NEVER leave empty - always provide meaningful title)
            - "leftContent": Left column text content
            - "rightContent": Right column text content (NEVER leave empty - split slide content between columns)
            - "leftImagePrompt": Image prompt for left column (if applicable)
            - "rightImagePrompt": Image prompt for right column (if applicable)
            - **CRITICAL**: Two-column slides MUST have content in BOTH columns. Split slide content intelligently between left and right sections.
            
            For "big-numbers":
            - "title": Main heading
            - "steps": Array with exactly 3 items, each having:
              - "value": Numerical value or short metric (e.g., "25%", "3x", "$42")
              - "label": Short descriptive label (e.g., "Performance Improvement") 
              - "description": Detailed explanation of the metric
            - **CRITICAL**: ALWAYS provide exactly 3 steps. If slide content has less, expand into 3 logical points. If more than 3, group into 3 main categories.
            
            For "metrics-analytics":
            - "title": Main heading
            - "metrics": Array of metric descriptions (strings)

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title
            - NEVER create duplicate content for title and subtitle - extract different content
            - ALWAYS generate imagePrompt for templates that support images - NEVER leave imagePrompt fields empty
            - CRITICAL: bullet-points and bullet-points-right templates MUST include detailed imagePrompt fields

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM  # Use video lesson template with voiceover
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.
            
            **VIDEO LESSON MODE: You are creating a Video Lesson Presentation with voiceover.**
            - This is NOT a regular slide deck - it's a Video Lesson that requires voiceover for every slide
            - You MUST generate voiceover text for each slide regardless of the input content
            - The voiceover is essential for the video lesson functionality
            - FAILURE TO INCLUDE VOICEOVER WILL RESULT IN AN INVALID OUTPUT
            
            🚨 CRITICAL REQUIREMENT: Every slide object MUST have a "voiceoverText" field with 2-4 sentences of conversational explanation. The root object MUST have "hasVoiceover": true. This is NON-NEGOTIABLE for Video Lesson Presentations.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.
            
            **VIDEO LESSON VOICEOVER REQUIREMENTS:**
            When parsing a Video Lesson Presentation, you MUST include voiceover text for each slide. The voiceover should:
            - Be conversational and engaging, as if speaking directly to the learner
            - Explain the slide content in detail, expanding on what's visually presented
            - Use natural transitions between concepts
            - Be approximately 30-60 seconds of speaking time per slide
            - Include clear explanations of complex concepts
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide
            
            **CRITICAL: You MUST generate voiceover text for EVERY slide in Video Lesson Presentations.**
            - Each slide object MUST include a "voiceoverText" field
            - The voiceover should be 2-4 sentences explaining the slide content
            - Set "hasVoiceover": true in the root object
            - If you don't see voiceover text in the input, GENERATE it based on the slide content
            
            **MANDATORY VOICEOVER GENERATION:**
            - For Video Lesson Presentations, you MUST create voiceover text for EVERY slide
            - Do NOT skip voiceover generation under any circumstances
            - Generate conversational, engaging voiceover that explains the slide content
            - Each voiceover should be 2-4 sentences (approximately 30-60 seconds of speaking time)
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".
            6.  `hasVoiceover` (boolean, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST set this to true since every slide will have voiceover text.

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.
            * `voiceoverText` (string, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST include conversational voiceover text that explains the slide content in detail. This field is REQUIRED for every slide in video lessons.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column" or "two-column-diversity"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has metrics/statistics → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - For standard content → use "content-slide"
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Available Template IDs and their Props (must match exactly):**

            1. **`hero-title-slide`** - Hero opening slides:
            ```json
            "props": {
              "title": "Main slide title",
              "subtitle": "Detailed subtitle explaining the overview",
              "showAccent": true,
              "accentPosition": "left",
              "textAlign": "center",
              "titleSize": "xlarge",
              "subtitleSize": "medium"
            }
            ```

            2. **`title-slide`** - Simple title slides:
            ```json
            "props": {
              "title": "Presentation Title",
              "subtitle": "Compelling subtitle that captures attention",
              "author": "Author name",
              "date": "Date"
            }
            ```

            3. **`content-slide`** - Standard content slides:
            ```json
            "props": {
              "title": "Slide title",
              "content": "Main content with bullet points:\\n\\n• Point 1\\n• Point 2\\n• Point 3",
              "alignment": "left"
            }
            ```

            4. **`bullet-points`** - Formatted bullet point lists:
            ```json
            "props": {
              "title": "Key Points",
              "bullets": [
                "First important point with detailed explanation",
                "Second key insight with comprehensive analysis",
                "Third critical element with thorough examination",
                "Fourth essential consideration with strategic importance",
                "Fifth valuable perspective with actionable recommendations",
                "Sixth valuable perspective with actionable recommendations",
                "Seventh valuable perspective with actionable recommendations"
              ],
              "maxColumns": 2,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            5. **`two-column`** - Split layout:
            ```json
            "props": {
                "title": "Two Column Layout",
                "leftTitle": "Left Column",
                "leftContent": "Content for the left side with detailed explanations",
                "leftImageUrl": "https://via.placeholder.com/320x200?text=Left+Image",
                "leftImageAlt": "Description of left image",
                "leftImagePrompt": "Prompt for left image",
                "rightTitle": "Right Column",
                "rightContent": "Content for the right side with detailed information",
                "rightImageUrl": "https://via.placeholder.com/320x200?text=Right+Image",
                "rightImageAlt": "Description of right image",
                "rightImagePrompt": "Prompt for right image",
                "columnRatio": "50-50"
            }
            ```

            6. **`process-steps`** - Numbered process steps:
            ```json
            "props": {
              "title": "Process Steps",
              "steps": [
                "Step 1 with detailed description explaining what to do",
                "Step 2 with comprehensive explanation covering the process",
                "Step 3 with thorough description of the methodology",
                "Step 4 with in-depth explanation covering the final phase"
              ],
              "layout": "horizontal"
            }
            ```

            

            7. **`challenges-solutions`** - Problems vs solutions:
            ```json
            "props": {
              "title": "Challenges and Solutions",
              "challengesTitle": "Challenges",
              "solutionsTitle": "Solutions",
              "challenges": [
                "Challenge 1 with detailed explanation of the problem",
                "Challenge 2 with comprehensive analysis of the issue"
              ],
              "solutions": [
                "Solution 1 with detailed approach and implementation strategy",
                "Solution 2 with comprehensive methodology and practical steps"
              ]
            }
            ```

            8. **`big-image-left`** - Large image on left:
            ```json
            "props": {
              "title": "Slide Title",
              "subtitle": "Subtitle or detailed description for the slide",
              "imageUrl": "https://via.placeholder.com/600x400?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration that visually represents the slide title",
              "imageSize": "large"
            }
            ```

            9. **`bullet-points-right`** - Title, subtitle, bullet points with image:
            ```json
            "props": {
              "title": "Key Points",
              "subtitle": "Short intro or context before the list",
              "bullets": [
                "First important point",
                "Second key insight",
                "Third critical element",
                "Fourth essential consideration",
                "Fifth valuable perspective"
              ],
              "maxColumns": 1,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            10. **`big-image-top`** - Large image on top:
            ```json
            "props": {
              "title": "Main Title",
              "subtitle": "Subtitle or content goes here",
              "imageUrl": "https://via.placeholder.com/700x350?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration for the topic",
              "imageSize": "large"
            }
            ```

            11. **`four-box-grid`** - Title and 4 boxes in 2x2 grid:
            ```json
            "props": {
              "title": "Main Title",
              "boxes": [
                { "heading": "Box 1", "text": "Detailed description with comprehensive explanations" },
                { "heading": "Box 2", "text": "Comprehensive explanation covering detailed insights" },
                { "heading": "Box 3", "text": "Thorough description spanning multiple sentences" },
                { "heading": "Box 4", "text": "In-depth explanation with actionable insights" }
              ]
            }
            ```

            12. **`timeline`** - Horizontal timeline with 4 steps:
            ```json
            "props": {
              "title": "History and Evolution",
              "steps": [
                { "heading": "Step 1", "description": "Detailed description of the first phase" },
                { "heading": "Step 2", "description": "Comprehensive explanation of the second phase" },
                { "heading": "Step 3", "description": "Thorough description of the third phase" },
                { "heading": "Step 4", "description": "In-depth explanation of the final phase" }
              ]
            }
            ```

            13. **`big-numbers`** - Three-column layout for metrics:
            ```json
            "props": {
              "title": "Key Metrics",
              "steps": [
                { "value": "25%", "label": "Performance Improvement", "description": "System performance improved by 25% after optimization" },
                { "value": "3x", "label": "Speed Increase", "description": "Processing speed increased 3 times faster than before" },
                { "value": "50%", "label": "Cost Reduction", "description": "Operating costs reduced by 50% through efficient design" }
              ]
            }
            ```

            14. **`pyramid`** - Pyramid diagram with 3 levels:
            ```json
            "props": {
              "title": "Hierarchical Structure",
              "subtitle": "Explanation of the hierarchical relationship between elements",
              "steps": [
                { "heading": "Top Level", "description": "Description of the highest level" },
                { "heading": "Middle Level", "description": "Description of the intermediate level" },
                { "heading": "Base Level", "description": "Description of the foundational level" }
              ]
            }
            ```

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "items" array with value/label/description
            - For timeline: parse chronological content into "steps" array
            - For pyramid: parse hierarchical content into "steps" array

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON:
            target_content_model = VideoLessonData
            default_error_instance = VideoLessonData(
                mainPresentationTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
            {
  "mainPresentationTitle": "Курс: Обучение для рекрутера",
  "slides": [
    {
      "slideId": "slide_1_znakomstvo",
      "slideNumber": 1,
      "slideTitle": "Знакомство",
      "displayedText": "Знакомимся с основами рекрутинга и ключевыми обязанностями.",
      "displayedPictureDescription": "Улыбающиеся профессионалы в современном офисе.",
      "displayedVideoDescription": "Анимация воронки рекрутинга: поиск, отбор, интервью, оффер.",
      "voiceoverText": "Приветствую вас на курсе 'Обучение для рекрутера'! Начнем с основ. Этот модуль посвящен ключевым аспектам профессии."
    },
    {
      "slideId": "slide_2_instrumenty",
      "slideNumber": 2,
      "slideTitle": "Инструменты Рекрутера",
      "displayedText": "Рассматриваем основные инструменты для современного рекрутера.",
      "displayedPictureDescription": "Коллаж логотипов: LinkedIn, ATS, GitHub, поиск, календарь.",
      "displayedVideoDescription": "Анимация кликов по иконкам инструментов с краткими пояснениями их функций.",
      "voiceoverText": "Для успеха рекрутеру нужен арсенал инструментов. Рассмотрим основные категории и их назначение. Эффективное использование повысит вашу производительность."
    }
  ],
  "detectedLanguage": "ru"
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant. Your task is to convert the provided presentation slide content, which is in a specific structured text format, into a perfectly structured JSON object.

Your output MUST be a single, valid JSON object, and it must strictly adhere to the exact structure provided in the example JSON you have been given separately. Do not include any additional text, explanations, or conversational fillers outside the JSON object.

Input Text Structure and Extraction Rules:
The input text will describe a presentation or video lesson. The content within the fields (like slide titles, descriptions) can be in various languages (e.g., Ukrainian, Russian, English). You must extract the content exactly as it appears, preserving its original language, including any original formatting like line breaks within the content where present (e.g., in "Відображуваний Текст").

Overall Presentation Title:

This will be identified by a header like "Загальний Заголовок Курсу:" (or its equivalent in other languages like "ОБЩИЙ ЗАГОЛОВОК КУРСА:" or "Overall Course Title:").
Extract the text that immediately follows this bolded header as the value for the mainPresentationTitle field.
Individual Slides:

Each slide's information is clearly marked by consistently bolded headers.
slideNumber (integer): Look for "Номер Слайда:" (or equivalent, e.g., "Номер Слайда:", "Slide Number:"). Extract the numerical value that immediately follows this bolded header.
slideTitle (string): Look for "Заголовок Слайда:" (or equivalent, e.g., "Заголовок Слайда:", "Slide Title:"). Extract the text that immediately follows this bolded header.
displayedText (string): Look for "Відображуваний Текст:" (or equivalent, e.g., "Відображуваний Текст:", "Displayed Text:"). Extract all text that immediately follows this bolded header, up until the next bolded header. Preserve any internal line breaks or numbering.
displayedPictureDescription (string): Look for "Опис Зображення:" (or equivalent, e.g., "Опис Зображення:", "Image Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
displayedVideoDescription (string): Look for "Опис Відео:" (or equivalent, e.g., "Опис Відео:", "Video Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
voiceoverText (string): Look for "Текст Озвучення:" (or equivalent, e.g., "Текст Озвучення:", "Voiceover Text:"). Extract the text that immediately follows this bolded header, up until the next bolded header or the end of the slide's content block.
slideId Generation:

For each slide, you must generate a unique slideId.
This ID should be a concatenation of the literal string "slide_", the slideNumber, and a simplified, lowercase version of the slideTitle.
To simplify the slideTitle for the ID, convert it to lowercase and replace all spaces with underscores (_). Remove any punctuation or special characters from the simplified title part of the ID. If the title is very long, consider using only the first few words to keep the ID concise, but ensure uniqueness. For example:
slideNumber: 1, slideTitle: "Вступ" -> slideId: "slide_1_вступ"
slideNumber: 2, slideTitle: "Питання 1" -> slideId: "slide_2_питання_1"
slideNumber: 3, slideTitle: "Варіанти відповіді" -> slideId: "slide_3_варіанти_відповіді"
slideNumber: 4, slideTitle: "Пояснення до Питання 1" -> slideId: "slide_4_пояснення_до_питання_1"
detectedLanguage (string):

This will be identified by a header like "Language of Content:" (or its equivalent, e.g., "Язык Контента:", "Мова Контенту:").
Extract the two-letter ISO 639-1 language code (e.g., "uk", "ru", "en") that immediately follows this label.
If this "Language of Content:" label is missing from the input, infer the primary language from the majority of the content (specifically the mainPresentationTitle and slideTitle fields) and use the appropriate two-letter ISO 639-1 code.
Key Parsing Rules & Constraints for 100% Reliability:

Header Recognition: Always identify fields by their bolded headers (e.g., "Номер Слайда:", "Заголовок Слайда:"). These bolded headers consistently precede the data you need to extract.
Exact Text Extraction: All extracted text content must be preserved exactly as it appears in the input, including its original capitalization, punctuation, and line breaks within the content block for a given field.
Field Presence: If a field's bolded header is present in the input but the text following it is empty before the next header, the corresponding JSON field should be an empty string (""). Do not use null or omit fields that are defined as strings in the target schema if their labels are present in the input.
Sequential Parsing: Process the text sequentially, extracting content associated with each bolded header until the next bolded header is encountered.
Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_QUIZ:
            target_content_model = QuizData
            default_error_instance = QuizData(
                quizTitle=f"LLM Parsing Error for {project_data.projectName}",
                questions=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
{
"quizTitle": "Advanced Sales Techniques Quiz",
"detectedLanguage": "en",
"questions": [
{
"question_type": "multiple-choice",
"question_text": "Which technique involves assuming the sale is made?",
"options": [
{"id": "A", "text": "The 'Question Close'"},
{"id": "B", "text": "The 'Presumptive Close'"}
],
"correct_option_id": "B",
"explanation": "A presumptive close assumes the sale is made."
},
{
"question_type": "multi-select",
"question_text": "Which of the following are primary colors? (Select all that apply)",
"options": [
{"id": "A", "text": "Red"},
{"id": "B", "text": "Green"},
{"id": "C", "text": "Orange"},
{"id": "D", "text": "Blue"}
],
"correct_option_ids": ["A", "D"],
"explanation": "In the traditional subtractive model, the primary colors are Red, Yellow, and Blue."
},
{
"question_type": "matching",
"question_text": "Match each sales technique with its description:",
"prompts": [
{"id": "A", "text": "The 'Alternative Close'"},
{"id": "B", "text": "The 'Summary Close"}
],
"options": [
{"id": "1", "text": "Presenting two options to the customer"},
{"id": "2", "text": "Recapping key benefits before asking for the sale"}
],
"correct_matches": {"A": "1", "B": "2"},
"explanation": "The Alternative Close gives customers a choice between options, while the Summary Close reinforces value before closing."
},
{
"question_type": "sorting",
"question_text": "Arrange these steps in the correct order for a successful sales call:",
"items_to_sort": [
{"id": "step1", "text": "Identify customer needs"},
{"id": "step2", "text": "Present solution"},
{"id": "step3", "text": "Handle objections"},
{"id": "step4", "text": "Close the sale"}
],
"correct_order": ["step1", "step2", "step3", "step4"],
"explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
},
{
"question_type": "open-answer",
"question_text": "What are the three key elements of an effective elevator pitch?",
"acceptable_answers": [
"Problem, Solution, Call to Action",
"Problem statement, Your solution, What you want them to do next",
"The issue, How you solve it, What action to take"
],
"explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
}
]
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Quiz' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the provided quiz content into a structured JSON object that captures all questions, their types, options, correct answers, and explanations.

            **Global Fields:**
            1. `quizTitle` (string): The main title of the quiz.
            2. `questions` (array): An array of question objects.
            3. `detectedLanguage` (string): e.g., "en", "ru".

            **Question Types and Their Structures:**

            1. **Multiple Choice (`question_type: "multiple-choice"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_id` (string): The ID of the correct option.
               * `explanation` (string, optional): Explanation of the correct answer.

            2. **Multi-Select (`question_type: "multi-select"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_ids` (array): Array of IDs of all correct options.
               * `explanation` (string, optional): Explanation of the correct answers.

            3. **Matching (`question_type: "matching"`)**
               * `question_text` (string): The question text.
               * `prompts` (array): List of `MatchingPrompt` objects with `id` and `text`.
               * `options` (array): List of `MatchingOption` objects with `id` and `text`.
               * `correct_matches` (object): Maps prompt IDs to option IDs.
               * `explanation` (string, optional): Explanation of the correct matches.

            4. **Sorting (`question_type: "sorting"`)**
               * `question_text` (string): The question text.
               * `items_to_sort` (array): List of `SortableItem` objects with `id` and `text`.
               * `correct_order` (array): Array of item IDs in the correct sequence.
               * `explanation` (string, optional): Explanation of the correct order.

            5. **Open Answer (`question_type: "open-answer"`)**
               * `question_text` (string): The question text.
               * `acceptable_answers` (array): List of acceptable answer strings.
               * `explanation` (string, optional): Explanation or additional context.

            **Key Parsing Rules:**
            1. Each question must have a unique type and appropriate fields for that type.
            2. Option IDs should be consistent (e.g., "A", "B", "C" for multiple choice).
            3. Maintain original language and formatting in all text fields.
            4. Include explanations where available to help users understand correct answers.
            5. Ensure all required fields are present for each question type.
            6. Validate that correct answers reference valid option IDs.

            Return ONLY the JSON object.
            """

        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # For lesson plans, preserve the original structure without parsing
            logger.info(f"Lesson plan detected for project {project_data.projectName}. Preserving original structure.")
            # Store the raw lesson plan data without parsing
            content_to_store_for_db = json.loads(project_data.aiResponse) if isinstance(project_data.aiResponse, str) else project_data.aiResponse
            derived_product_type = "lesson-plan"
            derived_microproduct_type = "Lesson Plan"
            
            # Skip the LLM parsing for lesson plans but continue with normal flow
            logger.info("Skipping LLM parsing for lesson plan - using raw data directly")
            # Set these variables to be used in the normal flow below
            target_content_model = None  # Not used for lesson plans
            default_error_instance = None  # Not used for lesson plans
            llm_json_example = ""  # Not used for lesson plans
            component_specific_instructions = ""  # Not used for lesson plans
            
        else:
            logger.warning(f"Unknown component_name '{selected_design_template.component_name}' for DT ID {selected_design_template.id}. Defaulting to TrainingPlanDetails for parsing.")
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Config Error for {project_data.projectName}", sections=[])
            llm_json_example = DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = "Parse the content according to the JSON example provided."


        if hasattr(default_error_instance, 'detectedLanguage'):
                default_error_instance.detectedLanguage = detect_language(project_data.aiResponse)

        # Skip LLM parsing for lesson plans
        if selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            logger.info("Lesson plan detected - skipping LLM parsing entirely")
            parsed_content_model_instance = None  # Will not be used
        elif selected_design_template.component_name == COMPONENT_NAME_TRAINING_PLAN:
            # Fast path: Check if aiResponse is already valid JSON with sections (from preview)
            try:
                logger.info(f"[FAST_PATH_DEBUG] Checking aiResponse for Training Plan: {project_data.aiResponse[:200]}...")
                cached_json = json.loads(project_data.aiResponse.strip())
                logger.info(f"[FAST_PATH_DEBUG] JSON parsed successfully, type: {type(cached_json)}")
                if isinstance(cached_json, dict) and "sections" in cached_json:
                    logger.info(f"[FAST_PATH_DEBUG] JSON has sections field with {len(cached_json.get('sections', []))} sections")
                    logger.info(f"[FAST_PATH] Training Plan JSON detected, bypassing LLM parsing for {project_data.projectName}")
                    parsed_content_model_instance = TrainingPlanDetails(**cached_json)
                    logger.info(f"[FAST_PATH_DEBUG] TrainingPlanDetails created successfully with {len(parsed_content_model_instance.sections)} sections")
                else:
                    logger.info(f"[FAST_PATH_DEBUG] JSON doesn't have sections, falling back to LLM parsing")
                    parsed_content_model_instance = await parse_ai_response_with_llm(
                        ai_response=project_data.aiResponse,
                        project_name=project_data.projectName,
                        target_model=target_content_model,
                        default_error_model_instance=default_error_instance,
                        dynamic_instructions=component_specific_instructions,
                        target_json_example=llm_json_example
                    )
            except (json.JSONDecodeError, KeyError, Exception) as e:
                logger.info(f"[FAST_PATH] JSON validation failed ({e}), falling back to LLM parsing")
                parsed_content_model_instance = await parse_ai_response_with_llm(
                    ai_response=project_data.aiResponse,
                    project_name=project_data.projectName,
                    target_model=target_content_model,
                    default_error_model_instance=default_error_instance,
                    dynamic_instructions=component_specific_instructions,
                    target_json_example=llm_json_example
                )
        else:
            parsed_content_model_instance = await parse_ai_response_with_llm(
                ai_response=project_data.aiResponse,
                project_name=project_data.projectName,
                target_model=target_content_model,
                default_error_model_instance=default_error_instance,
                dynamic_instructions=component_specific_instructions,
                target_json_example=llm_json_example
            )

        if selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            logger.info("Lesson plan detected - using raw data without parsing")
        else:    
            logger.info(f"LLM Parsing Result Type: {type(parsed_content_model_instance).__name__}")
            logger.info(f"LLM Parsed Content (first 200 chars): {str(parsed_content_model_instance.model_dump_json())[:200]}") # Use model_dump_json()

        # Inject theme for slide decks from the finalize request
        if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
            parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'theme') and 
            hasattr(project_data, 'theme') and 
            project_data.theme):
            parsed_content_model_instance.theme = project_data.theme
            logger.info(f"Injected theme '{project_data.theme}' into slide deck")

        # Post-process module IDs for training plans to ensure № character is preserved
        if (parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'sections') and parsed_content_model_instance.sections):
            for section in parsed_content_model_instance.sections:
                if hasattr(section, 'id') and section.id:
                    # Fix module IDs that lost the № character
                    if section.id.isdigit():
                        # Plain number like "2" -> "№2"
                        section.id = f"№{section.id}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                    elif section.id.startswith("#"):
                        # Hash format like "#2" -> "№2"
                        number = section.id[1:]
                        section.id = f"№{number}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                    elif not section.id.startswith("№"):
                        # Other formats without № - try to extract number and format correctly
                        import re
                        number_match = re.search(r'\d+', section.id)
                        if number_match:
                            number = number_match.group()
                            section.id = f"№{number}"
                            logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed ID format to '{section.id}'")

        # Apply slide prop normalization for slide decks and video lesson presentations
        if (selected_design_template.component_name in [COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION] and 
            parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'slides') and 
            parsed_content_model_instance.slides):
            
            # Normalize slide props to fix schema mismatches
            slides_dict = [slide.model_dump() if hasattr(slide, 'model_dump') else dict(slide) for slide in parsed_content_model_instance.slides]
            normalized_slides = normalize_slide_props(slides_dict, selected_design_template.component_name)
            
            # Update the content with normalized slides
            content_dict = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            content_dict['slides'] = normalized_slides
            
            # Remove hasVoiceover flag for regular slide decks
            if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
                'hasVoiceover' in content_dict):
                logger.info("Removing hasVoiceover flag for regular slide deck")
                content_dict.pop('hasVoiceover', None)
            
            content_to_store_for_db = content_dict
            
            logger.info(f"Applied slide prop normalization for {len(normalized_slides)} slides")
        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # For lesson plans, content_to_store_for_db was already set earlier - don't overwrite it
            logger.info("Lesson plan - using pre-set content_to_store_for_db")
        else:
            content_to_store_for_db = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            
        derived_product_type = selected_design_template.microproduct_type
        derived_microproduct_type = selected_design_template.template_name

        logger.info(f"Content prepared for DB storage (first 200 chars of JSON): {str(content_to_store_for_db)[:200]}")

        # Determine if this is a standalone product (default to True for general project creation)
        # For specific products like quizzes, this will be overridden in their dedicated endpoints
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_product = project_data.outlineId is None
        
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id,
            source_context_type, source_context_data
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10, $11, $12)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id,
                  source_context_type, source_context_data;
    """

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                project_data.projectName,
                derived_product_type,
                derived_microproduct_type,
                db_microproduct_name_to_store,
                content_to_store_for_db,
                project_data.design_template_id,
                project_data.chatSessionId,
                is_standalone_product,
                project_data.folder_id,
                project_data.source_context_type,
                project_data.source_context_data
            )
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create project entry.")

        db_content_dict = row["microproduct_content"]
        final_content_for_response: Optional[MicroProductContentType] = None
        if db_content_dict and isinstance(db_content_dict, dict):
            component_name_from_db = selected_design_template.component_name
            try:
                if component_name_from_db == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_response = PdfLessonDetails(**db_content_dict)
                    logger.info("Re-parsed as PdfLessonDetails.")
                elif component_name_from_db == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_response = TextPresentationDetails(**db_content_dict)
                    logger.info("Re-parsed as TextPresentationDetails.")
                elif component_name_from_db == COMPONENT_NAME_TRAINING_PLAN:
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    final_content_for_response = TrainingPlanDetails(**db_content_dict)
                    logger.info("Re-parsed as TrainingPlanDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_response = VideoLessonData(**db_content_dict)
                    logger.info("Re-parsed as VideoLessonData.")
                elif component_name_from_db == COMPONENT_NAME_QUIZ:
                    final_content_for_response = QuizData(**db_content_dict)
                    logger.info("Re-parsed as QuizData.")
                elif component_name_from_db == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails (Video Lesson Presentation).")
                elif component_name_from_db == COMPONENT_NAME_LESSON_PLAN:
                    # For lesson plans, preserve the original structure without parsing
                    logger.info("Re-parsing lesson plan - preserving original structure.")
                    final_content_for_response = db_content_dict
                else:
                    logger.warning(f"Unknown component_name '{component_name_from_db}' when re-parsing content from DB on add. Attempting generic TrainingPlanDetails fallback.")
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    # Preserve custom fields (e.g., recommended_content_types) for edit view
                    final_content_for_response = db_content_dict
            except Exception as e_parse:
                logger.error(f"Error parsing content from DB on add (proj ID {row['id']}): {e_parse}", exc_info=not IS_PRODUCTION)

        return ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_response,
            design_template_id=row["design_template_id"], created_at=row["created_at"]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error inserting project: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding project." if IS_PRODUCTION else f"DB error on project insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        # Always release the in-flight lock
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)


@app.get("/api/custom/projects/names", response_model=List[str], summary="Get unique project names for the user")
async def get_distinct_project_names_for_user(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT DISTINCT project_name
        FROM projects
        WHERE onyx_user_id = $1
        ORDER BY project_name ASC;
        """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        project_names: List[str] = [str(row["project_name"]) for row in rows if row["project_name"] is not None]
        return project_names
    except Exception as e:
        logger.error(f"Error fetching distinct project names for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project names." if IS_PRODUCTION else f"Database error while fetching project names: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/projects/{project_id}/edit", response_model=ProjectDetailForEditResponse)
async def get_project_details_for_edit(project_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT
            p.id, p.project_name, p.microproduct_name, p.microproduct_content, p.created_at,
            p.design_template_id, dt.template_name as design_template_name,
            dt.component_name as design_component_name,
            dt.design_image_path as design_image_path,
            p.product_type, p.microproduct_type
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND p.onyx_user_id = $2;
    """
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, project_id, onyx_user_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found.")

        row_dict = dict(row)
        db_content_json = row_dict.get("microproduct_content")
        parsed_content_for_response: Optional[MicroProductContentType] = None
        component_name = row_dict.get("design_component_name")

        # Round hours to integers in the content before parsing
        if db_content_json and isinstance(db_content_json, dict):
            db_content_json = round_hours_in_content(db_content_json)
            
        if db_content_json and isinstance(db_content_json, dict):
            try:
                if component_name == COMPONENT_NAME_PDF_LESSON:
                    parsed_content_for_response = PdfLessonDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    parsed_content_for_response = TextPresentationDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content_json = sanitize_training_plan_for_parse(db_content_json)
                    parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON:
                    parsed_content_for_response = VideoLessonData(**db_content_json)
                elif component_name == COMPONENT_NAME_QUIZ:
                    parsed_content_for_response = QuizData(**db_content_json)
                elif component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                else:
                    logger.warning(f"Unknown component_name '{component_name}' for project {project_id}. Trying fallbacks.", exc_info=not IS_PRODUCTION)
                    try: parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                    except:
                        try: parsed_content_for_response = PdfLessonDetails(**db_content_json)
                        except Exception as e_parse_fallback: logger.error(f"Fallback parsing failed for project {project_id}: {e_parse_fallback}", exc_info=not IS_PRODUCTION)
            except Exception as e_main_parse:
                logger.error(f"Pydantic validation error for DB JSON (project {project_id}, component {component_name}): {e_main_parse}", exc_info=not IS_PRODUCTION)
        elif isinstance(db_content_json, str) and component_name == COMPONENT_NAME_TRAINING_PLAN:
                parsed_content_for_response = parse_training_plan_from_string(db_content_json, row_dict["project_name"])

        return ProjectDetailForEditResponse(
            id=row_dict["id"], projectName=row_dict["project_name"], microProductName=row_dict.get("microproduct_name"),
            design_template_id=row_dict.get("design_template_id"), microProductContent=parsed_content_for_response,
            createdAt=row_dict.get("created_at"), design_template_name=row_dict.get("design_template_name"),
            design_component_name=component_name, design_image_path=row_dict.get("design_image_path")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching project {project_id} for edit: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project details." if IS_PRODUCTION else f"DB error fetching project details for edit: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def get_user_identifiers_for_workspace(request: Request) -> tuple[str, str]:
    """Get both user UUID and email for workspace access"""
    try:
        session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        if not session_cookie_value:
            dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
            if dev_user_id: 
                # For dev users, assume email format and return both
                return dev_user_id, dev_user_id
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authentication required")

        onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
        cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            
            user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email")
            
            if not user_id:
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="User ID extraction failed")
            
            return str(user_id), user_email or str(user_id)
    except Exception as e:
        logger.error(f"Error getting user identifiers: {e}")
        raise

@app.get("/api/custom/projects", response_model=List[ProjectApiResponse])
async def get_user_projects_list_from_db(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool),
    folder_id: Optional[int] = None
):
    # Get both UUID and email for the user
    user_uuid, user_email = await get_user_identifiers_for_workspace(request)
    
    # For backward compatibility with existing code
    onyx_user_id = user_uuid
    
    # For backward compatibility with existing code
    onyx_user_id = user_uuid
    
    # First, get projects owned by the user
    owned_projects_query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type,
               p.folder_id, p."order", p.microproduct_content, p.source_chat_session_id, p.is_standalone
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 {folder_filter}
    """
    
    # Then, get projects the user has access to through workspace permissions
    shared_projects_query = """
        SELECT DISTINCT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type,
               p.folder_id, p."order", p.microproduct_content, p.source_chat_session_id, p.is_standalone
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        INNER JOIN product_access pa ON p.id = pa.product_id
        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
        WHERE (wm.user_id = $1 OR wm.user_id = $2)
          AND wm.status = 'active'
          AND pa.access_type IN ('workspace', 'role', 'individual')
          AND (
              pa.access_type = 'workspace' 
              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
              OR (pa.access_type = 'individual' AND (pa.target_id = $1 OR pa.target_id = $2))
          )
          {folder_filter}
    """
    
    # Get both UUID and email for workspace access
    try:
        session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        if not session_cookie_value:
            dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
            if dev_user_id:
                user_uuid = dev_user_id
                user_email = dev_user_id  # For dev, assume email format
            else:
                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authentication required")
        else:
            onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
            cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
                response.raise_for_status()
                user_data = response.json()
                
                user_uuid = str(user_data.get("userId") or user_data.get("id"))
                user_email = user_data.get("email") or user_uuid
    except Exception as e:
        logger.error(f"Error getting user identifiers: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="User identification failed")
    
    folder_filter = ""
    owned_params = [user_uuid]
    shared_params = [user_uuid, user_email]
    
    if folder_id is not None:
        folder_filter = "AND p.folder_id = $2"
        owned_params.append(folder_id)
        folder_filter_shared = "AND p.folder_id = $3"
        shared_params.append(folder_id)
    else:
        folder_filter_shared = ""
    
    owned_query = owned_projects_query.format(folder_filter=folder_filter)
    shared_query = shared_projects_query.format(folder_filter=folder_filter_shared)
    
    async with pool.acquire() as conn:
        # Get owned projects (use UUID)
        owned_rows = await conn.fetch(owned_query, *owned_params)
        
        # Get shared projects (use email)
        shared_rows = await conn.fetch(shared_query, *shared_params)
        
        # 🔍 DEBUG: Log workspace access results
        logger.info(f"🔍 [WORKSPACE ACCESS] User {user_uuid} (email: {user_email}) projects query results:")
        logger.info(f"   - Owned projects: {len(owned_rows)}")
        logger.info(f"   - Shared projects: {len(shared_rows)}")
        logger.info(f"   - Folder filter: {folder_id}")
        
        if owned_rows:
            logger.info(f"   - Owned project IDs: {[row['id'] for row in owned_rows]}")
        if shared_rows:
            logger.info(f"   - Shared project IDs: {[row['id'] for row in shared_rows]}")
        else:
            # Debug why no shared projects found
            logger.info(f"🔍 [WORKSPACE DEBUG] No shared projects found for user {onyx_user_id}. Investigating...")
            
            # Check workspace memberships using email (not UUID)
            membership_check = await conn.fetch("""
                SELECT wm.*, w.name as workspace_name, wr.name as role_name
                FROM workspace_members wm
                JOIN workspaces w ON wm.workspace_id = w.id
                JOIN workspace_roles wr ON wm.role_id = wr.id
                WHERE wm.user_id = $1
            """, user_email)
            
            logger.info(f"   - User workspace memberships: {len(membership_check)}")
            for membership in membership_check:
                logger.info(f"     * Workspace: {membership['workspace_name']} (ID: {membership['workspace_id']})")
                logger.info(f"       Role: {membership['role_name']} (ID: {membership['role_id']})")
                logger.info(f"       Status: {membership['status']}")
            
            if not membership_check:
                logger.info(f"   ❌ User {user_uuid} is not a member of any workspace!")
                logger.info(f"   💡 Add user to a workspace to enable shared project access")
                logger.info(f"   - User workspace memberships: 0")
                logger.info(f"   - No workspace memberships found - user needs to be added to a workspace")
            
            # Check product access records
            if membership_check:
                workspace_ids = [m['workspace_id'] for m in membership_check]
                access_check = await conn.fetch("""
                    SELECT pa.*, p.project_name, w.name as workspace_name
                    FROM product_access pa
                    JOIN projects p ON pa.product_id = p.id
                    JOIN workspaces w ON pa.workspace_id = w.id
                    WHERE pa.workspace_id = ANY($1::int[])
                """, workspace_ids)
                
                logger.info(f"   - Product access records in user's workspaces: {len(access_check)}")
                for access in access_check:
                    logger.info(f"     * Project: {access['project_name']} (ID: {access['product_id']})")
                    logger.info(f"       Workspace: {access['workspace_name']} (ID: {access['workspace_id']})")
                    logger.info(f"       Access Type: {access['access_type']}")
                    logger.info(f"       Target ID: {access['target_id']}")
            else:
                logger.info(f"   - No workspace memberships found - user needs to be added to a workspace")
        
        # Combine and deduplicate projects
        all_projects = {}
        
        # Process owned projects first
        for row_data in owned_rows:
            row_dict = dict(row_data)
            project_slug = create_slug(row_dict.get('project_name'))
            source_chat_session_id = row_dict.get("source_chat_session_id")
            if source_chat_session_id:
                source_chat_session_id = str(source_chat_session_id)
            
            all_projects[row_dict["id"]] = ProjectApiResponse(
                id=row_dict["id"], projectName=row_dict["project_name"], projectSlug=project_slug,
                microproduct_name=row_dict.get("microproduct_name"),
                design_template_name=row_dict.get("design_template_name"),
                design_microproduct_type=row_dict.get("design_microproduct_type"),
                created_at=row_dict["created_at"], design_template_id=row_dict.get("design_template_id"),
                folder_id=row_dict.get("folder_id"), order=row_dict.get("order"),
                microproduct_content=row_dict.get("microproduct_content"),
                source_chat_session_id=source_chat_session_id,
                is_standalone=row_dict.get("is_standalone")
            )
        
        # Process shared projects (will override owned if same ID, which is fine)
        for row_data in shared_rows:
            row_dict = dict(row_data)
            project_slug = create_slug(row_dict.get('project_name'))
            source_chat_session_id = row_dict.get("source_chat_session_id")
            if source_chat_session_id:
                source_chat_session_id = str(source_chat_session_id)
            
            all_projects[row_dict["id"]] = ProjectApiResponse(
                id=row_dict["id"], projectName=row_dict["project_name"], projectSlug=project_slug,
                microproduct_name=row_dict.get("microproduct_name"),
                design_template_name=row_dict.get("design_template_name"),
                design_microproduct_type=row_dict.get("design_microproduct_type"),
                created_at=row_dict["created_at"], design_template_id=row_dict.get("design_template_id"),
                folder_id=row_dict.get("folder_id"), order=row_dict.get("order"),
                microproduct_content=row_dict.get("microproduct_content"),
                source_chat_session_id=source_chat_session_id,
                is_standalone=row_dict.get("is_standalone")
            )
    
    # Convert to list and sort
    projects_list = list(all_projects.values())
    projects_list.sort(key=lambda x: (x.order or 0, x.created_at), reverse=True)
    
    return projects_list

@app.get("/api/custom/projects/view/{project_id}", response_model=MicroProductApiResponse, responses={404: {"model": ErrorDetail}})
async def get_project_instance_detail(
    project_id: int, 
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    # Get user identifiers for workspace access
    user_uuid, user_email = await get_user_identifiers_for_workspace(request)
    
    # Check if user owns the project or has workspace access
    select_query = """
        SELECT p.*, dt.template_name as design_template_name, dt.microproduct_type as design_microproduct_type, dt.component_name
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND (
            p.onyx_user_id = $2 
            OR EXISTS (
                SELECT 1 FROM product_access pa
                INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                WHERE pa.product_id = p.id 
                  AND wm.user_id = $3 
                  AND wm.status = 'active'
                  AND pa.access_type IN ('workspace', 'role', 'individual')
                  AND (
                      pa.access_type = 'workspace' 
                      OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                      OR (pa.access_type = 'individual' AND pa.target_id = $3)
                  )
            )
        )
    """
    async with pool.acquire() as conn:
        row = await conn.fetchrow(select_query, project_id, user_uuid, user_email)
    if not row:
        raise HTTPException(status_code=404, detail="Project not found")
    row_dict = dict(row)
    project_instance_name = row_dict.get("microproduct_name") or row_dict.get("project_name")
    project_slug = create_slug(project_instance_name)
    component_name = row_dict.get("component_name")
    details_data = row_dict.get("microproduct_content")
    
    # 🔍 BACKEND VIEW LOGGING: What we retrieved from database for view
    logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Raw details_data type: {type(details_data)}")
    
    # Parse the details_data if it's a JSON string
    parsed_details = None
    if details_data:
        if isinstance(details_data, str):
            try:
                # Parse JSON string to dict
                details_dict = json.loads(details_data)
                # Round hours to integers before returning
                details_dict = round_hours_in_content(details_dict)
                parsed_details = details_dict
                logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Parsed from JSON string: {json.dumps(parsed_details, indent=2)}")
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Failed to parse microproduct_content JSON for project {project_id}: {e}")
                parsed_details = None
        else:
            # Already a dict, just round hours
            parsed_details = round_hours_in_content(details_data)
            logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Already dict, after round_hours: {json.dumps(parsed_details, indent=2)}")
    
    # 🔍 BACKEND VIEW RESULT LOGGING
    if parsed_details and 'contentBlocks' in parsed_details:
        image_blocks = [block for block in parsed_details['contentBlocks'] if block.get('type') == 'image']
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Final image blocks for frontend: {json.dumps(image_blocks, indent=2)}")
    else:
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - No contentBlocks in parsed_details or parsed_details is None")
    
    web_link_path = None
    pdf_link_path = None
    
    # Parse lesson_plan_data if it exists and is a JSON string
    lesson_plan_data = row_dict.get("lesson_plan_data")
    if lesson_plan_data and isinstance(lesson_plan_data, str):
        try:
            lesson_plan_data = json.loads(lesson_plan_data)
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"Failed to parse lesson_plan_data JSON for project {project_id}: {e}")
            lesson_plan_data = None
    
    return MicroProductApiResponse(
        name=project_instance_name, slug=project_slug, project_id=project_id,
        design_template_id=row_dict["design_template_id"], component_name=component_name,
        webLinkPath=web_link_path, pdfLinkPath=pdf_link_path, details=parsed_details,
        sourceChatSessionId=row_dict.get("source_chat_session_id"),
        parentProjectName=row_dict.get('project_name'),
        custom_rate=row_dict.get("custom_rate"),
        quality_tier=row_dict.get("quality_tier"),
        is_advanced=row_dict.get("is_advanced"),
        advanced_rates=row_dict.get("advanced_rates"),
        lesson_plan_data=lesson_plan_data
        # folder_id is not in MicroProductApiResponse, but can be added if needed
    )

@app.get("/api/custom/pdf/folder/{folder_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_folder_as_pdf(
    folder_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download all products in a folder as a single PDF, ordered by type and creation date."""
    try:
        # First, verify the folder exists and belongs to the user
        async with pool.acquire() as conn:
            folder_row = await conn.fetchrow(
                """
                SELECT name FROM project_folders 
                WHERE id = $1 AND onyx_user_id = $2;
                """,
                folder_id, onyx_user_id
            )
        if not folder_row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Folder not found for user.")

        folder_name = folder_row['name']
        
        # Get all projects in the folder, ordered by their position in the folder
        async with pool.acquire() as conn:
            projects = await conn.fetch(
                """
                SELECT p.id, p.project_name, p.microproduct_name, p.microproduct_content,
                       p.created_at, dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                ORDER BY p."order" ASC, p.created_at ASC;
                """,
                folder_id, onyx_user_id
            )
        
        if not projects:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="No projects found in folder.")
        
        # Generate individual PDFs for each project
        pdf_paths = []
        project_names = []
        
        for project in projects:
            project_id = project['id']
            project_name = project['microproduct_name'] or project['project_name']
            content_json = project['microproduct_content']
            component_name = project['design_component_name']
            
            # Skip unsupported project types
            if component_name not in ['TextPresentationDisplay', 'TrainingPlanTable']:
                continue
            
            try:
                # Generate PDF for this project using existing logic
                mp_name_for_pdf_context = project_name
                content_json = project['microproduct_content']
                component_name = project['design_component_name']
                data_for_template_render: Optional[Dict[str, Any]] = None
                pdf_template_file: str

                detected_lang_for_pdf = 'ru'  # Default language
                if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
                    detected_lang_for_pdf = content_json.get('detectedLanguage')
                elif mp_name_for_pdf_context:
                    detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
                
                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

                if component_name == 'TextPresentationDisplay':
                    pdf_template_file = "text_presentation_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        data_for_template_render = json.loads(json.dumps(content_json))
                        if not data_for_template_render.get('detectedLanguage'):
                            data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                        
                        # Log content blocks for debugging image issues
                        content_blocks = data_for_template_render.get('contentBlocks', [])
                        image_blocks = [block for block in content_blocks if block.get('type') == 'image']
                        
                        logger.info(f"🖼️ [PDF GEN] Processing {len(content_blocks)} content blocks, {len(image_blocks)} image blocks")
                        for i, block in enumerate(image_blocks):
                            logger.info(f"🖼️ [PDF GEN] Image block {i}: {json.dumps(block, indent=2)}")
                            if hasattr(block, 'keys'):
                                logger.info(f"🖼️ [PDF GEN] Image block {i} keys: {list(block.keys())}")
                            if 'src' in block:
                                logger.info(f"🖼️ [PDF GEN] Image block {i} src: '{block['src']}' (type: {type(block['src'])})")
                            else:
                                logger.info(f"🚨 [PDF GEN] Image block {i} missing 'src' property!")
                                
                    else:
                        data_for_template_render = {
                            "title": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "contentBlocks": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                elif component_name == 'TrainingPlanTable':
                    pdf_template_file = "training_plan_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        try:
                            content_json = round_hours_in_content(content_json)
                            parsed_model = TrainingPlanDetails(**content_json)
                            if parsed_model.detectedLanguage:
                                detected_lang_for_pdf = parsed_model.detectedLanguage
                                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                            
                            data_for_template_render = {
                                'mainTitle': parsed_model.mainTitle,
                                'sections': parsed_model.sections,
                                'detectedLanguage': detected_lang_for_pdf
                            }
                        except Exception as e:
                            logger.error(f"Error parsing training plan for project {project_id}: {e}")
                            data_for_template_render = {
                                "mainTitle": f"Error: {mp_name_for_pdf_context}",
                                "sections": [],
                                "detectedLanguage": detected_lang_for_pdf
                            }
                    else:
                        data_for_template_render = {
                            "mainTitle": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "sections": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                else:
                    continue  # Skip unsupported types
                
                if not isinstance(data_for_template_render, dict):
                    data_for_template_render = {"title": "Error", "contentBlocks": [], "detectedLanguage": "en"}
                
                unique_output_filename = f"folder_export_{folder_id}_project_{project_id}_{uuid.uuid4().hex[:8]}.pdf"
                
                context_for_jinja = {
                    'details': data_for_template_render,
                    'locale': current_pdf_locale_strings,
                    'pdf_context': {
                        'static_images_path': os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
                    }
                }
                
                pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
                if os.path.exists(pdf_path):
                    pdf_paths.append(pdf_path)
                    project_names.append(project_name)
                
            except Exception as e:
                logger.error(f"Error generating PDF for project {project_id}: {e}")
                continue
        
        if not pdf_paths:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs could be generated for projects in folder.")
        
        # Combine PDFs into a single file
        try:
            if PdfMerger is None:
                # If PyPDF2 is not available, return the first PDF as a fallback
                logger.warning("PyPDF2 not available, returning first PDF as fallback")
                if pdf_paths:
                    user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    return FileResponse(
                        path=pdf_paths[0],
                        filename=user_friendly_filename,
                        media_type='application/pdf',
                        headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
                    )
                else:
                    raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs generated and PyPDF2 not available")
            
            merger = PdfMerger()
            
            for pdf_path in pdf_paths:
                merger.append(pdf_path)
            
            combined_pdf_path = f"/tmp/folder_export_{folder_id}_{uuid.uuid4().hex[:8]}.pdf"
            merger.write(combined_pdf_path)
            merger.close()
            
            # Clean up individual PDF files
            for pdf_path in pdf_paths:
                try:
                    os.remove(pdf_path)
                except:
                    pass
            
            user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            return FileResponse(
                path=combined_pdf_path,
                filename=user_friendly_filename,
                media_type='application/pdf',
                headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
            )
            
        except Exception as e:
            logger.error(f"Error combining PDFs for folder {folder_id}: {e}")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to combine PDFs")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating folder PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate folder PDF: {str(e)[:200]}")
    

# Streaming slide deck PDF generation with progress updates
@app.get("/api/custom/pdf/slide-deck/{project_id}/stream")
async def stream_slide_deck_pdf_generation(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Stream slide deck PDF generation with progress updates"""
    from fastapi.responses import StreamingResponse
    import json
    
    async def generate_with_progress():
        try:
            # Get project data (same as the existing endpoint)
            async with pool.acquire() as conn:
                target_row_dict = await conn.fetchrow(
                    """
                    SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                           dt.component_name as design_component_name
                    FROM projects p
                    LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                    WHERE p.id = $1 AND p.onyx_user_id = $2;
                    """,
                    project_id, onyx_user_id
                )
            
            if not target_row_dict:
                yield f"data: {json.dumps({'error': 'Project not found'})}\n\n"
                return

            component_name = target_row_dict.get("design_component_name")
            if component_name != COMPONENT_NAME_SLIDE_DECK:
                yield f"data: {json.dumps({'error': 'This endpoint is only for slide deck projects'})}\n\n"
                return

            content_json = target_row_dict.get('microproduct_content')
            if not content_json or not isinstance(content_json, dict):
                yield f"data: {json.dumps({'error': 'Invalid slide deck content'})}\n\n"
                return

            # Prepare slide deck data
            slide_deck_data = {
                'slides': content_json.get('slides', []),
                'theme': theme or 'dark-purple'
            }

            total_slides = len(slide_deck_data['slides'])
            yield f"data: {json.dumps({'type': 'progress', 'message': f'Starting PDF generation for {total_slides} slides...', 'current': 0, 'total': total_slides})}\n\n"

            mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
            unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
            
            # Generate PDF with regular function and send progress updates
            from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
            
            # Send intermediate progress messages
            yield f"data: {json.dumps({'type': 'progress', 'message': 'Calculating slide dimensions...', 'current': 1, 'total': total_slides})}\n\n"
            
            # Simulate progress for user feedback during long operation
            import asyncio
            
            # Start PDF generation in background and send periodic updates
            pdf_task = asyncio.create_task(generate_slide_deck_pdf_with_dynamic_height(
                slides_data=slide_deck_data['slides'],
                theme=theme,
                output_filename=unique_output_filename,
                use_cache=True
            ))
            
            # Send progress updates while PDF is generating
            progress_step = 0
            max_steps = total_slides * 2  # Simulate steps for dimension calc + generation
            
            while not pdf_task.done():
                await asyncio.sleep(2)  # Update every 2 seconds
                progress_step += 1
                current_progress = min(progress_step, max_steps - 1)
                
                if progress_step <= total_slides:
                    message = f"Calculating dimensions for slide {progress_step}..."
                else:
                    slide_num = progress_step - total_slides
                    message = f"Generating slide {slide_num}..."
                
                yield f"data: {json.dumps({'type': 'progress', 'message': message, 'current': current_progress, 'total': max_steps})}\n\n"
            
            # Wait for PDF generation to complete
            pdf_path = await pdf_task
            
            # Send final progress update
            yield f"data: {json.dumps({'type': 'progress', 'message': 'PDF generation completed!', 'current': max_steps, 'total': max_steps})}\n\n"
                
            # Final success message with download info - FIXED: Return filename instead of URL that triggers regeneration
            user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
            final_message = {
                'type': 'complete',
                'message': 'PDF generation completed successfully!',
                'download_url': f'/pdf/slide-deck/{project_id}/download/{os.path.basename(pdf_path)}?theme={theme}',
                'filename': user_friendly_pdf_filename
            }
            yield f"data: {json.dumps(final_message)}\n\n"
            
        except Exception as e:
            logger.error(f"Error in streaming PDF generation: {e}", exc_info=True)
            error_message = {
                'type': 'error',
                'message': f'PDF generation failed: {str(e)[:200]}'
            }
            yield f"data: {json.dumps(error_message)}\n\n"

    return StreamingResponse(
        generate_with_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

# New endpoint to serve cached PDFs without regeneration
@app.get("/api/custom/pdf/slide-deck/{project_id}/download/{pdf_filename}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_cached_slide_deck_pdf(
    project_id: int,
    pdf_filename: str,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download cached slide deck PDF without regeneration"""
    try:
        # Verify the project exists and user has access
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        # Construct the path to the cached PDF
        from app.services.pdf_generator import PDF_CACHE_DIR
        pdf_path = PDF_CACHE_DIR / pdf_filename
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="PDF file not found. It may have expired or been deleted.")
        
        # Create user-friendly filename
        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
        
        return FileResponse(
            path=str(pdf_path), 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error serving cached slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to serve PDF: {str(e)[:200]}")

# Move slide deck route BEFORE the general route to avoid path conflicts
@app.get("/api/custom/pdf/slide-deck/{project_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_slide_deck_pdf(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download slide deck as multi-page PDF"""
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        if not content_json or not isinstance(content_json, dict):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slide deck content.")

        # Prepare slide deck data for PDF generation
        slide_deck_data = {
            'lessonTitle': content_json.get('lessonTitle', mp_name_for_pdf_context),
            'slides': content_json.get('slides', []),
            'theme': theme,
            'detectedLanguage': content_json.get('detectedLanguage', 'en')
        }

        # Validate slides structure
        if not isinstance(slide_deck_data['slides'], list):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slides structure.")

        logger.info(f"Slide Deck PDF Gen (Project {project_id}): Generating PDF with {len(slide_deck_data['slides'])} slides, theme: {theme}")

        # ✅ NEW: Detailed logging for slide data before PDF generation
        logger.info(f"=== SLIDE DATA ANALYSIS BEFORE PDF GENERATION ===")
        logger.info(f"Project ID: {project_id}")
        logger.info(f"Total slides: {len(slide_deck_data['slides'])}")
        logger.info(f"Theme: {theme}")
        
        # Analyze each slide for big-image-left template
        big_image_left_slides = []
        for i, slide in enumerate(slide_deck_data['slides']):
            if slide.get('templateId') == 'big-image-left':
                big_image_left_slides.append((i, slide))
                logger.info(f"Found big-image-left slide at index {i}")
                
                # Log slide structure
                logger.info(f"  Slide {i} structure:")
                logger.info(f"    templateId: {slide.get('templateId')}")
                logger.info(f"    slideId: {slide.get('slideId')}")
                logger.info(f"    props keys: {list(slide.get('props', {}).keys())}")
                logger.info(f"    metadata keys: {list(slide.get('metadata', {}).keys()) if slide.get('metadata') else 'None'}")
                
                # Log text content
                props = slide.get('props', {})
                logger.info(f"    title: '{props.get('title', 'NOT SET')}'")
                logger.info(f"    subtitle: '{props.get('subtitle', 'NOT SET')}'")
                
                # Log image info without base64 data
                image_path = props.get('imagePath', '')
                if image_path:
                    if image_path.startswith('data:'):
                        logger.info(f"    imagePath: [BASE64 DATA URL - {len(image_path)} characters]")
                    else:
                        logger.info(f"    imagePath: {image_path}")
                else:
                    logger.info(f"    imagePath: NOT SET")
                
                # Log positioning data
                metadata = slide.get('metadata', {})
                element_positions = metadata.get('elementPositions', {})
                logger.info(f"    elementPositions exists: {bool(element_positions)}")
                if element_positions:
                    logger.info(f"    elementPositions keys: {list(element_positions.keys())}")
                    
                    # Check for title and subtitle positions
                    slide_id = slide.get('slideId', 'unknown')
                    title_id = f'draggable-{slide_id}-0'
                    subtitle_id = f'draggable-{slide_id}-1'
                    
                    title_pos = element_positions.get(title_id)
                    subtitle_pos = element_positions.get(subtitle_id)
                    
                    logger.info(f"    title element ID: {title_id}")
                    logger.info(f"    title position: {title_pos}")
                    logger.info(f"    subtitle element ID: {subtitle_id}")
                    logger.info(f"    subtitle position: {subtitle_pos}")
        
        logger.info(f"Total big-image-left slides found: {len(big_image_left_slides)}")
        logger.info(f"=== END SLIDE DATA ANALYSIS ===")

        # Prepare template context
        context_for_jinja = {
            'details': slide_deck_data
        }

        unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Generate PDF using the new dynamic height slide deck generation
        from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
        
        pdf_path = await generate_slide_deck_pdf_with_dynamic_height(
            slides_data=slide_deck_data['slides'],
            theme=theme,
            output_filename=unique_output_filename,
            use_cache=True
        )
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate slide deck PDF: {str(e)[:200]}")


@app.post("/api/custom/pdf/debug/slides", response_class=JSONResponse)
async def debug_slide_generation(
    slides_data: List[Dict[str, Any]],
    theme: Optional[str] = Query("light-modern"),
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Debug endpoint to test individual slide generation and identify problematic slides."""
    try:
        from app.services.pdf_generator import test_all_slides_individually
        
        logger.info(f"Debug slide generation: Testing {len(slides_data)} slides with theme: {theme}")
        
        # Test all slides individually
        summary = await test_all_slides_individually(slides_data, theme)
        
        return JSONResponse(content=summary)
        
    except Exception as e:
        logger.error(f"Error in debug slide generation: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Debug failed: {str(e)[:200]}")


@app.get("/api/custom/pdf/{project_id}/", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf_no_slug(
    project_id: int,
    # all other parameters as in the main function
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    # Just call the main function with a default slug
    return await download_project_instance_pdf(
        project_id=project_id,
        document_name_slug="-",  # or any default value
        parentProjectName=parentProjectName,
        lessonNumber=lessonNumber,
        knowledgeCheck=knowledgeCheck,
        contentAvailability=contentAvailability,
        informationSource=informationSource,
        time=time,
        estCompletionTime=estCompletionTime,
        qualityTier=qualityTier,
        onyx_user_id=onyx_user_id,
        pool=pool,
    )    


@app.get("/api/custom/pdf/{project_id}/{document_name_slug}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf(
    project_id: int,
    document_name_slug: str,
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    quiz: Optional[str] = Query(None),
    onePager: Optional[str] = Query(None),
    videoPresentation: Optional[str] = Query(None),
    lessonPresentation: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    print("OPTIONAL DATA:", parentProjectName, lessonNumber)
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       p.lesson_plan_data, dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        component_name = target_row_dict.get("design_component_name")
        lesson_plan_data = target_row_dict.get("lesson_plan_data")
        data_for_template_render: Optional[Dict[str, Any]] = None
        pdf_template_file: str
        
        # Debug logging for LessonPlan data
        if component_name == COMPONENT_NAME_LESSON_PLAN:
            logger.info(f"PDF Gen (Proj {project_id}): Raw lesson_plan_data from DB: {lesson_plan_data}")
            if lesson_plan_data:
                logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data type: {type(lesson_plan_data)}")
                if isinstance(lesson_plan_data, dict):
                    logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data keys: {list(lesson_plan_data.keys())}")
                elif isinstance(lesson_plan_data, str):
                    logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data is string, length: {len(lesson_plan_data)}")
            else:
                logger.warning(f"PDF Gen (Proj {project_id}): No lesson_plan_data found in target_row_dict")

        detected_lang_for_pdf = 'ru'  # Default language
        if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
            detected_lang_for_pdf = content_json.get('detectedLanguage')
        elif mp_name_for_pdf_context: # Fallback if not in content_json
            detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
        
        # Get the locale strings for the detected language, defaulting to 'en' if not found
        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

        logger.info(f"Project {project_id} PDF Gen: Raw content_json from DB (type: {type(content_json)}). First 1000 chars: {str(content_json)[:1000]}")

        if component_name == COMPONENT_NAME_PDF_LESSON:
            pdf_template_file = "pdf_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                logger.info(f"Project {project_id} PDF Gen (PDF LESSON): Using raw content_json directly for template.")
                data_for_template_render = json.loads(json.dumps(content_json)) 
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model_for_fallback_lang = PdfLessonDetails(**content_json)
                        if parsed_model_for_fallback_lang and parsed_model_for_fallback_lang.detectedLanguage:
                            detected_lang_for_pdf = parsed_model_for_fallback_lang.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                logger.warning(f"Project {project_id} PDF Gen (PDF LESSON): content_json is not a valid dict or is None. Using fallback structure.")
                data_for_template_render = {
                    "lessonTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf}
        elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            pdf_template_file = "text_presentation_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                data_for_template_render = {
                    "textTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_TRAINING_PLAN:
            pdf_template_file = "training_plan_pdf_template.html"
            temp_dumped_dict = None
            if content_json and isinstance(content_json, dict):
                try:
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json type: {type(content_json)}")
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json keys: {list(content_json.keys()) if isinstance(content_json, dict) else 'Not a dict'}")
                    if 'sections' in content_json:
                        logger.info(f"PDF Gen (Proj {project_id}): sections type: {type(content_json['sections'])}, length: {len(content_json['sections']) if isinstance(content_json['sections'], list) else 'Not a list'}")
                    
                    # Round hours to integers before parsing to prevent float validation errors
                    content_json = round_hours_in_content(content_json)
                    
                    parsed_model = TrainingPlanDetails(**content_json)
                    logger.info(f"PDF Gen (Proj {project_id}): Parsed model sections length: {len(parsed_model.sections)}")
                    
                    if parsed_model.detectedLanguage: 
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    
                    # Calculate completion time for each section
                    for section in parsed_model.sections:
                        total_completion_minutes = 0
                        for lesson in section.lessons:
                            if lesson.completionTime:
                                time_str = str(lesson.completionTime).strip()
                                if time_str and time_str != '':
                                    # Extract numeric part using regex to handle all language units (m, м, хв)
                                    import re
                                    numbers = re.findall(r'\d+', time_str)
                                    if numbers:
                                        try:
                                            # If it contains 'h' (hour indicator), convert to minutes
                                            if 'h' in time_str.lower():
                                                total_completion_minutes += int(numbers[0]) * 60
                                            else:
                                                # For minutes (m, м, хв), just use the number
                                                total_completion_minutes += int(numbers[0])
                                        except (ValueError, IndexError):
                                            total_completion_minutes += 5  # Fallback to 5 minutes
                        
                        # Add the calculated completion time to the section
                        section.totalCompletionTime = total_completion_minutes
                    
                    temp_dumped_dict = parsed_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"PDF Gen (Proj {project_id}): Dumped dict sections length: {len(temp_dumped_dict.get('sections', []))}")
                    data_for_template_render = json.loads(json.dumps(temp_dumped_dict))
                    logger.info(f"PDF Gen (Proj {project_id}): Final data sections length: {len(data_for_template_render.get('sections', []))}")
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for TrainingPlan (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
            if data_for_template_render is None:
                 logger.warning(f"Project {project_id} PDF Gen (TRAINING PLAN): data_for_template_render is None. Using fallback.")
                 data_for_template_render = {"mainTitle": f"Content Error: {mp_name_for_pdf_context}", "sections": [], "detectedLanguage": detected_lang_for_pdf}
            
            current_lang_cfg_main = LANG_CONFIG.get(detected_lang_for_pdf, LANG_CONFIG['ru']) # Using main LANG_CONFIG for units
            data_for_template_render['time_unit_singular'] = current_lang_cfg_main.get('TIME_UNIT_SINGULAR', 'h')
            data_for_template_render['time_unit_decimal_plural'] = current_lang_cfg_main.get('TIME_UNIT_DECIMAL_PLURAL', 'h')
            data_for_template_render['time_unit_general_plural'] = current_lang_cfg_main.get('TIME_UNIT_GENERAL_PLURAL', 'h')
        elif component_name == COMPONENT_NAME_VIDEO_LESSON: # Updated logic for Video Lesson
            pdf_template_file = "video_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model = VideoLessonData(**content_json)
                        if parsed_model.detectedLanguage:
                            detected_lang_for_pdf = parsed_model.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass 
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                else: # If language IS in content_json, ensure locale strings match
                    detected_lang_for_pdf = data_for_template_render.get('detectedLanguage', detected_lang_for_pdf)
                    current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

            else:
                data_for_template_render = {
                    "mainPresentationTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "slides": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_QUIZ: # Quiz handling
            pdf_template_file = "quiz_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                try:
                    parsed_model = QuizData(**content_json)
                    if parsed_model.detectedLanguage:
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    data_for_template_render = parsed_model.model_dump(mode='json', exclude_none=True)
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for Quiz (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
                    data_for_template_render = {
                        "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                        "questions": [],
                        "detectedLanguage": detected_lang_for_pdf
                    }
            else:
                data_for_template_render = {
                    "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "questions": [],
                    "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_LESSON_PLAN: # Lesson Plan handling
            pdf_template_file = "lesson_plan_pdf_template.html"
            # Get lesson plan data from the separate lesson_plan_data column
            lesson_plan_data = target_row_dict.get('lesson_plan_data')
            
            # Handle case where lesson_plan_data might be a JSON string
            if lesson_plan_data and isinstance(lesson_plan_data, str):
                try:
                    lesson_plan_data = json.loads(lesson_plan_data)
                    logger.info(f"PDF Gen (Proj {project_id}): Parsed lesson_plan_data from JSON string")
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"PDF Gen (Proj {project_id}): Failed to parse lesson_plan_data JSON: {e}")
                    lesson_plan_data = None
            
            if lesson_plan_data and isinstance(lesson_plan_data, dict):
                data_for_template_render = {
                    "lessonTitle": lesson_plan_data.get('lessonTitle', mp_name_for_pdf_context),
                    "shortDescription": lesson_plan_data.get('shortDescription', ''),
                    "lessonObjectives": lesson_plan_data.get('lessonObjectives', []),
                    "materials": lesson_plan_data.get('materials', []),
                    "contentDevelopmentSpecifications": lesson_plan_data.get('contentDevelopmentSpecifications', []),
                    "suggestedPrompts": lesson_plan_data.get('suggestedPrompts', []),
                    "detectedLanguage": detected_lang_for_pdf
                }
                logger.info(f"PDF Gen (Proj {project_id}): LessonPlan data loaded successfully with {len(lesson_plan_data.get('lessonObjectives', []))} objectives")
            else:
                data_for_template_render = {
                    "lessonTitle": mp_name_for_pdf_context,
                    "shortDescription": "Lesson plan content not available",
                    "lessonObjectives": [],
                    "contentDevelopmentSpecifications": [],
                    "materials": [],
                    "suggestedPrompts": [],
                    "detectedLanguage": detected_lang_for_pdf
                }
                logger.warning(f"PDF Gen (Proj {project_id}): No lesson_plan_data found in database or failed to parse")
        else:
            logger.warning(f"PDF: Unknown component_name '{component_name}' for project {project_id}. Defaulting to simple PDF Lesson structure.")
            pdf_template_file = "pdf_lesson_pdf_template.html" # Or a generic template
            data_for_template_render = {
                "lessonTitle": f"Unknown Content Type: {mp_name_for_pdf_context}",
                "contentBlocks": [{"type":"paragraph", "text":"The content type of this project is not configured for PDF export."}],
                "detectedLanguage": detected_lang_for_pdf
            }

        if not isinstance(data_for_template_render, dict):
             logger.critical(f"Project {project_id} PDF Gen: data_for_template_render is NOT A DICT ({type(data_for_template_render)}) before final context prep.")
             data_for_template_render = {"lessonTitle": "Critical Data Preparation Error", "contentBlocks": [], "detectedLanguage": "en"}
             # Ensure locale is set for critical error case
             current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS['en']


        if isinstance(data_for_template_render, dict):
            logger.info(f"Project {project_id} PDF Gen: Starting deep inspection of data_for_template_render (to be passed as 'details' in template context)...")
            inspect_list_items_recursively(data_for_template_render.get('contentBlocks', []), "data_for_template_render.contentBlocks")

        unique_output_filename = f"{project_id}_{document_name_slug}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Pass the locale strings to the template context
        static_images_abs_path = os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Static images path: {static_images_abs_path}")
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Path exists: {os.path.exists(static_images_abs_path.rstrip('/'))}")
        
        context_for_jinja = {
            'details': data_for_template_render, 
            'locale': current_pdf_locale_strings,
            'parentProjectName': parentProjectName,
            'lessonNumber': lessonNumber,
            'pdf_context': {
                'static_images_path': static_images_abs_path
            }
        }
        
        # 🔍 PDF CONTEXT LOGGING: What we're passing to the template
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Full context keys: {list(context_for_jinja.keys())}")
        
        # Log image blocks without transforming them (let PDF generator handle the transformation)
        if 'details' in context_for_jinja and isinstance(context_for_jinja['details'], dict) and 'contentBlocks' in context_for_jinja['details']:
            content_blocks = context_for_jinja['details']['contentBlocks']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Processing {len(content_blocks)} content blocks")
            
            image_blocks = [block for block in content_blocks if block.get('type') == 'image']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Found {len(image_blocks)} image blocks")
            for img_block in image_blocks:
                original_src = img_block.get('src', '')
                logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Image block src (before PDF generation): {original_src}")
                if original_src.startswith('/static_design_images/'):
                    filename = original_src.replace('/static_design_images/', '')
                    full_path = static_images_abs_path + filename
                    logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Expected file path: {full_path} (exists: {os.path.exists(full_path)})")
        else:
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - No contentBlocks found in details")
        
        # Add column visibility settings for Training Plan PDFs
        if component_name == COMPONENT_NAME_TRAINING_PLAN:
            column_visibility = {
                'knowledgeCheck': knowledgeCheck == '1' if knowledgeCheck else True,
                'contentAvailability': contentAvailability == '1' if contentAvailability else True,
                'informationSource': informationSource == '1' if informationSource else True,
                'estCreationTime': time == '1' if time else True,
                'estCompletionTime': estCompletionTime == '1' if estCompletionTime else True,
                'qualityTier': qualityTier == '1' if qualityTier else False,  # Hidden by default
                'quiz': quiz == '1' if quiz else False,
                'onePager': onePager == '1' if onePager else False,
                'videoPresentation': videoPresentation == '1' if videoPresentation else False,
                'lessonPresentation': lessonPresentation == '1' if lessonPresentation else False,
            }
            context_for_jinja['columnVisibility'] = column_visibility

        logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']: {type(context_for_jinja.get('details'))}")
        if isinstance(context_for_jinja.get('details'), dict) and isinstance(context_for_jinja['details'].get('details'), dict):
            final_cb_source = context_for_jinja['details']['details']
            final_cb_type = type(final_cb_source.get('contentBlocks'))
            logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']['details']['contentBlocks']: {final_cb_type}")
            if isinstance(final_cb_source.get('contentBlocks'), list):
                 for block_idx, block_item_final_check in enumerate(final_cb_source.get('contentBlocks', [])):
                    if isinstance(block_item_final_check, dict) and block_item_final_check.get('type') in ('bullet_list', 'numbered_list'):
                        items_final_check_type = type(block_item_final_check.get('items'))
                        if not isinstance(block_item_final_check.get('items'), list):
                            logger.error(f"Project {project_id} PDF Gen: CRITICAL - 'items' in block_item_final_check for block #{block_idx} is STILL NOT A LIST (type: {items_final_check_type}) just before Jinja render.")
            elif final_cb_type is not None: # if it's not None and not a list
                logger.error(f"Project {project_id} PDF Gen: CRITICAL - context_for_jinja['details']['details']['contentBlocks'] is NOT A LIST (type: {final_cb_type}) just before Jinja render.")

        pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        return FileResponse(path=pdf_path, filename=user_friendly_pdf_filename, media_type='application/pdf', headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"})
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in PDF endpoint for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during PDF generation." if IS_PRODUCTION else f"Error during PDF generation: {str(e)[:200]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/projects/delete-multiple", status_code=status.HTTP_200_OK)
async def delete_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided."})

    project_ids_to_trash = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # If scope is 'all', find all associated lesson projects for any Training Plans
            if delete_request.scope == 'all':
                for project_id in delete_request.project_ids:
                    # Fetch outline project name
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM projects WHERE id=$1 AND onyx_user_id=$2",
                        project_id, onyx_user_id
                    )
                    if not row:
                        continue
                    outline_name: str = row["project_name"]
                    # Treat both 'Training Plan' and 'Course Outline' as outline types
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        continue

                    # Select IDs of all projects whose name equals outline_name OR starts with outline_name + ': '
                    pattern = outline_name + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM projects WHERE onyx_user_id=$1 AND (project_name = $2 OR project_name LIKE $3)",
                        onyx_user_id, outline_name, pattern
                    )
                    for lr in lesson_rows:
                        project_ids_to_trash.add(lr["id"])

            if not project_ids_to_trash:
                 return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            # First, fetch all the data we need to move to trash
            projects_to_trash = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(project_ids_to_trash), onyx_user_id)

            if not projects_to_trash:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_trash:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into trashed_projects with safe values
                    await conn.execute("""
                        INSERT INTO trashed_projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from projects table
                result_status = await conn.execute(
                    "DELETE FROM projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(project_ids_to_trash), onyx_user_id
                )
        
        deleted_count_match = re.search(r"DELETE\s+(\d+)", result_status)
        deleted_count = int(deleted_count_match.group(1)) if deleted_count_match else 0
        
        logger.info(f"User {onyx_user_id} moved IDs {list(project_ids_to_trash)} to trash. Count: {deleted_count}.")
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully moved {deleted_count} project(s) to trash."})

    except Exception as e:
        logger.error(f"Error moving projects to trash for user {onyx_user_id}, IDs {delete_request.project_ids}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while sending projects to trash." if IS_PRODUCTION else f"Database error during trash operation: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Analytics Endpoints ---

@app.get("/api/custom/analytics/dashboard", response_model=Dict[str, Any])
async def get_analytics_dashboard(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get comprehensive analytics dashboard data"""
    import json
    
    print(f"=== DASHBOARD DEBUG: Incoming parameters ===")
    print(f"date_from: {date_from}")
    print(f"date_to: {date_to}")
    print(f"=== END PARAMETERS ===")

    try:
        # DEBUG: Print the latest 10 rows from request_analytics
        async with pool.acquire() as conn:
            debug_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, created_at FROM request_analytics ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: Latest 10 rows from request_analytics ===")
            for row in debug_rows:
                print(dict(row))
            print("=== END DEBUG ===")
            
            # DEBUG: Check specifically for AI parser records
            ai_parser_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at FROM request_analytics WHERE is_ai_parser_request = true ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: AI Parser records from request_analytics ===")
            for row in ai_parser_rows:
                print(dict(row))
            print(f"Total AI parser records found: {len(ai_parser_rows)}")
            print("=== END AI PARSER DEBUG ===")
            
            # DEBUG: Check if the columns exist and have any data
            column_check = await conn.fetch(
                "SELECT column_name, data_type, is_nullable, column_default FROM information_schema.columns WHERE table_name = 'request_analytics' ORDER BY ordinal_position"
            )
            print("=== DEBUG: All request_analytics columns check ===")
            for row in column_check:
                print(dict(row))
            print("=== END COLUMN CHECK ===")
            
            # DEBUG: Check for any records with non-null ai_parser fields
            any_ai_parser_data = await conn.fetch(
                "SELECT id, endpoint, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name FROM request_analytics WHERE is_ai_parser_request IS NOT NULL OR ai_parser_tokens IS NOT NULL OR ai_parser_model IS NOT NULL OR ai_parser_project_name IS NOT NULL ORDER BY created_at DESC LIMIT 5"
            )
            print("=== DEBUG: Any AI parser data ===")
            for row in any_ai_parser_data:
                print(dict(row))
            print(f"Total records with any AI parser data: {len(any_ai_parser_data)}")
            print("=== END ANY AI PARSER DATA ===")
    except Exception as e:
        print(f"DEBUG ERROR: Could not fetch request_analytics: {e}")

    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        print(f"=== DASHBOARD DEBUG: Filter and params ===")
        print(f"where_clause: {where_clause}")
        print(f"params: {params}")
        print(f"=== END FILTER ===")

        async with pool.acquire() as conn:
            # Overall statistics
            stats_query = f"""
                SELECT 
                    COUNT(*) as total_requests,
                    COUNT(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 END) as successful_requests,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as failed_requests,
                    COUNT(CASE WHEN error_message IS NOT NULL THEN 1 END) as error_requests,
                    AVG(response_time_ms) as avg_response_time,
                    MAX(response_time_ms) as max_response_time,
                    MIN(response_time_ms) as min_response_time,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data_transferred,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT endpoint) as unique_endpoints,
                    COUNT(CASE WHEN is_ai_parser_request THEN 1 END) as ai_parser_requests,
                    AVG(ai_parser_tokens) as avg_ai_parser_tokens,
                    MAX(ai_parser_tokens) as max_ai_parser_tokens,
                    MIN(ai_parser_tokens) as min_ai_parser_tokens,
                    SUM(ai_parser_tokens) as total_ai_parser_tokens
                FROM request_analytics
                {where_clause}
            """
            print(f"=== DASHBOARD DEBUG: Stats query ===")
            print(f"Query: {stats_query}")
            print(f"Params: {params}")
            stats_row = await conn.fetchrow(stats_query, *params)
            print(f"Stats result: {dict(stats_row) if stats_row else 'None'}")
            
            # Debug AI parser specific data
            if stats_row:
                print(f"=== AI PARSER DEBUG ===")
                print(f"ai_parser_requests: {stats_row['ai_parser_requests']}")
                print(f"avg_ai_parser_tokens: {stats_row['avg_ai_parser_tokens']}")
                print(f"max_ai_parser_tokens: {stats_row['max_ai_parser_tokens']}")
                print(f"min_ai_parser_tokens: {stats_row['min_ai_parser_tokens']}")
                print(f"total_ai_parser_tokens: {stats_row['total_ai_parser_tokens']}")
                print(f"=== END AI PARSER DEBUG ===")
            
            print(f"=== END STATS ===")
            
            # Status code distribution
            status_query = f"""
                SELECT 
                    status_code,
                    COUNT(*) as count,
                    AVG(response_time_ms) as avg_time
                FROM request_analytics
                {where_clause}
                GROUP BY status_code
                ORDER BY count DESC
            """
            print(f"=== DASHBOARD DEBUG: Status query ===")
            print(f"Query: {status_query}")
            print(f"Params: {params}")
            status_rows = await conn.fetch(status_query, *params)
            print(f"Status rows count: {len(status_rows)}")
            print(f"Status results: {[dict(row) for row in status_rows]}")
            print(f"=== END STATUS ===")
            
            # Top endpoints by request count
            endpoints_query = f"""
                SELECT 
                    endpoint,
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data
                FROM request_analytics
                {where_clause}
                GROUP BY endpoint, method
                ORDER BY request_count DESC
                LIMIT 20
            """
            endpoints_rows = await conn.fetch(endpoints_query, *params)
            
            # Top users by request count
            users_query = f"""
                SELECT 
                    user_id,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    MAX(created_at) as last_request
                FROM request_analytics
                {where_clause}
                {"AND user_id IS NOT NULL" if where_clause else "WHERE user_id IS NOT NULL"}
                GROUP BY user_id
                ORDER BY request_count DESC
                LIMIT 20
            """
            users_rows = await conn.fetch(users_query, *params)
            
            # Hourly distribution
            hourly_query = f"""
                SELECT 
                    EXTRACT(HOUR FROM created_at) as hour,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time
                FROM request_analytics
                {where_clause}
                GROUP BY EXTRACT(HOUR FROM created_at)
                ORDER BY hour
            """
            hourly_rows = await conn.fetch(hourly_query, *params)
            
            # Daily distribution
            daily_query = f"""
                SELECT 
                    DATE(created_at) as date,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY DATE(created_at)
                ORDER BY date DESC
                LIMIT 30
            """
            daily_rows = await conn.fetch(daily_query, *params)
            
            # Method distribution
            method_query = f"""
                SELECT 
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY method
                ORDER BY request_count DESC
            """
            method_rows = await conn.fetch(method_query, *params)
            
            # Recent errors
            errors_query = f"""
                SELECT 
                    id,
                    endpoint,
                    method,
                    status_code,
                    response_time_ms,
                    error_message,
                    user_id,
                    created_at
                FROM request_analytics
                {where_clause}
                {"AND (error_message IS NOT NULL OR status_code >= 400)" if where_clause else "WHERE error_message IS NOT NULL OR status_code >= 400"}
                ORDER BY created_at DESC
                LIMIT 50
            """
            errors_rows = await conn.fetch(errors_query, *params)
            
            # Performance percentiles
            percentile_query = f"""
                SELECT 
                    percentile_cont(0.5) WITHIN GROUP (ORDER BY response_time_ms) as p50,
                    percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95,
                    percentile_cont(0.99) WITHIN GROUP (ORDER BY response_time_ms) as p99
                FROM request_analytics
                {where_clause}
            """
            percentile_row = await conn.fetchrow(percentile_query, *params)

        response_data = {
            "overview": {
                "total_requests": stats_row["total_requests"],
                "successful_requests": stats_row["successful_requests"],
                "failed_requests": stats_row["failed_requests"],
                "error_requests": stats_row["error_requests"],
                "success_rate": round((stats_row["successful_requests"] / stats_row["total_requests"]) * 100, 2) if stats_row["total_requests"] > 0 else 0,
                "avg_response_time": round(stats_row["avg_response_time"], 2) if stats_row["avg_response_time"] else 0,
                "max_response_time": stats_row["max_response_time"],
                "min_response_time": stats_row["min_response_time"],
                "total_data_transferred": stats_row["total_data_transferred"],
                "unique_users": stats_row["unique_users"],
                "unique_endpoints": stats_row["unique_endpoints"],
                "ai_parser_requests": stats_row["ai_parser_requests"] or 0,
                "avg_ai_parser_tokens": round(stats_row["avg_ai_parser_tokens"], 2) if stats_row["avg_ai_parser_tokens"] else 0,
                "max_ai_parser_tokens": stats_row["max_ai_parser_tokens"] or 0,
                "min_ai_parser_tokens": stats_row["min_ai_parser_tokens"] or 0,
                "total_ai_parser_tokens": stats_row["total_ai_parser_tokens"] or 0
            },
            "status_distribution": [{"status_code": row["status_code"], "count": row["count"], "avg_time": round(row["avg_time"], 2) if row["avg_time"] else 0} for row in status_rows],
            "top_endpoints": [{
                "endpoint": row["endpoint"],
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "error_rate": round((row["error_count"] / row["request_count"]) * 100, 2) if row["request_count"] > 0 else 0,
                "total_data": row["total_data"]
            } for row in endpoints_rows],
            "top_users": [{
                "user_id": row["user_id"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "last_request": row["last_request"].isoformat() if row["last_request"] else None
            } for row in users_rows],
            "hourly_distribution": [{"hour": int(row["hour"]), "request_count": row["request_count"], "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0} for row in hourly_rows],
            "daily_distribution": [{
                "date": row["date"].isoformat(),
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in daily_rows],
            "method_distribution": [{
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in method_rows],
            "recent_errors": [{
                "id": row["id"],
                "endpoint": row["endpoint"],
                "method": row["method"],
                "status_code": row["status_code"],
                "response_time_ms": row["response_time_ms"],
                "error_message": row["error_message"],
                "user_id": row["user_id"],
                "created_at": row["created_at"].isoformat()
            } for row in errors_rows],
            "performance_percentiles": {
                "p50": round(percentile_row["p50"], 2) if percentile_row["p50"] else 0,
                "p95": round(percentile_row["p95"], 2) if percentile_row["p95"] else 0,
                "p99": round(percentile_row["p99"], 2) if percentile_row["p99"] else 0
            }
        }
        
        print(f"=== DASHBOARD DEBUG: Final response ===")
        print(f"Response overview: {response_data['overview']}")
        print(f"Status distribution count: {len(response_data['status_distribution'])}")
        print(f"Top endpoints count: {len(response_data['top_endpoints'])}")
        print(f"=== END FINAL RESPONSE ===")
        
        return response_data
    except Exception as e:
        logger.error(f"Error fetching analytics dashboard: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch analytics data")


@app.get("/api/custom/analytics/requests", response_model=List[RequestAnalytics])
async def get_analytics_requests(
    page: int = Query(1, ge=1, description="Page number"),
    limit: int = Query(50, ge=1, le=1000, description="Items per page"),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    user_id: Optional[str] = Query(None, description="Filter by user ID"),
    min_response_time: Optional[int] = Query(None, description="Minimum response time in ms"),
    max_response_time: Optional[int] = Query(None, description="Maximum response time in ms"),
    has_error: Optional[bool] = Query(None, description="Filter by error status"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get paginated request analytics with filters"""
    try:
        # Build WHERE clause
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if user_id:
            param_count += 1
            conditions.append(f"user_id = ${param_count}")
            params.append(user_id)
        
        if min_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms >= ${param_count}")
            params.append(min_response_time)
        
        if max_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms <= ${param_count}")
            params.append(max_response_time)
        
        if has_error is not None:
            if has_error:
                conditions.append("(error_message IS NOT NULL OR status_code >= 400)")
            else:
                conditions.append("(error_message IS NULL AND status_code < 400)")
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        # Calculate offset
        offset = (page - 1) * limit
        
        # Build query
        query = f"""
            SELECT 
                id, endpoint, method, user_id, status_code, 
                response_time_ms, request_size_bytes, response_size_bytes,
                error_message, created_at
            FROM request_analytics
            {where_clause}
            ORDER BY created_at DESC
            LIMIT ${param_count + 1} OFFSET ${param_count + 2}
        """
        params.extend([limit, offset])
        
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
            
            return [RequestAnalytics(**dict(row)) for row in rows]
            
    except Exception as e:
        logger.error(f"Error fetching analytics requests: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch request data")


@app.get("/api/custom/analytics/export")
async def export_analytics_data(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    format: str = Query("csv", description="Export format (csv or json)"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Export analytics data as CSV or JSON"""
    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""

        async with pool.acquire() as conn:
            query = f"""
                SELECT 
                    id, endpoint, method, user_id, status_code, 
                    response_time_ms, request_size_bytes, response_size_bytes,
                    error_message, created_at
                FROM request_analytics
                {where_clause}
                ORDER BY created_at DESC
            """
            rows = await conn.fetch(query, *params)
            
            if format.lower() == "csv":
                import csv
                import io
                
                output = io.StringIO()
                writer = csv.writer(output)
                
                # Write header
                writer.writerow([
                    "ID", "Endpoint", "Method", "User ID", "Status Code",
                    "Response Time (ms)", "Request Size (bytes)", "Response Size (bytes)",
                    "Error Message", "Created At"
                ])
                
                # Write data
                for row in rows:
                    writer.writerow([
                        row["id"], row["endpoint"], row["method"], row["user_id"],
                        row["status_code"], row["response_time_ms"],
                        row["request_size_bytes"], row["response_size_bytes"],
                        row["error_message"], row["created_at"].isoformat()
                    ])
                
                return StreamingResponse(
                    io.BytesIO(output.getvalue().encode()),
                    media_type="text/csv",
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"}
                )
            else:
                # JSON format
                data = [{
                    "id": row["id"],
                    "endpoint": row["endpoint"],
                    "method": row["method"],
                    "user_id": row["user_id"],
                    "status_code": row["status_code"],
                    "response_time_ms": row["response_time_ms"],
                    "request_size_bytes": row["request_size_bytes"],
                    "response_size_bytes": row["response_size_bytes"],
                    "error_message": row["error_message"],
                    "created_at": row["created_at"].isoformat()
                } for row in rows]
                
                return JSONResponse(
                    content=data,
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"}
                )
                
    except Exception as e:
        logger.error(f"Error exporting analytics data: {e}")
        raise HTTPException(status_code=500, detail="Failed to export analytics data")


@app.get("/api/custom/health")
async def health_check():
    return {"status": "healthy"}

HeadlineBlock.model_rebuild()
ParagraphBlock.model_rebuild()
AlertBlock.model_rebuild()
SectionBreakBlock.model_rebuild()
BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

# ========================= Wizard Course Outline Endpoints =========================

class OutlineWizardPreview(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: full markdown string of the current outline so the assistant can apply
    # targeted changes when the user sends an incremental "edit" prompt.
    originalOutline: Optional[str] = None
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths
    theme: Optional[str] = None  # Selected theme from frontend

class OutlineWizardFinalize(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    editedOutline: Dict[str, Any]
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths
    theme: Optional[str] = None  # Selected theme from frontend
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder

_CONTENTBUILDER_PERSONA_CACHE: Optional[int] = None

async def map_smartdrive_paths_to_onyx_files(smartdrive_paths: List[str], user_id: str) -> List[int]:
    """
    Map SmartDrive file paths to corresponding Onyx file IDs.
    
    Args:
        smartdrive_paths: List of SmartDrive file paths to map
        user_id: Onyx user ID for context filtering
    
    Returns:
        List of Onyx file IDs that correspond to the SmartDrive paths
    """
    if not smartdrive_paths:
        return []
    
    try:
        pool = await get_db_pool()
        async with pool.acquire() as connection:
            # Query the smartdrive_imports table to find matching Onyx file IDs
            placeholders = ','.join(f'${i+2}' for i in range(len(smartdrive_paths)))
            query = f"""
                SELECT onyx_file_id, smartdrive_path 
                FROM smartdrive_imports 
                WHERE onyx_user_id = $1 
                AND smartdrive_path IN ({placeholders})
                AND onyx_file_id IS NOT NULL
            """
            
            params = [user_id] + smartdrive_paths
            rows = await connection.fetch(query, *params)
            
            onyx_file_ids = [row['onyx_file_id'] for row in rows]
            mapped_paths = [row['smartdrive_path'] for row in rows]
            
            logger.info(f"[SMARTDRIVE_MAPPING] Mapped {len(onyx_file_ids)} file IDs from {len(smartdrive_paths)} paths for user {user_id}")
            
            # Enhanced debugging: Show what we found vs what we were looking for
            logger.info(f"[SMARTDRIVE_MAPPING] Looking for paths: {smartdrive_paths}")
            logger.info(f"[SMARTDRIVE_MAPPING] Found mappings: {[(row['smartdrive_path'], row['onyx_file_id']) for row in rows]}")
            
            # Log any unmapped paths for debugging
            unmapped_paths = [path for path in smartdrive_paths if path not in mapped_paths]
            if unmapped_paths:
                logger.warning(f"[SMARTDRIVE_MAPPING] Unmapped paths: {unmapped_paths}")
                
                # Show what paths ARE available in the database for this user
                debug_query = "SELECT smartdrive_path FROM smartdrive_imports WHERE onyx_user_id = $1 LIMIT 10"
                debug_rows = await connection.fetch(debug_query, user_id)
                available_paths = [row['smartdrive_path'] for row in debug_rows]
                logger.info(f"[SMARTDRIVE_MAPPING] Sample available paths for user {user_id}: {available_paths[:5]}")
            
            return onyx_file_ids
            
    except Exception as e:
        logger.error(f"[SMARTDRIVE_MAPPING] Error mapping SmartDrive paths to Onyx files: {e}", exc_info=True)
        return []

async def get_contentbuilder_persona_id(cookies: Dict[str, str], use_search_persona: bool = False) -> int:
    """Return persona id of the default ContentBuilder assistant (cached).
    
    Args:
        cookies: Authentication cookies
        use_search_persona: If True, return the Search persona (ID 0) instead of ContentBuilder
    """
    # If Knowledge Base search is requested, use Search persona (ID 0)
    if use_search_persona:
        logger.info(f"[PERSONA_SELECTION] Using Search persona (ID 0) for Knowledge Base search")
        return 0
    
    global _CONTENTBUILDER_PERSONA_CACHE
    if _CONTENTBUILDER_PERSONA_CACHE is not None:
        return _CONTENTBUILDER_PERSONA_CACHE
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.get(f"{ONYX_API_SERVER_URL}/persona", cookies=cookies)
        resp.raise_for_status()
        personas = resp.json()
        # naive: first persona marked is_default_persona and has name 'ContentBuilder'
        for p in personas:
            if p.get("is_default_persona") or "contentbuilder" in p.get("name", "").lower():
                _CONTENTBUILDER_PERSONA_CACHE = p["id"]
                return _CONTENTBUILDER_PERSONA_CACHE
    raise HTTPException(status_code=500, detail="Could not locate ContentBuilder persona")

async def create_onyx_chat_session(persona_id: int, cookies: Dict[str, str]) -> str:
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/create-chat-session",
            json={"persona_id": persona_id, "description": None},
            cookies=cookies,
        )
        resp.raise_for_status()
        data = resp.json()
        return data.get("chat_session_id") or data.get("chatSessionId")

async def stream_chat_message(chat_session_id: str, message: str, cookies: Dict[str, str], enable_search: bool = False) -> str:
    """Send message via Onyx and return the full answer, handling both streaming and non-streaming responses."""
    logger.info(f"[stream_chat_message] chat_id={chat_session_id} len(message)={len(message)} enable_search={enable_search}")

    # Use longer timeout for Knowledge Base searches
    timeout_duration = 600.0 if enable_search else 300.0
    async with httpx.AsyncClient(timeout=timeout_duration) as client:
        # Enable search when needed for Knowledge Base searches
        retrieval_options = {
            "run_search": "always" if enable_search else "never",
            "real_time": False,
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": False,
        }
        # Prefer the non-streaming simplified endpoint if available (much faster and avoids nginx timeouts)
        simple_url = f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api"
        logger.debug(f"[stream_chat_message] POST {simple_url} (preferred) ...")
        try:
            resp = await client.post(simple_url, json=payload, cookies=cookies)
            if resp.status_code == 404:
                raise HTTPStatusError("simple api not found", request=resp.request, response=resp)
        except HTTPStatusError:
            logger.debug("[stream_chat_message] simple-api not available, falling back to generic endpoint")
            # Fallback to the generic endpoint (may stream)
            resp = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message",
                json=payload,
                cookies=cookies,
            )
        logger.info(f"[stream_chat_message] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        # Depending on deployment, Onyx may return SSE stream or JSON.
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[stream_chat_message] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            
            async for line in resp.aiter_lines():
                line_count += 1
                if not line:
                    continue
                    
                if not line.startswith("data: "):
                    logger.debug(f"[stream_chat_message] Skipping non-data line: {line[:100]}")
                    continue
                    
                payload_text = line.removeprefix("data: ").strip()
                if payload_text == "[DONE]":
                    logger.info(f"[stream_chat_message] Received [DONE] signal after {line_count} lines")
                    done_received = True
                    break
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[stream_chat_message] Failed to parse JSON: {payload_text[:100]} - {e}")
                    continue
                    
                if packet.get("answer_piece"):
                    answer_piece = packet["answer_piece"]
                    full_answer += answer_piece
                    if len(full_answer) % 500 == 0:  # Log progress every 500 chars
                        logger.debug(f"[stream_chat_message] Accumulated {len(full_answer)} chars so far...")
                        
            logger.info(f"[stream_chat_message] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}")
            return full_answer
        # Fallback JSON response
        try:
            data = resp.json()
            return data.get("answer") or data.get("answer_citationless") or ""
        except Exception:
            return resp.text.strip()

# ------------ utility to parse markdown outline (very simple) -------------

def _parse_outline_markdown(md: str) -> List[Dict[str, Any]]:
    """Parse the markdown outline produced by the assistant into a lightweight
    list-of-modules representation expected by the wizard UI.

    Enhanced to handle various markdown formats and create intelligent module divisions.
    """
    logger.info(f"[PARSE_OUTLINE] Starting parse with input length: {len(md)}")
    logger.info(f"[PARSE_OUTLINE] Input preview: {md[:200]}{'...' if len(md) > 200 else ''}")
    
    modules: List[Dict[str, Any]] = []
    current: Optional[Dict[str, Any]] = None

    list_item_regex = re.compile(r"^(?:- |\* |\d+\.)")
    _buf: List[str] = []  # buffer for current lesson lines

    def flush_current_lesson(buf: List[str]) -> Optional[str]:
        """Combine buffered lines into a single lesson string."""
        if not buf:
            return None
        return "\n".join(buf)

    lines_processed = 0
    for raw_line in md.splitlines():
        lines_processed += 1
        if not raw_line.strip():
            continue  # skip empty lines

        indent = len(raw_line) - len(raw_line.lstrip())
        line = raw_line.lstrip()

        # Enhanced module detection - look for ## headers OR ### headers OR "Module" patterns
        is_module_header = (
            line.startswith("## ") or 
            (line.startswith("# ") and "module" in line.lower()) or
            line.startswith("**Module") or
            re.match(r"^Module\s+\d+", line, re.IGNORECASE)
        )
        
        if is_module_header:
            # flush any buffered lesson into previous module before switching
            if current:
                last_lesson = flush_current_lesson(_buf)
                if last_lesson:
                    current["lessons"].append(last_lesson)
                _buf = []

            # Extract title from various formats
            title_part = line.lstrip("#* ").strip()
            if ":" in title_part:
                title_part = title_part.split(":", 1)[-1].strip()
            if title_part.lower().startswith("module"):
                # Keep the "Module X:" format if present
                pass
            
            current = {
                "id": f"mod{len(modules) + 1}",
                "title": title_part,
                "totalHours": 0.0,
                "lessons": [],
            }
            modules.append(current)
            logger.debug(f"[PARSE_OUTLINE] Found module: {title_part}")
            continue

        # Lesson detection – only consider top-level list items (indent == 0)
        if current:
            # Note: Total Time lines are now auto-calculated from lesson creation times
            # We still capture them for backward compatibility but will recalculate
            m_time = re.match(r"(?:Total Time|Общее время|Загальний час)\s*:\s*([0-9]+(?:\.[0-9]+)?)", line, re.IGNORECASE)
            if m_time:
                try:
                    # Store the original value for backward compatibility, but we'll recalculate
                    current["originalTotalHours"] = float(m_time.group(1))
                except ValueError:
                    pass  # leave default 0.0 if parsing fails

            if indent == 0 and list_item_regex.match(line):
                # Starting a new top-level lesson → flush previous buffer
                ls_string = flush_current_lesson(_buf) if '_buf' in locals() else None
                if ls_string:
                    current["lessons"].append(ls_string)
                _buf = []  # reset buffer for new lesson

                lesson_title = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if lesson_title.startswith("**") and "**" in lesson_title[2:]:
                    lesson_title = lesson_title.split("**", 2)[1].strip()
                _buf.append(lesson_title)
                logger.debug(f"[PARSE_OUTLINE] Found lesson: {lesson_title}")
                continue
            elif current.get('lessons') is not None and '_buf' in locals():
                # inside a lesson details block (indented)
                if indent > 0:
                    _buf.append(line)
                continue

    # flush buffer after loop to whichever module is active
    if current:
        last_lesson = flush_current_lesson(_buf)
        if last_lesson:
            current["lessons"].append(last_lesson)

    logger.info(f"[PARSE_OUTLINE] After main parsing: {len(modules)} modules found, {lines_processed} lines processed")

    # Enhanced fallback when no module headings present
    if not modules:
        logger.warning(f"[PARSE_OUTLINE] No modules found, using intelligent fallback parsing")
        
        # Collect all lessons first
        all_lessons = []
        for raw_line in md.splitlines():
            if not raw_line.strip():
                continue
            indent = len(raw_line) - len(raw_line.lstrip())
            line = raw_line.lstrip()
            if indent == 0 and list_item_regex.match(line):
                txt = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if txt.startswith("**") and "**" in txt[2:]:
                    txt = txt.split("**", 2)[1].strip()
                all_lessons.append(txt)
        
        # If we have lessons, try to intelligently divide them into modules
        if all_lessons:
            logger.info(f"[PARSE_OUTLINE] Found {len(all_lessons)} lessons for intelligent division")
            
            # Try to determine intended module count from lesson separators or natural breaks
            separator_indices = []
            for i, lesson in enumerate(all_lessons):
                if "---" in lesson or lesson.strip() == "---":
                    separator_indices.append(i)
            
            if separator_indices:
                # Use separator-based division
                logger.info(f"[PARSE_OUTLINE] Using separator-based division with {len(separator_indices)} separators")
                start_idx = 0
                for module_num, sep_idx in enumerate(separator_indices + [len(all_lessons)], 1):
                    if start_idx < sep_idx:
                        module_lessons = [l for l in all_lessons[start_idx:sep_idx] if "---" not in l]
                        if module_lessons:  # Only create module if it has lessons
                            module = {
                                "id": f"mod{module_num}",
                                "title": f"Module {module_num}",
                                "totalHours": 0.0,
                                "lessons": module_lessons
                            }
                            modules.append(module)
                    start_idx = sep_idx + 1
            else:
                # Intelligently divide lessons into reasonable groups (3-5 lessons per module)
                target_lessons_per_module = 4
                num_modules = max(1, min(6, (len(all_lessons) + target_lessons_per_module - 1) // target_lessons_per_module))
                lessons_per_module = len(all_lessons) // num_modules
                remainder = len(all_lessons) % num_modules
                
                logger.info(f"[PARSE_OUTLINE] Dividing {len(all_lessons)} lessons into {num_modules} modules (~{lessons_per_module} lessons each)")
                
                start_idx = 0
                for module_num in range(1, num_modules + 1):
                    # Add one extra lesson to some modules to handle remainder
                    current_module_size = lessons_per_module + (1 if module_num <= remainder else 0)
                    end_idx = start_idx + current_module_size
                    
                    module_lessons = all_lessons[start_idx:end_idx]
                    if module_lessons:  # Only create module if it has lessons
                        module = {
                            "id": f"mod{module_num}",
                            "title": f"Module {module_num}",
                            "totalHours": 0.0,
                            "lessons": module_lessons
                        }
                        modules.append(module)
                    start_idx = end_idx
        
        # Last resort fallback - create single module with all content
        if not modules:
            logger.warning(f"[PARSE_OUTLINE] No lessons found, dumping all non-empty lines")
            tmp_module = {"id": "mod1", "title": "Course Content", "lessons": [], "totalHours": 0.0}
            tmp_module["lessons"] = [l.strip() for l in md.splitlines() if l.strip()]
            modules.append(tmp_module)
        
        logger.info(f"[PARSE_OUTLINE] Intelligent fallback created {len(modules)} modules")

    logger.info(f"[PARSE_OUTLINE] Final result: {len(modules)} modules")
    
    # Auto-calculate total creation time for each module by summing lesson creation times
    for i, module in enumerate(modules):
        logger.info(f"[PARSE_OUTLINE] Module {i+1}: '{module.get('title', 'No title')}' with {len(module.get('lessons', []))} lessons")
        
        # Calculate total creation time from lesson creation times
        total_creation_hours = 0.0
        lessons = module.get('lessons', [])
        
        for lesson in lessons:
            if isinstance(lesson, str):
                # Parse lesson details from markdown format
                lesson_lines = lesson.split('\n')
                for line in lesson_lines:
                    # Look for Time field in markdown format: "- **Time**: 17h" or "- **Время**: 17h" or "- **Час**: 17h"
                    time_match = re.search(r'^\s*-\s*\*\*(?:Time|Время|Час)\*\*:\s*([0-9]+(?:\.[0-9]+)?)h?\s*$', line.strip())
                    if time_match:
                        try:
                            hours = float(time_match.group(1))
                            total_creation_hours += hours
                            logger.debug(f"[PARSE_OUTLINE] Found lesson time: {hours}h")
                        except (ValueError, TypeError):
                            logger.warning(f"[PARSE_OUTLINE] Could not parse lesson time from line: {line}")
        
        # Set the auto-calculated total hours
        module['totalHours'] = total_creation_hours
        logger.info(f"[PARSE_OUTLINE] Module {i+1} auto-calculated totalHours: {total_creation_hours}")

    return modules

# ----------------------- ENDPOINTS ---------------------------------------

@app.post("/api/custom/course-outline/preview")
async def wizard_outline_preview(payload: OutlineWizardPreview, request: Request):
    # EXTENSIVE DEBUG LOGGING: Log all incoming parameters
    logger.info(f"🔍 [STEP 6] Backend received request with payload attributes:")
    for attr in ['prompt', 'modules', 'lessonsPerModule', 'language', 'fromConnectors', 'connectorIds', 'connectorSources', 'selectedFiles', 'fromFiles', 'fileIds', 'folderIds', 'fromText', 'userText', 'fromKnowledgeBase']:
        value = getattr(payload, attr, 'NOT_SET')
        logger.info(f"🔍 [STEP 6] payload.{attr} = {value}")
    
    logger.info(f"🔍 [STEP 6] Raw request body size: {len(await request.body())} bytes")
    logger.info(f"[PREVIEW_START] Course outline preview initiated")
    logger.info(f"[PREVIEW_PARAMS] prompt='{payload.prompt[:50]}...' modules={payload.modules} lessonsPerModule={payload.lessonsPerModule} lang={payload.language}")
    logger.info(f"[PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    # EXTENSIVE DEBUG: Check all connector-related attributes
    logger.info(f"🔍 [CRITICAL] payload.fromConnectors = {getattr(payload, 'fromConnectors', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.connectorIds = {getattr(payload, 'connectorIds', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.connectorSources = {getattr(payload, 'connectorSources', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.selectedFiles = {getattr(payload, 'selectedFiles', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] HasAttr selectedFiles: {hasattr(payload, 'selectedFiles')}")
    logger.info(f"🔍 [CRITICAL] All payload attributes: {[attr for attr in dir(payload) if not attr.startswith('_')]}")
    logger.info(f"[PREVIEW_PARAMS] chatSessionId={payload.chatSessionId}")
    logger.info(f"[PREVIEW_PARAMS] originalOutline length={len(payload.originalOutline) if payload.originalOutline else 0}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")
    logger.info(f"[PREVIEW_AUTH] Cookie value: {cookies[ONYX_SESSION_COOKIE_NAME][:20] if cookies[ONYX_SESSION_COOKIE_NAME] else 'None'}...")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[PREVIEW_CHAT] Creating new chat session")
        try:
            logger.info(f"[PREVIEW_CHAT] Attempting to get contentbuilder persona ID")
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            logger.info(f"[PREVIEW_CHAT] Attempting to create Onyx chat session")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[PREVIEW_CHAT_ERROR] Failed to create chat session: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    logger.info(f"[PREVIEW_PAYLOAD] Building wizard payload")
    wiz_payload = {
        "product": "Course Outline",
        "prompt": payload.prompt,
        "language": payload.language,
    }
    logger.info(f"[PREVIEW_PAYLOAD] Base payload created with product={wiz_payload['product']}, language={wiz_payload['language']}")

    # Add file context if provided
    if payload.fromFiles:
        logger.info(f"[PREVIEW_PAYLOAD] Adding file context: fromFiles=True")
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
            logger.info(f"[PREVIEW_PAYLOAD] Added folderIds: {payload.folderIds}")
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds
            logger.info(f"[PREVIEW_PAYLOAD] Added fileIds: {payload.fileIds}")

    # Add connector context if provided
    if payload.fromConnectors:
        logger.info(f"[PREVIEW_PAYLOAD] Adding connector context: fromConnectors=True")
        wiz_payload["fromConnectors"] = True
        if payload.connectorIds:
            wiz_payload["connectorIds"] = payload.connectorIds
            logger.info(f"[PREVIEW_PAYLOAD] Added connectorIds: {payload.connectorIds}")
        if payload.connectorSources:
            wiz_payload["connectorSources"] = payload.connectorSources
            logger.info(f"[PREVIEW_PAYLOAD] Added connectorSources: {payload.connectorSources}")
        if payload.selectedFiles:
            wiz_payload["selectedFiles"] = payload.selectedFiles
            logger.info(f"[PREVIEW_PAYLOAD] Added selectedFiles: {payload.selectedFiles}")

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        logger.info(f"[PREVIEW_PAYLOAD] Adding text context: fromText=True, textMode={payload.textMode}")
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"[PREVIEW_PAYLOAD] Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"[PREVIEW_PAYLOAD] Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                logger.info(f"[PREVIEW_PAYLOAD] Attempting to create virtual file for large text")
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"[PREVIEW_PAYLOAD] Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"[PREVIEW_PAYLOAD] Failed to create virtual file for large text: {e}", exc_info=True)
                # Fallback to chunking if virtual file creation fails
                logger.info(f"[PREVIEW_PAYLOAD] Falling back to chunking for large text")
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    logger.info(f"[PREVIEW_PAYLOAD] Single chunk fallback: using compression")
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"[PREVIEW_PAYLOAD] Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    logger.info(f"[PREVIEW_PAYLOAD] Multiple chunks fallback: using first chunk with compression")
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"[PREVIEW_PAYLOAD] Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"[PREVIEW_PAYLOAD] Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"[PREVIEW_PAYLOAD] Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"[PREVIEW_PAYLOAD] Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        logger.info(f"[PREVIEW_PAYLOAD] Adding Knowledge Base context: fromKnowledgeBase=True")
        wiz_payload["fromKnowledgeBase"] = True

    if payload.originalOutline:
        logger.info(f"[PREVIEW_PAYLOAD] Adding originalOutline ({len(payload.originalOutline)} chars)")
        wiz_payload["originalOutline"] = payload.originalOutline
    else:
        logger.info(f"[PREVIEW_PAYLOAD] Adding module configuration: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
        wiz_payload.update({
            "modules": payload.modules,
            "lessonsPerModule": payload.lessonsPerModule,
        })

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        logger.info(f"[PREVIEW_PAYLOAD] Decompressing text for assistant")
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"[PREVIEW_PAYLOAD] Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"[PREVIEW_PAYLOAD] Failed to decompress text: {e}", exc_info=True)
            # Continue with original text if decompression fails
    
    logger.info(f"[PREVIEW_PAYLOAD] Final payload keys: {list(wiz_payload.keys())}")
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
    # Force JSON-ONLY preview output for Course Outline to enable immediate parsed preview
    try:
        json_preview_instructions = f"""
CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Course Outline preview, strictly following this example structure:
{DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
"""
        wizard_message = wizard_message + "\n" + json_preview_instructions
    except Exception as e:
        logger.warning(f"[PREVIEW_JSON_INSTR] Failed to append JSON-only preview instructions: {e}")
    logger.info(f"[PREVIEW_PAYLOAD] Created wizard message ({len(wizard_message)} chars)")

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[PREVIEW_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    if payload.selectedFiles:
                        # Combined context: connectors + SmartDrive files
                        logger.info(f"[HYBRID_CONTEXT] Extracting COMBINED context from connectors: {payload.connectorSources} and SmartDrive files: {payload.selectedFiles}")
                        
                        # Extract connector context
                        connector_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                        
                        # Map SmartDrive paths to Onyx file IDs with proper normalization
                        raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                        
                        # Normalize paths to handle URL encoding and character variations
                        smartdrive_file_paths = []
                        for path in raw_paths:
                            # Handle URL encoding
                            try:
                                from urllib.parse import unquote
                                normalized_path = unquote(path)
                            except:
                                normalized_path = path
                            
                            # Handle `+` character variations (some systems use `+` in filenames)
                            # Try both with and without `+` to match database records
                            smartdrive_file_paths.append(normalized_path)
                            if '+' in normalized_path:
                                smartdrive_file_paths.append(normalized_path.replace('+', ''))
                            
                        onyx_user_id = await get_current_onyx_user_id(request)
                        
                        # DEBUG: Log the mapping attempt
                        logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                        logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                        logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                        
                        file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                        
                        if file_ids:
                            logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                            # Extract file context and combine with connector context
                            file_context_from_smartdrive = await extract_file_context_from_onyx(file_ids, [], cookies)
                            
                            # Combine both contexts
                            file_context = f"{connector_context}\n\n=== ADDITIONAL CONTEXT FROM SELECTED FILES ===\n\n{file_context_from_smartdrive}"
                        else:
                            logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths, using only connector context")
                            file_context = connector_context
                    else:
                        # For connector-based filtering only, extract context from specific connectors
                        logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                        file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromConnectors and payload.selectedFiles:
                    # SmartDrive files only (no connectors)
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from SmartDrive files only: {payload.selectedFiles}")
                    
                    # Map SmartDrive paths to Onyx file IDs
                    raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                    
                    # Normalize paths to handle URL encoding and character variations
                    smartdrive_file_paths = []
                    for path in raw_paths:
                        # Try multiple variations to match database records
                        from urllib.parse import unquote, quote
                        import re
                        
                        candidates = []
                        # Base variants
                        candidates.append(path)
                        try:
                            decoded_path = unquote(path)
                            candidates.append(decoded_path)
                        except:
                            decoded_path = path
                        try:
                            encoded_path = quote(path, safe='/')
                            candidates.append(encoded_path)
                        except:
                            pass
                        if ' ' in path:
                            candidates.append(path.replace(' ', '%20'))
                        if '%20' in path:
                            candidates.append(path.replace('%20', ' '))
                        
                        # Derived variants: trim spaces before dot and collapse multiple spaces
                        derived = []
                        for c in list(candidates):
                            trimmed_dot = re.sub(r"\s+\.", ".", c)
                            if trimmed_dot != c:
                                derived.append(trimmed_dot)
                            collapsed = re.sub(r"\s{2,}", " ", c)
                            if collapsed != c:
                                derived.append(collapsed)
                        candidates.extend(derived)
                        
                        # Encode derived variants as well
                        for c in list(candidates):
                            try:
                                enc = quote(c, safe='/')
                                candidates.append(enc)
                            except:
                                pass
                        
                        # Deduplicate while preserving order
                        seen = set()
                        for c in candidates:
                            if c and c not in seen:
                                seen.add(c)
                                smartdrive_file_paths.append(c)
                    
                    onyx_user_id = await get_current_onyx_user_id(request)
                    
                    # DEBUG: Log the mapping attempt
                    logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                    logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                    logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                    
                    file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                    
                    if file_ids:
                        logger.info(f"[HYBRID_CONTEXT] Successfully mapped {len(file_ids)} SmartDrive files to Onyx file IDs: {file_ids}")
                        # Extract context from the mapped file IDs
                        file_context = await extract_file_context_from_onyx(file_ids, [], cookies)
                    else:
                        logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths: {smartdrive_file_paths}")
                        file_context = f"Selected files: {', '.join(raw_paths)}\nNote: These files are from SmartDrive but could not be mapped to indexed content. Please ensure the files have been properly imported and indexed."
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Course Outline"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache full raw outline for later finalize step
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

                if not assistant_reply.strip():
                    logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
                    error_packet = {"type": "error", "message": "No content received from AI service"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    return

                # Try JSON-first parsing for immediate structured preview
                def _extract_json_text(s: str) -> str:
                    try:
                        start = s.find('{')
                        end = s.rfind('}')
                        if start != -1 and end != -1 and start < end:
                            return s[start:end+1]
                        return s
                    except Exception:
                        return s

                modules_preview = []
                try:
                    json_text = _extract_json_text(assistant_reply)
                    parsed = json.loads(json_text)
                    sections = parsed.get('sections', []) if isinstance(parsed, dict) else []
                    for i, sec in enumerate(sections):
                        title = (sec.get('title') if isinstance(sec, dict) else str(sec)) or ''
                        lessons_src = sec.get('lessons', []) if isinstance(sec, dict) else []
                        lessons = []
                        for ls in lessons_src:
                            if isinstance(ls, dict):
                                lessons.append(ls.get('title') or '')
                            else:
                                lessons.append(str(ls))
                        modules_preview.append({
                            "id": f"mod{i+1}",
                            "title": title,
                            "totalHours": (sec.get('totalHours') if isinstance(sec, dict) else 0.0) or 0.0,
                            "lessons": lessons,
                        })
                    logger.info(f"[PREVIEW_JSON_PARSE] Parsed modules from JSON: {len(modules_preview)}")
                except Exception as e:
                    logger.warning(f"[PREVIEW_JSON_PARSE] Failed to parse JSON preview ({e}); falling back to markdown parser")
                    logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
                    modules_preview = _parse_outline_markdown(assistant_reply)
                    
                    # Validate the parsed result meets basic requirements
                    validation_passed = True
                    validation_messages = []
                    # Check if we have reasonable number of modules (not just 1 with many lessons)
                    if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                        validation_passed = False
                        validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
                    # Check if we have expected module count (if specified in payload)
                    expected_modules = getattr(payload, 'modules', None)
                    if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                        validation_passed = False
                        validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
                    if not validation_passed:
                        logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                        logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
                    else:
                        logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
                
                # Send completion packet with the parsed outline
                logger.info(f"[PREVIEW_DONE] Creating completion packet")
                done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
                yield (json.dumps(done_packet) + "\n").encode()
                logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[PREVIEW_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            # Force JSON-ONLY preview output for Course Outline in direct OpenAI path
            enhanced_wizard_message = wizard_message + """

CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Course Outline preview, strictly following this example structure:
""" + DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM + """
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
"""
            
            try:
                async for chunk_data in stream_openai_response(enhanced_wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            except Exception as e:
                logger.error(f"[OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        if not assistant_reply.strip():
            logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
            error_packet = {"type": "error", "message": "No content received from AI service"}
            yield (json.dumps(error_packet) + "\n").encode()
            return

        # Try JSON-first parsing for immediate structured preview
        def _extract_json_text(s: str) -> str:
            try:
                start = s.find('{')
                end = s.rfind('}')
                if start != -1 and end != -1 and start < end:
                    return s[start:end+1]
                return s
            except Exception:
                return s

        modules_preview = []
        try:
            json_text = _extract_json_text(assistant_reply)
            parsed = json.loads(json_text)
            sections = parsed.get('sections', []) if isinstance(parsed, dict) else []
            for i, sec in enumerate(sections):
                title = (sec.get('title') if isinstance(sec, dict) else str(sec)) or ''
                lessons_src = sec.get('lessons', []) if isinstance(sec, dict) else []
                lessons = []
                for ls in lessons_src:
                    if isinstance(ls, dict):
                        lessons.append(ls.get('title') or '')
                    else:
                        lessons.append(str(ls))
                modules_preview.append({
                    "id": f"mod{i+1}",
                    "title": title,
                    "totalHours": (sec.get('totalHours') if isinstance(sec, dict) else 0.0) or 0.0,
                    "lessons": lessons,
                })
            logger.info(f"[PREVIEW_JSON_PARSE] Parsed modules from JSON: {len(modules_preview)}")
        except Exception as e:
            logger.warning(f"[PREVIEW_JSON_PARSE] Failed to parse JSON preview ({e}); falling back to markdown parser")
            logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
            modules_preview = _parse_outline_markdown(assistant_reply)
            
            # Validate the parsed result meets basic requirements
            validation_passed = True
            validation_messages = []
            # Check if we have reasonable number of modules (not just 1 with many lessons)
            if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                validation_passed = False
                validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
            # Check if we have expected module count (if specified in payload)
            expected_modules = getattr(payload, 'modules', None)
            if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                validation_passed = False
                validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
            if not validation_passed:
                logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
            else:
                logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
        
                # Send completion packet with the parsed outline
        logger.info(f"[PREVIEW_DONE] Creating completion packet")
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        yield (json.dumps(done_packet) + "\n").encode()
        logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
        return
                

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )

async def _ensure_training_plan_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_TRAINING_PLAN)
        if row:
            return row["id"]
        # create minimal template
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Training Plan", DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM, "Training Plan", COMPONENT_NAME_TRAINING_PLAN
        )
        return row["id"]

# After you get the parsed content from the AI parser, insert it like this:
async def insert_ai_audit_onepager_to_db(
    pool: asyncpg.Pool,
    onyx_user_id: str,
    project_name: str,
    microproduct_content: dict,
    chat_session_id: str = None
) -> int:
    """Insert AI-audit one-pager into database with correct template and component"""
    
    # First, ensure we have a Text Presentation template
    template_id = await _ensure_text_presentation_template(pool)
    
    insert_query = """
    INSERT INTO projects (
        onyx_user_id, project_name, product_type, microproduct_type,
        microproduct_name, microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, NOW(), $9)
    RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id;
    """
    
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            insert_query,
            onyx_user_id,
            project_name,
            "Text Presentation",  # product_type
            "Text Presentation",  # microproduct_type
            project_name,  # microproduct_name
            microproduct_content,  # parsed content from AI parser
            template_id,  # design_template_id (from _ensure_text_presentation_template)
            chat_session_id,  # source_chat_session_id
            None,  # folder_id - AI audit doesn't support folder assignment yet
        )
    
    if not row:
        raise HTTPException(status_code=500, detail="Failed to create AI-audit one-pager project entry.")
    
    return row["id"]


async def create_audit_folder(pool, onyx_user_id, company_name):
    async with pool.acquire() as conn:
        query = """
        INSERT INTO project_folders (onyx_user_id, name)
        VALUES ($1, $2)
        RETURNING id;
        """
        row = await conn.fetchrow(query, onyx_user_id, f"AI-Аудит: {company_name}")
        return row["id"]
    

async def assign_projects_to_folder(pool, folder_id, project_ids):
    async with pool.acquire() as conn:
        await conn.executemany(
            "UPDATE projects SET folder_id = $1 WHERE id = $2",
            [(folder_id, pid) for pid in project_ids]
        )


def set_progress(job_id, message):
    AI_AUDIT_PROGRESS.setdefault(job_id, []).append(message)


@app.get("/api/custom/ai-audit/progress")
async def get_audit_progress(jobId: str):
    return {"messages": AI_AUDIT_PROGRESS.get(jobId, [])}


async def create_audit_onepager(duckduckgo_summary, example_text_path, payload):
    try:
        with open(example_text_path, encoding="utf-8") as f:
            example_text = f.read()
    except Exception as e:
        logger.error(f"[AI-Audit] Error reading example: {e}")
        example_text = "(Example not found)"
    if not duckduckgo_summary or duckduckgo_summary.strip() == "" or duckduckgo_summary.strip().startswith("(Нет релевантных данных"):
        duck_info = "(DuckDuckGo не дал информации. Используй только анкету.)"
    else:
        duck_info = duckduckgo_summary
    prompt = f"""
    Сгенерируй AI-аудит (one-pager) для компании, используя ВСЮ информацию из анкеты пользователя и результаты интернет-исследования (DuckDuckGo).

    ТВОЯ ЗАДАЧА:
    - СКОПИРУЙ ПРИМЕР НИЖЕ МАКСИМАЛЬНО ТОЧНО, ДОСЛОВНО.
    - Используй те же секции, тот же порядок, ту же длину, те же заголовки, те же таблицы, те же списки, те же абзацы, то же форматирование.
    - Если в примере есть таблица — в твоём ответе тоже должна быть таблица с тем же количеством строк и столбцов.
    - Если в примере 5 секций — в твоём ответе тоже должно быть 5 секций с теми же названиями и в том же порядке.
    - ЗАМЕНИ только данные, относящиеся к компании, на новые из анкеты и поиска.
    - НЕ сокращай, НЕ добавляй новых секций, НЕ меняй структуру, НЕ меняй форматирование, НЕ меняй количество строк, НЕ меняй количество столбцов в таблицах.
    - Если не уверен — лучше скопируй больше из примера, чем меньше.
    - Твой ответ должен быть на 90% буквальной копией примера, только с новыми данными.
    - Если DuckDuckGo не дал информации, используй только анкету.
    - Если в примере есть таблица, твоя таблица должна быть с тем же количеством строк и столбцов, только с новыми данными.
    - Если в примере есть абзац, твой ответ должен содержать такой же абзац на том же месте.
    - Если в примере есть список, твой ответ должен содержать такой же список с тем же количеством пунктов.
    - Не меняй ни одну структуру, даже если кажется, что это не подходит — просто замени данные.

    ---
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {payload.companyName}
    - Описание компании: {payload.companyDesc}
    - Сайт компании: {payload.companyWebsite}
    - Количество сотрудников: {payload.employees}
    - Франшиза: {payload.franchise}
    - Проблемы онбординга: {payload.onboardingProblems}
    - Документы: {', '.join(payload.documents)} {payload.documentsOther}
    - Приоритеты: {', '.join(payload.priorities)} {payload.priorityOther}

    ---
    РЕЗУЛЬТАТЫ ИНТЕРНЕТ-ИССЛЕДОВАНИЯ (DuckDuckGo):
    {duck_info}

    ---
    СКОПИРУЙ ПРИМЕР НИЖЕ, ЗАМЕНИВ ТОЛЬКО ДАННЫЕ О КОМПАНИИ:
    {example_text}

    Ответь только текстом one-pager по этим правилам, без пояснений.
    """
    logger.info(f"[AI-Audit] Final prompt (first 500 chars): {prompt[:500]}")
    client = get_openai_client()
    try:
        # Set a longer timeout for the OpenAI call
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Ты профессиональный AI-ассистент для генерации обучающих one-pager документов. Строго следуй правилам ContentBuilder.ai."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4096,
            temperature=0.2,
            timeout=httpx.Timeout(180.0)  # 180 seconds
        )
    except Exception as e:
        logger.error(f"[AI-Audit] OpenAI generation error: {e}", exc_info=True)
        return {"error": f"Ошибка генерации AI-аудита: {e}"}
    
    result = response.choices[0].message.content
    logger.info(f"[AI-Audit] OpenAI result (first 500 chars): {result[:500]}")

    with open("custom_assistants/content_builder_ai.txt", encoding="utf-8") as f:
        assistant_instructions = f.read()

    # Compose the parsing prompt
    parsing_prompt = (
        f"{assistant_instructions}\n\n"
        "WIZARD_REQUEST\n"
        + json.dumps({
            "product": "Text Presentation",
            "microproduct": "One-Pager",
            "prompt": "Приведи этот текст к нужному формату one-pager для ContentBuilder.ai",
            "language": "ru",
            "fromText": True,
            "textMode": "context",
            "userText": result,
            "strict": True,
            "parseMode": "onepager"
        }, ensure_ascii=False)
    )

    # Call OpenAI again (use gpt-4o-mini or your preferred model)
    parsed_response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Ты профессиональный AI-ассистент для парсинга продуктов ContentBuilder.ai."},
            {"role": "user", "content": parsing_prompt}
        ],
        max_tokens=4096,
        temperature=0.1,
        timeout=httpx.Timeout(120.0)
    )
    parsed_markdown = parsed_response.choices[0].message.content

    parsed_json = await parse_ai_response_with_llm(
        ai_response=parsed_markdown,
        project_name=payload.companyName,
        target_model=TextPresentationDetails,  # or your one-pager model
        default_error_model_instance=TextPresentationDetails(textTitle="Parse error", contentBlocks=[]),
        dynamic_instructions=f"""
        You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
        This product is for general text like introductions, goal descriptions, etc.
        Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

        **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

        **Global Fields:**
        1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
            - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
            - Extract ONLY the title part (the last part after the last "**")
            - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
            - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
            - Do NOT include the course name or "Text Presentation" label in the title
            - If no clear pattern is found, use the first meaningful title or heading
        2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
        3.  `detectedLanguage` (string): e.g., "en", "ru".

        **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

        1.  **`type: "headline"`**
            * `level` (integer):
                * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
            * `text` (string): Headline text.
            * `iconName` (string, optional): Based on level and context as described above.
            * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

        2.  **`type: "paragraph"`**
            * `text` (string): Full paragraph text.
            * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

        3.  **`type: "bullet_list"`**
            * `items` (array of `ListItem`): Can be strings or other nested content blocks.
            * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

        4.  **`type: "numbered_list"`**
            * `items` (array of `ListItem`):
                * Can be simple strings for basic numbered points.
                * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                    * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                    * The `items` of this *inner* `bullet_list` should then be:
                        1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                        2. A `paragraph` block (for the main descriptive text).
                        3. Optionally, another `paragraph` block with `isRecommendation: true`.
                * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

        5.  **`type: "table"`**
            * `headers` (array of strings): The column headers for the table.
            * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
            * `caption` (string, optional): A short description or title for the table, if present in the source text.
            * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


        6.  **`type: "alert"`**
            *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
            *   `title` (string, optional): The title of the alert.
            *   `text` (string): The body text of the alert.
            *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

        7.  **`type: "section_break"`**
            * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

        **General Parsing Rules & Icon Names:**
        * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
        * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
        * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
        * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
        * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
        * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

        Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
            • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
            • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
            • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

        Return ONLY the JSON object.
        """,
        target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
    )
    return parsed_json

    
@app.post("/api/custom/ai-audit/generate")
async def generate_ai_audit_onepager(payload: AiAuditQuestionnaireRequest, request: Request, background_tasks: BackgroundTasks, pool: asyncpg.Pool = Depends(get_db_pool)):
    job_id = str(uuid.uuid4())
    set_progress(job_id, "Starting AI-Audit generation...")
    background_tasks.add_task(_run_audit_generation, payload, request, pool, job_id)
    return {"jobId": job_id}


async def _run_audit_generation(payload, request, pool, job_id):
    try:
        set_progress(job_id, "Researching company info...")
        duckduckgo_summary = await serpapi_company_research(payload.companyName, payload.companyDesc, payload.companyWebsite)
        logger.info(f"[AI-Audit] SERPAPI summary: {duckduckgo_summary[:300]}")

        set_progress(job_id, "Generating first one-pager...")
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/First-one-pager.txt", payload)

        onyx_user_id = await get_current_onyx_user_id(request)

        # After you get the parsed content from the AI parser:
        project_id = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {payload.companyName}",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id}")

        set_progress(job_id, "Researching open positions...")
        positions = extract_open_positions_from_table(parsed_json)

        results = []
        for position in positions:
            set_progress(job_id, f"Generating onboarding for '{position.get('Позиция', 'New Position')}'")
            project = await generate_and_finalize_course_outline_for_position(
                payload.companyName, position, onyx_user_id, pool, request
            )
            results.append(project)

        logger.info(f"[AI-Audit] Created {len(results)} course outlines for positions")

        set_progress(job_id, "Generating closing one-pager...")
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/Second-one-pager.txt", payload)

        # After you get the parsed content from the AI parser:
        project_id_2 = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {payload.companyName} (2)",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id_2}")

        set_progress(job_id, "Finalizing and saving to folder...")
        all_project_ids = [project_id] + [p.id for p in results] + [project_id_2]

        # 1. Create a new folder
        folder_id = await create_audit_folder(pool, onyx_user_id, payload.companyName)

        # 2. Assign all projects to this folder
        await assign_projects_to_folder(pool, folder_id, all_project_ids)

        set_progress(job_id, "AI-Audit complete!")
        logger.info(f"[AI-Audit] Finished the AI-Audit Generation")
        return {
            "id": project_id,
            "id_2": project_id_2,
            "name": f"AI-Аудит: {payload.companyName}",
            "folderId": folder_id
        }
    
    except Exception as e:
        set_progress(job_id, f"Error: {str(e)}")
    

def extract_open_positions_from_table(parsed_json):
    """
    Extracts open positions from a TableBlock in parsed_json.contentBlocks.
    Returns a list of dicts, one per position, with keys matching the table headers.
    Removes trailing '*' from header keys.
    """
    def clean_key(key):
        # Remove all trailing and leading '*' and whitespace
        return key.strip().rstrip("*").lstrip("*").strip()

    for block in getattr(parsed_json, "contentBlocks", []):
        if getattr(block, "type", None) == "table":
            headers = getattr(block, "headers", [])
            rows = getattr(block, "rows", [])
            # Normalize header names for easier matching
            header_map = {clean_key(h).lower(): i for i, h in enumerate(headers)}
            print("HEADER MAP:", header_map)
            if "позиция" in header_map:
                positions = []
                for row in rows:
                    position = {clean_key(headers[i]): row[i] for i in range(min(len(headers), len(row)))}
                    positions.append(position)

                return positions
    return []


async def generate_and_finalize_course_outline_for_position(
    company_name: str,
    position: dict,
    onyx_user_id: str,
    pool,
    request: Request,
    language: str = "ru"):
    # 1. Build the prompt for the LLM
    wizard_request = {
        "product": "Course Outline",
        "prompt": (
            f"Создай курс аутлайн 'Онбординг для должности {position['Позиция']}' для новых сотрудников этой должности в компании '{company_name}'. \n"
            f"Структура должна охватывать все аспекты работы сотрудника на этой должности в данной среде. Не включай аспекты работы других должностей, только то, что касается должности '{position['Позиция']}'. \n"
        ),
        "modules": 4,
        "lessonsPerModule": "5-7",
        "language": language
    }
    # Convert to JSON string for the LLM
    prompt = json.dumps(wizard_request, ensure_ascii=False)

    outline_text = ""
    async for chunk_data in stream_openai_response(prompt):
        if chunk_data.get("type") == "delta":
            outline_text += chunk_data["text"]
        elif chunk_data.get("type") == "error":
            raise Exception(f"OpenAI error: {chunk_data['text']}")


    # 4. Finalize/save the project (reuse add_project_to_custom_db)
    template_id = await _ensure_training_plan_template(pool)

    project_data = ProjectCreateRequest(
        projectName=f"Онбординг: {position['Позиция']}",
        design_template_id=template_id,
        microProductName=f"Онбординг: {position['Позиция']}",
        aiResponse=outline_text,
        chatSessionId=None
    )
    project_db_candidate = await add_project_to_custom_db(
        project_data=project_data,
        onyx_user_id=onyx_user_id,
        pool=pool
    )

    try:
        async with pool.acquire() as conn:
            # Convert Pydantic model to dictionary for processing
            content = project_db_candidate.microproduct_content.model_dump(mode='json', exclude_none=True) if project_db_candidate.microproduct_content else {}
            
            if isinstance(content, dict) and content.get("sections"):
                sections = content["sections"]
                updated_sections = []
                
                for section in sections:
                    if isinstance(section, dict) and section.get("lessons"):
                        # Ensure each lesson has proper hours value and completionTime (default to 1 hour and 5m if missing)
                        updated_lessons = []
                        for lesson in section["lessons"]:
                            if isinstance(lesson, dict):
                                # Set default hours if missing or zero
                                if lesson.get("hours", 0) == 0:
                                    lesson["hours"] = 1
                                # Set default completionTime if missing
                                if not lesson.get("completionTime"):
                                    lesson["completionTime"] = "5m"
                                # Ensure all required lesson fields are present
                                lesson.setdefault("check", {"type": "none", "text": ""})
                                # Set content coverage based on source
                                source = lesson.get("source", "Create from scratch")
                                content_available = {"type": "yes", "text": "0%"} if source == "Create from scratch" else {"type": "yes", "text": "0%"}
                                lesson.setdefault("contentAvailable", content_available)
                                lesson.setdefault("source", source)
                                # Populate recommended content types if missing
                                try:
                                    existing_flags = {
                                        "presentation": False,
                                        "one-pager": False,
                                        "quiz": False,
                                        "video-lesson": False,
                                    }
                                    recommendations = analyze_lesson_content_recommendations(
                                        lesson.get("title", ""),
                                        lesson.get("quality_tier") or section.get("quality_tier") or content.get("quality_tier"),
                                        existing_flags
                                    )
                                    lesson.setdefault("recommended_content_types", recommendations)
                                    # Update completionTime from recommendations
                                    try:
                                        lesson["completionTime"] = compute_completion_time_from_recommendations(recommendations.get("primary", []))
                                        # Also generate completion_breakdown for advanced mode support
                                        primary = recommendations.get("primary", [])
                                        ranges = {
                                            'one-pager': (2,3),
                                            'presentation': (5,10),
                                            'quiz': (5,7),
                                            'video-lesson': (2,5),
                                        }
                                        breakdown = {}
                                        total_m = 0
                                        for p in primary:
                                            r = ranges.get(p)
                                            if r:
                                                mid = int(round((r[0]+r[1])/2))
                                                breakdown[p] = mid
                                                total_m += mid
                                        if total_m > 0:
                                            lesson['completion_breakdown'] = breakdown
                                    except Exception:
                                        lesson.setdefault("completionTime", "5m")
                                except Exception:
                                    pass
                                updated_lessons.append(lesson)
                            else:
                                # If lesson is just a string, convert to proper structure
                                updated_lessons.append({
                                    "title": str(lesson),
                                    "check": {"type": "none", "text": ""},
                                    "contentAvailable": {"type": "yes", "text": "0%"},
                                    "source": "Create from scratch",
                                    "hours": 1,
                                    "recommended_content_types": analyze_lesson_content_recommendations(str(lesson), content.get("quality_tier"), {"presentation": False, "one-pager": False, "quiz": False, "video-lesson": False})
                                })
                        
                        # Calculate total hours from lesson hours
                        total_hours = sum(lesson.get("hours", 0) for lesson in updated_lessons)
                        
                        # Update section with calculated total hours and set autoCalculateHours to true
                        updated_section = {
                            **section,
                            "lessons": updated_lessons,
                            "totalHours": total_hours,
                            "autoCalculateHours": True
                        }
                        # Ensure section has proper ID if missing
                        if not updated_section.get("id"):
                            updated_section["id"] = f"№{len(updated_sections) + 1}"
                        updated_sections.append(updated_section)
                    else:
                        updated_sections.append(section)
                
                # Update the project with recalculated totals and ensure mainTitle and detectedLanguage
                if updated_sections:
                    updated_content = {
                        **content, 
                        "sections": updated_sections,
                        "mainTitle": content.get("mainTitle") or f"Онбординг: {position['Позиция']}",
                        "detectedLanguage": content.get("detectedLanguage") or language
                    }
                    await conn.execute(
                        """
                        UPDATE projects
                        SET microproduct_content = $1::jsonb
                        WHERE id = $2
                        """,
                        json.dumps(updated_content), project_db_candidate.id
                    )
                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
    except Exception as e:
        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")


    return project_db_candidate


@app.post("/api/custom/course-outline/finalize")
async def wizard_outline_finalize(payload: OutlineWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Get user ID and deduct credits for course outline finalization
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        credits_needed = calculate_product_credits("course_outline")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Course outline finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for course outline finalization")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for course outline: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    # Ensure we have a chat session id (needed both for cache lookup and possible assistant fallback)
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        # Check if this is a Knowledge Base search request
        use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
        persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Helper: check whether the user made ANY changes (structure or content)
    def _any_changes_made(orig_modules: List[Dict[str, Any]], edited: Dict[str, Any]) -> bool:
        try:
            edited_sections = edited.get("sections") or edited.get("modules") or []
            
            # Debug logging to understand the data structures
            logger.info(f"Comparing changes: orig_modules count={len(orig_modules)}, edited_sections count={len(edited_sections)}")
            
            # Check structural changes first (modules/lessons added/removed)
            if len(orig_modules) != len(edited_sections):
                logger.info(f"Structural change detected: module count changed from {len(orig_modules)} to {len(edited_sections)}")
                return True
            
            # Check for content changes (titles modified)
            for i, (o, e) in enumerate(zip(orig_modules, edited_sections)):
                # Compare module titles
                orig_title = str(o.get("title", "")).strip()
                edited_title = str(e.get("title", "")).strip() if isinstance(e, dict) else str(e).strip()
                
                logger.debug(f"Module {i}: comparing titles '{orig_title}' vs '{edited_title}'")
                if orig_title != edited_title:
                    logger.info(f"Module title change detected at index {i}: '{orig_title}' -> '{edited_title}'")
                    return True
                
                # Compare lesson structure and content
                orig_lessons = o.get("lessons", [])
                edited_lessons = e.get("lessons", []) if isinstance(e, dict) else []
                
                if len(orig_lessons) != len(edited_lessons):
                    logger.info(f"Lesson count change detected in module {i}: {len(orig_lessons)} -> {len(edited_lessons)}")
                    return True
                
                # Compare individual lesson titles
                for j, (ol, el) in enumerate(zip(orig_lessons, edited_lessons)):
                    # Handle different lesson formats
                    if isinstance(ol, dict):
                        orig_lesson = str(ol.get("title", ol.get("name", ""))).strip()
                    else:
                        orig_lesson = str(ol).strip()
                    
                    if isinstance(el, dict):
                        edited_lesson = str(el.get("title", el.get("name", ""))).strip()
                    else:
                        edited_lesson = str(el).strip()
                    
                    logger.debug(f"Module {i}, Lesson {j}: comparing '{orig_lesson}' vs '{edited_lesson}'")
                    if orig_lesson != edited_lesson:
                        logger.info(f"Lesson change detected in module {i}, lesson {j}: '{orig_lesson}' -> '{edited_lesson}'")
                        return True
            
            logger.info("No changes detected - outline is identical")
            return False
        except Exception as e:
            # On any parsing issue assume changes were made so we use assistant
            logger.warning(f"Error during change detection (assuming changes made): {e}")
            return True



    # ---------- 1) Decide strategy ----------
    raw_outline_cached = OUTLINE_PREVIEW_CACHE.get(chat_id)
    
    # Debug cache lookup
    logger.info(f"DEBUG: Cache lookup for chat_id='{chat_id}', found cached outline: {bool(raw_outline_cached)}")
    if raw_outline_cached:
        logger.info(f"DEBUG: Cached outline preview (first 200 chars): {raw_outline_cached[:200]}...")
    else:
        logger.info(f"DEBUG: Available cache keys: {list(OUTLINE_PREVIEW_CACHE.keys())}")
    
    if raw_outline_cached:
        # Parse cached preview - try JSON first, fallback to markdown
        try:
            # Try to parse as JSON (new format)
            cached_json = json.loads(raw_outline_cached.strip())
            if isinstance(cached_json, dict) and "sections" in cached_json:
                # Convert JSON sections to modules format for comparison
                parsed_orig = []
                for section in cached_json["sections"]:
                    parsed_orig.append({
                        "id": section.get("id", ""),
                        "title": section.get("title", ""),
                        "lessons": [lesson.get("title", "") for lesson in section.get("lessons", [])],
                        "totalHours": section.get("totalHours", 0)
                    })
                logger.info(f"[FINALIZE_CACHE] Parsed {len(parsed_orig)} modules from JSON preview")
            else:
                # Fallback to markdown parsing
                parsed_orig = _parse_outline_markdown(raw_outline_cached)
                logger.info(f"[FINALIZE_CACHE] Used markdown fallback, parsed {len(parsed_orig)} modules")
        except (json.JSONDecodeError, KeyError) as e:
            # Fallback to markdown parsing for old format
            parsed_orig = _parse_outline_markdown(raw_outline_cached)
            logger.info(f"[FINALIZE_CACHE] JSON parse failed ({e}), used markdown fallback: {len(parsed_orig)} modules")
        
        # Debug: Log the data structures being compared
        logger.info(f"DEBUG: parsed_orig structure: {json.dumps(parsed_orig, indent=2)[:500]}...")
        logger.info(f"DEBUG: payload.editedOutline structure: {json.dumps(payload.editedOutline, indent=2)[:500]}...")
        
        any_changes = _any_changes_made(parsed_orig, payload.editedOutline)
        
        if not any_changes:
            # NO CHANGES: Use direct parser path (fastest)
            use_direct_parser = True
            use_assistant_then_parser = False
            logger.info("No changes detected - using direct parser path")
        else:
            # CHANGES DETECTED: Use assistant first, then parser
            use_direct_parser = False
            use_assistant_then_parser = True
            logger.info("Changes detected - using assistant + parser path")
    else:
        # No cached data available - use assistant + parser path
        use_direct_parser = False
        use_assistant_then_parser = True
        logger.info("No cached outline - using assistant + parser path")

    # ---------- 2) DIRECT PARSER PATH: No changes made, use cached data directly ----------
    if use_direct_parser:
        direct_path_project_id = None  # Track project ID for cleanup if needed
        try:
            # Use cached outline directly since no changes were made
            template_id = await _ensure_training_plan_template(pool)
            
            # Extract project name from JSON or markdown
            project_name_detected = None
            try:
                # Try JSON first
                cached_json = json.loads(raw_outline_cached.strip())
                if isinstance(cached_json, dict) and "mainTitle" in cached_json:
                    project_name_detected = cached_json["mainTitle"]
                    logger.info(f"[DIRECT_PATH] Extracted project name from JSON: {project_name_detected}")
            except (json.JSONDecodeError, KeyError):
                pass
            
            # Fallback to markdown extraction or payload prompt
            if not project_name_detected:
                project_name_detected = _extract_project_name_from_markdown(raw_outline_cached) or payload.prompt
                logger.info(f"[DIRECT_PATH] Using fallback project name: {project_name_detected}")
            
            logger.info(f"Direct parser path: Using cached outline with {len(raw_outline_cached)} characters")
            
            # Build source context from payload
            source_context_type, source_context_data = build_source_context(payload)
            
            project_request = ProjectCreateRequest(
                projectName=project_name_detected,
                design_template_id=template_id,
                microProductName=None,
                aiResponse=raw_outline_cached,
                chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                folder_id=int(payload.folderId) if payload.folderId else None,
                source_context_type=source_context_type,
                source_context_data=source_context_data,
            )
            onyx_user_id = await get_current_onyx_user_id(request)

            project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
            direct_path_project_id = project_db_candidate.id  # Store for potential cleanup
            
            logger.info(f"Direct parser path: Created project {direct_path_project_id}")
            logger.info(f"Direct parser path: Project content type: {type(project_db_candidate.microproduct_content)}")
            
            # Check if content was parsed successfully
            content_valid = False
            if project_db_candidate.microproduct_content:
                if hasattr(project_db_candidate.microproduct_content, "sections"):
                    sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                    content_valid = len(sections) > 0
                    logger.info(f"Direct parser path: Found {len(sections)} sections in parsed content")
                else:
                    logger.warning(f"Direct parser path: Content does not have sections attribute")
            else:
                logger.warning(f"Direct parser path: microproduct_content is None")

            # --- Patch theme into DB if provided (only for TrainingPlan components) ---
            if payload.theme and content_valid:
                async with pool.acquire() as conn:
                    design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                    if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                        await conn.execute(
                            """
                            UPDATE projects
                            SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                            WHERE id = $2
                            """,
                            payload.theme, project_db_candidate.id
                        )
                        row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                        if row_patch and row_patch["microproduct_content"] is not None:
                            project_db_candidate.microproduct_content = row_patch["microproduct_content"]

            # --- Recalculate module total hours after creation ---
            if content_valid and project_db_candidate.microproduct_content:
                try:
                    async with pool.acquire() as conn:
                        content = project_db_candidate.microproduct_content
                        if isinstance(content, dict) and content.get("sections"):
                            sections = content["sections"]
                            updated_sections = []
                            
                            for section in sections:
                                if isinstance(section, dict) and section.get("lessons"):
                                    # Calculate total hours from lesson hours
                                    total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                    # Update section with calculated total hours and set autoCalculateHours to true
                                    updated_section = {
                                        **section,
                                        "totalHours": total_hours,
                                        "autoCalculateHours": True
                                    }
                                    updated_sections.append(updated_section)
                                else:
                                    updated_sections.append(section)
                            
                            # Update the project with recalculated totals
                            if updated_sections:
                                updated_content = {**content, "sections": updated_sections}
                                await conn.execute(
                                    """
                                    UPDATE projects
                                    SET microproduct_content = $1::jsonb
                                    WHERE id = $2
                                    """,
                                    json.dumps(updated_content), project_db_candidate.id
                                )
                                logger.info(f"Direct parser path: Recalculated module total hours for project {project_db_candidate.id}")
                except Exception as e:
                    logger.warning(f"Direct parser path: Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

            # Success when we have valid parsed content
            if content_valid:
                logger.info(f"Direct parser path successful for project {direct_path_project_id}")
                logger.debug(f'Full content for project {direct_path_project_id}: {project_db_candidate.microproduct_content}')
                return JSONResponse(content={"type": "done", "id": project_db_candidate.id})
            else:
                # Direct parser path validation failed - clean up the created project and fall back to assistant
                logger.warning(f"Direct parser path validation failed for project {direct_path_project_id} - LLM parsing likely failed")
                logger.warning(f"Content details: {project_db_candidate.microproduct_content}")
                try:
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
                
                # Fall back to assistant path
                logger.info("Falling back to assistant + parser path due to direct parser failure")
                use_direct_parser = False
                use_assistant_then_parser = True
                
        except Exception as direct_e:
            # Clean up any project created during direct parser path failure
            if direct_path_project_id:
                logger.warning(f"Direct parser path failed with project {direct_path_project_id}, attempting cleanup...")
                try:
                    onyx_user_id = await get_current_onyx_user_id(request)
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
            
            logger.error(f"Direct parser path failed with error: {direct_e}")
            
            # If another concurrent request already started creation we patiently wait for it instead of kicking off assistant again
            if isinstance(direct_e, HTTPException) and direct_e.status_code == status.HTTP_429_TOO_MANY_REQUESTS:
                logger.info("wizard_outline_finalize detected in-progress creation. Waiting for completion…")
                max_wait_sec = 900  # 15 minutes
                poll_every_sec = 1
                waited = 0
                while waited < max_wait_sec:
                    async with pool.acquire() as conn:
                        if chat_id:
                            # Prefer locating the project by the wizard chat_session_id (unique identifier per outline wizard run)
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                                uuid.UUID(chat_id),
                            )
                        else:
                            # Fallback to the previous behaviour when we have no chat_id information available
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2 ORDER BY created_at DESC LIMIT 1",
                                onyx_user_id,
                                payload.prompt,
                            )
                    if row and row["microproduct_content"] is not None:
                        return JSONResponse(content={"type": "done", "id": row["id"]})
                    await asyncio.sleep(poll_every_sec)
                    waited += poll_every_sec
                logger.warning("wizard_outline_finalize waited too long for existing creation – giving up")
            else:
                logger.warning(f"wizard_outline_finalize direct parser path failed – will use assistant path. Details: {direct_e}")
            
            # Fall back to assistant path
            use_direct_parser = False
            use_assistant_then_parser = True

    # ---------- 3) ASSISTANT + PARSER PATH: Process changes with assistant, then parse ----------
    if use_assistant_then_parser:
        # Before starting assistant path, check if a project was already created successfully for this session
        if chat_id:
            try:
                async with pool.acquire() as conn:
                    existing_row = await conn.fetchrow(
                        "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                        uuid.UUID(chat_id),
                    )
                    if existing_row and existing_row["microproduct_content"] is not None:
                        # Check if the existing project has valid content
                        try:
                            content = existing_row["microproduct_content"]
                            if isinstance(content, dict) and content.get("sections"):
                                logger.info(f"Found existing valid project {existing_row['id']} for chat session, returning it")
                                return JSONResponse(content={"type": "done", "id": existing_row["id"]})
                        except Exception:
                            pass  # Continue with assistant path if content validation fails
            except Exception as e:
                logger.warning(f"Failed to check for existing project: {e}")
        
        # Build wizard payload for assistant path - different structure for finalization
        # CRITICAL: Don't send modules/lessonsPerModule during finalization as they conflict
        # with user edits and cause the AI to ignore the actual edited structure
        wiz_payload = {
            "product": "Course Outline",
            "action": "finalize",
            "prompt": payload.prompt,
            "language": payload.language,
            "editedOutline": payload.editedOutline,
        }
        
        # Only add structural parameters if no user edits exist (fallback case)
        edited_sections = payload.editedOutline.get("sections", payload.editedOutline.get("modules", [])) if payload.editedOutline else []
        user_edit_module_count = len(edited_sections)
        
        if not payload.editedOutline or user_edit_module_count == 0:
            logger.info(f"[FINALIZE_PAYLOAD] No user edits found, adding structural parameters as fallback: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
            wiz_payload["modules"] = payload.modules
            wiz_payload["lessonsPerModule"] = payload.lessonsPerModule
        else:
            logger.info(f"[FINALIZE_PAYLOAD] User edits present ({user_edit_module_count} modules) - omitting conflicting structural parameters (original: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}) to preserve user structure")
            # Log the first few modules to understand the structure
            for i, section in enumerate(edited_sections[:3]):
                if isinstance(section, dict):
                    section_title = section.get("title", "Unknown")
                    section_lessons = len(section.get("lessons", []))
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: '{section_title}' ({section_lessons} lessons)")
                else:
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: {type(section)} - {str(section)[:50]}...")
            if user_edit_module_count > 3:
                logger.info(f"[FINALIZE_PAYLOAD] ... and {user_edit_module_count - 3} more modules")

        # Add file context if provided
        if payload.fromFiles:
            wiz_payload["fromFiles"] = True
            if payload.folderIds:
                wiz_payload["folderIds"] = payload.folderIds
            if payload.fileIds:
                wiz_payload["fileIds"] = payload.fileIds

        # Add text context if provided
        if payload.fromText and payload.userText:
            wiz_payload["fromText"] = True
            wiz_payload["textMode"] = payload.textMode
            wiz_payload["userText"] = payload.userText

        wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
        logger.info(f"[FINALIZE_PAYLOAD] Final wizard message structure: {list(wiz_payload.keys())}")
        logger.info(f"[FINALIZE_PAYLOAD] Wizard message length: {len(wizard_message)} chars")

        async def streamer():
            assistant_reply: str = ""
            last_send = asyncio.get_event_loop().time()
            chunks_received = 0

            # Use longer timeout for large text processing to prevent AI memory issues
            timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
            logger.info(f"[FINALIZE_OPENAI_STREAM] Starting OpenAI finalization streamer with timeout: {timeout_duration} seconds")
            logger.info(f"[FINALIZE_OPENAI_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
            
            try:
                # Use OpenAI streaming for finalization instead of Onyx
                logger.info(f"[FINALIZE_OPENAI_STREAM] ✅ USING OPENAI DIRECT STREAMING for finalization")
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[FINALIZE_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[FINALIZE_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return

                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[FINALIZE_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[FINALIZE_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
            except Exception as e:
                logger.error(f"[FINALIZE_OPENAI_STREAM_ERROR] Error in OpenAI finalization streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

            # Cache full raw outline for later finalize step
            if chat_id:
                OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

            # Create the project using the assistant response
            try:
                template_id = await _ensure_training_plan_template(pool)
                project_name_detected = _extract_project_name_from_markdown(assistant_reply) or payload.prompt
                
                logger.info(f"Assistant + parser path: Creating project with {len(assistant_reply)} characters")
                
                # Build source context from payload
                source_context_type, source_context_data = build_source_context(payload)
                
                project_request = ProjectCreateRequest(
                    projectName=project_name_detected,
                    design_template_id=template_id,
                    microProductName=None,
                    aiResponse=assistant_reply,
                    chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                    folder_id=int(payload.folderId) if payload.folderId else None,
                    source_context_type=source_context_type,
                    source_context_data=source_context_data,
                )
                onyx_user_id = await get_current_onyx_user_id(request)

                project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
                
                logger.info(f"Assistant + parser path: Created project {project_db_candidate.id}")
                
                # Check if content was parsed successfully
                content_valid = False
                if project_db_candidate.microproduct_content:
                    if hasattr(project_db_candidate.microproduct_content, "sections"):
                        sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                        content_valid = len(sections) > 0
                        logger.info(f"Assistant + parser path: Found {len(sections)} sections in parsed content")
                    else:
                        logger.warning(f"Assistant + parser path: Content does not have sections attribute")
                else:
                    logger.warning(f"Assistant + parser path: microproduct_content is None")

                # --- Patch theme into DB if provided (only for TrainingPlan components) ---
                if payload.theme and content_valid:
                    async with pool.acquire() as conn:
                        design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                        if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                            await conn.execute(
                                """
                                UPDATE projects
                                SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                                WHERE id = $2
                                """,
                                payload.theme, project_db_candidate.id
                            )
                            row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                            if row_patch and row_patch["microproduct_content"] is not None:
                                project_db_candidate.microproduct_content = row_patch["microproduct_content"]

                # --- Recalculate module total hours after creation ---
                if content_valid and project_db_candidate.microproduct_content:
                    try:
                        async with pool.acquire() as conn:
                            content = project_db_candidate.microproduct_content
                            if isinstance(content, dict) and content.get("sections"):
                                sections = content["sections"]
                                updated_sections = []
                                
                                for section in sections:
                                    if isinstance(section, dict) and section.get("lessons"):
                                        # Calculate total hours from lesson hours
                                        total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                        # Update section with calculated total hours and set autoCalculateHours to true
                                        updated_section = {
                                            **section,
                                            "totalHours": total_hours,
                                            "autoCalculateHours": True
                                        }
                                        updated_sections.append(updated_section)
                                    else:
                                        updated_sections.append(section)
                                
                                # Update the project with recalculated totals
                                if updated_sections:
                                    updated_content = {**content, "sections": updated_sections}
                                    await conn.execute(
                                        """
                                        UPDATE projects
                                        SET microproduct_content = $1::jsonb
                                        WHERE id = $2
                                        """,
                                        json.dumps(updated_content), project_db_candidate.id
                                    )
                                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
                    except Exception as e:
                        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

                if content_valid:
                    logger.info(f"Assistant + parser path successful for project {project_db_candidate.id}")
                    # Send completion packet with the project ID
                    done_packet = {"type": "done", "id": project_db_candidate.id}
                    yield (json.dumps(done_packet) + "\n").encode()
                else:
                    logger.error(f"Assistant + parser path: Project {project_db_candidate.id} created but content validation failed")
                    # Clean up the failed project
                    try:
                        async with pool.acquire() as conn:
                            await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", project_db_candidate.id, onyx_user_id)
                        logger.info(f"Successfully cleaned up failed assistant + parser project {project_db_candidate.id}")
                    except Exception as cleanup_e:
                        logger.error(f"Failed to cleanup assistant + parser project {project_db_candidate.id}: {cleanup_e}")
                    
                    # Send error packet
                    error_packet = {"type": "error", "message": "Failed to parse the generated outline"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    
            except Exception as create_e:
                logger.error(f"Assistant + parser path: Failed to create project: {create_e}")
                # Send error packet
                error_packet = {"type": "error", "message": f"Failed to create project: {str(create_e)}"}
                yield (json.dumps(error_packet) + "\n").encode()

        return StreamingResponse(streamer(), media_type="application/json")

@app.post("/api/custom/course-outline/init-chat")
async def init_course_outline_chat(request: Request):
    """Pre-create Chat Session & persona so subsequent preview calls are faster."""
    cookies = request.cookies
    # For init-chat, we'll use the default ContentBuilder persona
    # The actual persona selection will happen in the preview endpoint based on the request payload
    persona_id = await get_contentbuilder_persona_id(cookies)
    chat_id = await create_onyx_chat_session(persona_id, cookies)
    return {"personaId": persona_id, "chatSessionId": chat_id}

# ======================= End Wizard Section ==============================

# === Wizard Outline helpers & cache ===
OUTLINE_PREVIEW_CACHE: Dict[str, str] = {}  # chat_session_id -> raw markdown outline
QUIZ_PREVIEW_CACHE: Dict[str, str] = {}  # chat_session_id -> raw quiz content

def _apply_title_edits_to_outline(original_md: str, edited_outline: Dict[str, Any]) -> str:
    """Return a markdown outline that reflects the *structure* provided in
    `edited_outline` (modules & lessons) while preserving the original header.

    Instead of trying to patch-in titles at the old positions, we rebuild each
    module's lesson list from scratch. This guarantees correctness even when
    lessons were inserted, removed or reordered in the UI.
    """

    # ---- 1. Normalise `edited_outline` ----
    sections: Optional[List[Any]] = None
    if isinstance(edited_outline, dict):
        sections = edited_outline.get("sections") or edited_outline.get("modules")
    elif isinstance(edited_outline, list):
        sections = edited_outline

    if not sections:
        return original_md  # nothing to merge -> return original untouched

    # ---- 2. Preserve the very first non-empty line (usually Universal Header) ----
    header_line = None
    for line in original_md.splitlines():
        if line.strip():
            header_line = line.rstrip()
            break

    out_lines: List[str] = []
    if header_line:
        out_lines.append(header_line)
        out_lines.append("")  # spacer line to match original formatting

    # ---- 3. Rebuild modules & lessons ----
    for idx, sec in enumerate(sections):
        # Module title
        title = sec.get("title") if isinstance(sec, dict) else str(sec)
        out_lines.append(f"## Module {idx + 1}: {title.strip()}")

        # Lessons
        lessons_list: List[Any] = []
        if isinstance(sec, dict):
            lessons_list = sec.get("lessons", []) or []
        elif isinstance(sec, list):
            lessons_list = sec

        for ls in lessons_list:
            ls_raw = ls.get("title") if isinstance(ls, dict) else str(ls)
            if not isinstance(ls_raw, str):
                ls_raw = str(ls_raw)

            segments = ls_raw.split("\n")
            main_line = segments[0].strip()
            out_lines.append(f"- **{main_line}**")

            for extra in segments[1:]:
                extra = extra.rstrip()
                if extra:
                    out_lines.append(f"  {extra}")

        out_lines.append("")  # blank line between modules for readability

    return "\n".join(out_lines).rstrip()  # drop trailing newline

# ------------------- Utility: extract project name from AI markdown header -------------------

_HEADER_RE = re.compile(r"^\*\*(?P<name>[^*]+)\*\*\s*:\s*\*\*.+")


def _extract_project_name_from_markdown(md: str) -> Optional[str]:
    """Return the first **Project Name** element found in the Universal Product Header.

    The header line looks like:
        **Project Name** : **Course Outline** : **Course Outline**
    We return "Project Name" (stripped).
    """
    if not md:
        return None
    first_line = md.splitlines()[0].strip()
    m = _HEADER_RE.match(first_line)
    if m:
        return m.group("name").strip()
    return None

# --- PDF Lesson helper and wizard endpoints ---

# Ensure a design template for PDF Lesson exists, return its ID
async def _ensure_pdf_lesson_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_PDF_LESSON)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "PDF Lesson", DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM, "PDF Lesson", COMPONENT_NAME_PDF_LESSON,
        )
        return row["id"]

# Ensure a design template for Slide Deck exists, return its ID
async def _ensure_slide_deck_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_SLIDE_DECK)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Slide Deck", DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM, "Slide Deck", COMPONENT_NAME_SLIDE_DECK,
        )
        return row["id"]


# Ensure a design template for Video Lesson Presentation exists, return its ID
async def _ensure_video_lesson_presentation_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Video Lesson Presentation", DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM, "Video Lesson Presentation", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION,
        )
        return row["id"]


# Ensure a design template for Text Presentation exists, return its ID
async def _ensure_text_presentation_template(pool: asyncpg.Pool) -> int:
    """Ensure text presentation template exists and return its ID"""
    try:
        # Check if text presentation template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Text Presentation' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create text presentation template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Text Presentation Template",
            "Create a comprehensive text presentation with clear structure, engaging content, and professional formatting.",
            "Text Presentation",
            COMPONENT_NAME_TEXT_PRESENTATION,
            "/text-presentation.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring text presentation template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure text presentation template")


# -------- Lesson Presentation (PDF Lesson) Wizard ---------

class LessonWizardPreview(BaseModel):
    outlineProjectId: Optional[int] = None  # Parent Training Plan project id
    lessonTitle: Optional[str] = None      # Specific lesson to generate, optional when prompt-based
    lengthRange: Optional[str] = None      # e.g. "400-500 words"
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources


class LessonWizardFinalize(BaseModel):
    outlineProjectId: Optional[int] = None
    lessonTitle: str
    lengthRange: Optional[str] = None
    aiResponse: str                        # User-edited markdown / plain text
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder


@app.post("/api/custom/lesson-presentation/preview")
async def wizard_lesson_preview(payload: LessonWizardPreview, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Ensure chat session
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        # Check if this is a Knowledge Base search request
        use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
        persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Build wizard request for assistant persona
    is_video_lesson = payload.productType == "video_lesson_presentation"
    wizard_dict: Dict[str, Any] = {
        "product": "Video Lesson Slides Deck" if is_video_lesson else "Slides Deck",
        "action": "preview",
        "language": payload.language,
        "slidesCount": payload.slidesCount or 5,
        "generateVoiceover": is_video_lesson,  # Flag to indicate voiceover generation
        "theme": payload.theme or "dark-purple",  # Use selected theme or default
    }
    if payload.outlineProjectId is not None:
        wizard_dict["outlineProjectId"] = payload.outlineProjectId
        
        # Fetch outline name to include in wizard request
        try:
            # Get current user ID to fetch the outline
            onyx_user_id = await get_current_onyx_user_id(request)
            
            # Fetch outline name from database
            async with pool.acquire() as conn:
                outline_row = await conn.fetchrow(
                    "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                    payload.outlineProjectId, onyx_user_id
                )
                if outline_row:
                    wizard_dict["outlineName"] = outline_row["project_name"]
        except Exception as e:
            logger.warning(f"Failed to fetch outline name for project {payload.outlineProjectId}: {e}")
            # Continue without outline name - not critical for preview
            
    if payload.lessonTitle:
        wizard_dict["lessonTitle"] = payload.lessonTitle
    if payload.prompt:
        wizard_dict["prompt"] = payload.prompt

    wizard_dict["importantRules"] = "IMPORTANT: DO NOT CREATE CONCLUSION SLIDES. ONLY CREATE EDUCATIONAL SLIDES. DO NOT CREATE SLIDES WITH TITLES LIKE 'Conclusion', 'Summary', 'Wrap-Up', 'Thank You', 'Further Reading', 'Additional Resources', 'Questions', 'Open Floor for Questions', 'Feedback'. DO NOT MAKE SECOND SLIDE BE A TITLE SLIDE. DO NOT USE 'content-slide' SLIDES"
    wizard_dict["importantRules"] += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.
    """
    
    # Add file context if provided
    if payload.fromFiles:
        wizard_dict["fromFiles"] = True
        if payload.folderIds:
            wizard_dict["folderIds"] = payload.folderIds
        if payload.fileIds:
            wizard_dict["fileIds"] = payload.fileIds

    # Add connector context if provided
    if payload.fromConnectors:
        wizard_dict["fromConnectors"] = True
        if payload.connectorIds:
            wizard_dict["connectorIds"] = payload.connectorIds
        if payload.connectorSources:
            wizard_dict["connectorSources"] = payload.connectorSources

    # Add text context if provided - use compression for large texts
    if payload.fromText and payload.userText:
        wizard_dict["fromText"] = True
        wizard_dict["textMode"] = payload.textMode
        
        if len(payload.userText) > TEXT_SIZE_THRESHOLD:
            # Compress large text to reduce payload size
            compressed_text = compress_text(payload.userText)
            wizard_dict["userText"] = compressed_text
            wizard_dict["textCompressed"] = True
            logger.info(f"Using compressed text for large lesson content ({len(payload.userText)} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for smaller content
            wizard_dict["userText"] = payload.userText
            wizard_dict["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wizard_dict["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for lesson generation")

    # Decompress text if it was compressed
    if wizard_dict.get("textCompressed") and wizard_dict.get("userText"):
        try:
            decompressed_text = decompress_text(wizard_dict["userText"])
            wizard_dict["userText"] = decompressed_text
            wizard_dict["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed lesson text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress lesson text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wizard_dict) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wizard_dict.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[LESSON_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    # For connector-based filtering, extract context from specific connectors
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                    file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wizard_dict.get("virtualFileId"):
                        file_ids_list.append(wizard_dict["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wizard_dict['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                chunks_received = 0
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Video Lesson Presentation" if is_video_lesson else "Lesson Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[LESSON_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                chunks_received = 0
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[LESSON_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[LESSON_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[LESSON_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[LESSON_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[LESSON_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")
        # Send completion packet with the parsed outline.
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}

        print("FULL RESPOSE:", assistant_reply)

        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="text/plain")


@app.post("/api/custom/lesson-presentation/finalize")
async def wizard_lesson_finalize(payload: LessonWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    logger.info(f"Finalizing lesson presentation: {payload.lessonTitle}")
    
    # Validate required fields early
    if not payload.lessonTitle or not payload.lessonTitle.strip():
        raise HTTPException(status_code=400, detail="Lesson title is required")
    
    if not payload.aiResponse or not payload.aiResponse.strip():
        raise HTTPException(status_code=400, detail="AI response content is required")

    # Parse AI response to determine slide count for credit calculation
    try:
        slides_data = json.loads(payload.aiResponse)
        credits_needed = calculate_product_credits("lesson_presentation", slides_data)
    except:
        # If parsing fails, use default credit cost
        credits_needed = calculate_product_credits("lesson_presentation")

    # Get user ID and deduct credits for lesson presentation
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Lesson presentation finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for lesson presentation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for lesson presentation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    try:
        # Determine if this is a video lesson presentation
        is_video_lesson = payload.productType == "video_lesson_presentation"
        
        # Get the appropriate template with retry mechanism
        max_retries = 3
        template_id = None
        for attempt in range(max_retries):
            try:
                if is_video_lesson:
                    template_id = await _ensure_video_lesson_presentation_template(pool)
                else:
                    template_id = await _ensure_slide_deck_template(pool)
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"Failed to get template after {max_retries} attempts: {e}")
                    raise HTTPException(status_code=500, detail="Unable to initialize template")
                await asyncio.sleep(0.5)  # Brief delay before retry

        if not template_id:
            raise HTTPException(status_code=500, detail="Template initialization failed")

        # Get user ID
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lessonTitle.strip()
        if payload.outlineProjectId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineProjectId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lessonTitle.strip()}"
            except Exception as e:
                logger.warning(f"Failed to fetch outline name for lesson naming: {e}")
                # Continue with plain lesson title if outline fetch fails

        # Build source context from payload
        source_context_type, source_context_data = build_source_context(payload)
        
        # Create project data
        project_data = ProjectCreateRequest(
            projectName=project_name,
            design_template_id=template_id,
            microProductName=None,
            aiResponse=payload.aiResponse.strip(),
            chatSessionId=payload.chatSessionId,
            outlineId=payload.outlineProjectId,  # Pass outlineId for consistent naming
            folder_id=int(payload.folderId) if payload.folderId else None,  # Add folder assignment
            theme=payload.theme,  # Pass selected theme
            source_context_type=source_context_type,
            source_context_data=source_context_data
        )
        
        # Create project with proper error handling
        try:
            created_project = await add_project_to_custom_db(project_data, onyx_user_id, pool)
        except HTTPException as e:
            # Re-raise HTTP exceptions as-is
            raise e
        except Exception as e:
            logger.error(f"Failed to create project: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Failed to create lesson project")

        # Validate the created project
        if not created_project or not created_project.id:
            logger.error("Project creation returned invalid result")
            raise HTTPException(status_code=500, detail="Project creation failed - invalid response")

        logger.info(f"Successfully finalized lesson presentation with project ID: {created_project.id}")

        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if row:
                    logger.info(f"[LESSON_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved presentation JSON for project {created_project.id}: {log_e}")

        # Return simple JSON response (not streaming for now)
        return {
            "id": created_project.id,
            "projectName": created_project.project_name,
            "message": "Lesson presentation finalized successfully"
        }
    
    except HTTPException:
        # Re-raise HTTP exceptions without modification
        raise
    except Exception as e:
        logger.error(f"Unexpected error in lesson finalization: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail="An unexpected error occurred during finalization"
        )

# --- Delete single project endpoint ---
@app.delete("/api/custom/projects/{project_id}", status_code=204)
async def delete_project(
    project_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Delete a single project by ID.
    """
    try:
        async with pool.acquire() as conn:
            # Check if project exists and belongs to user
            project_row = await conn.fetchrow(
                "SELECT id FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
            if not project_row:
                raise HTTPException(
                    status_code=404,
                    detail="Project not found"
                )
            
            # Delete the project
            await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
        logger.info(f"Successfully deleted project {project_id}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error deleting project {project_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred while deleting the project"
        )

# --- New endpoint: list trashed projects for user ---

def fix_product_descriptions(lesson_plan_data, logger):
    """
    Ensures all product_description fields are strings, not nested objects.
    """
    if "contentDevelopmentSpecifications" not in lesson_plan_data:
        return
    
    logger.info("🔧 Fixing product descriptions to ensure string format...")
    
    for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
        if block.get("type") == "product" and "product_description" in block:
            description = block["product_description"]
            
            if not isinstance(description, str):
                logger.warning(f"Block {i}: product_description is {type(description)}, converting to string")
                
                if isinstance(description, dict):
                    # Flatten dictionary to comprehensive string
                    text_parts = []
                    
                    def flatten_dict(d, prefix=""):
                        for key, value in d.items():
                            if isinstance(value, dict):
                                flatten_dict(value, f"{prefix}{key}: ")
                            elif isinstance(value, list):
                                text_parts.append(f"{prefix}{key}: {', '.join(map(str, value))}")
                            else:
                                text_parts.append(f"{prefix}{key}: {value}")
                    
                    flatten_dict(description)
                    flattened_description = ". ".join(text_parts) + "."
                    
                    # Ensure comprehensive content
                    if len(flattened_description) < 250:
                        flattened_description += " Additional specifications include technical quality standards, accessibility compliance (WCAG), target audience considerations, assessment criteria, and implementation guidelines for professional content development."
                    
                    block["product_description"] = flattened_description
                    logger.info(f"✅ Block {i}: Flattened to {len(flattened_description)} chars")
                    
                elif isinstance(description, list):
                    # Join list items
                    flattened_description = ". ".join(map(str, description)) + "."
                    block["product_description"] = flattened_description
                    logger.info(f"✅ Block {i}: Joined list to string")
                    
                else:
                    # Convert any other type
                    block["product_description"] = str(description)
                    logger.info(f"✅ Block {i}: Converted to string")
            else:
                logger.info(f"✅ Block {i}: product_description is already a string ({len(description)} chars)")

@app.post("/api/custom/lesson-plan/generate", response_model=LessonPlanResponse)
async def generate_lesson_plan(
    payload: LessonPlanGenerationRequest, 
    request: Request, 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Generate a lesson plan directly from a course outline using the hybrid approach.
    """
    logger.info(f"Generating lesson plan for outline project {payload.outlineProjectId}")
    
    try:
        # Get user ID
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Retrieve the source context from the course outline project
        async with pool.acquire() as conn:
            outline_row = await conn.fetchrow(
                """
                SELECT id, project_name, source_context_type, source_context_data, 
                       microproduct_content, microproduct_type
                FROM projects 
                WHERE id = $1 AND onyx_user_id = $2
                """,
                payload.outlineProjectId, onyx_user_id
            )
            
            if not outline_row:
                raise HTTPException(
                    status_code=404, 
                    detail="Course outline project not found"
                )
            
            if outline_row["microproduct_type"] not in ["Training Plan", "Course Outline"]:
                raise HTTPException(
                    status_code=400, 
                    detail="Specified project is not a course outline"
                )
        
        # Extract source context
        source_context_type = outline_row["source_context_type"]
        source_context_data = outline_row["source_context_data"]
        
        # Prepare context for OpenAI
        context_for_openai = ""
        
        if source_context_type == "files" and source_context_data:
            # Extract file context using hybrid approach
            file_ids = source_context_data.get("file_ids", [])
            folder_ids = source_context_data.get("folder_ids", [])
            
            if file_ids or folder_ids:
                # Get cookies from request for Onyx API calls
                cookies = dict(request.cookies)
                
                # Extract context using the existing hybrid approach
                file_context = await extract_file_context_from_onyx(
                    file_ids, folder_ids, cookies
                )
                
                # Build context string for OpenAI
                if file_context.get("file_summaries"):
                    context_for_openai += "File Content:\n" + "\n".join(file_context["file_summaries"]) + "\n\n"
                if file_context.get("key_topics"):
                    context_for_openai += "Key Topics:\n" + ", ".join(file_context["key_topics"]) + "\n\n"
        
        elif source_context_type == "connectors" and source_context_data:
            # Extract connector context
            connector_ids = source_context_data.get("connector_ids", [])
            connector_sources = source_context_data.get("connector_sources", [])
            
            if connector_ids:
                context_for_openai += f"Connector Sources: {', '.join(connector_sources)}\n\n"
        
        elif source_context_type == "text" and source_context_data:
            # Extract text context
            user_text = source_context_data.get("user_text", "")
            if user_text:
                context_for_openai += f"Source Text:\n{user_text}\n\n"
        
        elif source_context_type == "knowledge_base" and source_context_data:
            # Extract knowledge base context
            search_query = source_context_data.get("search_query", "")
            if search_query:
                context_for_openai += f"Knowledge Base Query: {search_query}\n\n"
        
        # Extract specific lesson data and course outline content
        lesson_completion_time = "6m"  # Default fallback
        lesson_context = ""
        
        if outline_row["microproduct_content"]:
            try:
                outline_content = outline_row["microproduct_content"]
                if isinstance(outline_content, dict):
                    # Extract relevant information from outline
                    if "sections" in outline_content:
                        sections_text = []
                        found_lesson = False
                        
                        for section in outline_content["sections"]:
                            if isinstance(section, dict):
                                section_title = section.get("title", "")
                                section_content = section.get("content", "")
                                
                                # Check if this section matches the module name
                                if section_title and payload.moduleName.lower() in section_title.lower():
                                    # Look for the specific lesson in this section
                                    if "lessons" in section and isinstance(section["lessons"], list):
                                        for lesson in section["lessons"]:
                                            if isinstance(lesson, dict):
                                                lesson_title = lesson.get("title", "")
                                                if lesson_title and payload.lessonTitle.lower() in lesson_title.lower():
                                                    # Found the specific lesson - extract its completion time and individual product times
                                                    lesson_completion_time = lesson.get("completionTime", "6m")
                                                    
                                                    # Extract individual product completion times if available
                                                    individual_completion_times = {}
                                                    if lesson.get("completionTimes"):
                                                        completion_times_data = lesson["completionTimes"]
                                                        if isinstance(completion_times_data, dict):
                                                            # Map frontend naming to backend naming
                                                            if 'presentation' in completion_times_data:
                                                                individual_completion_times['presentation'] = completion_times_data['presentation']
                                                            if 'onePager' in completion_times_data:
                                                                individual_completion_times['one-pager'] = completion_times_data['onePager']
                                                            if 'quiz' in completion_times_data:
                                                                individual_completion_times['quiz'] = completion_times_data['quiz']
                                                            if 'videoLesson' in completion_times_data:
                                                                individual_completion_times['video-lesson'] = completion_times_data['videoLesson']
                                                    
                                                    lesson_context = f"Lesson Context: {lesson_title}"
                                                    if lesson.get("content"):
                                                        lesson_context += f" - {lesson.get('content')}"
                                                    found_lesson = True
                                                    
                                                    if individual_completion_times:
                                                        logger.info(f"Found lesson '{lesson_title}' with individual completion times: {individual_completion_times}")
                                                    else:
                                                        logger.info(f"Found lesson '{lesson_title}' with total completion time: {lesson_completion_time}")
                                                    break
                                    if found_lesson:
                                        break
                                
                                # Add general section context
                                if section_title and section_content:
                                    sections_text.append(f"{section_title}: {section_content}")
                        
                        if sections_text:
                            context_for_openai += "Course Outline Content:\n" + "\n".join(sections_text) + "\n\n"
                        
                        if lesson_context:
                            context_for_openai += lesson_context + "\n\n"
                            
            except Exception as e:
                logger.warning(f"Failed to parse outline content: {e}")
                
        # Use individual completion times if available, otherwise use total lesson time
        if individual_completion_times:
            logger.info(f"Using individual product completion times: {individual_completion_times}")
        else:
            logger.info(f"Using total lesson completion time: {lesson_completion_time} for lesson plan generation")
        
        # Helper function to extract minutes from time string
        def extract_minutes_from_time(time_str):
            try:
                if not time_str:
                    return 6  # Default fallback
                # Extract numeric part from strings like "6m", "2h", etc.
                numeric_part = ''.join(filter(str.isdigit, str(time_str)))
                if not numeric_part:
                    return 6  # Default fallback
                minutes = int(numeric_part)
                if 'h' in str(time_str).lower():
                    minutes *= 60  # Convert hours to minutes
                return minutes
            except:
                return 6  # Default fallback
        
        # Prepare timing information for products using individual times if available
        timing_info = "Product Timing Guidelines:\n"
        
        # Calculate video lesson duration
        if individual_completion_times and 'video-lesson' in individual_completion_times:
            video_minutes = extract_minutes_from_time(individual_completion_times['video-lesson'])
            timing_info += f"- Video Lesson Duration: {video_minutes} minutes (from individual completion time)\n"
        else:
            # Fallback to calculated duration from total lesson time
            total_minutes = extract_minutes_from_time(lesson_completion_time)
            video_duration = max(2, min(5, total_minutes // 2))
            timing_info += f"- Video Lesson Duration: Approximately {video_duration} minutes (calculated from lesson completion time)\n"
        
        # Calculate presentation length
        if individual_completion_times and 'presentation' in individual_completion_times:
            presentation_minutes = extract_minutes_from_time(individual_completion_times['presentation'])
            # Convert presentation time to slide count (rough estimate: 1 slide per minute + buffer)
            presentation_slides = max(8, min(20, presentation_minutes + 3))
            timing_info += f"- Presentation Length: Approximately {presentation_slides} slides (based on {presentation_minutes}min completion time)\n"
        else:
            # Fallback to calculated slides from total lesson time
            total_minutes = extract_minutes_from_time(lesson_completion_time)
            presentation_slides = max(8, min(15, total_minutes + 2))
            timing_info += f"- Presentation Length: Approximately {presentation_slides} slides (calculated from lesson completion time)\n"
        
        # Calculate quiz length
        if individual_completion_times and 'quiz' in individual_completion_times:
            quiz_minutes = extract_minutes_from_time(individual_completion_times['quiz'])
            # Convert quiz time to question count (rough estimate: 1-2 questions per minute)
            quiz_questions = max(5, min(15, quiz_minutes * 2))
            timing_info += f"- Quiz Length: Approximately {quiz_questions} questions (based on {quiz_minutes}min completion time)\n"
        else:
            timing_info += "- Quiz Length: 8-12 questions (standard range)\n"
        
        # One-pager timing
        if individual_completion_times and 'one-pager' in individual_completion_times:
            onepager_minutes = extract_minutes_from_time(individual_completion_times['one-pager'])
            timing_info += f"- One-Pager: Single comprehensive page (based on {onepager_minutes}min completion time)\n"
        else:
            timing_info += "- One-Pager: Single comprehensive page\n"
        

        
        # Create specific prompts based on recommended products
        has_video = any('video' in product.lower() for product in payload.recommendedProducts)
        has_presentation = any('presentation' in product.lower() for product in payload.recommendedProducts)
        
        prompts_instruction = "AI TOOL PROMPTS: Create ready-to-use prompts for AI content creation tools (like Synthesia, Gamma, etc.) "
        if has_video and has_presentation:
            prompts_instruction += "Provide exactly 2 specific prompts - one for video lesson creation and one for presentation creation. Each prompt should be detailed and actionable."
        elif has_video:
            prompts_instruction += "Provide exactly 1 specific prompt for video lesson creation. The prompt should be detailed and actionable."
        elif has_presentation:
            prompts_instruction += "Provide exactly 1 specific prompt for presentation creation. The prompt should be detailed and actionable."
        else:
            prompts_instruction += "Provide 2-3 specific content creation prompts for the recommended product types. Each prompt should be detailed and actionable."

        # Extract source materials from the course outline context
        source_materials = []
        
        if source_context_type == "files" and source_context_data:
            file_ids = source_context_data.get("file_ids", [])
            folder_ids = source_context_data.get("folder_ids", [])
            
            # Get cookies from request for Onyx API calls
            cookies = dict(request.cookies)
            
            # Fetch actual file names
            if file_ids:
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        # Get the file system to find files by ID
                        response = await client.get(
                            f"{ONYX_API_SERVER_URL}/user/file-system",
                            cookies=cookies
                        )
                        response.raise_for_status()
                        folders_data = response.json()
                        
                        # Extract file names from the folder structure
                        file_names = {}
                        for folder in folders_data:
                            if 'files' in folder:
                                for file_info in folder['files']:
                                    if file_info.get('id') in file_ids:
                                        file_names[file_info['id']] = file_info.get('name', f'Document {file_info["id"]}')
                        
                        # Add files with their actual names
                        for file_id in file_ids:
                            file_name = file_names.get(file_id, f'Document {file_id}')
                            source_materials.append(file_name)
                            
                except Exception as e:
                    logger.error(f"Error fetching file names: {e}")
                    # Fallback to generic names
                    source_materials.extend([f"Document {file_id}" for file_id in file_ids])
            
            # Fetch actual folder names
            if folder_ids:
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        # Get the file system to find folders by ID
                        response = await client.get(
                            f"{ONYX_API_SERVER_URL}/user/file-system",
                            cookies=cookies
                        )
                        response.raise_for_status()
                        folders_data = response.json()
                        
                        # Extract folder names
                        folder_names = {}
                        for folder in folders_data:
                            if folder.get('id') in folder_ids:
                                folder_names[folder['id']] = folder.get('name', f'Folder {folder["id"]}')
                        
                        # Add folders with their actual names
                        for folder_id in folder_ids:
                            folder_name = folder_names.get(folder_id, f'Folder {folder_id}')
                            source_materials.append(f"{folder_name} (Folder)")
                            
                except Exception as e:
                    logger.error(f"Error fetching folder names: {e}")
                    # Fallback to generic names
                    source_materials.extend([f"Folder {folder_id}" for folder_id in folder_ids])
                
        elif source_context_type == "connectors" and source_context_data:
            connector_sources = source_context_data.get("connector_sources", [])
            if connector_sources:
                source_materials.extend([f"Connector: {source}" for source in connector_sources])
                
        elif source_context_type == "text" and source_context_data:
            source_materials.append("Custom Text Input")
            
        elif source_context_type == "knowledge_base" and source_context_data:
            search_query = source_context_data.get("search_query", "")
            if search_query:
                source_materials.append(f"Knowledge Base Query: {search_query}")
        
        # If no source materials found, use general knowledge
        if not source_materials:
            source_materials = ["General Knowledge"]

        print("context_for_openai:", context_for_openai)
        
        # Prepare OpenAI prompt
        openai_prompt = f"""
You are an expert instructional designer and educational content developer. Based on the following source context, create a comprehensive lesson plan that serves as a complete task specification for Content Developers to create high-quality educational materials.

Source Context:
{context_for_openai}

Lesson Information:
- Lesson Title: {payload.lessonTitle}
- Module Name: {payload.moduleName}
- Lesson Number: {payload.lessonNumber}
- Lesson Completion Time: {lesson_completion_time}
- Recommended Products: {', '.join(payload.recommendedProducts)}
- CRITICAL: Use these EXACT product names in product blocks within contentDevelopmentSpecifications: {payload.recommendedProducts}

{timing_info}

Create a detailed lesson plan following instructional design best practices:

LESSON OBJECTIVES: Write 3-5 specific, measurable learning objectives using Bloom's Taxonomy action verbs. Each objective should specify what learners will be able to DO after completing the lesson (not what will be taught to them). Include the performance/behavior, conditions, and success criteria where applicable.

SHORT DESCRIPTION: Write a compelling 2-3 sentence description that clearly communicates the lesson's value proposition to learners. Focus on practical outcomes and real-world applications they will gain, not just topics covered.

CONTENT DEVELOPMENT SPECIFICATIONS: Create a flowing, structured lesson format that combines educational text blocks with product specifications. This section should tell a complete story about the lesson topic, with product blocks seamlessly integrated. Structure as follows:

TEXT BLOCKS: Create 3-5 educational text blocks with:
- block_title: A clear, engaging title (e.g., "Understanding the Fundamentals", "Key Implementation Strategies", "Best Practices for Success")
- block_content: Rich educational content that should contain ONE of the following formats:
  * Plain text paragraphs only (for explanatory content)
  * Bullet lists only (using -) for key points and benefits
  * Numbered lists only (using 1.) for sequential steps or processes
  * Mixed format: Brief intro text followed by a list (if context requires both)

CRITICAL CASE STUDY REQUIREMENT: If any text block includes a case study, it MUST be a real, specific case study with actual details - including real company names, specific outcomes, actual dates/timeframes, and concrete results. NEVER use placeholder text like "Company X", "a major corporation", "recent study", or generic examples. Research and provide actual case studies with verifiable details.

PRODUCT BLOCKS: For each recommended product, create a product block with:
- product_name: Exact name from recommendedProducts list
- product_description: SIMPLE CONTENT OUTLINE ONLY. This should be a clear, concise description of WHAT TOPICS AND CONTENT should be covered in this product. Write as a single string that serves as a content roadmap for developers. Include:

CONTENT TOPICS TO COVER (as a simple list format):
- Main topics that must be addressed (3-5 key areas)
- Important subtopics within each main area
- Key concepts and terminology to explain
- Essential examples or case studies to include
- Practical applications to demonstrate

EXAMPLE FORMAT: "This [product type] should cover the following topics: Topic 1 including subtopic A and subtopic B, Topic 2 with focus on concept X and concept Y, Topic 3 demonstrating practical application Z. Key terminology to explain includes [terms]. Essential examples should include [specific example]. The content should help learners understand [main learning outcome]."

DO NOT INCLUDE: Detailed instructions, technical specs, formatting requirements, or step-by-step creation guidance. Keep it focused on WHAT content should be covered, not HOW to create it.

INTEGRATION PATTERN: Alternate between text blocks and product blocks to create educational flow:
- Start with 1-2 text blocks introducing the topic
- Insert first product block
- Add randomly 1-2 text blocks expanding on concepts (vary the count)
- Insert next product block (if applicable)
- Continue pattern with random 1-2 text blocks between each product block
- End with a text block for conclusion

IMPORTANT: Vary the number of text blocks between products (sometimes 1, sometimes 2) to create natural flow. Each text block should contain EITHER plain text paragraphs OR bullet/numbered lists, not necessarily both.

The content should flow naturally, building knowledge progressively while seamlessly incorporating product specifications that support the learning journey.

MATERIALS: List the actual source materials used to create this lesson plan:

Source Materials Used:
{', '.join(source_materials)}

AI TOOL PROMPTS: Create exactly one COMPLETE, COPY-PASTE READY prompt for each recommended product type. These prompts should be fully formed instructions that users can copy directly into AI tools (like ChatGPT, Claude, Synthesia, Gamma, etc.) without any modification. Each prompt must be self-contained and immediately executable.

CRITICAL REQUIREMENTS:
- Create exactly {len(payload.recommendedProducts)} prompts (one for each recommended product)
- Each prompt must be 200-400 words and READY TO USE as-is
- Fill in ALL placeholder values with actual lesson-specific content
- Prompts should be complete sentences that can be copied and pasted directly into AI tools

PRODUCT TYPE FORMATS:

IMPORTANT: Replace ALL bracketed placeholders with actual lesson-specific information. The final prompts should contain NO brackets or placeholders.

FOR VIDEO LESSONS:
Create prompts following this pattern (fill in all specific details):
"Create a professional training video for [specific target audience with experience level]. This is the [specific lesson context], titled [actual lesson title]. The video should welcome learners with [specific opening approach], explain that the main goal is [actual learning objective from the lesson plan], cover [specific topics from the lesson's product block content], and provide [specific overview elements]. The tone should be [appropriate tone for audience], and the duration should be around [X] minutes based on the lesson timing."

FOR PRESENTATIONS:
Create prompts following this pattern (fill in all specific details):
"Create a professional educational presentation for [specific target audience]. This is the [lesson context] for the unit on [topic area], titled '[actual lesson title].' The presentation should [specific opening approach], explain that the main goal is to [actual learning objective], and provide [specific content breakdown from lesson topics]. The presentation must cover [list specific topics from product block]. The tone should be [appropriate tone], with [visual style description]. The presentation should be around [X-Y] slides based on timing. For each slide, please generate concise on-slide text and provide detailed speaker notes to guide the teacher."

FOR QUIZZES:
Create prompts following this pattern (fill in all specific details):
"Create a [specific number]-question multiple-choice quiz for [specific target audience] to assess their understanding of the [lesson context], '[actual lesson title].' The quiz's primary goal is to test the students' knowledge of [specific concepts from lesson objectives]. It should cover [specific content areas from product block]. The questions should be clear, direct, and multiple-choice, with four distinct answer options. The tone should be educational and straightforward. For each question, provide one correct answer and three plausible but incorrect distractors. Additionally, include a brief rationale for each answer option explaining why it is correct or incorrect, and a hint for each question that guides students toward the correct concept without giving away the answer."

FOR ONE-PAGERS:
Create prompts following this pattern (fill in all specific details):
"Create a professional e-learning document for [specific target audience]. This document should act as a [specific document purpose] for the lesson titled '[actual lesson title].' The document should [specific content organization requirements]. It must cover [specific topics from the lesson's product block content]. The tone should be [appropriate tone]. The information should be highly organized for scannability, using headings, bullet points, and bold text to highlight key information. The final document should be approximately [specific length] long."

CRITICAL DISTINCTION - PRODUCT BLOCKS vs PROMPTS:
- PRODUCT BLOCKS = Simple content outline describing WHAT topics should be covered (example: "This video should cover topic A, topic B, and topic C with focus on practical applications")
- AI TOOL PROMPTS = Complete, copy-paste ready instructions for AI tools (example: "Create a professional training video for senior project managers. This is an advanced lesson on risk management, titled 'Advanced Risk Assessment Techniques.' The video should...")

CONTENT REQUIREMENTS FOR ALL PROMPTS:
- Target audience must be specific (not generic)
- Learning objectives must be measurable and clear
- Content must include specific topics and subtopics from the lesson context
- Examples and case studies must be detailed and relevant
- Tone and style must be appropriate for the audience and context
- Fill in ALL bracketed placeholders with actual lesson-specific information
- Final prompts should be ready to copy-paste into AI tools without modification

CRITICAL REQUIREMENT: 
- ONLY include products that are explicitly listed in the recommendedProducts array: {payload.recommendedProducts}
- Use the EXACT product names from the recommendedProducts list in product blocks within contentDevelopmentSpecifications
- Do NOT add any products that are not in the recommendedProducts array
- Do NOT change the spelling or format of product names from the recommendedProducts list

Focus on creating actionable specifications that enable Content Developers to produce effective, engaging educational materials.

Return your response as a valid JSON object with this exact structure. PAY SPECIAL ATTENTION to product_description - it must be a SINGLE STRING, not nested objects:

{{
  "lessonTitle": "string",
  "lessonObjectives": ["string"],
  "shortDescription": "string",
  "contentDevelopmentSpecifications": [
    {{
      "type": "text",
      "block_title": "string",
      "block_content": "string (can include bullet lists with - or numbered lists with 1.)"
    }},
    {{
      "type": "product",
      "product_name": "exact name from recommendedProducts",
      "product_description": "This must be a single string with all specifications in one paragraph. Example: Create a comprehensive 12-slide presentation targeting intermediate project managers with 3-5 years experience. Content must cover RAG methodology definitions, common applications in project contexts, specific limitations including data quality issues and implementation challenges, real-world case studies from software development and construction industries. Technical specifications: 16:9 aspect ratio, 1920x1080 resolution, corporate branding with blue/white color scheme, Arial font 24pt minimum for titles and 18pt for content. Each slide should include speaker notes with detailed talking points. Accessibility requirements: high contrast text (4.5:1 ratio), alt text for all images, screen reader compatibility. Structure: opening slide with agenda, 2 slides for definitions, 4 slides for applications, 3 slides for limitations with examples, 2 slides for case studies, closing slide with key takeaways. Include interactive elements like polls or discussion questions every 4 slides to maintain engagement."
    }}
  ],
  "materials": ["string"],
  "suggestedPrompts": ["string"]
}}

CRITICAL: The product_description field shown above is an example of the single string format required. Do NOT use nested objects like {{"contentSpecifications": {{}}, "technicalSpecs": {{}}}}. Everything must be in one comprehensive string.

Ensure the JSON is valid and follows the exact structure specified.
"""
        
        # Generate lesson plan using OpenAI
        openai_client = get_openai_client()
        
        response = await openai_client.chat.completions.create(
            model=LLM_DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": "You are an expert educational content creator. Always respond with valid JSON. Create extremely detailed, comprehensive content specifications."},
                {"role": "user", "content": openai_prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        # Parse OpenAI response
        ai_response = response.choices[0].message.content.strip()
        
        # Strip markdown code blocks if present
        if ai_response.startswith('```json'):
            ai_response = ai_response[7:]  # Remove ```json
        elif ai_response.startswith('```'):
            ai_response = ai_response[3:]   # Remove ```
        
        if ai_response.endswith('```'):
            ai_response = ai_response[:-3]  # Remove trailing ```
        
        ai_response = ai_response.strip()  # Clean up any remaining whitespace
        
        # Strip markdown code blocks if present (similar to existing LLM parsing logic)
        ai_response = re.sub(r"^```json\s*|\s*```$", "", ai_response.strip(), flags=re.MULTILINE)
        ai_response = re.sub(r"^```(?:json)?\s*|\s*```$", "", ai_response, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        try:
            lesson_plan_data = json.loads(ai_response)
            
            # CRITICAL FIX: Ensure product_description is always a string
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        if not isinstance(description, str):
                            logger.warning(f"Block {i}: Converting {type(description)} to string")
                            if isinstance(description, dict):
                                # Flatten nested structure to string
                                parts = []
                                def flatten_obj(obj, prefix=""):
                                    if isinstance(obj, dict):
                                        for k, v in obj.items():
                                            if isinstance(v, dict):
                                                flatten_obj(v, f"{prefix}{k}: ")
                                            elif isinstance(v, list):
                                                parts.append(f"{prefix}{k}: {', '.join(map(str, v))}")
                                            else:
                                                parts.append(f"{prefix}{k}: {v}")
                                    else:
                                        parts.append(str(obj))
                                flatten_obj(description)
                                flattened = ". ".join(parts) + "."
                                if len(flattened) < 200:
                                    flattened += " This includes comprehensive specifications, technical requirements, and quality standards."
                                block["product_description"] = flattened
                                logger.info(f"✅ Block {i}: Flattened to {len(flattened)} chars")
                            else:
                                block["product_description"] = str(description)
                                logger.info(f"✅ Block {i}: Converted to string")
            # Validate the structure
            required_fields = ["lessonTitle", "lessonObjectives", "shortDescription", "contentDevelopmentSpecifications", "materials", "suggestedPrompts"]
            for field in required_fields:
                if field not in lesson_plan_data:
                    raise ValueError(f"Missing required field: {field}")
            
            # COMPREHENSIVE FIX: Ensure all product descriptions are strings
            logger.info("Validating and fixing product descriptions...")
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        
                        if not isinstance(description, str):
                            logger.warning(f"Block {i}: Converting {type(description)} to string for product_description")
                            
                            if isinstance(description, dict):
                                # Convert dictionary to comprehensive string
                                parts = []
                                for key, value in description.items():
                                    if isinstance(value, dict):
                                        nested_parts = [f"{k}: {v}" for k, v in value.items()]
                                        parts.append(f"{key.upper()}: {'. '.join(nested_parts)}")
                                    elif isinstance(value, list):
                                        parts.append(f"{key.upper()}: {', '.join(map(str, value))}")
                                    else:
                                        parts.append(f"{key}: {value}")
                                
                                flattened = ". ".join(parts)
                                if len(flattened) < 200:
                                    flattened += ". This content should include comprehensive specifications, technical requirements, accessibility standards, and quality criteria."
                                
                                block["product_description"] = flattened
                                logger.info(f"Block {i}: Flattened to {len(flattened)} character string")
                                
                            else:
                                # Convert other types to string
                                block["product_description"] = str(description)
                                logger.info(f"Block {i}: Converted {type(description)} to string")
            
            # Fix product descriptions that might be nested objects instead of strings
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for block in lesson_plan_data["contentDevelopmentSpecifications"]:
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        if isinstance(description, dict):
                            # AI returned nested structure - flatten it to a comprehensive string
                            logger.warning(f"AI returned nested structure for product_description, flattening to string")
                            flattened_description = ""
                            
                            # Extract content specifications
                            if "contentSpecifications" in description:
                                content_specs = description["contentSpecifications"]
                                if isinstance(content_specs, dict):
                                    flattened_description += "CONTENT SPECIFICATIONS: "
                                    for key, value in content_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(content_specs, list):
                                    flattened_description += "CONTENT SPECIFICATIONS: " + ". ".join(map(str, content_specs)) + ". "
                            
                            # Extract technical specifications
                            if "technicalSpecifications" in description:
                                tech_specs = description["technicalSpecifications"]
                                flattened_description += "TECHNICAL SPECIFICATIONS: "
                                if isinstance(tech_specs, dict):
                                    for key, value in tech_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(tech_specs, list):
                                    flattened_description += ". ".join(map(str, tech_specs)) + ". "
                            
                            # Extract target audience specifications
                            if "targetAudienceAdaptations" in description:
                                audience_specs = description["targetAudienceAdaptations"]
                                flattened_description += "TARGET AUDIENCE: "
                                if isinstance(audience_specs, dict):
                                    for key, value in audience_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(audience_specs, list):
                                    flattened_description += ". ".join(map(str, audience_specs)) + ". "
                            
                            # If we couldn't extract specific sections, convert the whole thing to string
                            if not flattened_description.strip():
                                # Recursive function to extract all text from nested structures
                                def extract_all_text(obj):
                                    if isinstance(obj, dict):
                                        text_parts = []
                                        for key, value in obj.items():
                                            if isinstance(value, (str, int, float)):
                                                text_parts.append(f"{key}: {value}")
                                            else:
                                                text_parts.append(f"{key}: {extract_all_text(value)}")
                                        return ". ".join(text_parts)
                                    elif isinstance(obj, list):
                                        return ". ".join(str(item) for item in obj)
                                    else:
                                        return str(obj)
                                
                                flattened_description = extract_all_text(description)
                            
                            # Clean up the string and ensure it's comprehensive
                            flattened_description = flattened_description.strip()
                            if len(flattened_description) < 200:
                                # If too short, add padding with comprehensive details
                                flattened_description += " This content should include detailed specifications, technical requirements, learning objectives, assessment criteria, target audience considerations, accessibility requirements, and implementation guidelines to ensure comprehensive content development."
                            
                            # Update the block with flattened string
                            block["product_description"] = flattened_description
                            logger.info(f"Flattened product description to {len(flattened_description)} characters")
                        elif not isinstance(description, str):
                            # Handle other non-string types
                            logger.warning(f"Product description is not a string: {type(description)}, converting to string")
                            block["product_description"] = str(description)
            
            # Log the recommended products for debugging
            logger.info(f"Payload recommended products: {payload.recommendedProducts}")
            # Extract product names from contentDevelopmentSpecifications for validation
            ai_generated_products = []
            for block in lesson_plan_data.get('contentDevelopmentSpecifications', []):
                if block.get('type') == 'product':
                    ai_generated_products.append(block.get('product_name'))
            logger.info(f"AI generated product types: {ai_generated_products}")
            
            # Create a mapping of common product name variations
            product_name_mapping = {
                'video-lesson': ['video-lesson', 'videoLesson', 'video_lesson', 'video lesson'],
                'presentation': ['presentation', 'presentations'],
                'quiz': ['quiz', 'quizzes'],
                'one-pager': ['one-pager', 'onePager', 'one_pager', 'one pager']
            }
            
            # Create reverse mapping for validation
            normalized_payload_products = set()
            for product in payload.recommendedProducts:
                # Find the canonical name for this product
                canonical_name = None
                for canonical, variations in product_name_mapping.items():
                    if product.lower() in [v.lower() for v in variations]:
                        canonical_name = canonical
                        break
                if canonical_name:
                    normalized_payload_products.add(canonical_name)
                else:
                    normalized_payload_products.add(product.lower())
            
            # Validate product blocks only contain products from the request
            for product_name in ai_generated_products:
                # Normalize the AI-generated product name
                canonical_name = None
                for canonical, variations in product_name_mapping.items():
                    if product_name.lower() in [v.lower() for v in variations]:
                        canonical_name = canonical
                        break
                
                if canonical_name:
                    if canonical_name not in normalized_payload_products:
                        logger.warning(f"AI generated product '{product_name}' (canonical: '{canonical_name}') not in normalized recommended products: {normalized_payload_products}")
                        raise ValueError(f"Product {product_name} not in recommended products list")
                else:
                    if product_name.lower() not in normalized_payload_products:
                        logger.warning(f"AI generated unknown product '{product_name}' not in recommended products: {payload.recommendedProducts}")
                        raise ValueError(f"Product {product_name} not in recommended products list")
                    
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse OpenAI response: {e}")
            logger.error(f"Raw AI response: {ai_response}")
            raise HTTPException(
                status_code=500,
                detail="Failed to generate valid lesson plan structure"
            )
        
        # Override AI materials with our extracted source materials
        logger.info(f"Extracted source materials: {source_materials}")
        lesson_plan_data['materials'] = source_materials
        logger.info(f"Final materials for lesson plan: {lesson_plan_data['materials']}")
        
        # Debug the prompts to see what the AI returned
        logger.info(f"AI returned {len(lesson_plan_data.get('suggestedPrompts', []))} prompts")
        for i, prompt in enumerate(lesson_plan_data.get('suggestedPrompts', [])):
            logger.info(f"Prompt {i+1}: {prompt[:100]}...")  # Log first 100 chars of each prompt
            
        # Ensure prompts are properly formatted with titles if they're not
        if 'suggestedPrompts' in lesson_plan_data:
            formatted_prompts = []
            for i, prompt in enumerate(lesson_plan_data['suggestedPrompts']):
                # Check if prompt already has a title format
                if not prompt.strip().startswith('**'):
                    # Try to infer the product type from the recommended products
                    if i < len(ai_generated_products):
                        product_name = ai_generated_products[i]
                        # Format the product name as a title
                        if 'video' in product_name.lower():
                            title = "Video Lesson Creation Prompt"
                        elif 'presentation' in product_name.lower():
                            title = "Presentation Creation Prompt"
                        elif 'quiz' in product_name.lower():
                            title = "Quiz Creation Prompt"
                        elif 'one-pager' in product_name.lower():
                            title = "One-Pager Creation Prompt"
                        else:
                            title = f"{product_name.replace('-', ' ').title()} Creation Prompt"
                        
                        formatted_prompt = f"**{title}:**\n{prompt}"
                        formatted_prompts.append(formatted_prompt)
                        logger.info(f"Formatted prompt {i+1} with title: {title}")
                    else:
                        formatted_prompts.append(prompt)
                else:
                    formatted_prompts.append(prompt)
            
            lesson_plan_data['suggestedPrompts'] = formatted_prompts
        
        # Debug the prompts to see what the AI returned
        logger.info(f"AI returned prompts: {lesson_plan_data.get('suggestedPrompts', [])}")
        
        # Ensure prompts are properly formatted with titles if they're not
        formatted_prompts = []
        for i, prompt in enumerate(lesson_plan_data.get('suggestedPrompts', [])):
            # Check if prompt already has a title format
            if not prompt.strip().startswith('**'):
                # Try to infer the product type from the recommended products
                if i < len(ai_generated_products):
                    product_name = ai_generated_products[i]
                    # Format the product name as a title
                    if 'video' in product_name.lower():
                        title = "Video Lesson Creation Prompt"
                    elif 'presentation' in product_name.lower():
                        title = "Presentation Creation Prompt"
                    elif 'quiz' in product_name.lower():
                        title = "Quiz Creation Prompt"
                    elif 'one-pager' in product_name.lower():
                        title = "One-Pager Creation Prompt"
                    else:
                        title = f"{product_name.title()} Creation Prompt"
                    
                    formatted_prompt = f"**{title}:**\n{prompt}"
                    formatted_prompts.append(formatted_prompt)
                else:
                    formatted_prompts.append(prompt)
            else:
                formatted_prompts.append(prompt)
        
        lesson_plan_data['suggestedPrompts'] = formatted_prompts
        logger.info(f"Formatted prompts: {formatted_prompts}")
        
        # Create the lesson plan project in database
        project_name = f"{outline_row['project_name']}: {payload.lessonTitle}"
        
        # Get a design template for lesson plans
        async with pool.acquire() as conn:
            template_row = await conn.fetchrow(
                "SELECT id FROM design_templates WHERE component_name = 'LessonPlanDisplay' LIMIT 1"
            )
            
            if not template_row:
                # Create a basic template if none exists
                template_id = await conn.fetchval(
                    """
                    INSERT INTO design_templates 
                    (template_name, component_name, microproduct_type, template_structuring_prompt)
                    VALUES ($1, $2, $3, $4) RETURNING id
                    """,
                    "Lesson Plan Template",
                    "LessonPlanDisplay",
                    "Lesson Plan",
                    "Generate a lesson plan with objectives, materials, and product recommendations."
                )
            else:
                template_id = template_row["id"]
        
        # Create project data
        project_data = ProjectCreateRequest(
            projectName=project_name,
            design_template_id=template_id,
            microProductName="Lesson Plan",
            aiResponse=json.dumps(lesson_plan_data),
            chatSessionId=None,
            outlineId=payload.outlineProjectId,
            folder_id=None,
            theme="default",
            source_context_type=source_context_type,
            source_context_data=source_context_data
        )
        
        # Add the project to database
        created_project = await add_project_to_custom_db(project_data, onyx_user_id, pool)
        
        # Update the project with lesson plan specific data
        async with pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE projects 
                SET product_type = 'lesson-plan',
                   lesson_plan_data = $1,
                   parent_outline_id = $2
                WHERE id = $3
                """,
                json.dumps(lesson_plan_data),
                payload.outlineProjectId,
                created_project.id
            )
        
        logger.info(f"Successfully created lesson plan project {created_project.id}")
        
        return LessonPlanResponse(
            success=True,
            project_id=created_project.id,
            lesson_plan_data=LessonPlanData(**lesson_plan_data),
            message="Lesson plan generated successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in lesson plan generation: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail="An unexpected error occurred during lesson plan generation"
        )

@app.delete("/api/custom/projects/{project_id}", status_code=204)
async def delete_project(
    project_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Delete a single project by ID.
    """
    try:
        async with pool.acquire() as conn:
            # Check if project exists and belongs to user
            project_row = await conn.fetchrow(
                "SELECT id FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
            if not project_row:
                raise HTTPException(
                    status_code=404,
                    detail="Project not found"
                )
            
            # Delete the project
            await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
        logger.info(f"Successfully deleted project {project_id}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error deleting project {project_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred while deleting the project"
        )

@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Restore trashed projects ---

@app.post("/api/custom/projects/restore-multiple", status_code=status.HTTP_200_OK)
async def restore_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for restore."})

    ids_to_restore: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # Expand scope to related lessons when requested
            if delete_request.scope == 'all':
                for pid in delete_request.project_ids:
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                        pid, onyx_user_id
                    )
                    if not row:
                        continue
                    pname: str = row["project_name"]
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        continue
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_restore.add(lr["id"])

            if not ids_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            # First, fetch all the data we need to restore
            projects_to_restore = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM trashed_projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(ids_to_restore), onyx_user_id)

            if not projects_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_restore:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into projects with safe values
                    await conn.execute("""
                        INSERT INTO projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type,
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from trashed_projects table
                await conn.execute(
                    "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(ids_to_restore), onyx_user_id
                )

        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully restored {len(ids_to_restore)} project(s)."})

    except Exception as e:
        logger.error(f"Error restoring projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while restoring projects." if IS_PRODUCTION else f"DB error while restoring projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


# --- Permanently delete trashed projects ---

@app.post("/api/custom/projects/delete-permanently", status_code=status.HTTP_200_OK)
async def delete_permanently(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for permanent deletion."})

    ids_to_delete: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            for pid in delete_request.project_ids:
                row = await conn.fetchrow(
                    "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                    pid, onyx_user_id
                )
                if not row:
                    continue
                pname: str = row["project_name"]
                # If this is an outline, cascade to its lessons
                if row["microproduct_type"] in ("Training Plan", "Course Outline"):
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_delete.add(lr["id"])

            # Perform deletion of all collected ids
            result = await conn.execute(
                "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id=$2",
                list(ids_to_delete), onyx_user_id
            )

        deleted_count = int(result.split(" ")[1]) if result else 0
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully deleted {deleted_count} project(s) permanently."})
    except Exception as e:
        logger.error(f"Error permanently deleting projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during permanent deletion." if IS_PRODUCTION else f"DB error during permanent deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# Add the new model for training plan editing
class TrainingPlanEditRequest(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: Optional[str] = None
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during edit
    # File context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs

@app.post("/api/custom/training-plan/edit")
async def edit_training_plan_with_prompt(payload: TrainingPlanEditRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Edit an existing training plan using AI prompt"""
    logger.info(f"[edit_training_plan_with_prompt] projectId={payload.projectId} prompt='{payload.prompt[:50]}...'")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    # Get or create chat session
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        persona_id = await get_contentbuilder_persona_id(cookies)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Convert existing training plan to markdown format for AI processing
    existing_content = row["microproduct_content"]
    current_outline = ""
    
    if existing_content:
        # Convert existing training plan to markdown format with full details
        content_data = existing_content
        if isinstance(content_data, dict):
            main_title = content_data.get("mainTitle", "Training Plan")
            current_outline = f"# {main_title}\n\n"
            
            sections = content_data.get("sections", [])
            for section in sections:
                section_id = section.get("id", "")
                section_title = section.get("title", "")
                total_hours = section.get("totalHours", 0.0)
                # Get module quality tier information for preservation
                section_quality_tier = section.get("quality_tier", "")
                
                # Convert special characters to safe ASCII for AI processing
                # We'll convert back after AI response to preserve user-visible format
                if section_id and section_title:
                    # Replace № with # for AI processing (encoding-safe)
                    safe_section_id = section_id.replace("№", "#")
                    if section_id != safe_section_id:
                        logger.info(f"[SMART_EDIT_ENCODING] Converted '{section_id}' to '{safe_section_id}' for AI processing")
                    # Check if section_id already contains "Module" keyword
                    if "Module" in safe_section_id or "Модуль" in safe_section_id:
                        current_outline += f"## {safe_section_id}: {section_title}\n"
                    else:
                        # For other formats (#1, mod1, etc.), preserve them exactly as they are
                        current_outline += f"## {safe_section_id}: {section_title}\n"
                else:
                    # Fallback for empty IDs
                    current_outline += f"## {section_title}\n"
                current_outline += f"**Total Hours:** {total_hours}\n"
                if section_quality_tier:
                    current_outline += f"**Module Quality Tier:** {section_quality_tier}\n"
                current_outline += "\n"
                
                lessons = section.get("lessons", [])
                if lessons:
                    current_outline += "### Lessons:\n"
                    for idx, lesson in enumerate(lessons, 1):
                        lesson_title = lesson.get("title", "")
                        lesson_hours = lesson.get("hours", 1.0)
                        lesson_source = lesson.get("source", "Create from scratch")
                        
                        # Get check details
                        check = lesson.get("check", {})
                        check_type = check.get("type", "none")
                        check_text = check.get("text", "No")
                        
                        # Get content availability
                        content_available = lesson.get("contentAvailable", {})
                        content_type = content_available.get("type", "yes")
                        content_text = content_available.get("text", "100%")
                        
                        # Get quality tier information for preservation
                        lesson_quality_tier = lesson.get("quality_tier", "")
                        
                        current_outline += f"{idx}. **{lesson_title}**\n"
                        current_outline += f"   - Hours: {lesson_hours}\n"
                        current_outline += f"   - Source: {lesson_source}\n"
                        current_outline += f"   - Assessment: {check_type} ({check_text})\n"
                        current_outline += f"   - Content Available: {content_type} ({content_text})\n"
                        if lesson_quality_tier:
                            current_outline += f"   - Quality Tier: {lesson_quality_tier}\n"
                        current_outline += "\n"
                else:
                    current_outline += "*No lessons defined*\n\n"
                current_outline += "\n"

    # Prepare wizard payload
    wiz_payload = {
        "product": "Training Plan Edit",
        "prompt": payload.prompt,
        "language": payload.language,
        "originalOutline": current_outline,
        "editMode": True
    }

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

    # Stream the response
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
        
        # NEW: Check if we should use OpenAI directly instead of Onyx
        if should_use_openai_direct(payload):
            logger.info(f"[SMART_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[SMART_EDIT_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                chunks_received = 0
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[SMART_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[SMART_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[SMART_EDIT_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[SMART_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
            except Exception as e:
                logger.error(f"[SMART_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # EXISTING: Use Onyx when file context is present
        else:
            logger.info(f"[SMART_EDIT_STREAM] ❌ USING ONYX API (file context detected)")
            logger.info(f"[SMART_EDIT_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                async with httpx.AsyncClient(timeout=timeout_duration) as client:
                    # Parse folder and file IDs for Onyx
                    folder_ids_list = []
                    file_ids_list = []
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    send_payload = {
                        "chat_session_id": chat_id,
                        "message": wizard_message,
                        "parent_message_id": None,
                        "file_descriptors": [],
                        "user_file_ids": file_ids_list,
                        "user_folder_ids": folder_ids_list,
                        "prompt_id": None,
                        "search_doc_ids": None,
                        "retrieval_options": {"run_search": "never", "real_time": False},
                        "stream_response": True,
                    }
                    logger.info(f"[PREVIEW_ONYX] Sending request to Onyx /chat/send-message with payload: user_file_ids={file_ids_list}, user_folder_ids={folder_ids_list}")
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=send_payload, cookies=cookies) as resp:
                        logger.info(f"[PREVIEW_ONYX] Response status: {resp.status_code}")
                        async for raw_line in resp.aiter_lines():
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info("[PREVIEW_ONYX] Received [DONE] from Onyx stream")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    delta_text = pkt["answer_piece"].replace("\\n", "\n")
                                    assistant_reply += delta_text
                                    logger.debug(f"[PREVIEW_ONYX] Received chunk: {delta_text[:80]}")
                                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                            except Exception as e:
                                logger.error(f"[PREVIEW_ONYX] Error parsing chunk: {e} | Raw: {line[:100]}")
                                continue

                            # send keep-alive every 8s
                            now = asyncio.get_event_loop().time()
                            if now - last_send > 8:
                                yield b" "
                                last_send = now
            except Exception as e:
                logger.error(f"[PREVIEW_ONYX] Exception in streaming: {e}")
                raise

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")

        # Convert back from safe ASCII characters to original special characters
        # Replace # back to № to restore original format for user display
        assistant_reply_restored = assistant_reply.replace("## #", "## №")
        if assistant_reply_restored != assistant_reply:
            logger.info(f"[SMART_EDIT_ENCODING] Restored special characters in AI response")
        
        # Update the cached version and the one used for parsing
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply_restored
        
        # Use the restored version for all subsequent processing
        assistant_reply = assistant_reply_restored
        
        # NEW: Parse AI response into structured TrainingPlanDetails but DON'T save to database yet
        # This is for preview - user will confirm before saving
        updated_content_dict: Optional[Dict[str, Any]] = None
        try:
            # Use the proper LLM parser to convert AI response to TrainingPlanDetails
            # Use the SAME parsing instructions as normal generation to ensure consistent ID handling
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `quality_tier` (string, optional): Quality tier for this module. PRESERVE EXACTLY from source if mentioned as "Module Quality Tier: X". If not specified, omit this field entirely.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `quality_tier` (string, optional): Quality tier for this lesson. PRESERVE EXACTLY from source if mentioned as "Quality Tier: X". If not specified, omit this field entirely.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
            
            # Create a default TrainingPlanDetails instance for error handling
            # Preserve theme from existing content or use payload theme
            theme_to_use = "cherry"
            if existing_content and isinstance(existing_content, dict):
                theme_to_use = existing_content.get("theme", "cherry")
            else:
                theme_to_use = payload.theme or "cherry"
                
            default_training_plan = TrainingPlanDetails(
                mainTitle=row["project_name"],
                sections=[],
                detectedLanguage=detect_language(assistant_reply),
                theme=theme_to_use
            )
            
            # Example JSON structure for the LLM parser
            llm_json_example = json.dumps({
                "mainTitle": "Example Training Plan",
                "sections": [
                    {
                        "id": "№1",
                        "title": "Introduction to Topic",
                        "totalHours": 10,
                        "quality_tier": "premium",
                        "lessons": [
                            {
                                "title": "Lesson 1: Basics",
                                "hours": 2,
                                "source": "Create from scratch",
                                "completionTime": "5m",
                                "quality_tier": "interactive",
                                "check": {"type": "test", "text": "Test"},
                                "contentAvailable": {"type": "yes", "text": "100%"}
                            }
                        ],
                        "autoCalculateHours": True
                    }
                ],
                "detectedLanguage": "en",
                "theme": theme_to_use
            })
            
            logger.info(f"[SMART_EDIT_PARSER] Parsing AI response with length: {len(assistant_reply)}")
            logger.info(f"[SMART_EDIT_PARSER] AI response preview: {assistant_reply[:300]}{'...' if len(assistant_reply) > 300 else ''}")
            
            parsed_training_plan = await parse_ai_response_with_llm(
                ai_response=assistant_reply,
                project_name=row["project_name"],
                target_model=TrainingPlanDetails,
                default_error_model_instance=default_training_plan,
                dynamic_instructions=component_specific_instructions,
                target_json_example=llm_json_example
            )
            
            if parsed_training_plan:
                # Preserve the original language and theme
                if existing_content and isinstance(existing_content, dict):
                    # Preserve original language
                    original_language = existing_content.get("detectedLanguage", payload.language)
                    parsed_training_plan.detectedLanguage = original_language
                    logger.info(f"[SMART_EDIT_LANGUAGE] Preserved original language: {original_language}")
                    
                    # Preserve original theme
                    original_theme = existing_content.get("theme", "cherry")
                    parsed_training_plan.theme = original_theme
                    logger.info(f"[SMART_EDIT_THEME] Preserved original theme: {original_theme}")
                else:
                    # Use the language and theme from the request payload if available
                    parsed_training_plan.detectedLanguage = payload.language or "en"
                    parsed_training_plan.theme = payload.theme or "cherry"
                    logger.info(f"[SMART_EDIT_LANGUAGE] Using language from payload: {payload.language}")
                    logger.info(f"[SMART_EDIT_THEME] Using theme from payload: {payload.theme}")
                
                # Post-process module IDs to ensure № character is preserved
                for section in parsed_training_plan.sections:
                    if section.id:
                        # Fix module IDs that lost the № character
                        if section.id.isdigit():
                            # Plain number like "2" -> "№2"
                            section.id = f"№{section.id}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                        elif section.id.startswith("#"):
                            # Hash format like "#2" -> "№2"
                            number = section.id[1:]
                            section.id = f"№{number}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                        elif not section.id.startswith("№"):
                            # Other formats without № - try to extract number and format correctly
                            import re
                            number_match = re.search(r'\d+', section.id)
                            if number_match:
                                number = number_match.group()
                                section.id = f"№{number}"
                                logger.info(f"[SMART_EDIT_ID_FIX] Fixed ID format to '{section.id}'")
                
                updated_content_dict = parsed_training_plan.model_dump(mode='json', exclude_none=True)
                
                logger.info(f"[SMART_EDIT_PREVIEW] Generated preview for training plan projectId={payload.projectId}")
            
        except Exception as e:
            logger.error(f"[SMART_EDIT_ERROR] Error parsing training plan: {e}")
            # Fall back to the preview-only mode if parsing fails
            updated_content_dict = None

        # Send completion packet with updatedContent for frontend preview
        # Note: This is now a PREVIEW - user must confirm to save to database
        if updated_content_dict:
            done_packet = {"type": "done", "updatedContent": updated_content_dict, "isPreview": True}
        else:
            # Fallback to old format if parsing failed
            done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        
        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="application/json")

class SmartEditConfirmRequest(BaseModel):
    projectId: int
    updatedContent: dict
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during confirmation

@app.post("/api/custom/training-plan/confirm-edit")
async def confirm_training_plan_edit(payload: SmartEditConfirmRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Confirm and save smart-edit changes to the database"""
    logger.info(f"[confirm_training_plan_edit] projectId={payload.projectId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Verify the project exists and belongs to the user
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    try:
        # Log the content structure for debugging
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content structure: {type(payload.updatedContent)}")
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content keys: {list(payload.updatedContent.keys()) if isinstance(payload.updatedContent, dict) else 'Not a dict'}")
        
        # Save the confirmed changes to the database
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE projects 
                SET microproduct_content = $1
                WHERE id = $2 AND onyx_user_id = $3
            """, payload.updatedContent, payload.projectId, onyx_user_id)
        
        logger.info(f"[SMART_EDIT_CONFIRMED] Successfully saved changes for training plan projectId={payload.projectId}")
        
        return {"success": True, "message": "Changes confirmed and saved successfully"}
        
    except Exception as e:
        logger.error(f"[SMART_EDIT_CONFIRM_ERROR] Error saving confirmed changes: {e}")
        raise HTTPException(status_code=500, detail="Failed to save changes")

# Add the finalize model for training plan editing
class TrainingPlanEditFinalize(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: str
    editedOutline: Dict[str, Any]
    language: str = "en"

@app.post("/api/custom/training-plan/finalize")
async def finalize_training_plan_edit(payload: TrainingPlanEditFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize and apply the edited training plan"""
    logger.info(f"[finalize_training_plan_edit] projectId={payload.projectId} chatSessionId={payload.chatSessionId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get the cached preview
    cached_preview = OUTLINE_PREVIEW_CACHE.get(payload.chatSessionId)
    if not cached_preview:
        raise HTTPException(status_code=400, detail="No preview found for this session. Please regenerate the preview.")
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")
    
    # Parse the edited outline from the cached preview using LLM-based parsing
    try:
        # Create a default TrainingPlanDetails instance for error handling
        default_training_plan = TrainingPlanDetails(
            mainTitle=row["project_name"],
            sections=[],
            detectedLanguage=detect_language(cached_preview)
        )
        
        # Component-specific instructions for TrainingPlanDetails parsing
        component_specific_instructions = """
            Parse the training plan outline into a structured JSON format. Extract all modules and their lessons with complete details.

            **Main Object:**
            * `mainTitle` (string): The main title of the training plan.
            * `sections` (array): List of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
        
        # Example JSON structure for the LLM parser
        llm_json_example = json.dumps({
            "mainTitle": "Example Training Plan",
            "sections": [
                {
                    "id": "№1",
                    "title": "Introduction to Topic",
                    "totalHours": 10,
                    "lessons": [
                        {
                            "title": "Lesson 1: Basics",
                            "hours": 2,
                            "source": "Create from scratch",
                            "completionTime": "5m",
                            "check": {"type": "test", "text": "Test"},
                            "contentAvailable": {"type": "yes", "text": "100%"}
                        }
                    ],
                    "autoCalculateHours": True
                }
            ],
            "detectedLanguage": "en",
            "theme": "cherry"
        })
        
        # First, parse the outline to get auto-calculated totalHours
        parsed_orig = _parse_outline_markdown(cached_preview)
        logger.info(f"[FINALIZE] Parsed outline with {len(parsed_orig)} modules")
        
        # Create a mapping of module titles to auto-calculated totalHours
        auto_calculated_hours = {}
        for module in parsed_orig:
            title = module.get('title', '')
            total_hours = module.get('totalHours', 0.0)
            auto_calculated_hours[title] = total_hours
            logger.info(f"[FINALIZE] Auto-calculated hours for '{title}': {total_hours}")
        
        training_plan_details = await parse_ai_response_with_llm(
            ai_response=cached_preview,
            project_name=row["project_name"],
            target_model=TrainingPlanDetails,
            default_error_model_instance=default_training_plan,
            dynamic_instructions=component_specific_instructions,
            target_json_example=llm_json_example
        )
        
        if not training_plan_details:
            raise HTTPException(status_code=400, detail="Failed to parse the edited training plan")
        
        # Override LLM-calculated totalHours with auto-calculated values
        for section in training_plan_details.sections:
            section_title = section.title
            if section_title in auto_calculated_hours:
                original_hours = section.totalHours
                section.totalHours = auto_calculated_hours[section_title]
                logger.info(f"[FINALIZE] Overrode totalHours for '{section_title}': {original_hours} -> {section.totalHours}")
            else:
                logger.warning(f"[FINALIZE] No auto-calculated hours found for section: '{section_title}'")
        
        # Post-process module IDs to ensure № character is preserved
        for section in training_plan_details.sections:
            if section.id:
                # Fix module IDs that lost the № character
                if section.id.isdigit():
                    # Plain number like "2" -> "№2"
                    section.id = f"№{section.id}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                elif section.id.startswith("#"):
                    # Hash format like "#2" -> "№2"
                    number = section.id[1:]
                    section.id = f"№{number}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                elif not section.id.startswith("№"):
                    # Other formats without № - try to extract number and format correctly
                    import re
                    number_match = re.search(r'\d+', section.id)
                    if number_match:
                        number = number_match.group()
                        section.id = f"№{number}"
                        logger.info(f"[FINALIZE_ID_FIX] Fixed ID format to '{section.id}'")
        
        # Update the project with the new content
        await conn.execute("""
            UPDATE projects 
            SET microproduct_content = $1
            WHERE id = $2 AND onyx_user_id = $3
        """, json.dumps(training_plan_details.dict()), payload.projectId, onyx_user_id)
        
        logger.info(f"[FINALIZE_SUCCESS] Updated training plan projectId={payload.projectId}")
        
        # Clean up the cache
        OUTLINE_PREVIEW_CACHE.pop(payload.chatSessionId, None)
        
        return {"success": True, "message": "Training plan updated successfully"}
        
    except Exception as e:
        logger.error(f"[FINALIZE_ERROR] Error finalizing training plan: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to finalize training plan: {str(e)}")

# --- Folders API Models ---
class ProjectFolderCreateRequest(BaseModel):
    name: str
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None  # { presentation, one_pager, quiz, video_lesson }

class ProjectFolderResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

class ProjectFolderListResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    order: int
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None
    project_count: int
    total_lessons: int
    total_hours: int
    total_completion_time: int
    model_config = {"from_attributes": True}

class ProjectFolderRenameRequest(BaseModel):
    name: str

class ProjectFolderMoveRequest(BaseModel):
    parent_id: Optional[int] = None

class ProjectFolderTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

# --- Folders API Endpoints ---
@app.get("/api/custom/projects/folders", response_model=List[ProjectFolderListResponse])
async def list_folders(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT 
            pf.id, 
            pf.name, 
            pf.created_at, 
            pf."order", 
            pf.parent_id,
            COALESCE(pf.quality_tier, 'medium') as quality_tier,
            COALESCE(pf.custom_rate, 200) as custom_rate,
            pf.is_advanced as is_advanced,
            pf.advanced_rates as advanced_rates,
            pf.completion_times as completion_times,
            COUNT(p.id) as project_count,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COUNT(*)::int 
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_lessons,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'hours' IS NOT NULL AND lesson->>'hours' != '' 
                                    THEN (lesson->>'hours')::float
                                    ELSE 0 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_hours,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                    THEN (
                                        -- Extract numeric part using regex, handling all language units (m, м, хв)
                                        CASE 
                                            WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                            THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                            ELSE 5
                                        END
                                    )
                                    ELSE 5 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_completion_time
        FROM project_folders pf
        LEFT JOIN projects p ON pf.id = p.folder_id
        WHERE pf.onyx_user_id = $1
        GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id, pf.is_advanced, pf.advanced_rates
        ORDER BY pf."order" ASC, pf.created_at ASC;
    """
    async with pool.acquire() as conn:
        rows = await conn.fetch(query, onyx_user_id)
    return [ProjectFolderListResponse(**dict(row)) for row in rows]

@app.get("/api/custom/projects/folders/{folder_id}", response_model=ProjectFolderResponse)
async def get_folder(folder_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Get a specific folder by ID"""
    query = """
        SELECT 
            pf.id, 
            pf.name, 
            pf.created_at, 
            pf.parent_id,
            COALESCE(pf.quality_tier, 'medium') as quality_tier,
            COALESCE(pf.custom_rate, 200) as custom_rate,
            pf.is_advanced as is_advanced,
            pf.advanced_rates as advanced_rates,
            pf.completion_times as completion_times
        FROM project_folders pf
        WHERE pf.id = $1 AND pf.onyx_user_id = $2
    """
    async with pool.acquire() as conn:
        row = await conn.fetchrow(query, folder_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Folder not found")
    return ProjectFolderResponse(**dict(row))

@app.post("/api/custom/projects/folders", response_model=ProjectFolderResponse)
async def create_folder(req: ProjectFolderCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    async with pool.acquire() as conn:
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
        
        query = "INSERT INTO project_folders (onyx_user_id, name, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates;"
        row = await conn.fetchrow(query, onyx_user_id, req.name, req.parent_id, req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None)
    return ProjectFolderResponse(**dict(row))

@app.patch("/api/custom/projects/folders/{folder_id}", response_model=ProjectFolderResponse)
async def rename_folder(folder_id: int, req: ProjectFolderRenameRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "UPDATE project_folders SET name = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates;"
    async with pool.acquire() as conn:
        row = await conn.fetchrow(query, req.name, folder_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Folder not found")
    return ProjectFolderResponse(**dict(row))

@app.delete("/api/custom/projects/folders/{folder_id}", status_code=204)
async def delete_folder(folder_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # Set folder_id to NULL for all projects in this folder (preserve projects)
    async with pool.acquire() as conn:
        await conn.execute("UPDATE projects SET folder_id = NULL WHERE folder_id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
        result = await conn.execute("DELETE FROM project_folders WHERE id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
    if result == "DELETE 0":
        raise HTTPException(status_code=404, detail="Folder not found")
    return JSONResponse(status_code=204, content={})

@app.put("/api/custom/projects/folders/{folder_id}/move", response_model=ProjectFolderResponse)
async def move_folder(folder_id: int, req: ProjectFolderMoveRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Move a folder to a different parent folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
            
            # Prevent circular references - check if the target parent is a descendant of this folder
            if req.parent_id == folder_id:
                raise HTTPException(status_code=400, detail="Cannot move folder into itself")
            
            # Check for circular references by traversing up the tree
            current_parent_id = req.parent_id
            while current_parent_id is not None:
                if current_parent_id == folder_id:
                    raise HTTPException(status_code=400, detail="Cannot move folder into its own descendant")
                parent = await conn.fetchrow(
                    "SELECT parent_id FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                    current_parent_id, onyx_user_id
                )
                if not parent:
                    break
                current_parent_id = parent['parent_id']
        
        # Update the folder's parent_id
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET parent_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at, parent_id",
            req.parent_id, folder_id, onyx_user_id
        )
        
        return ProjectFolderResponse(**dict(updated_folder))

@app.patch("/api/custom/projects/folders/{folder_id}/tier", response_model=ProjectFolderResponse)
async def update_folder_tier(folder_id: int, req: ProjectFolderTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier of a folder and recalculate creation hours for all projects in the folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the folder's quality_tier/custom_rate and advanced fields
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET quality_tier = $1, custom_rate = $2, is_advanced = COALESCE($3, is_advanced), advanced_rates = COALESCE($4, advanced_rates), completion_times = COALESCE($5, completion_times) WHERE id = $6 AND onyx_user_id = $7 RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates, completion_times",
            req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None, json.dumps(req.completion_times) if req.completion_times is not None else None, folder_id, onyx_user_id
        )
        
        # Get all projects in this folder (including subfolders recursively)
        projects_to_update = await conn.fetch("""
            WITH RECURSIVE folder_tree AS (
                -- Base case: the target folder
                SELECT id, parent_id FROM project_folders WHERE id = $1
                UNION ALL
                -- Recursive case: child folders
                SELECT pf.id, pf.parent_id 
                FROM project_folders pf
                INNER JOIN folder_tree ft ON pf.parent_id = ft.id
            )
            SELECT DISTINCT p.id, p.microproduct_content, p.folder_id
            FROM projects p
            INNER JOIN folder_tree ft ON p.folder_id = ft.id
            WHERE p.microproduct_content IS NOT NULL 
            AND p.microproduct_content->>'sections' IS NOT NULL
        """, folder_id)
        
        # Update creation hours for each project based on the new tier and custom rate
        for project in projects_to_update:
            try:
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    total_completion_time = 0
                    
                    # Calculate total completion time and update tier names and hours
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            # Clear any existing module-level tier settings to ensure folder-level tier takes precedence
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            
                            # Update the module's tier name to match the new folder tier
                            section['quality_tier'] = req.quality_tier
                                
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Clear any existing lesson-level tier settings to ensure folder-level tier takes precedence
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    
                                    # Update the tier name to match the new folder tier
                                    lesson['quality_tier'] = req.quality_tier

                                    # Always update recommendations when tier changes to ensure they match the new tier
                                    try:
                                        lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                                lesson.get('title', ''),
                                                req.quality_tier,
                                                {
                                                    'presentation': False,
                                                    'one-pager': False,
                                                    'quiz': False,
                                                    'video-lesson': False,
                                                }
                                            )
                                        # Also record a deterministic completion_breakdown and completionTime
                                        try:
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                            ranges = {
                                                'one-pager': (2,3),
                                                'presentation': (5,10),
                                                'quiz': (5,7),
                                                'video-lesson': (2,5),
                                            }
                                            breakdown = {}
                                            total_m = 0
                                            for p in primary:
                                                r = ranges.get(p)
                                                if r:
                                                    mid = int(round((r[0]+r[1])/2))
                                                    breakdown[p] = mid
                                                    total_m += mid
                                            if total_m > 0:
                                                lesson['completion_breakdown'] = breakdown
                                                lesson['completionTime'] = f"{total_m}m"
                                        except Exception:
                                            pass
                                    except Exception:
                                        pass
                                    
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to total completion time
                                    total_completion_time += completion_time_minutes
                                    
                                    # Recalculate hours considering advanced per-product rates if enabled
                                    try:
                                        primary = []
                                        if isinstance(lesson.get('recommended_content_types'), dict):
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                        is_adv = bool(updated_folder.get('is_advanced'))
                                        adv_rates = updated_folder.get('advanced_rates') if is_adv else None
                                        if is_adv and primary:
                                            breakdown = lesson.get('completion_breakdown') if isinstance(lesson.get('completion_breakdown'), dict) else None
                                            rates = {
                                                'presentation': (adv_rates or {}).get('presentation') or req.custom_rate,
                                                'one_pager': (adv_rates or {}).get('one_pager') or req.custom_rate,
                                                'quiz': (adv_rates or {}).get('quiz') or req.custom_rate,
                                                'video_lesson': (adv_rates or {}).get('video_lesson') or req.custom_rate,
                                            }
                                            total_hours = 0.0
                                            if breakdown:
                                                for p in primary:
                                                    key = 'one_pager' if p == 'one-pager' else ('video_lesson' if p == 'video-lesson' else p)
                                                    minutes = breakdown.get(p, 0)
                                                    total_hours += (minutes / 60.0) * float(rates.get(key, req.custom_rate))
                                            else:
                                                per = max(1, int(round(completion_time_minutes / max(1, len(primary)))))
                                                for p in primary:
                                                    key = 'one_pager' if p == 'one-pager' else ('video_lesson' if p == 'video-lesson' else p)
                                                    total_hours += (per / 60.0) * float(rates.get(key, req.custom_rate))
                                            lesson_creation_hours = int(round(total_hours))
                                        else:
                                            lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    except Exception:
                                        lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project['id']} creation hours: {e}")
                continue
        
        return ProjectFolderResponse(**dict(updated_folder))

# --- Update project queries to support folder_id (backward compatible) ---
# In all project list endpoints, add folder_id to SELECT and response models, and allow filtering by folder_id (optional)
# ... existing code ...

class ProjectFolderUpdateRequest(BaseModel):
    folder_id: Optional[int] = None
    model_config = {"from_attributes": True}

@app.put("/api/custom/projects/update/{project_id}", response_model=ProjectDB)
async def update_project_in_db(project_id: int, project_update_data: ProjectUpdateRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        # Get user identifiers for workspace access
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        
        db_microproduct_name_to_store = project_update_data.microProductName
        current_component_name = None
        # Fetch current component_name, project_name and content to detect renames/diffs
        old_project_name: Optional[str] = None
        old_microproduct_content: Optional[dict] = None
        async with pool.acquire() as conn:
            # Check if user owns the project or has workspace access
            project_row = await conn.fetchrow("""
                SELECT p.project_name, p.microproduct_content, p.onyx_user_id, dt.component_name 
                FROM projects p 
                JOIN design_templates dt ON p.design_template_id = dt.id 
                WHERE p.id = $1 AND (
                    p.onyx_user_id = $2 
                    OR EXISTS (
                        SELECT 1 FROM product_access pa
                        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                        WHERE pa.product_id = p.id 
                          AND wm.user_id = $3 
                          AND wm.status = 'active'
                          AND pa.access_type IN ('workspace', 'role', 'individual')
                          AND (
                              pa.access_type = 'workspace' 
                              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                              OR (pa.access_type = 'individual' AND pa.target_id = $3)
                          )
                    )
                )
            """, project_id, user_uuid, user_email)
            if not project_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or not owned by user.")
            current_component_name = project_row["component_name"]
            project_owner_id = project_row["onyx_user_id"]
            old_project_name = project_row["project_name"]
            try:
                old_microproduct_content = dict(project_row["microproduct_content"]) if project_row["microproduct_content"] else None
            except Exception:
                old_microproduct_content = project_row["microproduct_content"] if isinstance(project_row["microproduct_content"], dict) else None

        if (not db_microproduct_name_to_store or not db_microproduct_name_to_store.strip()) and project_update_data.design_template_id:
            async with pool.acquire() as conn: design_row = await conn.fetchrow("SELECT template_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
            if design_row: db_microproduct_name_to_store = design_row["template_name"]

        content_to_store_for_db = project_update_data.microProductContent if project_update_data.microProductContent else None
        
        # 🔍 BACKEND SAVE LOGGING: What we're about to store in database
        if content_to_store_for_db:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Storing content to DB: {json.dumps(content_to_store_for_db, indent=2)}")
            if 'contentBlocks' in content_to_store_for_db:
                image_blocks = [block for block in content_to_store_for_db['contentBlocks'] if block.get('type') == 'image']
                logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Image blocks to store: {json.dumps(image_blocks, indent=2)}")
        else:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - No content to store (content_to_store_for_db is None)")

        derived_product_type = None; derived_microproduct_type = None
        if project_update_data.design_template_id is not None:
            async with pool.acquire() as conn: design_template = await conn.fetchrow("SELECT microproduct_type, template_name, component_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
            if design_template:
                derived_product_type = design_template["microproduct_type"]
                derived_microproduct_type = design_template["template_name"]
                current_component_name = design_template["component_name"]

        update_clauses = []; update_values = []; arg_idx = 1
        
        # Handle project name updates and sync with Training Plan mainTitle
        project_name_updated = False
        if project_update_data.projectName is not None: 
            update_clauses.append(f"project_name = ${arg_idx}")
            update_values.append(project_update_data.projectName)
            arg_idx += 1
            project_name_updated = True
        if db_microproduct_name_to_store is not None: update_clauses.append(f"microproduct_name = ${arg_idx}"); update_values.append(db_microproduct_name_to_store); arg_idx +=1
        if project_update_data.design_template_id is not None:
            update_clauses.append(f"design_template_id = ${arg_idx}"); update_values.append(project_update_data.design_template_id); arg_idx +=1
            if derived_product_type: update_clauses.append(f"product_type = ${arg_idx}"); update_values.append(derived_product_type); arg_idx += 1
            if derived_microproduct_type: update_clauses.append(f"microproduct_type = ${arg_idx}"); update_values.append(derived_microproduct_type); arg_idx += 1
        if project_update_data.microProductContent is not None: 
            update_clauses.append(f"microproduct_content = ${arg_idx}")
            update_values.append(content_to_store_for_db); arg_idx += 1
            
            # Ensure lessons have recommendations when storing training plan
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN and content_to_store_for_db:
                try:
                    for section in (content_to_store_for_db.get('sections') or []):
                        lessons = section.get('lessons') or []
                        for lesson in lessons:
                            if isinstance(lesson, dict) and ('recommended_content_types' not in lesson or not lesson['recommended_content_types']):
                                lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                    lesson.get('title', ''),
                                    lesson.get('quality_tier') or section.get('quality_tier') or content_to_store_for_db.get('quality_tier'),
                                    {'presentation': False, 'one-pager': False, 'quiz': False, 'video-lesson': False}
                                )
                                # Also generate completion_breakdown for advanced mode support
                                try:
                                    primary = lesson['recommended_content_types'].get('primary', [])
                                    ranges = {
                                        'one-pager': (2,3),
                                        'presentation': (5,10),
                                        'quiz': (5,7),
                                        'video-lesson': (2,5),
                                    }
                                    breakdown = {}
                                    total_m = 0
                                    for p in primary:
                                        r = ranges.get(p)
                                        if r:
                                            mid = int(round((r[0]+r[1])/2))
                                            breakdown[p] = mid
                                            total_m += mid
                                    if total_m > 0:
                                        lesson['completion_breakdown'] = breakdown
                                        lesson['completionTime'] = f"{total_m}m"
                                except Exception:
                                    pass
                except Exception:
                    pass
            
            # SYNC TITLES: For Training Plans, keep project_name and mainTitle synchronized
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN and content_to_store_for_db:
                try:
                    # Extract mainTitle from the content
                    main_title = content_to_store_for_db.get('mainTitle')
                    if main_title and isinstance(main_title, str) and main_title.strip():
                        # Update project_name to match mainTitle
                        update_clauses.append(f"project_name = ${arg_idx}")
                        update_values.append(main_title.strip())
                        arg_idx += 1
                        project_name_updated = True
                except Exception as e:
                    logger.warning(f"Could not sync mainTitle to project_name for project {project_id}: {e}")

        # SYNC TITLES: If only project_name was updated (not content), sync it to mainTitle for Training Plans
        if (project_name_updated and project_update_data.microProductContent is None and 
            current_component_name == COMPONENT_NAME_TRAINING_PLAN):
            try:
                # Get current content to update mainTitle
                async with pool.acquire() as conn:
                    current_row = await conn.fetchrow(
                        "SELECT microproduct_content FROM projects WHERE id = $1 AND onyx_user_id = $2", 
                        project_id, onyx_user_id
                    )
                    if current_row and current_row["microproduct_content"]:
                        current_content = dict(current_row["microproduct_content"])
                        current_content["mainTitle"] = project_update_data.projectName
                        update_clauses.append(f"microproduct_content = ${arg_idx}")
                        update_values.append(current_content)
                        arg_idx += 1
            except Exception as e:
                logger.warning(f"Could not sync project_name to mainTitle for project {project_id}: {e}")

        if not update_clauses:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No update data provided.")

        update_values.extend([project_id])
        update_query = f"UPDATE projects SET {', '.join(update_clauses)} WHERE id = ${arg_idx} RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name, microproduct_content, design_template_id, created_at, custom_rate, quality_tier, is_advanced, advanced_rates;"

        async with pool.acquire() as conn: row = await conn.fetchrow(update_query, *update_values)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or update failed.")

        # --- Propagate outline/lesson renames to connected products (best-effort) ---
        try:
            # Only for Training Plans
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                new_project_name = row["project_name"]
                # 1) If outline name changed, update prefix for all connected products
                if project_name_updated and old_project_name and new_project_name and old_project_name.strip() != new_project_name.strip():
                    old_prefix = f"{old_project_name.strip()}: "
                    new_prefix = f"{new_project_name.strip()}: "
                    async with pool.acquire() as conn:
                        children = await conn.fetch(
                            "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND is_standalone = FALSE AND project_name LIKE $2",
                            project_owner_id, old_prefix + "%"
                        )
                        for child in children:
                            child_id = child["id"]
                            child_pn = child["project_name"] or ""
                            child_mpname = child["microproduct_name"]
                            # Compute new names
                            if ": " in child_pn:
                                suffix = child_pn.split(": ", 1)[1]
                                updated_project_name = new_prefix + suffix
                            else:
                                updated_project_name = child_pn  # unexpected, skip
                            # Update microproduct_name if it matches the old full or is None
                            updated_micro_name = child_mpname
                            if isinstance(child_mpname, str):
                                if child_mpname == child_pn:
                                    updated_micro_name = updated_project_name
                            elif child_mpname is None:
                                updated_micro_name = updated_project_name
                            await conn.execute(
                                "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name) WHERE id = $3 AND onyx_user_id = $4",
                                updated_project_name, updated_micro_name, child_id, project_owner_id
                            )
                
                # 2) If lesson titles changed inside outline content, rename exact-matching children
                # Compute simple diff by position (module, lesson index)
                def extract_titles(content: Optional[dict]) -> list[list[str]]:
                    titles: list[list[str]] = []
                    if not content or not isinstance(content, dict):
                        return titles
                    sections = content.get("sections") or []
                    for sec in sections:
                        sec_titles: list[str] = []
                        lessons = sec.get("lessons") if isinstance(sec, dict) else []
                        for les in lessons:
                            if isinstance(les, dict):
                                title = str(les.get("title") or les.get("name") or "").strip()
                            else:
                                title = str(les).strip()
                            sec_titles.append(title)
                        titles.append(sec_titles)
                    return titles
                old_titles_by_section = extract_titles(old_microproduct_content)
                new_titles_by_section = extract_titles(content_to_store_for_db if project_update_data.microProductContent is not None else old_microproduct_content)
                rename_pairs: list[tuple[str, str]] = []
                if old_titles_by_section and new_titles_by_section and len(old_titles_by_section) == len(new_titles_by_section):
                    for sec_idx in range(len(old_titles_by_section)):
                        old_ls = old_titles_by_section[sec_idx]
                        new_ls = new_titles_by_section[sec_idx]
                        for li in range(min(len(old_ls), len(new_ls))):
                            old_t = (old_ls[li] or "").strip()
                            new_t = (new_ls[li] or "").strip()
                            if old_t and new_t and old_t != new_t:
                                rename_pairs.append((old_t, new_t))
                if rename_pairs:
                    async with pool.acquire() as conn:
                        for (old_title, new_title) in rename_pairs:
                            old_full = f"{(row['project_name'] or new_project_name).strip()}: {old_title}"
                            new_full = f"{(row['project_name'] or new_project_name).strip()}: {new_title}"
                            children = await conn.fetch(
                                "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2",
                                project_owner_id, old_full
                            )
                            for child in children:
                                child_id = child["id"]
                                child_mpname = child["microproduct_name"]
                                child_content = child["microproduct_content"]
                                # Update microproduct_name smartly: replace exact matches or lesson-only
                                updated_micro_name = child_mpname
                                if isinstance(child_mpname, str):
                                    if child_mpname == old_full:
                                        updated_micro_name = new_full
                                    elif child_mpname == old_title:
                                        updated_micro_name = new_title
                                elif child_mpname is None:
                                    updated_micro_name = new_full
                                # Update content titles if present
                                updated_content = child_content
                                try:
                                    if isinstance(child_content, dict):
                                        # Quiz
                                        if 'quizTitle' in child_content and isinstance(child_content['quizTitle'], str):
                                            if child_content['quizTitle'].strip() in (old_title, old_full):
                                                child_content['quizTitle'] = new_title
                                        # Text Presentation
                                        if 'textTitle' in child_content and isinstance(child_content['textTitle'], str):
                                            if child_content['textTitle'].strip() in (old_title, old_full):
                                                child_content['textTitle'] = new_title
                                        updated_content = child_content
                                except Exception as e:
                                    logger.warning(f"[RENAME_PROPAGATION] Failed to update content titles for child {child_id}: {e}")
                                await conn.execute(
                                    "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name), microproduct_content = COALESCE($3, microproduct_content) WHERE id = $4 AND onyx_user_id = $5",
                                    new_full, updated_micro_name, updated_content, child_id, project_owner_id
                                )
        except Exception as e:
            logger.error(f"[RENAME_PROPAGATION] Error during rename propagation for project {project_id}: {e}", exc_info=not IS_PRODUCTION)

        db_content = row["microproduct_content"]
        
        # 🔍 BACKEND RETRIEVE LOGGING: What we got back from database
        logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Retrieved content from DB: {json.dumps(db_content, indent=2) if db_content else 'None'}")
        if db_content and isinstance(db_content, dict) and 'contentBlocks' in db_content:
            image_blocks = [block for block in db_content['contentBlocks'] if block.get('type') == 'image']
            logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Image blocks retrieved: {json.dumps(image_blocks, indent=2)}")
        
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - About to validate with component: {current_component_name}")
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - Validating as TextPresentationDetails")
                    final_content_for_model = TextPresentationDetails(**db_content)
                    logger.info(f"✅ [BACKEND VALIDATION] Project {project_id} - TextPresentationDetails validation successful")
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content and db_content['slides']:
                        db_content['slides'] = normalize_slide_props(db_content['slides'], current_component_name)
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                
                # 🔍 BACKEND VALIDATION RESULT LOGGING
                if final_content_for_model and hasattr(final_content_for_model, 'contentBlocks'):
                    result_dict = final_content_for_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final validated content: {json.dumps(result_dict, indent=2)}")
                    if 'contentBlocks' in result_dict:
                        result_image_blocks = [block for block in result_dict['contentBlocks'] if block.get('type') == 'image']
                        logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final image blocks: {json.dumps(result_image_blocks, indent=2)}")
                
            except Exception as e_parse:
                logger.error(f"❌ [BACKEND VALIDATION ERROR] Project {project_id} - Error parsing updated content from DB: {e_parse}", exc_info=not IS_PRODUCTION)

        return ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_model,
            design_template_id=row["design_template_id"], created_at=row["created_at"],
            custom_rate=row["custom_rate"], quality_tier=row["quality_tier"]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating project." if IS_PRODUCTION else f"DB error on project update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/projects/{project_id}/folder", response_model=ProjectDB)
async def update_project_folder(project_id: int, update_data: ProjectFolderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update a project's folder assignment and recalculate creation hours based on the new folder's tier"""
    async with pool.acquire() as conn:
        # Verify project belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # If folder_id is provided, verify it exists and belongs to user
        if update_data.folder_id is not None:
            folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                update_data.folder_id, onyx_user_id
            )
            if not folder:
                raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the project's folder_id
        updated_project = await conn.fetchrow(
            "UPDATE projects SET folder_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING *",
            update_data.folder_id, project_id, onyx_user_id
        )
        
        # If the project has content and is being moved to a folder, recalculate creation hours
        if update_data.folder_id is not None and project['microproduct_content']:
            try:
                # Get the folder's custom rate
                folder_custom_rate = await get_folder_custom_rate(update_data.folder_id, pool)
                
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    content = sanitize_training_plan_for_parse(dict(content))
                    sections = content['sections']
                    
                    # Update the hours in each lesson and recalculate section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Calculate hours using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with tier-adjusted sum
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Round all hours in the content to ensure they are integers
                    content = round_hours_in_content(content)
                    
                    # Update the project in the database with new hours
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project_id
                    )
                    
                    # Update the returned project data
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        project_id, onyx_user_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours after folder move: {e}")
        
        # Parse the content properly based on component type
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        
        if db_content and isinstance(db_content, dict):
            try:
                # Get the component name to determine the content type
                component_row = await conn.fetchrow(
                    "SELECT dt.component_name FROM projects p JOIN design_templates dt ON p.design_template_id = dt.id WHERE p.id = $1",
                    project_id
                )
                current_component_name = component_row["component_name"] if component_row else COMPONENT_NAME_TRAINING_PLAN
                
                # Round hours to integers before parsing to prevent float validation errors
                if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = round_hours_in_content(db_content)
                
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"],
            custom_rate=updated_project.get("custom_rate"),
            quality_tier=updated_project.get("quality_tier"),
            is_advanced=updated_project.get("is_advanced"),
            advanced_rates=updated_project.get("advanced_rates")
        )

@app.patch("/api/custom/projects/{project_id}/tier", response_model=ProjectDB)
async def update_project_tier(project_id: int, req: ProjectTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier, custom rate, and advanced rates of a project and recalculate creation hours"""
    async with pool.acquire() as conn:
        # Verify the project exists and belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # Update the project's quality_tier, custom_rate, and advanced fields
        updated_project = await conn.fetchrow(
            "UPDATE projects SET quality_tier = $1, custom_rate = $2, is_advanced = COALESCE($3, is_advanced), advanced_rates = COALESCE($4, advanced_rates), completion_times = COALESCE($5, completion_times) WHERE id = $6 AND onyx_user_id = $7 RETURNING *",
            req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None, json.dumps(req.completion_times) if req.completion_times is not None else None, project_id, onyx_user_id
        )
        
        # If the project has content, recalculate creation hours
        if project['microproduct_content']:
            try:
                content = dict(project['microproduct_content'])
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    
                    # Update tier names, update recommendations, and sum existing hours for section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            section['quality_tier'] = req.quality_tier
                            
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    lesson['quality_tier'] = req.quality_tier

                                    try:
                                        # Always update recommendations when tier changes to ensure they match the new tier
                                        lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                            lesson.get('title', ''),
                                            req.quality_tier,
                                            {
                                                'presentation': False,
                                                'one-pager': False,
                                                'quiz': False,
                                                'video-lesson': False,
                                            }
                                        )
                                        # Also generate completion_breakdown for advanced mode support
                                        try:
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                            ranges = {
                                                'one-pager': (2,3),
                                                'presentation': (5,10),
                                                'quiz': (5,7),
                                                'video-lesson': (2,5),
                                            }
                                            breakdown = {}
                                            total_m = 0
                                            for p in primary:
                                                r = ranges.get(p)
                                                if r:
                                                    mid = int(round((r[0]+r[1])/2))
                                                    breakdown[p] = mid
                                                    total_m += mid
                                            if total_m > 0:
                                                lesson['completion_breakdown'] = breakdown
                                                lesson['completionTime'] = f"{total_m}m"
                                        except Exception:
                                            pass
                                    except Exception:
                                        pass
                                    
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Recalculate hours with new project rate using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
                    # Re-fetch the updated project
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1",
                        project_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours: {e}")
        
        # Get current component name for proper content parsing
        current_component_name = None
        if updated_project["design_template_id"]:
            design_template = await conn.fetchrow(
                "SELECT component_name FROM design_templates WHERE id = $1", 
                updated_project["design_template_id"]
            )
            if design_template:
                current_component_name = design_template["component_name"]
        
        # Parse the content into the appropriate model
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"],
            custom_rate=updated_project["custom_rate"],
            quality_tier=updated_project["quality_tier"],
            is_advanced=updated_project.get("is_advanced"),
            advanced_rates=updated_project.get("advanced_rates")
        )

@app.get("/api/custom/projects/{project_id}/effective-rates")
async def get_effective_rates(
    project_id: int, 
    section_index: Optional[int] = None, 
    lesson_index: Optional[int] = None, 
    onyx_user_id: str = Depends(get_current_onyx_user_id), 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get effective advanced rates for a project/section/lesson following inheritance chain"""
    async with pool.acquire() as conn:
        # Get project and folder data
        project_row = await conn.fetchrow(
            """
            SELECT p.*, pf.is_advanced as folder_is_advanced, pf.advanced_rates as folder_advanced_rates, 
                   pf.custom_rate as folder_custom_rate, pf.completion_times as folder_completion_times
            FROM projects p
            LEFT JOIN project_folders pf ON p.folder_id = pf.id
            WHERE p.id = $1 AND p.onyx_user_id = $2
            """,
            project_id, onyx_user_id
        )
        if not project_row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        project = dict(project_row)
        
        # Extract section and lesson if specified
        section = None
        lesson = None
        if project.get("microproduct_content"):
            content = project["microproduct_content"]
            if isinstance(content, dict) and isinstance(content.get('sections'), list):
                sections = content['sections']
                if section_index is not None and 0 <= section_index < len(sections):
                    section = sections[section_index]
                    if isinstance(section, dict) and isinstance(section.get('lessons'), list):
                        lessons = section['lessons']
                        if lesson_index is not None and 0 <= lesson_index < len(lessons):
                            lesson = lessons[lesson_index]
        
        # Resolve effective advanced config following inheritance: lesson > section > project > folder
        is_advanced = False
        rates = {}
        completion_times = {}
        completion_times = {}
        
        # Start with folder defaults
        if project.get('folder_is_advanced'):
            is_advanced = True
            if project.get('folder_advanced_rates'):
                try:
                    folder_rates = project['folder_advanced_rates']
                    if isinstance(folder_rates, str):
                        folder_rates = json.loads(folder_rates)
                    rates.update(folder_rates)
                except:
                    pass
            if project.get('folder_completion_times'):
                try:
                    folder_completion_times = project['folder_completion_times']
                    if isinstance(folder_completion_times, str):
                        folder_completion_times = json.loads(folder_completion_times)
                    completion_times.update(folder_completion_times)
                except:
                    pass
        folder_single_rate = project.get('folder_custom_rate') or 200
        
        # Override with project level
        if project.get('is_advanced') is not None:
            is_advanced = bool(project['is_advanced'])
        if project.get('advanced_rates'):
            try:
                project_rates = project['advanced_rates']
                if isinstance(project_rates, str):
                    project_rates = json.loads(project_rates)
                rates.update(project_rates)
            except:
                pass
        if project.get('completion_times'):
            try:
                project_completion_times = project['completion_times']
                if isinstance(project_completion_times, str):
                    project_completion_times = json.loads(project_completion_times)
                completion_times.update(project_completion_times)
            except:
                pass
        project_single_rate = project.get('custom_rate') or folder_single_rate
        
        # Override with section level
        if section:
            if section.get('advanced') is not None:
                is_advanced = bool(section['advanced'])
            if section.get('advancedRates'):
                section_rates = section['advancedRates']
                if isinstance(section_rates, dict):
                    # Convert frontend naming to backend naming
                    backend_rates = {}
                    if 'presentation' in section_rates:
                        backend_rates['presentation'] = section_rates['presentation']
                    if 'onePager' in section_rates:
                        backend_rates['one_pager'] = section_rates['onePager']
                    if 'quiz' in section_rates:
                        backend_rates['quiz'] = section_rates['quiz']
                    if 'videoLesson' in section_rates:
                        backend_rates['video_lesson'] = section_rates['videoLesson']
                    rates.update(backend_rates)
            if section.get('completionTimes'):
                section_completion_times = section['completionTimes']
                if isinstance(section_completion_times, dict):
                    # Convert frontend naming to backend naming
                    backend_completion_times = {}
                    if 'presentation' in section_completion_times:
                        backend_completion_times['presentation'] = section_completion_times['presentation']
                    if 'onePager' in section_completion_times:
                        backend_completion_times['one_pager'] = section_completion_times['onePager']
                    if 'quiz' in section_completion_times:
                        backend_completion_times['quiz'] = section_completion_times['quiz']
                    if 'videoLesson' in section_completion_times:
                        backend_completion_times['video_lesson'] = section_completion_times['videoLesson']
                    completion_times.update(backend_completion_times)
            section_single_rate = section.get('custom_rate') or project_single_rate
        else:
            section_single_rate = project_single_rate
        
        # Override with lesson level
        if lesson:
            if lesson.get('advanced') is not None:
                is_advanced = bool(lesson['advanced'])
            if lesson.get('advancedRates'):
                lesson_rates = lesson['advancedRates']
                if isinstance(lesson_rates, dict):
                    # Convert frontend naming to backend naming
                    backend_rates = {}
                    if 'presentation' in lesson_rates:
                        backend_rates['presentation'] = lesson_rates['presentation']
                    if 'onePager' in lesson_rates:
                        backend_rates['one_pager'] = lesson_rates['onePager']
                    if 'quiz' in lesson_rates:
                        backend_rates['quiz'] = lesson_rates['quiz']
                    if 'videoLesson' in lesson_rates:
                        backend_rates['video_lesson'] = lesson_rates['videoLesson']
                    rates.update(backend_rates)
            if lesson.get('completionTimes'):
                lesson_completion_times = lesson['completionTimes']
                if isinstance(lesson_completion_times, dict):
                    # Convert frontend naming to backend naming
                    backend_completion_times = {}
                    if 'presentation' in lesson_completion_times:
                        backend_completion_times['presentation'] = lesson_completion_times['presentation']
                    if 'onePager' in lesson_completion_times:
                        backend_completion_times['one_pager'] = lesson_completion_times['onePager']
                    if 'quiz' in lesson_completion_times:
                        backend_completion_times['quiz'] = lesson_completion_times['quiz']
                    if 'videoLesson' in lesson_completion_times:
                        backend_completion_times['video_lesson'] = lesson_completion_times['videoLesson']
                    completion_times.update(backend_completion_times)
            lesson_single_rate = lesson.get('custom_rate') or section_single_rate
        else:
            lesson_single_rate = section_single_rate
        
        fallback_single_rate = lesson_single_rate
        
        # Fill in missing rates with fallback
        default_rates = {
            'presentation': fallback_single_rate,
            'one_pager': fallback_single_rate,
            'quiz': fallback_single_rate,
            'video_lesson': fallback_single_rate
        }
        for key in default_rates:
            if key not in rates:
                rates[key] = default_rates[key]
        
        return {
            "is_advanced": is_advanced,
            "rates": {
                "presentation": rates.get('presentation', fallback_single_rate),
                "one_pager": rates.get('one_pager', fallback_single_rate),
                "quiz": rates.get('quiz', fallback_single_rate),
                "video_lesson": rates.get('video_lesson', fallback_single_rate),
            },
            "completion_times": {
                "presentation": completion_times.get('presentation', 8),  # Will be replaced with proper inheritance logic
                "one_pager": completion_times.get('one_pager', 3),
                "quiz": completion_times.get('quiz', 6),
                "video_lesson": completion_times.get('video_lesson', 4),
            },
            "fallback_single_rate": fallback_single_rate
        }

class ProjectOrderUpdateRequest(BaseModel):
    orders: List[Dict[str, int]]  # List of {projectId: int, order: int}

@app.put("/api/custom/projects/update-order", status_code=status.HTTP_200_OK)
async def update_project_order(order_data: ProjectOrderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the order of multiple projects"""
    try:
        async with pool.acquire() as conn:
            # Update each project's order
            for order_item in order_data.orders:
                project_id = order_item.get('projectId')
                order = order_item.get('order')
                
                if project_id is not None and order is not None:
                    # Verify project belongs to user and update order
                    result = await conn.execute(
                        "UPDATE projects SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                        order, project_id, onyx_user_id
                    )
                    
                    if result == "UPDATE 0":
                        logger.warning(f"Project {project_id} not found or not owned by user {onyx_user_id}")
        
        return {"message": "Project order updated successfully"}
    except Exception as e:
        logger.error(f"Error updating project order: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update project order")

@app.put("/api/custom/projects/folders/update-order")
async def update_folder_order(
    orders: List[Dict[str, int]], 
    onyx_user_id: str = Depends(get_current_onyx_user_id), 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Update the order of folders"""
    async with pool.acquire() as conn:
        for order_data in orders:
            folder_id = order_data.get("folderId")
            order = order_data.get("order")
            if folder_id is not None and order is not None:
                await conn.execute(
                    "UPDATE project_folders SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                    order, folder_id, onyx_user_id
                )
    return {"message": "Folder order updated successfully"}

@app.get("/api/custom/projects/{project_id}/lesson-data")
async def get_project_lesson_data(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Get lesson data for a project with tier-adjusted creation hours"""
    try:
        # Get user identifiers for workspace access
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        
        async with pool.acquire() as conn:
            # Get project details including folder_id (with workspace access check)
            project = await conn.fetchrow("""
                SELECT p.microproduct_content, p.folder_id, dt.component_name 
                FROM projects p 
                JOIN design_templates dt ON p.design_template_id = dt.id 
                WHERE p.id = $1 AND (
                    p.onyx_user_id = $2 
                    OR EXISTS (
                        SELECT 1 FROM product_access pa
                        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                        WHERE pa.product_id = p.id 
                          AND wm.user_id = $3 
                          AND wm.status = 'active'
                          AND pa.access_type IN ('workspace', 'role', 'individual')
                          AND (
                              pa.access_type = 'workspace' 
                              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                              OR (pa.access_type = 'individual' AND pa.target_id = $3)
                          )
                    )
                )
            """, project_id, user_uuid, user_email)
            
            if not project:
                raise HTTPException(status_code=404, detail="Project not found")
            
            content = project["microproduct_content"]
            component_name = project["component_name"]
            folder_id = project["folder_id"]
            
            # Only Training Plans have lesson data
            if component_name != COMPONENT_NAME_TRAINING_PLAN or not content:
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            
            # Get the folder's custom rate (with inheritance from parent)
            folder_custom_rate = 200  # Default custom rate
            if folder_id:
                folder_custom_rate = await get_folder_custom_rate(folder_id, pool)
            
            # Parse the training plan content
            try:
                if isinstance(content, dict):
                    sections = content.get("sections", [])
                    total_lessons = 0
                    total_hours = 0
                    total_completion_time = 0
                    sections_data = []
                    
                    for section in sections:
                        if isinstance(section, dict):
                            lessons = section.get("lessons", [])
                            section_lessons = len(lessons)
                            section_hours = 0
                            section_completion_time = 0
                            
                            total_lessons += section_lessons
                            
                            # Sum up completion time and use existing lesson hours for this section
                            for lesson in lessons:
                                if isinstance(lesson, dict):
                                    # Parse completion time (handles all language units: m, м, хв) - treat missing as 5 minutes
                                    completion_time_str = lesson.get("completionTime", "")
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            # Extract numeric part using regex to handle all language units
                                            import re
                                            numbers = re.findall(r'\d+', time_str)
                                            if numbers:
                                                try:
                                                    # If it contains 'h' (hour indicator), convert to minutes
                                                    if 'h' in time_str.lower():
                                                        completion_time_minutes = int(numbers[0]) * 60
                                                    else:
                                                        # For minutes (m, м, хв), just use the number
                                                        completion_time_minutes = int(numbers[0])
                                                except (ValueError, IndexError):
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # No numbers found, use 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to totals
                                    section_completion_time += completion_time_minutes
                                    total_completion_time += completion_time_minutes
                                    
                                    # Use existing lesson hours if available, otherwise calculate with folder rate
                                    if lesson.get('hours'):
                                        try:
                                            lesson_creation_hours = float(lesson['hours'])
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                        except (ValueError, TypeError):
                                            # If hours parsing fails, calculate with completion time
                                            lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                    else:
                                        # No existing hours, calculate with completion time
                                        lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                        section_hours += lesson_creation_hours
                                        total_hours += lesson_creation_hours
                            
                            # Add section data with tier-adjusted totals
                            sections_data.append({
                                "id": section.get("id", ""),
                                "title": section.get("title", ""),
                                "totalHours": round(section_hours),
                                "totalCompletionTime": section_completion_time,
                                "lessonCount": section_lessons
                            })
                    
                    return {
                        "lessonCount": total_lessons, 
                        "totalHours": round(total_hours), 
                        "completionTime": total_completion_time,
                        "sections": sections_data
                    }
                else:
                    return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            except Exception as e:
                logger.warning(f"Error parsing lesson data for project {project_id}: {e}")
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting lesson data for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to get lesson data")

@app.get("/api/custom/pdf/projects-list", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_projects_list_pdf(
    folder_id: Optional[int] = Query(None),
    column_visibility: Optional[str] = Query(None),  # JSON string of column visibility settings
    client_name: Optional[str] = Query(None),  # Client name for PDF header customization
    selected_folders: Optional[str] = Query(None),  # JSON string of selected folder IDs
    selected_projects: Optional[str] = Query(None),  # JSON string of selected project IDs
    column_widths: Optional[str] = Query(None),  # JSON string of column width settings
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download projects list as PDF with all folders expanded, deduplicated like the products page."""
    try:
        # Parse column visibility settings
        column_visibility_settings = {
            'title': True,
            'created': False,
            'creator': False,
            'numberOfLessons': True,
            'estCreationTime': True,
            'estCompletionTime': True
        }
        if column_visibility:
            try:
                parsed_settings = json.loads(column_visibility)
                column_visibility_settings.update(parsed_settings)
            except json.JSONDecodeError:
                logger.warning("Invalid column_visibility JSON, using defaults")

        # Parse selected projects first to use in the query
        selected_project_ids = set()
        if selected_projects:
            try:
                selected_project_ids = set(json.loads(selected_projects))
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing selected_projects: {e}")

        # Fetch projects and folders data
        async with pool.acquire() as conn:
            # Fetch projects
            projects_query = """
                SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
                       dt.template_name as design_template_name,
                       dt.microproduct_type as design_microproduct_type,
                       p.folder_id, p."order", p.microproduct_content, p.quality_tier
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.onyx_user_id = $1
            """
            projects_params = [onyx_user_id]
            param_count = 1
            
            if folder_id is not None:
                projects_query += f" AND p.folder_id = ${param_count + 1}"
                projects_params.append(folder_id)
                param_count += 1
            
            # Add selected projects filter if provided
            if selected_project_ids:
                placeholders = ','.join([f'${i + param_count + 1}' for i in range(len(selected_project_ids))])
                projects_query += f" AND p.id IN ({placeholders})"
                projects_params.extend(selected_project_ids)
            
            projects_query += " ORDER BY p.\"order\" ASC, p.created_at DESC;"
            
            projects_rows = await conn.fetch(projects_query, *projects_params)
            
            # Fetch folders with hierarchical structure (only if not viewing a specific folder)
            folders_data = []
            if folder_id is None:
                folders_query = """
                    SELECT 
                        pf.id, 
                        pf.name, 
                        pf.created_at, 
                        pf."order", 
                        pf.parent_id,
                        pf.quality_tier,
                        pf.custom_rate,
                        COUNT(p.id) as project_count,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COUNT(*)::int 
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_lessons,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM((lesson->>'hours')::float), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_hours,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM(
                                            CASE 
                                                WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                                THEN (
                                                    -- Extract numeric part using regex, handling all language units (m, м, хв)
                                                    CASE 
                                                        WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                                        THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                                        ELSE 5
                                                    END
                                                )
                                                ELSE 5 
                                            END
                                        ), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_completion_time
                    FROM project_folders pf
                    LEFT JOIN projects p ON pf.id = p.folder_id
                    WHERE pf.onyx_user_id = $1
                    GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id, pf.quality_tier, pf.custom_rate
                    ORDER BY pf."order" ASC, pf.created_at ASC;
                """
                folders_rows = await conn.fetch(folders_query, onyx_user_id)
                folders_data = [dict(row) for row in folders_rows]

        # Process projects data
        projects_data = []
        for row in projects_rows:
            row_dict = dict(row)
            
            # Calculate individual project times
            total_lessons = 0
            total_hours = 0.0
            total_completion_time = 0
            
            if row_dict.get('microproduct_content') and isinstance(row_dict['microproduct_content'], dict):
                content = row_dict['microproduct_content']
                if content.get('sections') and isinstance(content['sections'], list):
                    for section in content['sections']:
                        if section.get('lessons') and isinstance(section['lessons'], list):
                            for lesson in section['lessons']:
                                total_lessons += 1
                                if lesson.get('hours'):
                                    try:
                                        total_hours += float(lesson['hours'])
                                    except (ValueError, TypeError):
                                        pass
                                
                                # Calculate completion time - treat missing completion time as 5 minutes
                                completion_time_str = lesson.get('completionTime', '')
                                if completion_time_str:
                                    time_str = str(completion_time_str).strip()
                                    if time_str and time_str != '':
                                        if time_str.endswith('m'):
                                            try:
                                                minutes = int(time_str[:-1])
                                                total_completion_time += minutes
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.endswith('h'):
                                            try:
                                                hours = int(time_str[:-1])
                                                total_completion_time += (hours * 60)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.isdigit():
                                            try:
                                                total_completion_time += int(time_str)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        else:
                                            total_completion_time += 5  # Fallback to 5 minutes
                                    else:
                                        total_completion_time += 5  # Empty string, use 5 minutes
                                else:
                                    total_completion_time += 5  # No completion time, use 5 minutes
            
            projects_data.append({
                'id': row_dict['id'],
                'title': row_dict.get('project_name') or row_dict.get('microproduct_name') or 'Untitled',
                'created_at': row_dict['created_at'],
                'created_by': 'You',
                'design_microproduct_type': row_dict.get('design_microproduct_type'),
                'folder_id': row_dict.get('folder_id'),
                'order': row_dict.get('order', 0),
                'microproduct_content': row_dict.get('microproduct_content'),
                'total_lessons': total_lessons,
                'total_hours': round(total_hours),
                'total_completion_time': total_completion_time
            })

        # --- Deduplicate projects: only show top-level products and outlines, hide lessons/quizzes that belong to an outline ---
        def deduplicate_projects(projects_arr):
            outline_names = set()
            filtered_projects = []
            grouped = {}
            # First pass: collect all outline names and group by title
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    outline_names.add(proj['title'].strip())
                if proj['title'] not in grouped:
                    grouped[proj['title']] = {'outline': None, 'others': []}
                if is_outline:
                    if not grouped[proj['title']]['outline']:
                        grouped[proj['title']]['outline'] = proj
                else:
                    grouped[proj['title']]['others'].append(proj)
            # Second pass: filter projects
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    filtered_projects.append(proj)
                else:
                    project_title = proj['title'].strip()
                    belongs_to_outline = False
                    group_for_this_title = grouped[proj['title']]
                    if group_for_this_title and group_for_this_title['outline']:
                        belongs_to_outline = True
                    if not belongs_to_outline and ': ' in project_title:
                        outline_part = project_title.split(': ')[0].strip()
                        if outline_part in outline_names:
                            belongs_to_outline = True
                    if not belongs_to_outline:
                        filtered_projects.append(proj)
            return filtered_projects

        projects_data = deduplicate_projects(projects_data)

        # Build folder tree structure
        def build_folder_tree(folders):
            folder_map = {}
            root_folders = []
            
            # Create folder map
            for folder in folders:
                folder['children'] = []
                folder_map[folder['id']] = folder
            
            # Build tree structure
            for folder in folders:
                if folder['parent_id'] is None:
                    root_folders.append(folder)
                else:
                    parent = folder_map.get(folder['parent_id'])
                    if parent:
                        parent['children'].append(folder)
            
            return root_folders

        # Group projects by folder
        folder_projects = {}
        unassigned_projects = []
        
        for project in projects_data:
            if project['folder_id']:
                if project['folder_id'] not in folder_projects:
                    folder_projects[project['folder_id']] = []
                folder_projects[project['folder_id']].append(project)
            else:
                unassigned_projects.append(project)

        # Build hierarchical folder structure
        folder_tree = build_folder_tree(folders_data) if folders_data else []

        # Calculate recursive totals for folders (including subfolder projects)
        def calculate_recursive_totals(folder):
            # Start with direct project totals
            direct_projects = folder_projects.get(folder['id'], [])
            total_lessons = sum(p['total_lessons'] for p in direct_projects)
            total_hours = sum(p['total_hours'] for p in direct_projects)
            total_completion_time = sum(p['total_completion_time'] for p in direct_projects)
            total_items = len(direct_projects)
            
            # Add subfolder totals recursively
            if folder.get('children'):
                for child in folder['children']:
                    child_totals = calculate_recursive_totals(child)
                    total_lessons += child_totals['total_lessons']
                    total_hours += child_totals['total_hours']
                    total_completion_time += child_totals['total_completion_time']
                    total_items += child_totals['total_items']
            
            # Update folder with recursive totals
            folder['total_lessons'] = total_lessons
            folder['total_hours'] = total_hours
            folder['total_completion_time'] = total_completion_time
            folder['project_count'] = total_items
            
            return {
                'total_lessons': total_lessons,
                'total_hours': total_hours,
                'total_completion_time': total_completion_time,
                'total_items': total_items
            }

        # Calculate recursive totals for all root folders
        for folder in folder_tree:
            calculate_recursive_totals(folder)

        # Helper function to get tier color
        def get_tier_color(tier):
            tier_colors = {
                'basic': '#22c55e',        # green-500
                'interactive': '#f97316',  # orange-500
                'advanced': '#a855f7',     # purple-500
                'immersive': '#3b82f6',    # blue-500
                # Legacy tier support
                'starter': '#22c55e',      # green-500 (mapped to basic)
                'medium': '#f97316',       # orange-500 (mapped to interactive)
                'professional': '#3b82f6'  # blue-500 (mapped to immersive)
            }
            return tier_colors.get(tier, '#f97316')  # default to interactive

        # Helper function to check if folder has course outlines
        def has_course_outlines(folder_id):
            projects = folder_projects.get(folder_id, [])
            return any(p.get('design_microproduct_type', '').lower() == 'training plan' for p in projects)

        # Helper function to check if folder or any subfolder has course outlines
        def has_course_outlines_recursive(folder):
            # Check direct projects
            if has_course_outlines(folder['id']):
                return True
            
            # Check subfolders recursively
            if folder.get('children'):
                for child in folder['children']:
                    if has_course_outlines_recursive(child):
                        return True
            
            return False

        # Add tier information and check for course outlines
        def add_tier_info(folder):
            folder['tier_color'] = get_tier_color(folder.get('quality_tier', 'interactive'))
            folder['has_course_outlines'] = has_course_outlines_recursive(folder)
            
            # Recursively process children
            if folder.get('children'):
                for child in folder['children']:
                    add_tier_info(child)

        # Add tier information to all folders
        for folder in folder_tree:
            add_tier_info(folder)

        # Filter data based on selected folders (projects are already filtered in the query)
        if selected_folders:
            try:
                selected_folder_ids = set()
                
                # Parse selected folders
                if selected_folders:
                    selected_folder_ids = set(json.loads(selected_folders))
                
                # Filter folders - only include selected folders and their children
                def filter_folders_recursive(folders_list):
                    filtered_folders = []
                    for folder in folders_list:
                        # Include folder if it's selected or if any of its children are selected
                        if folder['id'] in selected_folder_ids:
                            filtered_folders.append(folder)
                        else:
                            # Check if any children are selected
                            if folder.get('children'):
                                filtered_children = filter_folders_recursive(folder['children'])
                                if filtered_children:
                                    folder_copy = folder.copy()
                                    folder_copy['children'] = filtered_children
                                    filtered_folders.append(folder_copy)
                    return filtered_folders
                
                filtered_folder_tree = filter_folders_recursive(folder_tree)
                
                # Filter folder projects - only include projects from selected folders
                filtered_folder_projects = {}
                for folder_id, projects in folder_projects.items():
                    if folder_id in selected_folder_ids:
                        filtered_folder_projects[folder_id] = projects
                
                # Use filtered data
                folder_tree = filtered_folder_tree
                folder_projects = filtered_folder_projects
                # Note: unassigned_projects are already filtered by the query when selected_projects is provided
                
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing selected folders: {e}. Using all data.")
                # If parsing fails, use all data (fallback)

        # Parse column widths if provided
        column_widths_settings = {}
        if column_widths:
            try:
                column_widths_settings = json.loads(column_widths)
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing column widths: {e}. Using default widths.")
                column_widths_settings = {}

        # Calculate summary statistics for the mini table
        def calculate_summary_stats(folders, folder_projects, unassigned_projects):
            total_projects = 0
            total_lessons = 0
            total_creation_time = 0
            total_completion_time = 0
            
            # Calculate from folders and their projects
            for folder in folders:
                if folder['id'] in folder_projects:
                    for project in folder_projects[folder['id']]:
                        total_projects += 1
                        total_lessons += project.get('total_lessons', 0) or 0
                        total_creation_time += project.get('total_hours', 0) or 0
                        total_completion_time += project.get('total_completion_time', 0) or 0
                
                # Recursively calculate from subfolders
                if folder.get('children'):
                    child_stats = calculate_summary_stats(folder['children'], folder_projects, [])
                    total_projects += child_stats['total_projects']
                    total_lessons += child_stats['total_lessons']
                    total_creation_time += child_stats['total_creation_time']
                    total_completion_time += child_stats['total_completion_time']
            
            # Add unassigned projects
            for project in unassigned_projects:
                total_projects += 1
                total_lessons += project.get('total_lessons', 0) or 0
                total_creation_time += project.get('total_hours', 0) or 0
                total_completion_time += project.get('total_completion_time', 0) or 0
            
            return {
                'total_projects': total_projects,
                'total_lessons': total_lessons,
                'total_creation_time': total_creation_time,
                'total_completion_time': total_completion_time
            }

        # Calculate summary statistics
        summary_stats = calculate_summary_stats(folder_tree, folder_projects, unassigned_projects)

        # Collect all project IDs that are actually included in the filtered PDF data
        included_project_ids = set()
        
        # Add projects from filtered folders
        for folder_id_key, projects in folder_projects.items():
            for project in projects:
                included_project_ids.add(project['id'])
        
        # Add filtered unassigned projects
        for project in unassigned_projects:
            included_project_ids.add(project['id'])
        
        logger.info(f"[PDF_ANALYTICS] Found {len(included_project_ids)} projects included in PDF: {list(included_project_ids)}")

        # Fetch real analytics data for pie charts using only the projects actually included in the PDF
        product_distribution = None
        quality_distribution = None
        
        logger.info(f"[PDF_ANALYTICS] Starting analytics data fetch for user: {onyx_user_id}, included projects: {len(included_project_ids)}")
        
        try:
            # Use a new connection for analytics queries since the original conn might be released
            async with pool.acquire() as analytics_conn:
                # Get product distribution data - since lessons don't have pre-computed recommendations,
                # we'll generate them on-the-fly using Python logic
                if included_project_ids:
                    # Convert project IDs to a format suitable for SQL IN clause
                    project_ids_list = list(included_project_ids)
                    project_ids_placeholder = ','.join(['$' + str(i+2) for i in range(len(project_ids_list))])
                    
                    lessons_query = f"""
                        SELECT 
                            p.id as project_id,
                            lesson->>'title' as lesson_title,
                            lesson->>'quality_tier' as lesson_quality_tier,
                            p.quality_tier as project_quality_tier,
                            pf.quality_tier as folder_quality_tier
                        FROM projects p
                        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                        LEFT JOIN project_folders pf ON p.folder_id = pf.id
                        CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        WHERE p.onyx_user_id = $1
                        AND p.id IN ({project_ids_placeholder})
                        AND p.microproduct_content IS NOT NULL
                        AND p.microproduct_content->>'sections' IS NOT NULL
                        AND dt.component_name = 'TrainingPlanTable'
                    """
                    product_params = [onyx_user_id] + project_ids_list
                else:
                    # No projects included, use empty result set
                    lessons_query = "SELECT NULL as project_id, NULL as lesson_title, NULL as lesson_quality_tier, NULL as project_quality_tier, NULL as folder_quality_tier WHERE FALSE"
                    product_params = []
                
                logger.info(f"[PDF_ANALYTICS] Lessons query: {lessons_query}")
                logger.info(f"[PDF_ANALYTICS] Lessons params: {product_params}")
                
                lessons_rows = await analytics_conn.fetch(lessons_query, *product_params)
                logger.info(f"[PDF_ANALYTICS] Found {len(lessons_rows)} lessons for product analysis")
                
                # Generate recommendations for each lesson using the existing function
                product_counts = {}
                total_products = 0
                
                for row in lessons_rows:
                    lesson_title = row['lesson_title'] or ''
                    # Determine effective quality tier
                    quality_tier = (
                        row['lesson_quality_tier'] or 
                        row['project_quality_tier'] or 
                        row['folder_quality_tier'] or 
                        'interactive'
                    )
                    
                    logger.info(f"[PDF_ANALYTICS] Processing lesson: '{lesson_title}' (quality: {quality_tier})")
                    
                    # Generate recommendations using the existing function
                    recommendations = analyze_lesson_content_recommendations(lesson_title, quality_tier)
                    primary_types = recommendations.get('primary', [])
                    
                    logger.info(f"[PDF_ANALYTICS] Generated recommendations: {primary_types}")
                    
                    # Count each recommended product type
                    for product_type_str in primary_types:
                        total_products += 1
                        
                        # Map to our ProductType enum
                        product_type_mapping = {
                            'one-pager': ProductType.ONE_PAGER,
                            'presentation': ProductType.PRESENTATION,
                            'quiz': ProductType.QUIZ,
                            'video-lesson': ProductType.VIDEO_LESSON
                        }
                        
                        product_type = product_type_mapping.get(product_type_str)
                        if product_type:
                            if product_type not in product_counts:
                                product_counts[product_type] = 0
                            product_counts[product_type] += 1
                            logger.info(f"[PDF_ANALYTICS] Added {product_type_str} -> {product_type}, count now: {product_counts[product_type]}")
                        else:
                            logger.warning(f"[PDF_ANALYTICS] Unknown product type: {product_type_str}")
                
                # We've already computed product_counts and total_products above using Python logic
                
                logger.info(f"[PDF_ANALYTICS] Total products: {total_products}")
                logger.info(f"[PDF_ANALYTICS] Product counts: {product_counts}")
                
                # Create product distribution data for template
                product_distribution = {
                    'total_products': total_products,
                    'one_pager_count': product_counts.get(ProductType.ONE_PAGER, 0),
                    'presentation_count': product_counts.get(ProductType.PRESENTATION, 0),
                    'quiz_count': product_counts.get(ProductType.QUIZ, 0),
                    'video_lesson_count': product_counts.get(ProductType.VIDEO_LESSON, 0)
                }
                
                logger.info(f"[PDF_ANALYTICS] Product distribution before percentages: {product_distribution}")
                
                # Calculate percentages
                if total_products > 0:
                    product_distribution['one_pager_percentage'] = round((product_distribution['one_pager_count'] / total_products * 100), 1)
                    product_distribution['presentation_percentage'] = round((product_distribution['presentation_count'] / total_products * 100), 1)
                    product_distribution['quiz_percentage'] = round((product_distribution['quiz_count'] / total_products * 100), 1)
                    product_distribution['video_lesson_percentage'] = round((product_distribution['video_lesson_count'] / total_products * 100), 1)
                else:
                    product_distribution['one_pager_percentage'] = 0
                    product_distribution['presentation_percentage'] = 0
                    product_distribution['quiz_percentage'] = 0
                    product_distribution['video_lesson_percentage'] = 0
                
                logger.info(f"[PDF_ANALYTICS] Final product distribution: {product_distribution}")
                
                # Get quality distribution data using the same project filtering
                if included_project_ids:
                    # Use the same project IDs that were included in the product analysis
                    quality_query = f"""
                        WITH lesson_quality_tiers AS (
                            SELECT 
                                COALESCE(
                                    lesson->>'quality_tier',
                                    section->>'quality_tier', 
                                    p.quality_tier,
                                    pf.quality_tier,
                                    'interactive'
                                ) as effective_quality_tier
                            FROM projects p
                            LEFT JOIN project_folders pf ON p.folder_id = pf.id
                            CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                            WHERE p.onyx_user_id = $1
                            AND p.id IN ({project_ids_placeholder})
                            AND p.microproduct_content IS NOT NULL
                            AND p.microproduct_content->>'sections' IS NOT NULL
                        )
                        SELECT 
                            LOWER(effective_quality_tier) as quality_tier,
                            COUNT(*) as count
                        FROM lesson_quality_tiers
                        GROUP BY LOWER(effective_quality_tier)
                        ORDER BY count DESC
                    """
                    quality_params = [onyx_user_id] + project_ids_list
                else:
                    # No projects included, use empty result set
                    quality_query = "SELECT NULL as quality_tier, 0 as count WHERE FALSE"
                    quality_params = []
                
                logger.info(f"[PDF_ANALYTICS] Quality query: {quality_query}")
                logger.info(f"[PDF_ANALYTICS] Quality params: {quality_params}")
                
                quality_rows = await analytics_conn.fetch(quality_query, *quality_params)
                logger.info(f"[PDF_ANALYTICS] Quality query returned {len(quality_rows)} rows")
                
                # Process quality distribution
                tier_counts = {}
                total_lessons = 0
                
                for row in quality_rows:
                    tier_name = row['quality_tier'].lower()
                    count = row['count']
                    total_lessons += count
                    logger.info(f"[PDF_ANALYTICS] Raw quality tier: {tier_name}, count: {count}")
                    
                    # Map tier names to enum values
                    tier_mapping = {
                        'basic': 'basic',
                        'interactive': 'interactive',
                        'advanced': 'advanced',
                        'immersive': 'immersive',
                        'medium': 'interactive',  # Map medium to interactive
                        'premium': 'advanced',    # Map premium to advanced
                    }
                    
                    tier = tier_mapping.get(tier_name, 'interactive')
                    logger.info(f"[PDF_ANALYTICS] Mapped quality tier {tier_name} -> {tier}")
                    if tier not in tier_counts:
                        tier_counts[tier] = 0
                    tier_counts[tier] += count
                    logger.info(f"[PDF_ANALYTICS] Added {count} to {tier}, total now: {tier_counts[tier]}")
                
                logger.info(f"[PDF_ANALYTICS] Total lessons: {total_lessons}")
                logger.info(f"[PDF_ANALYTICS] Tier counts: {tier_counts}")
                
                # Create quality distribution data for template
                quality_distribution = {
                    'total_lessons': total_lessons,
                    'basic_count': tier_counts.get('basic', 0),
                    'interactive_count': tier_counts.get('interactive', 0),
                    'advanced_count': tier_counts.get('advanced', 0),
                    'immersive_count': tier_counts.get('immersive', 0)
                }
                
                logger.info(f"[PDF_ANALYTICS] Quality distribution before percentages: {quality_distribution}")
                
                # Calculate percentages
                if total_lessons > 0:
                    quality_distribution['basic_percentage'] = round((quality_distribution['basic_count'] / total_lessons * 100), 1)
                    quality_distribution['interactive_percentage'] = round((quality_distribution['interactive_count'] / total_lessons * 100), 1)
                    quality_distribution['advanced_percentage'] = round((quality_distribution['advanced_count'] / total_lessons * 100), 1)
                    quality_distribution['immersive_percentage'] = round((quality_distribution['immersive_count'] / total_lessons * 100), 1)
                else:
                    quality_distribution['basic_percentage'] = 0
                    quality_distribution['interactive_percentage'] = 0
                    quality_distribution['advanced_percentage'] = 0
                    quality_distribution['immersive_percentage'] = 0
                
                logger.info(f"[PDF_ANALYTICS] Final quality distribution: {quality_distribution}")
                
        except Exception as e:
            logger.error(f"[PDF_ANALYTICS] Failed to fetch analytics data for PDF: {str(e)}", exc_info=True)
            # Use fallback data if analytics fetch fails
            product_distribution = {
                'total_products': 0,
                'one_pager_count': 0,
                'presentation_count': 0,
                'quiz_count': 0,
                'video_lesson_count': 0,
                'one_pager_percentage': 0,
                'presentation_percentage': 0,
                'quiz_percentage': 0,
                'video_lesson_percentage': 0
            }
            quality_distribution = {
                'total_lessons': 0,
                'basic_count': 0,
                'interactive_count': 0,
                'advanced_count': 0,
                'immersive_count': 0,
                'basic_percentage': 0,
                'interactive_percentage': 0,
                'advanced_percentage': 0,
                'immersive_percentage': 0
            }
            logger.info(f"[PDF_ANALYTICS] Using fallback data due to error")

        # Prepare data for template
        template_data = {
            'folders': folder_tree,  # Use hierarchical structure
            'folder_projects': folder_projects,
            'unassigned_projects': unassigned_projects,
            'column_visibility': column_visibility_settings,
            'column_widths': column_widths_settings,
            'folder_id': folder_id,
            'client_name': client_name,  # Client name for header customization
            'generated_at': datetime.now().isoformat(),
            'summary_stats': summary_stats,  # Add summary statistics to template data
            'product_distribution': product_distribution,  # Add real product distribution data
            'quality_distribution': quality_distribution   # Add real quality distribution data
        }
        
        logger.info(f"[PDF_ANALYTICS] Template data prepared:")
        logger.info(f"[PDF_ANALYTICS] - product_distribution: {product_distribution}")
        logger.info(f"[PDF_ANALYTICS] - quality_distribution: {quality_distribution}")
        logger.info(f"[PDF_ANALYTICS] - summary_stats: {summary_stats}")

        # Generate PDF
        logger.info(f"[PDF_ANALYTICS] About to generate PDF with template data keys: {list(template_data.keys())}")
        logger.info(f"[PDF_ANALYTICS] Template data summary:")
        logger.info(f"[PDF_ANALYTICS] - folders count: {len(template_data.get('folders', []))}")
        logger.info(f"[PDF_ANALYTICS] - folder_projects count: {len(template_data.get('folder_projects', {}))}")
        logger.info(f"[PDF_ANALYTICS] - unassigned_projects count: {len(template_data.get('unassigned_projects', []))}")
        logger.info(f"[PDF_ANALYTICS] - product_distribution: {template_data.get('product_distribution')}")
        logger.info(f"[PDF_ANALYTICS] - quality_distribution: {template_data.get('quality_distribution')}")
        
        unique_output_filename = f"projects_list_{onyx_user_id}_{uuid.uuid4().hex[:12]}.pdf"
        pdf_path = await generate_pdf_from_html_template("projects_list_pdf_template.html", template_data, unique_output_filename)
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        user_friendly_filename = f"projects_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating projects list PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate PDF: {str(e)[:200]}")


# Quiz endpoints
class QuizWizardPreview(BaseModel):
    outlineId: Optional[int] = None  # Parent Training Plan project id
    lesson: Optional[str] = None      # Specific lesson to generate quiz for, optional when prompt-based
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"  # comma-separated question types
    questionCount: int = 10  # Number of questions to generate
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources

class QuizWizardFinalize(BaseModel):
    outlineId: Optional[int] = None
    lesson: str
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    aiResponse: str                        # User-edited quiz data
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"
    questionCount: int = 10  # Number of questions to generate
    language: str = "en"
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: user edits tracking (like in Course Outline)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False

class QuizEditRequest(BaseModel):
    currentContent: str
    editPrompt: str
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    questionTypes: Optional[str] = None
    language: str = "en"
    fromFiles: bool = False
    fromText: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    textMode: Optional[str] = None
    questionCount: int = 10
    chatSessionId: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False

async def _ensure_quiz_template(pool: asyncpg.Pool) -> int:
    """Ensure quiz design template exists, return template ID"""
    try:
        # Check if quiz template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Quiz' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create quiz template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Quiz Template",
            "Create an interactive quiz with various question types including multiple choice, multi-select, matching, sorting, and open answer questions.",
            "Quiz",
            COMPONENT_NAME_QUIZ,
            "/quiz.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring quiz template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure quiz template")

@app.post("/api/custom/quiz/generate")
async def quiz_generate(payload: QuizWizardPreview, request: Request):
    """Generate quiz content with streaming response"""
    logger.info(f"[QUIZ_PREVIEW_START] Quiz preview initiated")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] questionTypes={payload.questionTypes} lang={payload.language}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_PREVIEW_CHAT] Creating new chat session")
        try:
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz",
        "prompt": payload.prompt or "Create a quiz",
        "language": payload.language,
        "questionTypes": payload.questionTypes,
        "questionCount": payload.questionCount,
    }

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"Failed to create virtual file for large text: {e}")
                # Fallback to chunking if virtual file creation fails
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wiz_payload["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for quiz generation")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    # Enforce diverse question types by default unless explicitly told otherwise
    diversity_note = ""
    try:
        chosen_types_csv = (payload.questionTypes or "").strip()
        if not chosen_types_csv:
            default_types = ["multiple-choice", "multi-select", "matching", "sorting", "open-answer"]
            wiz_payload["questionTypes"] = ",".join(default_types)
            chosen_types = default_types
            enforce_diverse = True
        else:
            chosen_types = [t.strip() for t in chosen_types_csv.split(",") if t.strip()]
            enforce_diverse = len(chosen_types) > 1
        if enforce_diverse:
            total = payload.questionCount or 10
            base = total // len(chosen_types)
            remainder = total % len(chosen_types)
            # Prioritize non-multiple-choice types for remainders to reduce MC dominance
            remainder_order = [t for t in chosen_types if t != "multiple-choice"] + [t for t in chosen_types if t == "multiple-choice"]
            counts = {t: base for t in chosen_types}
            for i in range(remainder):
                counts[remainder_order[i % len(remainder_order)]] += 1
            plan_str = ", ".join([f"{t}: {counts[t]}" for t in chosen_types])
            diversity_note = (
                f"CRITICAL QUIZ DIVERSITY INSTRUCTION: Generate a balanced mix of question types. "
                f"Unless the user's prompt explicitly requests a different mix, distribute the {total} questions across types exactly as follows: {plan_str}. "
                f"Do not exceed a difference of +1 between any two types. Avoid making 'multiple-choice' more than any other type unless required by the user."
            )
    except Exception as e:
        logger.warning(f"[QUIZ_DIVERSITY_NOTE] Failed to build diversity instruction: {e}")
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations - For quizzes: questions, answers, explanations ALL must be in {payload.language}" + (("\n" + diversity_note) if diversity_note else "")  

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[QUIZ_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[QUIZ_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[QUIZ_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    # For connector-based filtering, extract context from specific connectors
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                    file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Quiz"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[QUIZ_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[QUIZ_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[QUIZ_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[QUIZ_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[QUIZ_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[QUIZ_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/quiz/edit")
async def quiz_edit(payload: QuizEditRequest, request: Request):
    """Edit quiz content with streaming response"""
    logger.info(f"[QUIZ_EDIT_START] Quiz edit initiated")
    logger.info(f"[QUIZ_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,
        "originalContent": payload.currentContent,
        "editMode": True,
        "isCleanContent": payload.isCleanContent
    }

    # Add context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName
    if payload.questionTypes:
        wiz_payload["questionTypes"] = payload.questionTypes
    if payload.questionCount:
        wiz_payload["questionCount"] = payload.questionCount

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided
    if payload.fromText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[QUIZ_EDIT_STREAM] Starting streamer")
        logger.info(f"[QUIZ_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for quiz editing
        logger.info(f"[QUIZ_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for quiz editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[QUIZ_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[QUIZ_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[QUIZ_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[QUIZ_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[QUIZ_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        
        # NEW: Cache the quiz content for later finalization
        if chat_id:
            QUIZ_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[QUIZ_PREVIEW_CACHE] Cached quiz content for chat_id={chat_id}, length={len(assistant_reply)}")
        
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/quiz/finalize")
async def quiz_finalize(payload: QuizWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize quiz creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get user ID and deduct credits for quiz creation
    try:
        credits_needed = calculate_product_credits("quiz")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Quiz creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for quiz creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for quiz creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this quiz finalization to prevent duplicates
    quiz_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this quiz is already being processed
    if quiz_key in ACTIVE_QUIZ_FINALIZE_KEYS:
        logger.warning(f"[QUIZ_FINALIZE_DUPLICATE] Quiz finalization already in progress for key: {quiz_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Quiz finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(quiz_key)
    QUIZ_FINALIZE_TIMESTAMPS[quiz_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Cleaned up stale quiz key: {stale_key}")
    
    try:
        # NEW: Check for user edits and decide strategy (like in Course Outline)
        use_direct_parser = False
        use_ai_parser = True
        
        if payload.hasUserEdits and payload.originalContent:
            # User has made edits - check if they're significant
            any_changes = _any_quiz_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No quiz changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Quiz changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # Ensure quiz template exists
        template_id = await _ensure_quiz_template(pool)
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lesson.strip()
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip()}"
                        logger.info(f"[QUIZ_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[QUIZ_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[QUIZ_FINALIZE_NAMING] Failed to fetch outline name for quiz naming: {e}")
                # Continue with plain lesson title if outline fetch fails
        else:
            logger.info(f"[QUIZ_FINALIZE_NAMING] No outline ID provided, using standalone naming: {project_name}")
        
        logger.info(f"[QUIZ_FINALIZE_START] Starting quiz finalization for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] quiz_key: {quiz_key}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] isCleanContent: {payload.isCleanContent}")
        
        # NEW: Choose parsing strategy based on user edits
        if use_direct_parser:
            # DIRECT PARSER PATH: Use cached content directly since no changes were made
            logger.info("Using direct parser path for quiz finalization")
            
            # Use the original content for parsing since no changes were made
            content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
            
            parsed_quiz = await parse_ai_response_with_llm(
                ai_response=content_to_parse,
                project_name=project_name,
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
            logger.info("Direct parser path completed successfully")
        else:
            # AI PARSER PATH: Use AI for parsing (original behavior)
            logger.info("Using AI parser path for quiz finalization")
            
            # NEW: Handle clean content (questions only) differently
            if payload.isCleanContent:
                logger.info("Processing clean content (questions only) - will generate options and answers")
                # For clean content, we need to generate complete quiz with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains ONLY quiz questions without options or answers. You need to generate a complete quiz with:
                1. Multiple choice options (A, B, C, D) for each question
                2. Correct answers
                3. Explanations for each answer

                REQUIREMENTS:
                1. Extract the quiz title from the content or use the lesson name
                2. For each question, generate:
                   - "question_type": "multiple-choice" (default)
                   - "question_text": The question text
                   - "options": Array with {{"id": "A", "text": "option text"}} for 4 options
                   - "correct_option_id": "A" (or appropriate letter)
                   - "explanation": Detailed explanation for the correct answer

                CRITICAL RULES:
                - Generate realistic and relevant options for each question
                - Make sure only one option is correct
                - Provide detailed explanations
                - Language: {payload.language}
                - Question types: {payload.questionTypes}
                """
            else:
                # Regular content with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """
            
                        # Parse the quiz data using LLM - only call once with consistent project name
            parsed_quiz = await parse_ai_response_with_llm(
                ai_response=payload.aiResponse,
                project_name=project_name,  # Use consistent project name
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=dynamic_instructions,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
        
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsed quiz title: {parsed_quiz.quizTitle}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Number of questions: {len(parsed_quiz.questions)}")
        
        # NEW: Hardcoded title extraction from first line of AI response
        try:
            extracted_title = project_name.split(":")[0].replace("Quiz - ", "").strip()
        except Exception as e:
            logger.error(f"[QUIZ_FINALIZE_TITLE_EXTRACTION] Error extracting title: {e}")
            extracted_title = None
        
        # Use extracted title if available, otherwise use parsed title or fallback
        if extracted_title:
            parsed_quiz.quizTitle = project_name.split(":")[-1].strip()
            logger.info(f"[QUIZ_FINALIZE_TITLE_EXTRACTION] Using hardcoded title: '{parsed_quiz.quizTitle}'")
        
        # Detect language if not provided
        if not parsed_quiz.detectedLanguage:
            parsed_quiz.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no questions, create a basic quiz structure
        if not parsed_quiz.questions:
            logger.warning(f"[QUIZ_FINALIZE_FALLBACK] LLM parsing failed for quiz, creating fallback structure")
            # Create a simple quiz with the AI response as content
            parsed_quiz.quizTitle = project_name
            parsed_quiz.questions = [
                {
                    "question_type": "open-answer",
                    "question_text": "Please review the quiz content and answer the questions.",
                    "acceptable_answers": ["See quiz content for answers"],
                    "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                }
            ]
        else:
            # Validate that all questions have the required question_type field
            valid_questions = []
            for i, question in enumerate(parsed_quiz.questions):
                if hasattr(question, 'question_type') and question.question_type:
                    valid_questions.append(question)
                else:
                    logger.warning(f"[QUIZ_FINALIZE_VALIDATION] Question {i} missing question_type, converting to open-answer")
                    # Convert to open-answer if question_type is missing
                    if hasattr(question, 'question_text'):
                        valid_questions.append({
                            "question_type": "open-answer",
                            "question_text": question.question_text,
                            "acceptable_answers": ["See original content for answer"],
                            "explanation": "This question was converted from the original format."
                        })
            
            if not valid_questions:
                logger.warning(f"[QUIZ_FINALIZE_VALIDATION] No valid questions found, creating fallback structure")
                parsed_quiz.questions = [
                    {
                        "question_type": "open-answer",
                        "question_text": "Please review the quiz content and answer the questions.",
                        "acceptable_answers": ["See quiz content for answers"],
                        "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                    }
                ]
            else:
                parsed_quiz.questions = valid_questions
        
        # Always use the consistent project name for database storage
        # The quiz title from parsed_quiz.quizTitle is used for display purposes only
        final_project_name = project_name
        
        logger.info(f"[QUIZ_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # Determine if this is a standalone quiz or part of an outline
        is_standalone_quiz = payload.outlineId is None
        
        # For quiz components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Quiz",  # product_type
                COMPONENT_NAME_QUIZ,  # microproduct_type - use the correct component name
                final_project_name,  # microproduct_name
                parsed_quiz.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_quiz,  # is_standalone
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create quiz project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        logger.info(f"[QUIZ_FINALIZE_SUCCESS] Quiz finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_quiz}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[QUIZ_FINALIZE_ERROR] Error in quiz finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(quiz_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(quiz_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Removed quiz_key from active set: {quiz_key}")

@app.delete("/api/custom/lessons/{lesson_id}", status_code=204)
async def delete_lesson(lesson_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Delete a lesson project permanently"""
    try:
        async with pool.acquire() as conn:
            # First, verify the lesson exists and belongs to the user
            lesson = await conn.fetchrow(
                "SELECT id, project_name, microproduct_type FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if not lesson:
                raise HTTPException(status_code=404, detail="Lesson not found or not owned by user")
            
            # Check if this is actually a lesson (not a training plan/course outline)
            if lesson['microproduct_type'] in ('Training Plan', 'Course Outline'):
                raise HTTPException(status_code=400, detail="Cannot delete training plans or course outlines. Please delete individual lessons instead.")
            
            # Delete the lesson
            result = await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(status_code=404, detail="Lesson not found")
            
            logger.info(f"User {onyx_user_id} deleted lesson {lesson_id} ({lesson['project_name']})")
            return JSONResponse(status_code=204, content={})
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting lesson {lesson_id} for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the lesson." if IS_PRODUCTION else f"Database error during lesson deletion: {str(e)}"
        raise HTTPException(status_code=500, detail=detail_msg)


# Default quiz JSON example for LLM parsing
DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM = """
{
  "quizTitle": "Example Quiz with All Question Types",
  "questions": [
    {
      "question_type": "multiple-choice",
      "question_text": "What is the capital of France?",
      "options": [
        {"id": "A", "text": "London"},
        {"id": "B", "text": "Paris"},
        {"id": "C", "text": "Berlin"},
        {"id": "D", "text": "Madrid"}
      ],
      "correct_option_id": "B",
      "explanation": "Paris is the capital and largest city of France."
    },
    {
      "question_type": "multi-select",
      "question_text": "Which of the following are programming languages?",
      "options": [
        {"id": "A", "text": "Python"},
        {"id": "B", "text": "HTML"},
        {"id": "C", "text": "JavaScript"},
        {"id": "D", "text": "CSS"}
      ],
      "correct_option_ids": ["A", "C"],
      "explanation": "Python and JavaScript are programming languages, while HTML and CSS are markup/styling languages."
    },
    {
      "question_type": "matching",
      "question_text": "Match the countries with their capitals:",
      "prompts": [
        {"id": "A", "text": "Germany"},
        {"id": "B", "text": "Italy"},
        {"id": "C", "text": "Spain"}
      ],
      "options": [
        {"id": "1", "text": "Berlin"},
        {"id": "2", "text": "Rome"},
        {"id": "3", "text": "Madrid"}
      ],
      "correct_matches": {"A": "1", "B": "2", "C": "3"},
      "explanation": "Germany-Berlin, Italy-Rome, Spain-Madrid are the correct country-capital pairs."
    },
    {
      "question_type": "sorting",
      "question_text": "Arrange the following steps in the correct order for a sales process:",
      "items_to_sort": [
        {"id": "step1", "text": "Identify customer needs"},
        {"id": "step2", "text": "Present solution"},
        {"id": "step3", "text": "Handle objections"},
        {"id": "step4", "text": "Close the sale"}
      ],
      "correct_order": ["step1", "step2", "step3", "step4"],
      "explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
    },
    {
      "question_type": "open-answer",
      "question_text": "What are the three key elements of an effective elevator pitch?",
      "acceptable_answers": [
        "Problem, Solution, Call to Action",
        "Problem statement, Your solution, What you want them to do next",
        "The issue, How you solve it, What action to take"
      ],
      "explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
    }
  ],
  "detectedLanguage": "en"
}

CRITICAL REQUIREMENTS:
- Output ONLY the JSON object, no other text or formatting
- Every question MUST have "question_type" field with exact values: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
- Use exact field names as shown above
- All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
- The "question_type" field is MANDATORY for every question
"""

# Default text presentation JSON example for LLM parsing
DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Example Text Presentation with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 2, "text": "Main Title of the Presentation" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

# Text Presentation Pydantic models
class TextPresentationWizardPreview(BaseModel):
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    prompt: Optional[str] = None
    language: str = "en"
    length: str = "medium"
    styles: Optional[str] = None
    fromFiles: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    fromText: bool = False
    textMode: Optional[str] = None
    userText: Optional[str] = None
    fromKnowledgeBase: bool = False
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    chatSessionId: Optional[str] = None

class TextPresentationWizardFinalize(BaseModel):
    aiResponse: str
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: User edits tracking (like in Quiz)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    isCleanContent: Optional[bool] = False
    # Connector context fields
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None
    connectorSources: Optional[str] = None

class TextPresentationEditRequest(BaseModel):
    content: str
    editPrompt: str
    language: Optional[str] = "en"  # Add language field with default fallback
    chatSessionId: Optional[str] = None
    isCleanContent: Optional[bool] = False

@app.post("/api/custom/text-presentation/generate")
async def text_presentation_generate(payload: TextPresentationWizardPreview, request: Request):
    """Generate text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_START] Text presentation preview initiated")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] lang={payload.language}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Creating new chat session")
        try:
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation",
        "prompt": payload.prompt or "Create a comprehensive text presentation",
        "language": payload.language,
        "length": payload.length,
    }

    # Add styles if provided
    if payload.styles:
        wiz_payload["styles"] = payload.styles

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"Failed to create virtual file for large text: {e}")
                # Fallback to chunking if virtual file creation fails
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wiz_payload["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for text presentation generation")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[TEXT_PRESENTATION_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    # For connector-based filtering, extract context from specific connectors
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                    file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Text Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[TEXT_PRESENTATION_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[TEXT_PRESENTATION_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[TEXT_PRESENTATION_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[TEXT_PRESENTATION_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/text-presentation/edit")
async def text_presentation_edit(payload: TextPresentationEditRequest, request: Request):
    """Edit text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_EDIT_START] Text presentation edit initiated")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,  # Use the language from the request
        "originalContent": payload.content,
        "editMode": True,
        "isCleanContent": getattr(payload, 'isCleanContent', False)
    }

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Starting streamer")
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for text presentation editing
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for text presentation editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[TEXT_PRESENTATION_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/text-presentation/finalize")
async def text_presentation_finalize(payload: TextPresentationWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize text presentation creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    styles_param = getattr(payload, 'styles', None)
    logger.info(f"[TEXT_PRESENTATION_FINALIZE] styles param: {styles_param}")
    
    # Get user ID and deduct credits for one-pager creation
    try:
        credits_needed = calculate_product_credits("one_pager")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "One-pager creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for one-pager creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for one-pager creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this text presentation finalization to prevent duplicates
    text_presentation_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this text presentation is already being processed
    if text_presentation_key in ACTIVE_QUIZ_FINALIZE_KEYS:  # Reuse the same set for simplicity
        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_DUPLICATE] Text presentation finalization already in progress for key: {text_presentation_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Text presentation finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(text_presentation_key)
    QUIZ_FINALIZE_TIMESTAMPS[text_presentation_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Cleaned up stale text presentation key: {stale_key}")
    
    try:
        # Ensure text presentation template exists
        template_id = await _ensure_text_presentation_template(pool)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_TEMPLATE] Template ID: {template_id}")
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lesson.strip() if payload.lesson else "Standalone Presentation"
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip() if payload.lesson else 'Standalone Presentation'}"
                        logger.info(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Failed to fetch outline name for text presentation naming: {e}")
                # Continue with plain lesson title if outline fetch fails
        else:
            logger.info(f"[TEXT_PRESENTATION_FINALIZE_NAMING] No outline ID provided, using standalone naming: {project_name}")
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_START] Starting text presentation finalization for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] text_presentation_key: {text_presentation_key}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] hasUserEdits: {getattr(payload, 'hasUserEdits', False)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
        
        # NEW: Check for user edits and decide strategy (like in Quiz)
        use_direct_parser = False
        use_ai_parser = True
        
        if getattr(payload, 'hasUserEdits', False) and getattr(payload, 'originalContent', None):
            # User has made edits - check if they're significant
            any_changes = _any_text_presentation_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No text presentation changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Text presentation changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # NEW: Choose parsing strategy based on user edits
        if use_direct_parser:
            # DIRECT PARSER PATH: Use cached content directly since no changes were made
            logger.info("Using direct parser path for text presentation finalization")
            
            # Use the original content for parsing since no changes were made
            content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
        else:
            # AI PARSER PATH: Use the provided content (which may be clean titles only)
            logger.info("Using AI parser path for text presentation finalization")
            
            # NEW: Check if we have clean content (only titles without descriptions)
            if getattr(payload, 'isCleanContent', False):
                logger.info("Detected clean content - titles only, will generate descriptions for empty sections")
                
                # Parse the clean content to identify sections that need content generation
                content_to_parse = await _generate_content_for_clean_titles(
                    clean_content=payload.aiResponse,
                    original_content=payload.originalContent,
                    language=payload.language
                )
            else:
                content_to_parse = payload.aiResponse
        
        # Parse the text presentation data using LLM - only call once with consistent project name
        parsed_text_presentation = await parse_ai_response_with_llm(
            ai_response=content_to_parse,
            project_name=project_name,  # Use consistent project name
            target_model=TextPresentationDetails,
            default_error_model_instance=TextPresentationDetails(
                textTitle=project_name,
                contentBlocks=[],
                detectedLanguage=payload.language
            ),
            dynamic_instructions=f"""
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

            1.  **`type: "headline"`**
                * `level` (integer):
                    * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                    * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                    * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                    * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
                * `text` (string): Headline text.
                * `iconName` (string, optional): Based on level and context as described above.
                * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be strings or other nested content blocks.
                * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`):
                    * Can be simple strings for basic numbered points.
                    * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                        * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                        * The `items` of this *inner* `bullet_list` should then be:
                            1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                            2. A `paragraph` block (for the main descriptive text).
                            3. Optionally, another `paragraph` block with `isRecommendation: true`.
                    * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **General Parsing Rules & Icon Names:**
            * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
            * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
            * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
            * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
            * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
            * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """,
            target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
        )
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsed text title: {parsed_text_presentation.textTitle}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Number of content blocks: {len(parsed_text_presentation.contentBlocks)}")

        logger.info(parsed_text_presentation.contentBlocks)
        
        # Detect language if not provided
        if not parsed_text_presentation.detectedLanguage:
            parsed_text_presentation.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no content blocks, create a basic structure
        if not parsed_text_presentation.contentBlocks:
            logger.warning(f"[TEXT_PRESENTATION_FINALIZE_FALLBACK] LLM parsing failed for text presentation, creating fallback structure")
            # Create a simple text presentation with the AI response as content
            parsed_text_presentation.textTitle = project_name
            parsed_text_presentation.contentBlocks = [
                {
                    "type": "paragraph",
                    "text": payload.aiResponse
                }
            ]
        else:
            # Validate that all content blocks have the required type field
            valid_content_blocks = []
            for i, block in enumerate(parsed_text_presentation.contentBlocks):
                if hasattr(block, 'type') and block.type:
                    valid_content_blocks.append(block)
                else:
                    logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] Content block {i} missing type, converting to paragraph")
                    # Convert to paragraph if type is missing
                    if hasattr(block, 'text'):
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": block.text
                        })
                    elif hasattr(block, 'items'):
                        valid_content_blocks.append({
                            "type": "bullet_list",
                            "items": block.items
                        })
                    else:
                        # Fallback to paragraph with string representation
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": str(block)
                        })
            
            if not valid_content_blocks:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] No valid content blocks found, creating fallback structure")
                parsed_text_presentation.contentBlocks = [
                    {
                        "type": "paragraph",
                        "text": payload.aiResponse
                    }
                ]
            else:
                parsed_text_presentation.contentBlocks = valid_content_blocks
        
        # Always use the consistent project name for database storage
        # The text title from parsed_text_presentation.textTitle is used for display purposes only
        final_project_name = project_name
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_text_presentation = payload.outlineId is None
        
        # For text presentation components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Text Presentation",  # product_type
                COMPONENT_NAME_TEXT_PRESENTATION,  # microproduct_type - use the correct component name
                project_name,  # microproduct_name
                parsed_text_presentation.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_text_presentation,  # is_standalone - consistent with outline connection
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create text presentation project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                content_row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if content_row:
                    logger.info(f"[TEXT_PRESENTATION_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(content_row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved text presentation JSON for project {created_project.id}: {log_e}")
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_SUCCESS] Text presentation finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_text_presentation}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[TEXT_PRESENTATION_FINALIZE_ERROR] Error in text presentation finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(text_presentation_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(text_presentation_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Removed text_presentation_key from active set: {text_presentation_key}")

@app.get("/api/custom/projects/latest-by-chat")
async def get_latest_project_by_chat(chatId: str = Query(..., alias="chatId"), onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Return the most recently created project for the given source_chat_session_id
    for the current user. This is used by finalize fallbacks to navigate reliably
    even when the original finalize request times out.
    """
    try:
        chat_uuid = uuid.UUID(chatId)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid chatId format. Must be UUID")

    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id, project_name, design_template_id, product_type, microproduct_type
                FROM projects
                WHERE onyx_user_id = $1 AND source_chat_session_id = $2
                ORDER BY created_at DESC
                LIMIT 1
                """,
                onyx_user_id, chat_uuid
            )
        if not row:
            return JSONResponse(status_code=404, content={"detail": "No project found for chat session"})
        return {
            "id": row["id"],
            "projectName": row["project_name"],
            "productType": row.get("product_type"),
            "microproductType": row.get("microproduct_type"),
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching latest project by chat: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to fetch latest project by chat session")

# ============================
# CREDITS MANAGEMENT ENDPOINTS
# ============================

@app.get("/api/custom/credits/me", response_model=UserCredits)
async def get_my_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get current user's credit balance (auto-creates if new user)"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # This will auto-create the user if they don't exist yet
        credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        return credits
    except Exception as e:
        logger.error(f"Error getting user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve credits")

@app.get("/api/custom/admin/credits/users", response_model=List[UserCredits])
async def list_all_user_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to list all user credits"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM user_credits 
                ORDER BY updated_at DESC
            """)
            return [UserCredits(**dict(row)) for row in rows]
    except Exception as e:
        logger.error(f"Error listing user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: Usage analytics across all users
@app.get("/api/custom/admin/credits/usage-analytics", response_model=CreditUsageAnalyticsResponse)
async def get_usage_analytics(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT COALESCE(product_type, 'Unknown') as product_type,
                       COALESCE(SUM(credits), 0) AS credits_used
                FROM credit_transactions
                WHERE type = 'product_generation'
                GROUP BY COALESCE(product_type, 'Unknown')
                ORDER BY credits_used DESC
                """
            )
            usage_by_product = [ProductUsage(product_type=row["product_type"], credits_used=int(row["credits_used"] or 0)) for row in rows]
            total_credits = sum(u.credits_used for u in usage_by_product)
            return CreditUsageAnalyticsResponse(usage_by_product=usage_by_product, total_credits_used=total_credits)
    except Exception as e:
        logger.error(f"Error fetching usage analytics: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch usage analytics")

@app.post("/api/custom/admin/credits/migrate-users")
async def migrate_onyx_users_to_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to manually trigger migration of Onyx users to credits table"""
    await verify_admin_user(request)
    
    try:
        # Use the same migration function as startup
        migrated_count = await migrate_onyx_users_to_credits_table()
        
        return {
            "success": True,
            "message": f"Successfully migrated {migrated_count} new users with 100 credits each",
            "users_migrated": migrated_count
        }
    except Exception as e:
        logger.error(f"Error migrating users: {e}")
        raise HTTPException(status_code=500, detail="Failed to migrate users")

@app.post("/api/custom/admin/credits/modify", response_model=CreditTransactionResponse)
async def modify_user_credits(
    transaction: CreditTransactionRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to add or remove credits for a user by email"""
    await verify_admin_user(request)
    
    try:
        if transaction.amount <= 0:
            raise HTTPException(status_code=400, detail="Amount must be positive")
        
        updated_credits = await modify_user_credits_by_email(
            transaction.user_email,
            transaction.amount,
            transaction.action,
            pool,
            transaction.reason
        )
        
        action_msg = "added to" if transaction.action == "add" else "removed from"
        message = f"Successfully {action_msg} {transaction.user_email}: {transaction.amount} credits"
        
        return CreditTransactionResponse(
            success=True,
            message=message,
            new_balance=updated_credits.credits_balance,
            user_credits=updated_credits
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error modifying user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to modify credits")

@app.get("/api/custom/admin/credits/user/{user_email}", response_model=UserCredits)
async def get_user_credits_by_email(
    user_email: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to get specific user's credits by email"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM user_credits WHERE onyx_user_id = $1",
                user_email
            )
            
            if not row:
                # Create entry for user if doesn't exist
                row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                    VALUES ($1, $2, $3)
                    RETURNING *
                """, user_email, user_email.split('@')[0], 0)
            
            return UserCredits(**dict(row))
            
    except Exception as e:
        logger.error(f"Error getting user credits by email: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: User transaction history (purchases + product generations)
@app.get("/api/custom/admin/credits/user/{user_id}/transactions", response_model=UserTransactionHistoryResponse)
async def get_user_transactions(
    user_id: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    async with pool.acquire() as conn:
        # Resolve user by numeric id or email
        if user_id.isdigit():
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE id = $1", int(user_id))
        else:
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE onyx_user_id = $1", user_id)

        if not user_row:
            raise HTTPException(status_code=404, detail="User not found")

        tx_rows = await conn.fetch(
            """
            SELECT id, type, title, credits, created_at, product_type
            FROM credit_transactions
            WHERE onyx_user_id = $1
            ORDER BY created_at DESC
            LIMIT 200
            """,
            user_row["onyx_user_id"]
        )

        activities = [
            TimelineActivity(
                id=str(r["id"]),
                type=r["type"],
                title=r["title"] or (r["type"].replace('_',' ').title()),
                credits=int(r["credits"] or 0),
                timestamp=r["created_at"],
                product_type=r["product_type"]
            )
            for r in tx_rows
        ]

        return UserTransactionHistoryResponse(
            user_id=int(user_row["id"]),
            user_email=user_row["onyx_user_id"],
            user_name=user_row["name"],
            transactions=activities
        )

# --- Feature Management Endpoints ---

@app.get("/api/custom/admin/features/definitions")
async def get_feature_definitions(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get all feature definitions"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM feature_definitions 
                WHERE is_active = true 
                ORDER BY category, display_name
            """)
            
            return [dict(row) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching feature definitions: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch feature definitions")

@app.get("/api/custom/admin/features/users")
async def get_users_with_features(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get all users and their feature permissions"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT 
                    uc.onyx_user_id AS user_id,
                    uc.onyx_user_id AS user_display_id,
                    uc.name AS user_name,
                    uf.feature_name,
                    uf.is_enabled,
                    uf.created_at,
                    uf.updated_at,
                    fd.display_name,
                    fd.description,
                    fd.category
                FROM user_credits uc
                LEFT JOIN user_features uf ON uc.onyx_user_id = uf.user_id
                LEFT JOIN feature_definitions fd 
                    ON uf.feature_name = fd.feature_name AND fd.is_active = true
                ORDER BY uc.onyx_user_id, fd.category, fd.display_name
            """)
            
            users_features = {}
            for row in rows:
                user_id = row['user_id']
                if user_id not in users_features:
                    users_features[user_id] = {
                        'user_id': user_id,
                        'user_email': row['user_display_id'] or user_id,
                        'user_name': row['user_name'] or 'Unknown User',
                        'features': []
                    }
                
                if row['feature_name']:
                    users_features[user_id]['features'].append({
                        'feature_name': row['feature_name'],
                        'display_name': row['display_name'],
                        'description': row['description'],
                        'category': row['category'],
                        'is_enabled': row['is_enabled'],
                        'created_at': row['created_at'],
                        'updated_at': row['updated_at']
                    })
            
            return list(users_features.values())
    except Exception as e:
        logger.error(f"Error fetching users with features: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch users with features")

@app.post("/api/custom/admin/features/toggle")
async def toggle_user_feature(
    feature_request: FeatureToggleRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Enable or disable a feature for a single user"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            # Verify feature exists
            feature_row = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_request.feature_name
            )
            
            if not feature_row:
                raise HTTPException(status_code=404, detail="Feature not found")
            
            # Insert or update user feature
            await conn.execute("""
                INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                VALUES ($1, $2, $3, NOW())
                ON CONFLICT (user_id, feature_name) 
                DO UPDATE SET 
                    is_enabled = $3,
                    updated_at = NOW()
            """, feature_request.user_id, feature_request.feature_name, feature_request.is_enabled)
            
            action = "enabled" if feature_request.is_enabled else "disabled"
            return {
                "success": True,
                "message": f"Feature '{feature_row['display_name']}' {action} for user {feature_request.user_id}"
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error toggling user feature: {e}")
        raise HTTPException(status_code=500, detail="Failed to toggle feature")

@app.post("/api/custom/admin/features/bulk-toggle")
async def bulk_toggle_user_features(
    bulk_request: BulkFeatureToggleRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Enable or disable a feature for multiple users"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            # Verify feature exists
            feature_row = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                bulk_request.feature_name
            )
            
            if not feature_row:
                raise HTTPException(status_code=404, detail="Feature not found")
            
            # Bulk insert/update user features
            updated_count = 0
            for user_id in bulk_request.user_ids:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                    VALUES ($1, $2, $3, NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = $3,
                        updated_at = NOW()
                """, user_id, bulk_request.feature_name, bulk_request.is_enabled)
                updated_count += 1
            
            action = "enabled" if bulk_request.is_enabled else "disabled"
            return {
                "success": True,
                "message": f"Feature '{feature_row['display_name']}' {action} for {updated_count} users",
                "users_updated": updated_count
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error bulk toggling user features: {e}")
        raise HTTPException(status_code=500, detail="Failed to bulk toggle features")

@app.get("/api/custom/features/check/{feature_name}")
async def check_user_feature(
    feature_name: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Check if a feature is enabled for the current user"""
    try:
        user_id = await get_current_onyx_user_id(request)
        if not user_id:
            return {"is_enabled": False}
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT uf.is_enabled 
                FROM user_features uf
                JOIN feature_definitions fd ON uf.feature_name = fd.feature_name
                WHERE uf.user_id = $1 AND uf.feature_name = $2 AND fd.is_active = true
            """, user_id, feature_name)
            
            return {"is_enabled": bool(row['is_enabled']) if row else False}
    except Exception as e:
        logger.error(f"Error checking user feature: {e}")
        return {"is_enabled": False}

@app.get("/api/custom/admin/features/user-types")
async def get_user_types(request: Request):
    """Get available user types and their features"""
    await verify_admin_user(request)
    return USER_TYPES

@app.post("/api/custom/admin/features/assign-user-type")
async def assign_user_type(
    assignment_request: UserTypeAssignmentRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Assign a user type to multiple users, enabling features for that type and disabling others"""
    await verify_admin_user(request)
    
    if assignment_request.user_type not in USER_TYPES:
        raise HTTPException(status_code=400, detail="Invalid user type")
    
    try:
        user_type_info = USER_TYPES[assignment_request.user_type]
        features_to_enable = set(user_type_info["features"])
        
        async with pool.acquire() as conn:
            # Get all active features
            all_features_rows = await conn.fetch(
                "SELECT feature_name FROM feature_definitions WHERE is_active = true"
            )
            all_features = {row['feature_name'] for row in all_features_rows}
            
            # Features to disable = all features - features to enable
            features_to_disable = all_features - features_to_enable
            
            users_updated = 0
            features_enabled = 0
            features_disabled = 0
            
            for user_id in assignment_request.user_ids:
                # Enable features for this user type
                for feature_name in features_to_enable:
                    if feature_name in all_features:  # Verify feature exists
                        await conn.execute("""
                            INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                            VALUES ($1, $2, true, NOW())
                            ON CONFLICT (user_id, feature_name) 
                            DO UPDATE SET 
                                is_enabled = true,
                                updated_at = NOW()
                        """, user_id, feature_name)
                        features_enabled += 1
                
                # Disable features NOT in this user type
                for feature_name in features_to_disable:
                    if feature_name in all_features:  # Verify feature exists
                        await conn.execute("""
                            INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                            VALUES ($1, $2, false, NOW())
                            ON CONFLICT (user_id, feature_name) 
                            DO UPDATE SET 
                                is_enabled = false,
                                updated_at = NOW()
                        """, user_id, feature_name)
                        features_disabled += 1
                
                users_updated += 1
            
            return {
                "success": True,
                "message": f"Assigned '{user_type_info['display_name']}' type to {users_updated} users ({features_enabled} features enabled, {features_disabled} features disabled)",
                "users_updated": users_updated,
                "features_enabled": features_enabled,
                "features_disabled": features_disabled,
                "user_type": user_type_info["display_name"]
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error assigning user type: {e}")
        raise HTTPException(status_code=500, detail="Failed to assign user type")

async def assign_default_user_type(user_id: str, conn: asyncpg.Connection):
    """Assign default 'Normal (HR)' user type to a new user"""
    try:
        default_user_type = "normal_hr"
        if default_user_type not in USER_TYPES:
            logger.warning(f"Default user type {default_user_type} not found in USER_TYPES")
            return
        
        user_type_info = USER_TYPES[default_user_type]
        features_to_enable = user_type_info["features"]
        
        # Enable features for the default user type
        features_assigned = 0
        for feature_name in features_to_enable:
            # Check if feature exists before trying to assign it
            feature_exists = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_name
            )
            
            if feature_exists:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, created_at, updated_at)
                    VALUES ($1, $2, true, NOW(), NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = true,
                        updated_at = NOW()
                """, user_id, feature_name)
                features_assigned += 1
            else:
                logger.warning(f"Feature {feature_name} not found or inactive for new user {user_id}")
        
        logger.info(f"Assigned default user type '{user_type_info['display_name']}' to new user {user_id} ({features_assigned} features enabled)")
        
    except Exception as e:
        logger.error(f"Error assigning default user type to new user {user_id}: {e}")
        # Don't raise exception to avoid blocking user creation

@app.post("/api/custom/projects/duplicate/{project_id}", response_model=ProjectDuplicationResponse)
async def duplicate_project(project_id: int, request: Request, user_id: str = Depends(get_current_onyx_user_id)):
    """
    Duplicate a project. If it's a Training Plan, also duplicate all connected products (lessons, quizzes, etc.).
    Enhanced with proper transaction management and complete field mapping.
    """
    async with DB_POOL.acquire() as conn:
        # Start transaction for atomic operations
        async with conn.transaction():
            try:
                # Fetch original project with all fields
                orig = await conn.fetchrow("SELECT * FROM projects WHERE id = $1", project_id)
                if not orig:
                    raise HTTPException(status_code=404, detail="Project not found")
                
                # Verify user ownership
                if orig['onyx_user_id'] != user_id:
                    raise HTTPException(status_code=403, detail="Access denied")
                
                new_name = f"Copy of {orig['project_name']}"
                now = datetime.now(timezone.utc)
                
                logger.info(f"Starting duplication of project {project_id} (type: {orig['microproduct_type']}) for user {user_id}")
                
                if orig['microproduct_type'] == "Training Plan":
                    # Training Plan duplication - handle connected products
                    new_session_id = str(uuid4())
                    
                    # Duplicate the main Training Plan with all fields
                    new_outline_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                        RETURNING id
                        """,
                        user_id,
                        new_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],  # JSONB will be handled automatically by asyncpg
                        orig['design_template_id'],
                        now,
                        new_session_id,
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Created new Training Plan with ID {new_outline_id}")
                    
                    # Find all connected products using the same naming patterns as frontend
                    # Get all user's projects to search through
                    all_projects = await conn.fetch(
                        "SELECT * FROM projects WHERE onyx_user_id = $1 ORDER BY created_at",
                        user_id
                    )
                    
                    # Find connected products using frontend naming patterns
                    connected = []
                    original_outline_name = orig['project_name'].strip()
                    
                    for project in all_projects:
                        if project['id'] == orig['id']:
                            continue  # Skip the original training plan.
                        
                        project_name = project['project_name'].strip()
                        micro_name = project['microproduct_name']
                        
                        # Skip other Training Plans
                        if project['microproduct_type'] == "Training Plan":
                            continue
                        
                        is_connected = False
                        
                        # Method 1: Legacy matching - project name matches outline and microProductName matches lesson
                        if project_name == original_outline_name and micro_name:
                            is_connected = True
                            logger.info(f"Found connected product via legacy matching: {project_name} (micro: {micro_name})")
                        
                        # Method 2: New naming convention - project name follows "Outline Name: Lesson Title" pattern
                        elif ': ' in project_name:
                            outline_part = project_name.split(': ')[0].strip()
                            if outline_part == original_outline_name:
                                is_connected = True
                                logger.info(f"Found connected product via new pattern: {project_name}")
                        
                        # Method 3: Legacy patterns for backward compatibility
                        # Legacy Quiz pattern - "Quiz - Outline Name: Lesson Title"
                        elif project_name.startswith('Quiz - ') and ': ' in project_name:
                            quiz_part = project_name.replace('Quiz - ', '', 1)
                            if ': ' in quiz_part:
                                outline_part = quiz_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy quiz pattern: {project_name}")
                        
                        # Legacy Text Presentation pattern - "Text Presentation - Outline Name: Lesson Title"
                        elif project_name.startswith('Text Presentation - ') and ': ' in project_name:
                            text_part = project_name.replace('Text Presentation - ', '', 1)
                            if ': ' in text_part:
                                outline_part = text_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy text presentation pattern: {project_name}")
                        
                        # Method 4: Alternative pattern - project name matches lesson title directly
                        # This is for cases where the lesson title became the project name
                        elif orig['microproduct_content']:
                            # Check if this project name matches any lesson title in the training plan
                            try:
                                content = orig['microproduct_content']
                                if isinstance(content, dict) and 'sections' in content:
                                    for section in content['sections']:
                                        if 'lessons' in section:
                                            for lesson in section['lessons']:
                                                lesson_title = lesson.get('title', '').strip()
                                                if lesson_title and lesson_title == project_name:
                                                    is_connected = True
                                                    logger.info(f"Found connected product via lesson title matching: {project_name}")
                                                    break
                                        if is_connected:
                                            break
                                    if is_connected:
                                        break
                            except Exception as e:
                                logger.warning(f"Error checking lesson title matching for {project_name}: {e}")
                        
                        if is_connected:
                            connected.append(project)
                    
                    logger.info(f"Found {len(connected)} connected products to duplicate")
                    
                    # Duplicate each connected product
                    duplicated_products = []
                    for i, prod in enumerate(connected):
                        try:
                            # Smart name replacement - handle various naming patterns
                            prod_name = prod['project_name']
                            if prod_name.startswith(orig['project_name']):
                                prod_name = prod_name.replace(orig['project_name'], new_name, 1)
                            else:
                                # If name doesn't start with parent name, just add "Copy of" prefix
                                prod_name = f"Copy of {prod_name}"
                            
                            # Update microproduct name if it references the parent
                            micro_name = prod['microproduct_name']
                            if micro_name and micro_name.startswith(orig['project_name']):
                                micro_name = micro_name.replace(orig['project_name'], new_name, 1)
                            
                            # Insert the duplicated product with all fields
                            new_prod_id = await conn.fetchval(
                                """
                                INSERT INTO projects (
                                    onyx_user_id, project_name, product_type, microproduct_type, 
                                    microproduct_name, microproduct_content, design_template_id, 
                                    created_at, source_chat_session_id, folder_id, "order", 
                                    is_standalone, completion_time, custom_rate, quality_tier
                                )
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                                RETURNING id
                                """,
                                user_id,
                                prod_name,
                                prod['product_type'],
                                prod['microproduct_type'],
                                micro_name,
                                prod['microproduct_content'],  # JSONB content preserved
                                prod['design_template_id'],
                                now,
                                new_session_id,  # Link to new Training Plan
                                prod['folder_id'],
                                prod['order'],
                                prod['is_standalone'],
                                prod['completion_time'],
                                prod['custom_rate'],
                                prod['quality_tier']
                            )
                            
                            duplicated_products.append({
                                'original_id': prod['id'],
                                'new_id': new_prod_id,
                                'type': prod['microproduct_type'],
                                'name': prod_name
                            })
                            
                            logger.info(f"Duplicated {prod['microproduct_type']} '{prod['project_name']}' -> '{prod_name}' (ID: {new_prod_id})")
                            
                        except Exception as e:
                            logger.error(f"Failed to duplicate connected product {prod['id']} ({prod['microproduct_type']}): {str(e)}")
                            # Re-raise to trigger transaction rollback
                            raise HTTPException(
                                status_code=500, 
                                detail=f"Failed to duplicate {prod['microproduct_type']} '{prod['project_name']}': {str(e)}"
                            )
                    
                    logger.info(f"Successfully duplicated Training Plan and {len(duplicated_products)} connected products")
                    
                    return {
                        "id": new_outline_id,
                        "name": new_name,
                        "type": "Training Plan",
                        "connected_products": duplicated_products,
                        "total_products_duplicated": len(duplicated_products) + 1
                    }
                    
                else:
                    # Regular product duplication (non-Training Plan)
                    new_prod_name = f"Copy of {orig['project_name']}"
                    
                    new_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                        RETURNING id
                        """,
                        user_id,
                        new_prod_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],
                        orig['design_template_id'],
                        now,
                        str(uuid4()),  # New session ID for standalone product
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Successfully duplicated {orig['microproduct_type']} '{orig['project_name']}' -> '{new_prod_name}' (ID: {new_id})")
                    
                    return {
                        "id": new_id,
                        "name": new_prod_name,
                        "type": orig['microproduct_type'],
                        "total_products_duplicated": 1
                    }
                    
            except HTTPException:
                # Re-raise HTTP exceptions (these are expected errors)
                raise
            except Exception as e:
                logger.error(f"Unexpected error during project duplication: {str(e)}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to duplicate project: {str(e)}"
                )

# --- Video Generation API Endpoints ---

# Import video generation service safely
video_generation_service = None
presentation_service = None
try:
    from app.services.video_generation_service import video_generation_service
    logger.info("Video generation service imported successfully")
except Exception as e:
    logger.warning(f"Video generation service not available: {e}")
    video_generation_service = None

# Import presentation service safely
try:
    from app.services.presentation_service import presentation_service, PresentationRequest
    logger.info("Presentation service imported successfully")
except Exception as e:
    logger.warning(f"Presentation service not available: {e}")
    presentation_service = None

@app.get("/api/custom/video/avatars")
async def get_avatars():
    """Get available avatars from Elai API."""
    try:
        if not video_generation_service:
            return {
                "success": False, 
                "error": "Video generation service not available. Please check backend configuration.",
                "avatars": []
            }
        
        result = await video_generation_service.get_avatars()
        
        if result["success"]:
            return {"success": True, "avatars": result["avatars"]}
        else:
            return {"success": False, "error": result["error"], "avatars": []}
            
    except Exception as e:
        logger.error(f"Error fetching avatars: {str(e)}")
        return {
            "success": False, 
            "error": f"Failed to fetch avatars: {str(e)}",
            "avatars": []
        }

@app.post("/api/custom/video/generate")
async def generate_video(request: Request):
    """Generate video from slides and avatar data."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        avatar_data = body.get("avatar", {})
        
        # Validate request data
        if not slides_data:
            return {"success": False, "error": "No slides data provided"}
        
        if not avatar_data:
            return {"success": False, "error": "No avatar data provided"}
        
        # Generate video
        result = await video_generation_service.generate_video(slides_data, avatar_data)
        
        if result["success"]:
            return {
                "success": True,
                "video_id": result["video_id"],
                "download_url": result["download_url"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.get("/api/custom/video/status/{video_id}")
async def get_video_status(video_id: str):
    """Get the status of a video generation."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        status_data = await video_generation_service.check_video_status(video_id)
        
        if status_data:
            return {"success": True, "status": status_data}
        else:
            return {"success": False, "error": "Video not found"}
            
    except Exception as e:
        logger.error(f"Error checking video status: {str(e)}")
        return {"success": False, "error": f"Failed to check video status: {str(e)}"}

@app.post("/api/custom/video/create")
async def create_video(request: Request):
    """Create a new video with Elai API."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        project_name = body.get("projectName", "Generated Video")
        voiceover_texts = body.get("voiceoverTexts", [])
        avatar_code = body.get("avatarCode")  # None will trigger auto-selection
        
        # Validate request data
        if not voiceover_texts:
            return {"success": False, "error": "No voiceover texts provided"}
        
        # Create video
        logger.info(f"Creating video with project name: {project_name}")
        logger.info(f"Voiceover texts count: {len(voiceover_texts)}")
        logger.info(f"Avatar code: {avatar_code}")
        
        result = await video_generation_service.create_video_from_texts(project_name, voiceover_texts, avatar_code)
        
        logger.info(f"Video creation result: {result}")
        
        if result["success"]:
            return {
                "success": True,
                "videoId": result["videoId"],
                "message": "Video created successfully"
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error creating video: {str(e)}")
        return {"success": False, "error": f"Failed to create video: {str(e)}"}

@app.post("/api/custom/video/render/{video_id}")
async def render_video(video_id: str):
    """Start rendering a video."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Start rendering
        result = await video_generation_service.render_video(video_id)
        
        if result["success"]:
            return {
                "success": True,
                "message": "Video rendering started successfully"
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error starting video render: {str(e)}")
        return {"success": False, "error": f"Failed to start video render: {str(e)}"}

# ============================================================================
# Clean Video Generation API Endpoints (HTML → PNG → Video Pipeline)
# ============================================================================

@app.post("/api/custom/clean-video/avatar-slide")
async def generate_clean_avatar_slide_video(request: Request):
    """Generate video for a single avatar slide using clean HTML → PNG → Video pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slide_props = body.get("slideProps", {})
        theme = body.get("theme", "dark-purple")
        slide_duration = body.get("slideDuration", 5.0)
        quality = body.get("quality", "high")
        
        # Validate slide props
        validation = await clean_video_generation_service.validate_slide_props(slide_props)
        if not validation["valid"]:
            return {
                "success": False,
                "error": f"Invalid slide props: {validation['error']}"
            }
        
        # Generate video
        result = await clean_video_generation_service.generate_avatar_slide_video(
            slide_props=slide_props,
            theme=theme,
            slide_duration=slide_duration,
            quality=quality
        )
        
        if result["success"]:
            return {
                "success": True,
                "video_url": result["video_url"],
                "video_path": result["video_path"],
                "file_size": result["file_size"],
                "duration": result["duration"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating clean avatar slide video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.post("/api/custom/clean-video/presentation")
async def generate_clean_presentation_video(request: Request):
    """Generate video for multiple avatar slides using clean HTML → PNG → Video pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slides_props = body.get("slidesProps", [])
        theme = body.get("theme", "dark-purple")
        slide_duration = body.get("slideDuration", 5.0)
        quality = body.get("quality", "high")
        
        # Validate request
        if not slides_props:
            return {
                "success": False,
                "error": "No slides provided"
            }
        
        # Validate each slide
        for i, slide_props in enumerate(slides_props):
            validation = await clean_video_generation_service.validate_slide_props(slide_props)
            if not validation["valid"]:
                return {
                    "success": False,
                    "error": f"Invalid slide {i+1} props: {validation['error']}"
                }
        
        # Generate video
        result = await clean_video_generation_service.generate_presentation_video(
            slides_props=slides_props,
            theme=theme,
            slide_duration=slide_duration,
            quality=quality
        )
        
        if result["success"]:
            return {
                "success": True,
                "video_url": result["video_url"],
                "video_path": result["video_path"],
                "file_size": result["file_size"],
                "duration": result["duration"],
                "slides_count": result["slides_count"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating clean presentation video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.get("/api/custom/clean-video/test")
async def test_clean_video_pipeline():
    """Test the clean video generation pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Run pipeline test
        result = await clean_video_generation_service.test_pipeline()
        
        return result
        
    except Exception as e:
        logger.error(f"Error testing clean video pipeline: {str(e)}")
        return {"success": False, "error": f"Pipeline test failed: {str(e)}"}

@app.get("/api/custom/clean-video/templates")
async def get_supported_avatar_templates():
    """Get list of supported avatar template IDs."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        templates = await clean_video_generation_service.get_supported_templates()
        
        return {
            "success": True,
            "templates": templates
        }
        
    except Exception as e:
        logger.error(f"Error getting supported templates: {str(e)}")
        return {"success": False, "error": f"Failed to get templates: {str(e)}"}

@app.get("/api/custom/video-system/status")
async def get_video_system_status():
    """Get video generation system status."""
    try:
        # Check HTML to Image service status
        from app.services.html_to_image_service import html_to_image_service
        image_service_status = html_to_image_service.get_status()
        
        return {
            "success": True,
            "system": "Clean Video Generation Pipeline",
            "screenshot_services": "DISABLED",
            "chromium_browser": "NOT REQUIRED",
            "clean_pipeline": "ACTIVE",
            "avatar_selection": "DYNAMIC",
            "image_conversion": image_service_status,
            "supported_formats": ["avatar-checklist", "avatar-crm", "avatar-service", "avatar-buttons", "avatar-steps"],
            "output_resolution": "1920x1080",
            "pipeline": "Props → HTML → PNG → Video"
        }
        
    except Exception as e:
        logger.error(f"Error getting video system status: {str(e)}")
        return {"success": False, "error": f"Failed to get status: {str(e)}"}

# ============================================================================
# Professional Presentation API Endpoints
# ============================================================================

@app.post("/api/custom/presentations")
async def create_presentation(request: Request):
    """Create a new professional video presentation."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slide_url = body.get("slideUrl")
        voiceover_texts = body.get("voiceoverTexts", [])
        # NEW: Accept actual slide data
        slides_data = body.get("slidesData")  # Optional - actual slide content with text, props, etc.
        theme = body.get("theme", "dark-purple")  # Theme for slide generation
        avatar_code = body.get("avatarCode")  # None will trigger auto-selection
        use_avatar_mask = body.get("useAvatarMask", True)  # NEW: Use avatar mask service by default
        duration = body.get("duration", 30.0)
        layout = body.get("layout", "picture_in_picture")
        quality = body.get("quality", "high")
        resolution = body.get("resolution", [1920, 1080])
        project_name = body.get("projectName", "Generated Presentation")
        
        # Add detailed logging for debugging
        logger.info("🎬 [MAIN_ENDPOINT] Received presentation request parameters:")
        logger.info(f"  - slide_url: {slide_url}")
        logger.info(f"  - voiceover_texts_count: {len(voiceover_texts) if voiceover_texts else 0}")
        logger.info(f"  - slides_data_count: {len(slides_data) if slides_data else 0}")
        logger.info(f"  - theme: {theme}")
        logger.info(f"  - avatar_code: {avatar_code}")
        logger.info(f"  - use_avatar_mask: {use_avatar_mask}")
        logger.info(f"  - duration: {duration}")
        logger.info(f"  - layout: {layout}")
        logger.info(f"  - quality: {quality}")
        logger.info(f"  - resolution: {resolution}")
        logger.info(f"  - project_name: {project_name}")
        
        # Validate required parameters  
        # slideUrl is required only if no slidesData provided
        if not slide_url and not slides_data:
            return {"success": False, "error": "Either slideUrl or slidesData is required"}
        
        if not voiceover_texts or len(voiceover_texts) == 0:
            return {"success": False, "error": "voiceoverTexts is required"}
        
        # Validate layout
        allowed_layouts = ["side_by_side", "picture_in_picture", "split_screen"]
        if layout not in allowed_layouts:
            return {"success": False, "error": f"layout must be one of {allowed_layouts}"}
        
        # Create presentation request
        logger.info("🎬 [MAIN_ENDPOINT] Creating PresentationRequest object...")
        presentation_request = PresentationRequest(
            slide_url=slide_url or "",  # Provide empty string if None
            voiceover_texts=voiceover_texts,
            slides_data=slides_data,  # NEW: Pass actual slide data
            theme=theme,  # NEW: Pass theme
            avatar_code=avatar_code,
            use_avatar_mask=use_avatar_mask,  # NEW: Pass avatar mask flag
            duration=duration,
            layout=layout,
            quality=quality,
            resolution=tuple(resolution),
            project_name=project_name
        )
        logger.info(f"🎬 [MAIN_ENDPOINT] PresentationRequest created with use_avatar_mask: {presentation_request.use_avatar_mask}")
        
        # Create presentation
        job_id = await presentation_service.create_presentation(presentation_request)
        
        # Immediate response to prevent timeout
        response = {
            "success": True,
            "jobId": job_id,
            "status": "processing",
            "progress": 0,
            "message": "Presentation generation started - check status with job ID",
            "estimatedTime": "60-90 seconds"
        }
        
        logger.info(f"Returning immediate response for job {job_id}")
        return response
        
    except Exception as e:
        logger.error(f"Error creating presentation: {str(e)}")
        return {"success": False, "error": f"Failed to create presentation: {str(e)}"}

@app.get("/api/custom/presentations/test/quick")
async def test_quick_response():
    """Quick test endpoint to verify no timeout issues."""
    from datetime import datetime
    return {
        "success": True,
        "message": "Quick response test successful",
        "timestamp": datetime.now().isoformat(),
        "backend_status": "active"
    }

@app.get("/api/custom/presentations/{job_id}")
async def get_presentation_status(job_id: str):
    """Get presentation processing status."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        job = await presentation_service.get_job_status(job_id)
        
        if not job:
            return {"success": False, "error": "Job not found"}
        
        return {
            "success": True,
            "jobId": job.job_id,
            "status": job.status,
            "progress": job.progress,
            "error": job.error,
            "videoUrl": job.video_url,
            "thumbnailUrl": job.thumbnail_url,
            "slideImageUrl": f"/api/custom/presentations/{job.job_id}/slide-image" if job.slide_image_path else None,
            "createdAt": job.created_at.isoformat() if job.created_at else None,
            "completedAt": job.completed_at.isoformat() if job.completed_at else None
        }
        
    except Exception as e:
        logger.error(f"Error getting presentation status: {str(e)}")
        return {"success": False, "error": f"Failed to get presentation status: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/video")
async def download_presentation_video(job_id: str):
    """Download the completed presentation video."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        video_path = await presentation_service.get_presentation_video(job_id)
        
        if not video_path:
            return {"success": False, "error": "Video not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=video_path,
            media_type="video/mp4",
            filename=f"presentation_{job_id}.mp4"
        )
        
    except Exception as e:
        logger.error(f"Error downloading presentation video: {str(e)}")
        return {"success": False, "error": f"Failed to download video: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/thumbnail")
async def get_presentation_thumbnail(job_id: str):
    """Get the presentation thumbnail."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        thumbnail_path = await presentation_service.get_presentation_thumbnail(job_id)
        
        if not thumbnail_path:
            return {"success": False, "error": "Thumbnail not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=thumbnail_path,
            media_type="image/jpeg",
            filename=f"thumbnail_{job_id}.jpg"
        )
        
    except Exception as e:
        logger.error(f"Error getting presentation thumbnail: {str(e)}")
        return {"success": False, "error": f"Failed to get thumbnail: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/slide-image")
async def download_presentation_slide_image(job_id: str):
    """Download the generated slide image for debugging."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        slide_image_path = await presentation_service.get_presentation_slide_image(job_id)
        
        if not slide_image_path:
            return {"success": False, "error": "Slide image not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=slide_image_path,
            media_type="image/png",
            filename=f"slide_image_{job_id}.png"
        )
        
    except Exception as e:
        logger.error(f"Error downloading presentation slide image: {str(e)}")
        return {"success": False, "error": f"Failed to download slide image: {str(e)}"}

@app.post("/api/custom/slide-image/generate")
async def generate_slide_image(request: Request):
    """Generate slide image from current slide data (standalone, no video generation)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Generating slide image")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Import the HTML to image service
        from app.services.html_to_image_service import html_to_image_service
        
        # Generate a unique ID for this image generation
        import uuid
        image_id = str(uuid.uuid4())
        
        # Create output directory
        from pathlib import Path
        output_dir = Path("output/slide_images")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate image for the first slide (or all slides if needed)
        slide_props = slides_data[0]  # Use first slide
        template_id = slide_props.get("templateId")
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Template ID: {template_id}")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Slide props keys: {list(slide_props.keys())}")
        
        if not template_id:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Actual props keys: {list(actual_props.keys())}")
        
        # Log some key props for debugging
        for key, value in actual_props.items():
            if isinstance(value, str):
                logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] {key}: '{value[:100]}...'")
            else:
                logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] {key}: {value}")
        
        # Generate output filename
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"slide_image_{template_id}_{timestamp}_{image_id[:8]}.png"
        output_path = str(output_dir / output_filename)
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Output path: {output_path}")
        
        # Convert slide to PNG
        success = await html_to_image_service.convert_slide_to_png(
            template_id=template_id,
            props=actual_props,
            theme=theme,
            output_path=output_path
        )
        
        if success:
            # Verify file was created
            if not os.path.exists(output_path):
                logger.error(f"📷 [STANDALONE_SLIDE_IMAGE] File not found after generation: {output_path}")
                return {"success": False, "error": "Generated file not found"}
            
            file_size = os.path.getsize(output_path)
            logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Successfully generated slide image: {output_path} ({file_size} bytes)")
            
            # Return the image directly
            return FileResponse(
                path=output_path,
                media_type="image/png",
                filename=output_filename
            )
        else:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] Failed to generate slide image")
            return {"success": False, "error": "Failed to generate slide image"}
        
    except Exception as e:
        logger.error(f"📷 [STANDALONE_SLIDE_IMAGE] Error generating slide image: {str(e)}")
        return {"success": False, "error": f"Failed to generate slide image: {str(e)}"}

@app.post("/api/custom/slide-html/preview")
async def preview_slide_html(request: Request):
    """Preview the static HTML for a slide (debugging feature)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"🔍 [HTML_PREVIEW] Generating HTML preview")
        logger.info(f"🔍 [HTML_PREVIEW] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"🔍 [HTML_PREVIEW] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("🔍 [HTML_PREVIEW] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Get the first slide
        slide_props = slides_data[0]
        template_id = slide_props.get("templateId")
        
        logger.info(f"🔍 [HTML_PREVIEW] Template ID: {template_id}")
        logger.info(f"🔍 [HTML_PREVIEW] Slide props keys: {list(slide_props.keys())}")
        
        if not template_id:
            logger.error("🔍 [HTML_PREVIEW] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        logger.info(f"🔍 [HTML_PREVIEW] Actual props keys: {list(actual_props.keys())}")
        
        # Log some key props for debugging
        for key, value in actual_props.items():
            if isinstance(value, str):
                logger.info(f"🔍 [HTML_PREVIEW] {key}: '{value[:100]}...'")
            else:
                logger.info(f"🔍 [HTML_PREVIEW] {key}: {value}")
        
        # Import the HTML template service
        from app.services.html_template_service import html_template_service
        
        # Generate clean HTML
        logger.info(f"🔍 [HTML_PREVIEW] Generating HTML content...")
        html_content = html_template_service.generate_clean_html_for_video(
            template_id, actual_props, theme
        )
        
        logger.info(f"🔍 [HTML_PREVIEW] HTML content generated")
        logger.info(f"🔍 [HTML_PREVIEW] HTML content length: {len(html_content)} characters")
        
        # Return the HTML content
        return {
            "success": True,
            "html": html_content,
            "template_id": template_id,
            "theme": theme,
            "props": actual_props
        }
        
    except Exception as e:
        logger.error(f"🔍 [HTML_PREVIEW] Error generating HTML preview: {str(e)}")
        return {"success": False, "error": f"Failed to generate HTML preview: {str(e)}"}

@app.post("/api/custom/slide-video/generate")
async def generate_slide_video(request: Request):
    """Generate video from slide image only (no AI avatar)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"🎬 [SLIDE_VIDEO] Generating slide-only video")
        logger.info(f"🎬 [SLIDE_VIDEO] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"🎬 [SLIDE_VIDEO] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("🎬 [SLIDE_VIDEO] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Import the presentation service
        from app.services.presentation_service import presentation_service
        
        # Create a presentation request with slide-only flag
        from app.services.presentation_service import PresentationRequest
        
        # Get the first slide
        slide_props = slides_data[0]
        template_id = slide_props.get("templateId")
        
        if not template_id:
            logger.error("🎬 [SLIDE_VIDEO] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        
        # Create presentation request with required arguments
        presentation_request = PresentationRequest(
            slide_url="",  # Empty for slide-only mode
            voiceover_texts=[],  # Empty for slide-only mode
            slides_data=slides_data,
            theme=theme,
            slide_only=True,  # Flag to indicate slide-only video
            use_avatar_mask=False  # Disable avatar mask for slide-only videos
        )
        
        logger.info(f"🎬 [SLIDE_VIDEO] Creating slide-only presentation...")
        
        # Start the presentation generation
        job_id = await presentation_service.create_presentation(presentation_request)
        
        logger.info(f"🎬 [SLIDE_VIDEO] Slide-only video generation started with job ID: {job_id}")
        
        return {
            "success": True,
            "jobId": job_id,
            "message": "Slide-only video generation started"
        }
        
    except Exception as e:
        logger.error(f"🎬 [SLIDE_VIDEO] Error generating slide-only video: {str(e)}")
        return {"success": False, "error": f"Failed to generate slide-only video: {str(e)}"}

@app.get("/api/custom/presentations")
async def list_presentations(limit: int = 50):
    """List recent presentation jobs."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        jobs = await presentation_service.list_jobs(limit)
        
        return {
            "success": True,
            "jobs": [
                {
                    "jobId": job.job_id,
                    "status": job.status,
                    "progress": job.progress,
                    "error": job.error,
                    "videoUrl": job.video_url,
                    "thumbnailUrl": job.thumbnail_url,
                    "createdAt": job.created_at.isoformat() if job.created_at else None,
                    "completedAt": job.completed_at.isoformat() if job.completed_at else None
                }
                for job in jobs
            ]
        }
        
    except Exception as e:
        logger.error(f"Error listing presentations: {str(e)}")
        return {"success": False, "error": f"Failed to list presentations: {str(e)}"}

def _any_quiz_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited quiz content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Quiz content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No quiz changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during quiz change detection (assuming changes made): {e}")
        return True

def _any_text_presentation_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited text presentation content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Text presentation content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No text presentation changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during text presentation change detection (assuming changes made): {e}")
        return True

async def _generate_content_for_clean_titles(clean_content: str, original_content: str, language: str) -> str:
    """Generate content for clean titles (titles without descriptions)"""
    try:
        logger.info("Starting content generation for clean titles")
        
        # Parse the clean content to identify sections
        sections = []
        lines = clean_content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if this is a header (## Title)
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if header_match:
                # Save previous section if exists
                if current_section:
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    'title': header_match.group(2).strip(),
                    'content': '',
                    'needs_content': True
                }
            elif current_section:
                # This is content for the current section
                current_section['content'] += line + '\n'
                current_section['needs_content'] = False
        
        # Add the last section
        if current_section:
            sections.append(current_section)
        
        logger.info(f"Found {len(sections)} sections, {sum(1 for s in sections if s['needs_content'])} need content generation")
        
        # Generate content for sections that need it
        for section in sections:
            if section['needs_content']:
                logger.info(f"Generating content for section: {section['title']}")
                
                # Create prompt for content generation
                prompt = f"""Generate comprehensive content for the following section title in {language} language:

Title: {section['title']}

Please provide detailed, informative content that explains the topic thoroughly. The content should be:
- Educational and informative
- Well-structured with paragraphs
- Include relevant examples or explanations
- Match the tone and style of a professional presentation

Generate the content:"""
                
                try:
                    # Use OpenAI to generate content
                    response = await stream_openai_response_direct(prompt)
                    section['content'] = response
                    logger.info(f"Generated {len(response)} characters for section: {section['title']}")
                except Exception as e:
                    logger.error(f"Failed to generate content for section {section['title']}: {e}")
                    # Fallback to a simple description
                    section['content'] = f"This section covers {section['title']}. Please refer to the original content for detailed information."
        
        # Reconstruct the content with generated descriptions
        result_content = ""
        for section in sections:
            result_content += f"## {section['title']}\n\n{section['content']}\n\n"
        
        logger.info(f"Content generation completed. Total length: {len(result_content)} characters")
        return result_content.strip()
        
    except Exception as e:
        logger.error(f"Error in content generation for clean titles: {e}")
        # Fallback to original content
        return clean_content

async def stream_openai_response_direct(prompt: str, model: str = None) -> str:
    """
    Get a complete response directly from OpenAI API (non-streaming).
    Returns the full response as a string.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_DIRECT] Starting direct OpenAI request with model {model}")
        logger.info(f"[OPENAI_DIRECT] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_DIRECT] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the chat completion (non-streaming)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.2
        )
        
        if response.choices and len(response.choices) > 0:
            content = response.choices[0].message.content
            logger.info(f"[OPENAI_DIRECT] Response received: {len(content)} characters")
            return content
        else:
            logger.error(f"[OPENAI_DIRECT] No content in response")
            return ""
            
    except Exception as e:
        logger.error(f"[OPENAI_DIRECT] Error in OpenAI direct request: {e}", exc_info=True)
        return f"Error generating content: {str(e)}"


# --- Analytics Response Models ---
from enum import Enum

class ProductType(str, Enum):
    ONE_PAGER = "one_pager"
    PRESENTATION = "presentation"
    QUIZ = "quiz"
    VIDEO_LESSON = "video_lesson"

class QualityTier(str, Enum):
    BASIC = "basic"
    INTERACTIVE = "interactive"
    ADVANCED = "advanced"
    IMMERSIVE = "immersive"

class ProductTypeDistribution(BaseModel):
    type: ProductType
    count: int
    percentage: float
    color: str

class ProductsDistributionResponse(BaseModel):
    total_products: int
    distribution: List[ProductTypeDistribution]

class QualityTierDistribution(BaseModel):
    tier: QualityTier
    count: int
    percentage: float
    color: str

class QualityTiersDistributionResponse(BaseModel):
    total_lessons: int
    distribution: List[QualityTierDistribution]

# Color mappings for consistency
PRODUCT_TYPE_COLORS = {
    ProductType.ONE_PAGER: "#9333ea",     # Purple
    ProductType.PRESENTATION: "#2563eb",   # Blue  
    ProductType.QUIZ: "#16a34a",          # Green
    ProductType.VIDEO_LESSON: "#ea580c"   # Orange
}

QUALITY_TIER_COLORS = {
    QualityTier.BASIC: "#059669",        # Green
    QualityTier.INTERACTIVE: "#ea580c",   # Orange  
    QualityTier.ADVANCED: "#7c3aed",      # Purple
    QualityTier.IMMERSIVE: "#2563eb"      # Blue
}

# Component to Product Type mapping
COMPONENT_TO_PRODUCT_TYPE = {
    COMPONENT_NAME_TEXT_PRESENTATION: ProductType.ONE_PAGER,
    COMPONENT_NAME_SLIDE_DECK: ProductType.PRESENTATION,
    COMPONENT_NAME_QUIZ: ProductType.QUIZ,
    COMPONENT_NAME_VIDEO_LESSON: ProductType.VIDEO_LESSON,
    COMPONENT_NAME_VIDEO_LESSON_PRESENTATION: ProductType.VIDEO_LESSON,
    COMPONENT_NAME_PDF_LESSON: ProductType.ONE_PAGER,  # PDF lessons are considered one-pagers
    # Note: TrainingPlanTable components contain lessons, we need to count the lesson products, not the outline itself
}

@app.get("/api/custom/projects/analytics/product-distribution", response_model=ProductsDistributionResponse)
async def get_product_distribution(
    folder_id: Optional[int] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get product type distribution for analytics pie chart."""
    try:
        async with pool.acquire() as conn:
            # Build query to get all projects with their component types
            query = """
                SELECT dt.component_name, COUNT(*) as count
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.onyx_user_id = $1
            """
            params = [onyx_user_id]
            
            if folder_id is not None:
                query += " AND p.folder_id = $2"
                params.append(folder_id)
            
            query += " GROUP BY dt.component_name ORDER BY count DESC"
            
            rows = await conn.fetch(query, *params)
            
            # Process results
            product_counts = {}
            total_products = 0
            
            for row in rows:
                component_name = row['component_name']
                count = row['count']
                total_products += count
                
                # Map component to product type
                product_type = COMPONENT_TO_PRODUCT_TYPE.get(component_name)
                if product_type:
                    if product_type not in product_counts:
                        product_counts[product_type] = 0
                    product_counts[product_type] += count
            
            # Create distribution list
            distribution = []
            for product_type in ProductType:
                count = product_counts.get(product_type, 0)
                percentage = (count / total_products * 100) if total_products > 0 else 0
                
                distribution.append(ProductTypeDistribution(
                    type=product_type,
                    count=count,
                    percentage=round(percentage, 1),
                    color=PRODUCT_TYPE_COLORS[product_type]
                ))
            
            return ProductsDistributionResponse(
                total_products=total_products,
                distribution=distribution
            )
            
    except Exception as e:
        logger.error(f"Error getting product distribution: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get product distribution: {str(e)}")

@app.get("/api/custom/projects/analytics/quality-distribution", response_model=QualityTiersDistributionResponse)
async def get_quality_distribution(
    folder_id: Optional[int] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get quality tiers distribution for analytics pie chart."""
    try:
        async with pool.acquire() as conn:
            # Build query to get all lessons with their quality tiers
            query = """
                WITH lesson_quality_tiers AS (
                    SELECT 
                        COALESCE(
                            lesson->>'quality_tier',
                            section->>'quality_tier', 
                            p.quality_tier,
                            pf.quality_tier,
                            'interactive'
                        ) as effective_quality_tier
                    FROM projects p
                    LEFT JOIN project_folders pf ON p.folder_id = pf.id
                    CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                    CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                    WHERE p.onyx_user_id = $1
                    AND p.microproduct_content IS NOT NULL
                    AND p.microproduct_content->>'sections' IS NOT NULL
            """
            params = [onyx_user_id]
            
            if folder_id is not None:
                query += " AND p.folder_id = $2"
                params.append(folder_id)
            
            query += """
                )
                SELECT 
                    LOWER(effective_quality_tier) as quality_tier,
                    COUNT(*) as count
                FROM lesson_quality_tiers
                GROUP BY LOWER(effective_quality_tier)
                ORDER BY count DESC
            """
            
            rows = await conn.fetch(query, *params)
            
            # Process results
            tier_counts = {}
            total_lessons = 0
            
            for row in rows:
                tier_name = row['quality_tier'].lower()
                count = row['count']
                total_lessons += count
                
                # Map tier names to enum values
                tier_mapping = {
                    'basic': QualityTier.BASIC,
                    'interactive': QualityTier.INTERACTIVE,
                    'advanced': QualityTier.ADVANCED,
                    'immersive': QualityTier.IMMERSIVE,
                    'medium': QualityTier.INTERACTIVE,  # Map medium to interactive
                    'premium': QualityTier.ADVANCED,    # Map premium to advanced
                }
                
                tier = tier_mapping.get(tier_name, QualityTier.INTERACTIVE)
                if tier not in tier_counts:
                    tier_counts[tier] = 0
                tier_counts[tier] += count
            
            # Create distribution list
            distribution = []
            for tier in QualityTier:
                count = tier_counts.get(tier, 0)
                percentage = (count / total_lessons * 100) if total_lessons > 0 else 0
                
                distribution.append(QualityTierDistribution(
                    tier=tier,
                    count=count,
                    percentage=round(percentage, 1),
                    color=QUALITY_TIER_COLORS[tier]
                ))
            
            return QualityTiersDistributionResponse(
                total_lessons=total_lessons,
                distribution=distribution
            )
            
    except Exception as e:
        logger.error(f"Error getting quality distribution: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get quality distribution: {str(e)}")

# ============================
# SMART DRIVE API ENDPOINTS
# ============================

@app.post("/api/custom/smartdrive/login-token")
async def create_smartdrive_login_token(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a short-lived, one-time token to allow Nextcloud autologin script to fetch credentials.
    The token is bound to the current user and expires quickly to reduce risk.
    """
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        import secrets
        from datetime import datetime, timedelta, timezone

        token = secrets.token_urlsafe(32)
        now = datetime.now(timezone.utc)
        expires_at = now + timedelta(minutes=2)

        async with pool.acquire() as conn:
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS smartdrive_login_tokens (
                    token TEXT PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    created_at TIMESTAMPTZ NOT NULL,
                    expires_at TIMESTAMPTZ NOT NULL,
                    used BOOLEAN NOT NULL DEFAULT FALSE
                )
                """
            )
            await conn.execute(
                """
                INSERT INTO smartdrive_login_tokens (token, onyx_user_id, created_at, expires_at, used)
                VALUES ($1, $2, $3, $4, FALSE)
                """,
                token, str(onyx_user_id), now, expires_at
            )

        return {"success": True, "token": token, "expires_in_seconds": 120}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating SmartDrive login token: {e}")
        raise HTTPException(status_code=500, detail="Failed to create login token")


@app.get("/api/custom/smartdrive/login-credentials")
async def get_smartdrive_login_credentials(
    request: Request,
    token: str = Query(..., description="One-time autologin token"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Redeem a short-lived token for Nextcloud credentials for the bound user.
    Marks the token as used. Returns username/password/base_url for the user's account.
    """
    try:
        from datetime import datetime, timezone

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT token, onyx_user_id, created_at, expires_at, used
                FROM smartdrive_login_tokens
                WHERE token = $1
                """,
                token
            )
            if not row:
                raise HTTPException(status_code=404, detail="Invalid token")
            if row["used"]:
                raise HTTPException(status_code=400, detail="Token already used")
            if row["expires_at"] < datetime.now(timezone.utc):
                raise HTTPException(status_code=400, detail="Token expired")

            account = await conn.fetchrow(
                """
                SELECT nextcloud_username, nextcloud_password_encrypted, nextcloud_base_url
                FROM smartdrive_accounts
                WHERE onyx_user_id = $1
                """,
                row["onyx_user_id"]
            )
            if not account or not account.get("nextcloud_username") or not account.get("nextcloud_password_encrypted"):
                # Auto-provision a Nextcloud account for this user
                try:
                    import secrets, re
                    base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
                    nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
                    nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
                    if not (nc_admin_user and nc_admin_pass):
                        raise HTTPException(status_code=400, detail="Credentials not configured for user and auto-provisioning is not configured")

                    raw_id = str(row["onyx_user_id"])  # uuid or string
                    sanitized = re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
                    userid = f"sd_{sanitized[:24]}"
                    new_password = secrets.token_urlsafe(16)

                    # Normalize base to HTTPS to avoid 301 on POST and preserve method
                    from urllib.parse import urlparse
                    parsed = urlparse(base_url)
                    if parsed.scheme == "http":
                        base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
                    else:
                        base_url = (base_url or "").rstrip("/")
                    ocs_base = base_url

                    headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}

                    async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                        create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                        create_resp = await client.post(
                            create_url,
                            data={"userid": userid, "password": new_password},
                            headers=headers,
                            auth=(nc_admin_user, nc_admin_pass)
                        )

                        # Try to parse OCS response if possible
                        ocs_ok = False
                        reset_needed = False
                        try:
                            j = create_resp.json()
                            sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                            # 100 = OK, 102 = Already exists
                            if sc == 100:
                                ocs_ok = True
                            elif sc == 102:
                                reset_needed = True
                        except Exception:
                            pass

                        if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                            # User likely exists; attempt password reset
                            update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                            update_resp = await client.put(
                                update_url,
                                data={"key": "password", "value": new_password},
                                headers=headers,
                                auth=(nc_admin_user, nc_admin_pass)
                            )
                            if update_resp.status_code not in (200, 201, 204):
                                logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                        elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                            logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                            raise HTTPException(status_code=500, detail="Failed to auto-provision Nextcloud user")

                    encrypted = encrypt_password(new_password)
                    await conn.execute(
                        """
                        UPDATE smartdrive_accounts 
                        SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                        WHERE onyx_user_id = $1
                        """,
                        row["onyx_user_id"], userid, encrypted, base_url, datetime.now(timezone.utc)
                    )
                    account = {"nextcloud_username": userid, "nextcloud_password_encrypted": encrypted, "nextcloud_base_url": base_url}
                except HTTPException:
                    raise
                except Exception as provision_err:
                    logger.error(f"Auto-provision failed: {provision_err}")
                    raise HTTPException(status_code=500, detail="Failed to auto-provision Nextcloud account")

            try:
                password_plain = decrypt_password(account["nextcloud_password_encrypted"])  # type: ignore
            except Exception:
                raise HTTPException(status_code=500, detail="Failed to decrypt credentials")

            await conn.execute(
                "UPDATE smartdrive_login_tokens SET used = TRUE WHERE token = $1",
                token
            )

        headers = {
            "Cache-Control": "no-store, no-cache, must-revalidate, max-age=0",
            "Pragma": "no-cache",
        }
        return JSONResponse(
            content={
                "success": True,
                "username": account["nextcloud_username"],
                "password": password_plain,
                "base_url": account.get("nextcloud_base_url") or "http://nc1.contentbuilder.ai:8080",
            },
            headers=headers
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error redeeming SmartDrive login token: {e}")
        raise HTTPException(status_code=500, detail="Failed to redeem login token")

@app.post("/api/custom/smartdrive/session")
async def bootstrap_smartdrive_session(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Bootstrap SmartDrive access for the current user"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Bootstrapping SmartDrive session for user: {onyx_user_id}")

        async with pool.acquire() as conn:
            # Check if user already has SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                # Create new SmartDrive account placeholder
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    """,
                    onyx_user_id,
                    '{}',  # Empty JSON cursor
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )
                logger.info(f"Created SmartDrive account placeholder for user: {onyx_user_id}")
                has_credentials = False
            else:
                logger.info(f"SmartDrive account already exists for user: {onyx_user_id}")
                has_credentials = bool(account.get('nextcloud_username') and account.get('nextcloud_password_encrypted'))

        return {
            "success": True, 
            "message": "SmartDrive session initialized",
            "has_credentials": has_credentials,
            "setup_required": not has_credentials
        }
        
    except Exception as e:
        logger.error(f"Error bootstrapping SmartDrive session: {e}")
        raise HTTPException(status_code=500, detail="Failed to initialize SmartDrive session")


@app.post("/api/custom/smartdrive/credentials")
async def set_smartdrive_credentials(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Set or update user's Nextcloud credentials"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get request data
        data = await request.json()
        nextcloud_username = data.get('nextcloud_username', '').strip()
        nextcloud_password = data.get('nextcloud_password', '').strip()
        nextcloud_base_url = data.get('nextcloud_base_url', 'http://nc1.contentbuilder.ai:8080').strip()
        
        if not nextcloud_username or not nextcloud_password:
            raise HTTPException(status_code=400, detail="Username and password are required")
        
        # Encrypt password
        encrypted_password = encrypt_password(nextcloud_password)
        
        async with pool.acquire() as conn:
            # Update or insert credentials
            await conn.execute(
                """
                UPDATE smartdrive_accounts 
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
                """,
                onyx_user_id,
                nextcloud_username,
                encrypted_password,
                nextcloud_base_url,
                datetime.now(timezone.utc)
            )
            
        logger.info(f"Updated Nextcloud credentials for user: {onyx_user_id}")
        return {"success": True, "message": "Nextcloud credentials saved successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error setting SmartDrive credentials: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/custom/smartdrive/credentials")
async def set_smartdrive_credentials(request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Set or update user's Nextcloud credentials"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        data = await request.json()
        
        nextcloud_username = data.get('nextcloud_username', '').strip()
        nextcloud_password = data.get('nextcloud_password', '').strip()
        nextcloud_base_url = data.get('nextcloud_base_url', 'http://nc1.contentbuilder.ai:8080').strip()
        
        if not nextcloud_username or not nextcloud_password:
            raise HTTPException(status_code=400, detail="Username and password are required")
        
        # Encrypt password
        encrypted_password = encrypt_password(nextcloud_password)
        
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE smartdrive_accounts 
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
            """, onyx_user_id, nextcloud_username, encrypted_password, nextcloud_base_url, datetime.now(timezone.utc))
            
        logger.info(f"Updated Nextcloud credentials for user: {onyx_user_id}")
        return {"success": True, "message": "Nextcloud credentials saved successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error setting SmartDrive credentials: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/custom/smartdrive/list")
async def list_smartdrive_files(
    request: Request,
    path: str = Query("/", description="Path to list files from"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """List files/folders in the user's SmartDrive"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Listing SmartDrive files for user: {onyx_user_id}, path: {path}")

        async with pool.acquire() as conn:
            # Get user's SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                raise HTTPException(status_code=404, detail="SmartDrive account not found")

            # Use Nextcloud WebDAV API to list files
            # Use a shared Nextcloud account for all Smart Drive access
            nextcloud_username = os.getenv("NEXTCLOUD_USERNAME", "smart_drive_user")
            nextcloud_password = os.getenv("NEXTCLOUD_PASSWORD", "nextcloud_password")
            
            # Create user-specific folder path within the shared account
            nextcloud_user_folder = account['nextcloud_user_id']  # This becomes the folder name
            
            # Ensure user folder exists
            await ensure_user_folder_exists(nextcloud_username, nextcloud_password, nextcloud_user_folder)
            
            webdav_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{nextcloud_user_folder}{path}"
            
            auth = (nextcloud_username, nextcloud_password)
            
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.request(
                        "PROPFIND", 
                        webdav_url,
                        auth=auth,
                        headers={
                            "Depth": "1",
                            "Content-Type": "application/xml"
                        },
                        content="""<?xml version="1.0"?>
                        <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                            <d:prop>
                                <d:resourcetype/>
                                <d:getcontentlength/>
                                <d:getlastmodified/>
                                <d:getcontenttype/>
                            </d:prop>
                        </d:propfind>"""
                    )
                    
                    if response.status_code == 207:  # Multi-Status response
                        # Parse WebDAV XML response
                        files = await parse_webdav_response(response.text, path)
                        return {
                            "files": files,
                            "path": path,
                            "total_count": len(files)
                        }
                    else:
                        logger.error(f"Nextcloud WebDAV error: {response.status_code} - {response.text}")
                        # Fallback to mock data if Nextcloud is unavailable
                        return await get_mock_files_response(path)
                        
            except Exception as nextcloud_error:
                logger.error(f"Failed to connect to Nextcloud: {nextcloud_error}")
                # Fallback to mock data if Nextcloud is unavailable
                return await get_mock_files_response(path)
        
    except Exception as e:
        logger.error(f"Error listing SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to list SmartDrive files")

async def parse_webdav_response(xml_content: str, base_path: str) -> List[Dict]:
    """Parse WebDAV PROPFIND XML response into file list"""
    # Simple XML parsing for WebDAV response
    import xml.etree.ElementTree as ET
    
    files = []
    try:
        root = ET.fromstring(xml_content)
        
        for response in root.findall('.//{DAV:}response'):
            href = response.find('.//{DAV:}href')
            if href is None:
                continue
                
            file_path = href.text
            if file_path.endswith('/remote.php/dav/files/'):
                continue  # Skip the root directory entry
                
            # Extract relative path by removing the WebDAV prefix
            # file_path looks like: /smartdrive/remote.php/dav/files/username/Documents/file.txt
            # We want just: /Documents/file.txt
            if '/remote.php/dav/files/' in file_path:
                # Find the username part and extract everything after it
                parts = file_path.split('/remote.php/dav/files/')
                if len(parts) > 1:
                    # parts[1] is like "username/Documents/file.txt"
                    username_and_path = parts[1]
                    # Split by first "/" to separate username from the actual path
                    path_parts = username_and_path.split('/', 1)
                    if len(path_parts) > 1:
                        # This is the actual relative path we want
                        relative_path = '/' + path_parts[1]
                    else:
                        # Just the username, so root path
                        relative_path = '/'
                else:
                    relative_path = '/'
            else:
                relative_path = file_path
                
            # Extract file name
            name = relative_path.split('/')[-1] if not relative_path.endswith('/') else relative_path.split('/')[-2]
            if not name:
                continue
                
            # Check if it's a directory
            resourcetype = response.find('.//{DAV:}resourcetype')
            is_directory = resourcetype is not None and resourcetype.find('.//{DAV:}collection') is not None
            
            # Get file size
            size_elem = response.find('.//{DAV:}getcontentlength')
            size = int(size_elem.text) if size_elem is not None and size_elem.text else None
            
            # Get last modified
            modified_elem = response.find('.//{DAV:}getlastmodified')
            modified = modified_elem.text if modified_elem is not None else None
            
            # Get content type
            content_type_elem = response.find('.//{DAV:}getcontenttype')
            mime_type = content_type_elem.text if content_type_elem is not None else None
            
            # Get ETag
            etag_elem = response.find('.//{DAV:}getetag')
            etag = etag_elem.text if etag_elem is not None else None
            # Remove quotes from ETag if present
            if etag and etag.startswith('"') and etag.endswith('"'):
                etag = etag[1:-1]
            
            files.append({
                "name": name,
                "path": relative_path,
                "type": "directory" if is_directory else "file",
                "size": size,
                "modified": modified,
                "mime_type": mime_type,
                "etag": etag
            })
            
    except Exception as e:
        logger.error(f"Error parsing WebDAV response: {e}")
        
    return files

# --- Date Parsing Functions ---
def parse_http_date(date_string: str) -> datetime:
    """Parse HTTP date string (RFC 2822 format) to datetime object"""
    try:
        # Parse HTTP date format like "Wed, 13 Aug 2025 23:31:13 GMT"
        from email.utils import parsedate_to_datetime
        return parsedate_to_datetime(date_string)
    except Exception as e:
        logger.warning(f"Failed to parse date '{date_string}': {e}")
        return datetime.now(timezone.utc)

# --- Encryption Functions ---
def get_or_create_encryption_key():
    """Get or create a Fernet encryption key for the system"""
    key = os.getenv("SMARTDRIVE_ENCRYPTION_KEY")
    if not key:
        # Generate a new key and save it to environment (in production, store securely)
        from cryptography.fernet import Fernet
        key = Fernet.generate_key().decode()
        logger.warning(f"Generated new encryption key. Please set SMARTDRIVE_ENCRYPTION_KEY={key} in your environment for production!")
    return key.encode()

def encrypt_password(password: str) -> str:
    """Encrypt a password for storage"""
    try:
        from cryptography.fernet import Fernet
        f = Fernet(get_or_create_encryption_key())
        return f.encrypt(password.encode()).decode()
    except Exception as e:
        logger.error(f"Failed to encrypt password: {e}")
        raise HTTPException(status_code=500, detail="Encryption failed")

def decrypt_password(encrypted_password: str) -> str:
    """Decrypt a password from storage"""
    try:
        from cryptography.fernet import Fernet
        f = Fernet(get_or_create_encryption_key())
        return f.decrypt(encrypted_password.encode()).decode()
    except Exception as e:
        logger.error(f"Failed to decrypt password: {e}")
        raise HTTPException(status_code=500, detail="Decryption failed")

async def get_mock_files_response(path: str) -> Dict:
    """Fallback mock data when Nextcloud is unavailable"""
    mock_files = [
        {
            "name": "Documents",
            "path": f"{path}Documents/",
            "type": "directory",
            "size": None,
            "modified": "2024-01-15T10:30:00Z"
        },
        {
            "name": "Training_Materials.pdf",
            "path": f"{path}Training_Materials.pdf",
            "type": "file",
            "size": 2048576,
            "modified": "2024-01-14T15:45:00Z",
            "mime_type": "application/pdf"
        },
        {
            "name": "Project_Notes.docx",
            "path": f"{path}Project_Notes.docx",
            "type": "file",
            "size": 1024000,
            "modified": "2024-01-13T09:20:00Z",
            "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        }
    ]
    
    return {
        "files": mock_files,
        "path": path,
        "total_count": len(mock_files)
    }

@app.post("/api/custom/smartdrive/import")
async def import_smartdrive_files(
    request: Request,
    file_paths: List[str] = None,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Stream files into Onyx; returns fileIds"""
    try:
        if file_paths is None:
            payload = await request.json()
            file_paths = payload.get('paths', [])
            
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Importing SmartDrive files for user: {onyx_user_id}, paths: {file_paths}")

        imported_file_ids = []
        
        # Get SmartDrive account details for the user
        async with pool.acquire() as conn:
            account = await conn.fetchrow(
                "SELECT nextcloud_username, nextcloud_password_encrypted, nextcloud_base_url FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                logger.error(f"No SmartDrive account found for user {onyx_user_id}")
                raise HTTPException(status_code=400, detail="SmartDrive not configured for this user")
            
            # Decrypt password (simplified - you may need proper decryption)
            # For now, assuming password is stored in plain text or you have decryption logic
            nextcloud_username = account['nextcloud_username']
            nextcloud_password = account['nextcloud_password_encrypted']  # TODO: Implement proper decryption
            nextcloud_base_url = account.get('nextcloud_base_url', 'http://nc1.contentbuilder.ai:8080')
            
            logger.info(f"Using SmartDrive account: {nextcloud_username} at {nextcloud_base_url}")

        # Process each file
            for file_path in file_paths:
                try:
                    logger.info(f"Processing SmartDrive file: {file_path}")
                    
                    # Download file from Nextcloud
                    file_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{file_path}"
                    
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        response = await client.get(
                            file_url,
                            auth=(nextcloud_username, nextcloud_password)
                        )
                        response.raise_for_status()
                        
                        file_content = response.content
                        file_name = os.path.basename(file_path)
                        
                        logger.info(f"Downloaded {file_name} ({len(file_content)} bytes)")
                    
                    # Create a temporary UploadFile object for Onyx
                    file_obj = io.BytesIO(file_content)
                    temp_file = UploadFile(
                        file=file_obj,
                        filename=file_name,
                        headers={"content-type": response.headers.get("content-type", "application/octet-stream")}
                    )
                    
                    # Import into Onyx using the standard process
                    # This will create real Onyx file records and return proper file IDs
                    from onyx.server.documents.connector import upload_files
                    from onyx.db.engine import get_session_with_tenant
                    
                    # Get a database session for Onyx operations
                    db_session = next(get_session_with_tenant())
                    
                    try:
                        # Upload file to Onyx file store
                        upload_response = upload_files([temp_file], db_session)
                        real_file_id = upload_response.file_paths[0]  # Get the real Onyx file ID
                        
                        logger.info(f"Uploaded to Onyx with file ID: {real_file_id}")
                        
                        # Store mapping in smartdrive_imports with REAL file ID
                        async with pool.acquire() as conn:
                            import_record_id = await conn.fetchval(
                    """
                    INSERT INTO smartdrive_imports (onyx_user_id, smartdrive_path, onyx_file_id, etag, checksum, imported_at)
                    VALUES ($1, $2, $3, $4, $5, $6)
                                ON CONFLICT (onyx_user_id, smartdrive_path) 
                                DO UPDATE SET 
                                    onyx_file_id = EXCLUDED.onyx_file_id,
                                    etag = EXCLUDED.etag,
                                    checksum = EXCLUDED.checksum,
                                    imported_at = EXCLUDED.imported_at
                    RETURNING id
                    """,
                    onyx_user_id,
                    file_path,
                                real_file_id,  # REAL Onyx file ID!
                                response.headers.get("etag", f"etag_{hash(file_path)}"),
                                f"imported_{int(time.time())}",  # Simple checksum
                    datetime.now(timezone.utc)
                )
                        
                        imported_file_ids.append(import_record_id)
                        logger.info(f"✅ Successfully imported {file_path} -> Onyx file ID: {real_file_id}")
                        
                    finally:
                        db_session.close()
                        
                except Exception as e:
                    logger.error(f"❌ Failed to import {file_path}: {e}", exc_info=True)
                    # Continue with other files even if one fails
                    continue

        return {
            "success": True,
            "fileIds": imported_file_ids,
            "imported_count": len(imported_file_ids)
        }
        
    except Exception as e:
        logger.error(f"Error importing SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to import SmartDrive files")

@app.post("/api/custom/smartdrive/import-new")
async def import_new_smartdrive_files(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Import new/updated files since the last sync"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Importing new SmartDrive files for user: {onyx_user_id}")
        
        # Extract session cookies for Onyx authentication
        session_cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}

        async with pool.acquire() as conn:
            # Get user's SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                raise HTTPException(status_code=404, detail="SmartDrive account not found")

            # Check if user has set up their Nextcloud credentials
            if not account['nextcloud_username'] or not account['nextcloud_password_encrypted']:
                raise HTTPException(status_code=400, detail="Please set up your Nextcloud credentials first")

            # Decrypt user's credentials
            nextcloud_username = account['nextcloud_username']
            nextcloud_password = decrypt_password(account['nextcloud_password_encrypted'])
            nextcloud_base_url = account['nextcloud_base_url'] or 'http://nc1.contentbuilder.ai:8080'
            
            # Parse JSON cursor from database
            sync_cursor = json.loads(account['sync_cursor']) if account['sync_cursor'] else {}
            last_sync = sync_cursor.get('last_sync') if sync_cursor else None
            
            # Get list of all files from user's individual Nextcloud account
            all_files = await get_all_nextcloud_files_individual(nextcloud_username, nextcloud_password, nextcloud_base_url, "/")
            
            imported_count = 0
            imported_files = []
            
            logger.info(f"Processing {len(all_files)} items from Nextcloud")
            
            for file_info in all_files:
                logger.debug(f"Processing item: {file_info}")
                
                if file_info['type'] == 'directory':
                    logger.debug(f"Skipping directory: {file_info['path']}")
                    continue  # Skip directories for now
                    
                file_path = file_info['path']
                file_modified = file_info['modified']
                file_etag = file_info.get('etag')
                
                logger.info(f"Processing file: {file_path} (type: {file_info['type']}, etag: {file_etag}, modified: {file_modified})")
                
                # Check if already imported
                existing = await conn.fetchrow(
                    "SELECT id, etag FROM smartdrive_imports WHERE onyx_user_id = $1 AND smartdrive_path = $2",
                    onyx_user_id, file_path
                )
                
                logger.info(f"Existing record for {file_path}: {existing}")
                
                # Skip if already imported with same etag (only if both etags exist and match)
                if existing and existing.get('etag') and file_etag and existing['etag'] == file_etag:
                    logger.info(f"Skipping {file_path} - already imported with same etag: {file_etag}")
                    continue
                
                # For debugging: let's import all files for now and check the etag logic later
                logger.info(f"Will import file: {file_path} (etag: {file_etag}, existing_etag: {existing.get('etag') if existing else None})")
                
                # Try to import the file into Onyx
                try:
                    # Extract session cookies for Onyx authentication from the request object
                    session_cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
                    
                    onyx_file_id = await import_file_to_onyx_individual(
                        nextcloud_username,
                        nextcloud_password,
                        nextcloud_base_url, 
                        file_path, 
                        file_info, 
                        onyx_user_id,
                        session_cookies
                    )
                    
                    if onyx_file_id:
                        # Update or insert import record
                        if existing:
                            await conn.execute(
                                """
                                UPDATE smartdrive_imports 
                                SET onyx_file_id = $1, etag = $2, checksum = $3, imported_at = $4, last_modified = $5
                                WHERE id = $6
                                """,
                                onyx_file_id,
                                file_info.get('etag', ''),
                                file_info.get('checksum', ''),
                                datetime.now(timezone.utc),
                                parse_http_date(file_modified) if file_modified else None,
                                existing['id']
                            )
                        else:
                            await conn.execute(
                                """
                                INSERT INTO smartdrive_imports 
                                (onyx_user_id, smartdrive_path, onyx_file_id, etag, checksum, file_size, mime_type, imported_at, last_modified)
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                                """,
                                onyx_user_id,
                                file_path,
                                onyx_file_id,
                                file_info.get('etag', ''),
                                file_info.get('checksum', ''),
                                file_info.get('size'),
                                file_info.get('mime_type'),
                                datetime.now(timezone.utc),
                                parse_http_date(file_modified) if file_modified else None
                            )
                        
                        imported_count += 1
                        imported_files.append({
                            "name": file_info['name'],
                            "path": file_path,
                            "onyx_file_id": onyx_file_id
                        })
                        logger.info(f"Successfully imported {file_path} as Onyx file {onyx_file_id}")
                        
                except Exception as import_error:
                    logger.error(f"Failed to import {file_path}: {import_error}")
                    continue

            # Update sync cursor
            await conn.execute(
                "UPDATE smartdrive_accounts SET sync_cursor = $1, updated_at = $2 WHERE onyx_user_id = $3",
                json.dumps({"last_sync": datetime.now(timezone.utc).isoformat()}),
                datetime.now(timezone.utc),
                onyx_user_id
            )
            
            logger.info(f"Import completed: {imported_count} files imported for user {onyx_user_id}")

        return {
            "success": True,
            "imported_count": imported_count,
            "imported_files": imported_files,
            "message": f"Imported {imported_count} new files"
        }
        
    except Exception as e:
        logger.error(f"Error importing new SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to import new SmartDrive files")

async def ensure_user_folder_exists(nextcloud_username: str, nextcloud_password: str, user_folder: str):
    """Ensure the user's folder exists in Nextcloud, create if it doesn't"""
    try:
        folder_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{user_folder}/"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient() as client:
            # Check if folder exists
            response = await client.request("PROPFIND", folder_url, auth=auth, headers={"Depth": "0"})
            
            if response.status_code == 404:
                # Folder doesn't exist, create it
                logger.info(f"Creating Nextcloud folder for user: {user_folder}")
                create_response = await client.request("MKCOL", folder_url, auth=auth)
                
                if create_response.status_code in [201, 405]:  # 201 = Created, 405 = Already exists
                    logger.info(f"Successfully created/verified folder: {user_folder}")
                else:
                    logger.error(f"Failed to create folder {user_folder}: {create_response.status_code}")
            elif response.status_code == 207:
                # Folder exists
                logger.debug(f"User folder already exists: {user_folder}")
            else:
                logger.warning(f"Unexpected response checking folder {user_folder}: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Error ensuring user folder exists: {e}")

async def get_all_nextcloud_files_individual(nextcloud_username: str, nextcloud_password: str, nextcloud_base_url: str, base_path: str = "/") -> List[Dict]:
    """Get all files from user's individual Nextcloud account recursively"""
    all_files = []
    visited_paths = set()  # Prevent infinite recursion
    
    async def traverse_directory(path: str, depth: int = 0):
        # Prevent infinite recursion
        if depth > 10:  # Max depth limit
            logger.warning(f"Max recursion depth reached for path: {path}")
            return
            
        if path in visited_paths:
            logger.warning(f"Already visited path, skipping: {path}")
            return
            
        visited_paths.add(path)
        
        try:
            webdav_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{path}"
            auth = (nextcloud_username, nextcloud_password)
            
            logger.debug(f"Traversing directory: {path} (depth: {depth}, URL: {webdav_url})")
            
            async with httpx.AsyncClient() as client:
                response = await client.request(
                    "PROPFIND", 
                    webdav_url, 
                    auth=auth, 
                    headers={"Depth": "1", "Content-Type": "application/xml"},
                    content="""<?xml version="1.0"?>
                    <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                        <d:prop>
                            <d:resourcetype/>
                            <d:getcontentlength/>
                            <d:getlastmodified/>
                            <d:getcontenttype/>
                            <d:getetag/>
                        </d:prop>
                    </d:propfind>"""
                )
                
                if response.status_code == 404:
                    logger.warning(f"Directory not found: {path}")
                    return
                elif response.status_code != 207:
                    logger.error(f"WebDAV PROPFIND failed: {response.status_code} - {response.text}")
                    return
                    
                files = await parse_webdav_response(response.text, nextcloud_base_url)
                logger.debug(f"Found {len(files)} items in {path}")
                
                for file_info in files:
                    if file_info['name'] != '.' and file_info['name'] != '..':  # Skip current and parent directory entries
                        all_files.append(file_info)
                        
                        if file_info['type'] == 'directory':
                            # For subdirectories, traverse recursively
                            subdir_path = file_info['path']
                            logger.debug(f"Found subdirectory: {file_info['name']} -> {subdir_path}")
                            await traverse_directory(subdir_path, depth + 1)
                            
        except Exception as e:
            logger.error(f"Error traversing directory {path}: {e}")
    
    await traverse_directory(base_path)
    logger.info(f"Directory traversal completed. Found {len(all_files)} total items.")
    return all_files


async def get_all_nextcloud_files(nextcloud_user_folder: str, base_path: str = "/") -> List[Dict]:
    """Recursively get all files from Nextcloud using shared account"""
    all_files = []
    
    try:
        # Use shared Nextcloud account
        nextcloud_username = os.getenv("NEXTCLOUD_USERNAME", "smart_drive_user")
        nextcloud_password = os.getenv("NEXTCLOUD_PASSWORD", "nextcloud_password")
        
        webdav_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{nextcloud_user_folder}{base_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient() as client:
            response = await client.request(
                "PROPFIND",
                webdav_url,
                auth=auth,
                headers={
                    "Depth": "infinity",  # Get all files recursively
                    "Content-Type": "application/xml"
                },
                content="""<?xml version="1.0"?>
                <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                    <d:prop>
                        <d:resourcetype/>
                        <d:getcontentlength/>
                        <d:getlastmodified/>
                        <d:getcontenttype/>
                        <d:getetag/>
                    </d:prop>
                </d:propfind>"""
            )
            
            if response.status_code == 207:
                files = await parse_webdav_response(response.text, base_path)
                all_files.extend(files)
                
    except Exception as e:
        logger.error(f"Error getting files from Nextcloud: {e}")
        
    return all_files

async def import_file_to_onyx(nextcloud_user_folder: str, file_path: str, file_info: Dict, onyx_user_id: str) -> str:
    """Download file from Nextcloud and upload to Onyx"""
    try:
        # Download file from Nextcloud using shared account
        nextcloud_username = os.getenv("NEXTCLOUD_USERNAME", "smart_drive_user")
        nextcloud_password = os.getenv("NEXTCLOUD_PASSWORD", "nextcloud_password")
        
        download_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{nextcloud_user_folder}{file_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            download_response = await client.get(download_url, auth=auth)
            
            if download_response.status_code != 200:
                logger.error(f"Failed to download {file_path}: {download_response.status_code}")
                return None
                
            file_content = download_response.content
            file_name = file_info['name']
            mime_type = file_info.get('mime_type', 'application/octet-stream')
            
            # Upload to Onyx using the user file upload endpoint (same as frontend)
            onyx_upload_url = f"{ONYX_API_SERVER_URL}/user/file/upload"
            
            # Create multipart form data with folder_id parameter
            files = {
                'files': (file_name, file_content, mime_type)
            }
            data = {
                'folder_id': '-1'  # Use RECENT_DOCS_FOLDER_ID (default "Recent Documents" folder)
            }
            
            # Upload to Onyx with session authentication
            upload_response = await client.post(
                onyx_upload_url,
                files=files,
                data=data,
                timeout=60.0
            )
            
            if upload_response.status_code in [200, 201]:
                response_data = upload_response.json()
                # Extract file ID from Onyx response
                if isinstance(response_data, list) and len(response_data) > 0:
                    return str(response_data[0].get('id'))
                elif isinstance(response_data, dict):
                    return str(response_data.get('id'))
                else:
                    logger.error(f"Unexpected Onyx response format: {response_data}")
                    return None
            else:
                logger.error(f"Failed to upload to Onyx: {upload_response.status_code} - {upload_response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error importing file {file_path} to Onyx: {e}")
        return None


async def import_file_to_onyx_individual(
    nextcloud_username: str, 
    nextcloud_password: str, 
    nextcloud_base_url: str, 
    file_path: str, 
    file_info: Dict, 
    onyx_user_id: str,
    session_cookies: Dict[str, str]
) -> str:
    """Download file from individual Nextcloud account and upload to Onyx"""
    try:
        # Build download URL for individual account
        download_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{file_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            download_response = await client.get(download_url, auth=auth)
            
            if download_response.status_code != 200:
                logger.error(f"Failed to download {file_path}: {download_response.status_code}")
                return None
                
            file_content = download_response.content
            file_name = file_info['name']
            mime_type = file_info.get('mime_type', 'application/octet-stream')
            
            # Upload to Onyx using the user file upload endpoint (same as frontend)
            onyx_upload_url = f"{ONYX_API_SERVER_URL}/user/file/upload"
            
            # Create multipart form data with folder_id parameter
            files = {
                'files': (file_name, file_content, mime_type)
            }
            data = {
                'folder_id': '-1'  # Use RECENT_DOCS_FOLDER_ID (default "Recent Documents" folder)
            }
            
            # Upload to Onyx with session authentication
            upload_response = await client.post(
                onyx_upload_url,
                files=files,
                data=data,
                cookies=session_cookies,
                timeout=60.0
            )
            
            if upload_response.status_code in [200, 201]:
                response_data = upload_response.json()
                # Extract file ID from Onyx response
                if isinstance(response_data, list) and len(response_data) > 0:
                    return str(response_data[0].get('id'))
                elif isinstance(response_data, dict):
                    return str(response_data.get('id'))
                else:
                    logger.error(f"Unexpected Onyx response format: {response_data}")
                    return None
            else:
                logger.error(f"Failed to upload to Onyx: {upload_response.status_code} - {upload_response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error importing individual file {file_path}: {e}")
        return None


@app.post("/api/custom/smartdrive/webhook")
async def smartdrive_webhook(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Handle Nextcloud webhooks for file changes"""
    try:
        payload = await request.json()
        logger.info(f"Received SmartDrive webhook: {payload}")

        # TODO: Implement webhook processing
        # This would handle notifications from Nextcloud about file changes
        
        return {"success": True, "message": "Webhook processed"}
        
    except Exception as e:
        logger.error(f"Error processing SmartDrive webhook: {e}")
        raise HTTPException(status_code=500, detail="Failed to process webhook")

# ============================
# PER-USER CONNECTORS API (DEPRECATED)
# ============================
# 
# NOTE: We now use Onyx's native connector system with AccessType.PRIVATE
# instead of our custom connector implementation. The frontend redirects users
# to /admin/connectors/{source}?access_type=private to create connectors using
# Onyx's existing OAuth-enabled forms and configuration system.
#
# Users' private connectors are managed through:
# - Creation: /admin/connectors/{source} with access_type=private
# - Listing: /api/manage/admin/connector (filtered for private connectors)
# - Management: /admin/connector/{id} (Onyx's existing connector management UI)
# - Syncing: /api/manage/admin/connector/{id}/index (Onyx's existing sync API)
#
# This gives users the full Onyx connector experience (including OAuth support)
# while keeping connectors private to each user.

# New SmartDrive connector creation endpoint (bypasses admin requirements)
@app.post("/api/custom/smartdrive/connectors/create")
async def create_smartdrive_connector(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Create a private connector for the current user without requiring admin privileges.
    This allows non-admin users to create connectors for Smart Drive.
    """
    try:
        # Get the main app domain (remove /custom-projects-ui path)
        host = request.headers.get('host', 'localhost')
        # Force HTTPS for production domains, use HTTP only for localhost
        if 'localhost' in host or '127.0.0.1' in host:
            protocol = 'http'
        else:
            protocol = 'https'
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies (using the same pattern as other endpoints)
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'x-smart-drive-connector': 'true'  # Smart Drive header to bypass admin checks
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Get connector data from request
        connector_data = await request.json()
        connector_id = connector_data.get('connector_id')  # This is the source type (e.g., 'notion', 'slack')
        credential_id = connector_data.get('credential_id')  # ID of existing credential to use
        name = connector_data.get('name', f'Smart Drive {connector_id}')
        
        if not connector_id:
            raise HTTPException(status_code=400, detail="Connector ID is required")
        
        if not credential_id:
            raise HTTPException(status_code=400, detail="Credential ID is required")
        
        # Define which fields are credentials vs connector config
        credential_fields = {
            'notion': ['notion_integration_token'],
            'slack': ['slack_bot_token'],
            'github': ['github_access_token'],
            'zendesk': ['zendesk_subdomain', 'zendesk_email', 'zendesk_token'],
            'asana': ['asana_api_token_secret'],
            'dropbox': ['dropbox_access_token'],
            'confluence': ['confluence_username', 'confluence_access_token'],
            'jira': ['jira_user_email', 'jira_api_token'],
            'linear': ['linear_access_token'],
            'hubspot': ['hubspot_access_token'],
            'clickup': ['clickup_api_token', 'clickup_team_id'],
            'google_drive': ['google_tokens', 'google_primary_admin'],
            'gmail': ['google_tokens', 'google_primary_admin'],
            'salesforce': ['sf_username', 'sf_password', 'sf_security_token', 'is_sandbox'],
            'sharepoint': ['sp_client_id', 'sp_client_secret', 'sp_directory_id'],
            'airtable': ['airtable_access_token'],
            'document360': ['portal_id', 'document360_api_token'],
            'slab': ['slab_bot_token'],
            'guru': ['guru_user', 'guru_user_token'],
            'gong': ['gong_access_key', 'gong_access_key_secret'],
            'loopio': ['loopio_subdomain', 'loopio_client_id', 'loopio_client_token'],
            'productboard': ['productboard_access_token'],
            'zulip': ['zuliprc_content'],
            'gitbook': ['gitbook_api_key'],
            'gitlab': ['gitlab_url', 'gitlab_access_token'],
            'bookstack': ['bookstack_base_url', 'bookstack_api_token_id', 'bookstack_api_token_secret'],
            's3': ['aws_access_key_id', 'aws_secret_access_key', 'aws_role_arn'],
            'r2': ['account_id', 'r2_access_key_id', 'r2_secret_access_key'],
            'google_cloud_storage': ['access_key_id', 'secret_access_key'],
            'oci_storage': ['namespace', 'region', 'access_key_id', 'secret_access_key'],
            'teams': ['teams_client_id', 'teams_client_secret', 'teams_directory_id']
        }
        
        # Separate credential fields from connector config fields
        connector_credential_fields = credential_fields.get(connector_id, [])
        credential_json = {}
        connector_specific_config = {}
        
        for key, value in connector_data.items():
            if key not in ['connector_id', 'name', 'access_type', 'smart_drive', 'credential_id']:
                if key in connector_credential_fields:
                    credential_json[key] = value
                else:
                    connector_specific_config[key] = value
        
        # Create the connector payload
        connector_payload = {
            "name": name,
            "source": connector_id,
            "input_type": "poll",
            "access_type": "private",  # Required field for Smart Drive connectors
            "connector_specific_config": connector_specific_config,
            "refresh_freq": 3600,  # 1 hour
            "prune_freq": 86400,   # 1 day
            "indexing_start": None
        }
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        # Create the connector using httpx
        async with httpx.AsyncClient(timeout=30.0) as client:
            connector_url = ensure_https_url("/api/manage/admin/connector")
            
            connector_response = await client.post(
                connector_url,
                headers=auth_headers,
                json=connector_payload
            )
            
            if not connector_response.is_success:
                logger.error(f"Failed to create connector: {connector_response.text}")
                raise HTTPException(
                    status_code=connector_response.status_code,
                    detail=f"Failed to create connector: {connector_response.text}"
                )
            
            connector_result = connector_response.json()
            connector_id = connector_result.get('id')
            
            if not connector_id:
                raise HTTPException(status_code=500, detail="Failed to get connector ID")
            
            # Use the existing credential instead of creating a new one
            # Verify the credential exists and is accessible
            credential_response = await client.get(
                ensure_https_url(f"/api/manage/credential/{credential_id}"),
                headers=auth_headers
            )
            
            if not credential_response.is_success:
                logger.error(f"Failed to access credential: {credential_response.text}")
                # If credential access fails, try to delete the connector
                try:
                    await client.delete(
                        ensure_https_url(f"/api/manage/admin/connector/{connector_id}"),
                        headers=auth_headers
                    )
                except:
                    pass
                
                raise HTTPException(
                    status_code=credential_response.status_code,
                    detail=f"Failed to access credential: {credential_response.text}"
                )
            
            credential_result = credential_response.json()
            
            # Link the credential to the connector using Onyx's linkCredential approach
            auto_sync_options = {
                "enabled": True,
                "frequency": 3600
            }
            
            # Add Smart Drive header for credential linking
            linking_headers = auth_headers.copy()
            linking_headers['x-smart-drive-credential'] = 'true'
            
            cc_pair_response = await client.put(
                ensure_https_url(f"/api/manage/connector/{connector_id}/credential/{credential_id}"),
                headers=linking_headers,
                json={
                    "name": name,
                    "access_type": "private",
                    "groups": [],  # Must be an empty list, not None
                    "auto_sync_options": auto_sync_options
                }
            )
            
            if not cc_pair_response.is_success:
                logger.error(f"Failed to create connector-credential pair: {cc_pair_response.text}")
                # If CC pair creation fails, try to delete both connector and credential
                try:
                    await client.delete(
                        ensure_https_url(f"/api/manage/admin/connector/{connector_id}"),
                        headers=auth_headers
                    )
                    await client.delete(
                        ensure_https_url(f"/api/manage/credential/{credential_id}"),
                        headers=auth_headers
                    )
                except:
                    pass
                
                raise HTTPException(
                    status_code=cc_pair_response.status_code,
                    detail=f"Failed to create connector-credential pair: {cc_pair_response.text}"
                )
            
            cc_pair_result = cc_pair_response.json()
            
            return {
                "success": True,
                "message": "Connector created successfully",
                "connector": connector_result,
                "credential": credential_result,
                "cc_pair": cc_pair_result
            }
            
    except httpx.RequestError as e:
        logger.error(f"Network error creating connector: {e}")
        raise HTTPException(status_code=500, detail="Network error occurred")
    except Exception as e:
        logger.error(f"Error creating connector: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Legacy endpoint stub (kept for backwards compatibility)
@app.get("/api/custom/smartdrive/connectors/")
async def list_user_connectors(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use Onyx's native connector system instead"""
    return {
        "message": "This endpoint is deprecated. Use Onyx's native connector system with AccessType.PRIVATE",
        "redirect_url": "/admin/connectors/",
        "api_endpoint": "/api/manage/admin/connector",
        "connectors": []
    }

@app.post("/api/custom/smartdrive/connectors/")
async def create_user_connector(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connectors/{source}?access_type=private instead"""
    payload = await request.json()
    source = payload.get('source', 'unknown')
    
    raise HTTPException(
        status_code=410,  # Gone
        detail={
            "message": "This endpoint is deprecated. Create connectors using Onyx's native system.",
            "redirect_url": f"/admin/connectors/{source}?access_type=private",
            "instructions": "Visit the connector creation page to set up your private connector with OAuth support."
        }
    )

@app.put("/api/custom/smartdrive/connectors/{connector_id}")
async def update_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connector/{connector_id} instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Manage connectors using Onyx's native system.",
            "redirect_url": f"/admin/connector/{connector_id}",
            "instructions": "Visit the connector management page to update your connector configuration."
        }
    )

@app.delete("/api/custom/smartdrive/connectors/{connector_id}")
async def delete_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connector/{connector_id} instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Manage connectors using Onyx's native system.",
            "redirect_url": f"/admin/connector/{connector_id}",
            "instructions": "Visit the connector management page to delete your connector."
        }
    )

@app.post("/api/custom/smartdrive/connectors/{connector_id}/sync")
async def sync_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /api/manage/admin/connector/{connector_id}/index instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Sync connectors using Onyx's native API.",
            "api_endpoint": f"/api/manage/admin/connector/{connector_id}/index",
            "instructions": "Use Onyx's connector sync API or the connector management UI."
        }
    )


# Credential proxy endpoints for non-admin users
@app.get("/api/custom/credentials/{source_type}")
async def get_credentials_for_source(source_type: str, request: Request):
    """
    Proxy endpoint to fetch credentials for a specific source type.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Accept': 'application/json',
            'x-smart-drive-credential': 'true'  # Smart Drive header to bypass admin checks
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's credential endpoint
            credentials_url = ensure_https_url(f"/api/manage/admin/similar-credentials/{source_type}")
            
            response = await client.get(
                credentials_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            elif response.status_code == 403:
                # User doesn't have admin access, return empty list
                # This allows the frontend to show "No existing credentials" 
                # and proceed with credential creation
                logger.info(f"Non-admin user accessing credentials for {source_type}, returning empty list")
                return []
            else:
                logger.error(f"Failed to fetch credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to fetch credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error fetching credentials: {str(e)}")
        # For any error, return empty list to allow credential creation
        return []


@app.post("/api/custom/credentials")
async def create_credential(request: Request):
    """
    Proxy endpoint to create credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        # Get credential data from request
        credential_data = await request.json()
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's credential creation endpoint
            credentials_url = ensure_https_url("/api/manage/credential")
            
            response = await client.post(
                credentials_url,
                headers=auth_headers,
                json=credential_data
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to create credential: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to create credential: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error creating credential: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error creating credential: {str(e)}")

# --- Offers API Endpoints ---

@app.get("/api/custom/offers", response_model=List[OfferResponse])
async def get_offers(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    company_id: Optional[int] = Query(None, description="Filter by company ID"),
    status: Optional[str] = Query(None, description="Filter by status"),
    search: Optional[str] = Query(None, description="Search in offer name or manager")
):
    """Get all offers for the current user with optional filtering"""
    try:
        
        # Build the query with optional filters
        query = """
            SELECT o.*, pf.name as company_name
            FROM offers o
            LEFT JOIN project_folders pf ON o.company_id = pf.id
            WHERE o.onyx_user_id = $1
        """
        params = [onyx_user_id]
        param_count = 1
        
        if company_id is not None:
            param_count += 1
            query += f" AND o.company_id = ${param_count}"
            params.append(company_id)
        
        if status is not None:
            param_count += 1
            query += f" AND o.status = ${param_count}"
            params.append(status)
        
        if search is not None:
            param_count += 1
            query += f" AND (o.offer_name ILIKE ${param_count} OR o.manager ILIKE ${param_count})"
            params.append(f"%{search}%")
        
        query += " ORDER BY o.created_on DESC"
        
        async with DB_POOL.acquire() as connection:
            rows = await connection.fetch(query, *params)
            
        offers = []
        for row in rows:
            offers.append(OfferResponse(
                id=row['id'],
                onyx_user_id=row['onyx_user_id'],
                company_id=row['company_id'],
                offer_name=row['offer_name'],
                created_on=row['created_on'],
                manager=row['manager'],
                status=row['status'],
                total_hours=row['total_hours'],
                link=row['link'],
                created_at=row['created_at'],
                updated_at=row['updated_at'],
                company_name=row['company_name']
            ))
        
        return offers
        
    except Exception as e:
        logger.error(f"Error fetching offers: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offers")

@app.get("/api/custom/offers/counts")
async def get_offer_counts(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Return a mapping of company_id to number of offers for the current user."""
    try:
        async with DB_POOL.acquire() as connection:
            rows = await connection.fetch(
                """
                SELECT company_id, COUNT(*) AS cnt
                FROM offers
                WHERE onyx_user_id = $1
                GROUP BY company_id
                """,
                onyx_user_id,
            )
        result = {row["company_id"]: int(row["cnt"]) for row in rows}
        return result
    except Exception as e:
        logger.error(f"Error fetching offer counts: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offer counts")

@app.post("/api/custom/offers", response_model=OfferResponse)
async def create_offer(
    offer_data: OfferCreate,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Create a new offer"""
    try:
        
        # Validate that the company exists and belongs to the user
        async with DB_POOL.acquire() as connection:
            company_exists = await connection.fetchval(
                "SELECT id FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                offer_data.company_id, onyx_user_id
            )
            
            if not company_exists:
                raise HTTPException(status_code=404, detail="Company not found")
            
            # First insert the offer without link to get the ID
            row = await connection.fetchrow("""
                INSERT INTO offers (onyx_user_id, company_id, offer_name, manager, status, total_hours, link)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                RETURNING *, (SELECT name FROM project_folders WHERE id = $2) as company_name
            """, onyx_user_id, offer_data.company_id, offer_data.offer_name, 
                 offer_data.manager, offer_data.status, offer_data.total_hours, None)
            
            # Generate auto link based on the offer ID
            offer_id = row['id']
            # Get the base URL from the request
            base_url = str(request.base_url).rstrip('/')
            
            # Log the base URL for debugging
            logger.info(f"Original base URL: {base_url}")
            
            # Remove /api/custom-projects-backend from the base URL if present
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            auto_link = f"{base_url}/custom-projects-ui/offer/{offer_id}"
            
            # Log the generated link for debugging
            logger.info(f"Generated auto link for offer {offer_id}: {auto_link}")
            
            # Update the offer with the auto-generated link
            update_result = await connection.execute(
                "UPDATE offers SET link = $1 WHERE id = $2",
                auto_link, offer_id
            )
            
            # Log the update result
            logger.info(f"Update result for offer {offer_id}: {update_result}")
            
            # Update the row dict with the new link
            row_dict = dict(row)
            row_dict['link'] = auto_link
        
        return OfferResponse(
            id=row_dict['id'],
            onyx_user_id=row_dict['onyx_user_id'],
            company_id=row_dict['company_id'],
            offer_name=row_dict['offer_name'],
            created_on=row_dict['created_on'],
            manager=row_dict['manager'],
            status=row_dict['status'],
            total_hours=row_dict['total_hours'],
            link=row_dict['link'],
            created_at=row_dict['created_at'],
            updated_at=row_dict['updated_at'],
            company_name=row_dict['company_name']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create offer")

@app.put("/api/custom/offers/{offer_id}", response_model=OfferResponse)
async def update_offer(
    offer_id: int,
    offer_data: OfferUpdate,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Update an existing offer"""
    try:
        
        # Build dynamic update query
        update_fields = []
        params = [onyx_user_id, offer_id]
        param_count = 2
        
        if offer_data.company_id is not None:
            param_count += 1
            update_fields.append(f"company_id = ${param_count}")
            params.append(offer_data.company_id)
        
        if offer_data.offer_name is not None:
            param_count += 1
            update_fields.append(f"offer_name = ${param_count}")
            params.append(offer_data.offer_name)
        
        if offer_data.manager is not None:
            param_count += 1
            update_fields.append(f"manager = ${param_count}")
            params.append(offer_data.manager)
        
        if offer_data.status is not None:
            param_count += 1
            update_fields.append(f"status = ${param_count}")
            params.append(offer_data.status)
        
        if offer_data.total_hours is not None:
            param_count += 1
            update_fields.append(f"total_hours = ${param_count}")
            params.append(offer_data.total_hours)
        
        if offer_data.created_on is not None:
            param_count += 1
            update_fields.append(f"created_on = ${param_count}")
            params.append(offer_data.created_on)
        
        # Note: link is auto-generated and not editable
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")
        
        # Add updated_at timestamp
        param_count += 1
        update_fields.append(f"updated_at = CURRENT_TIMESTAMP")
        
        query = f"""
            UPDATE offers 
            SET {', '.join(update_fields)}
            WHERE id = $2 AND onyx_user_id = $1
            RETURNING *, (SELECT name FROM project_folders WHERE id = company_id) as company_name
        """
        
        async with DB_POOL.acquire() as connection:
            row = await connection.fetchrow(query, *params)
            
            if not row:
                raise HTTPException(status_code=404, detail="Offer not found")
        
        return OfferResponse(
            id=row['id'],
            onyx_user_id=row['onyx_user_id'],
            company_id=row['company_id'],
            offer_name=row['offer_name'],
            created_on=row['created_on'],
            manager=row['manager'],
            status=row['status'],
            total_hours=row['total_hours'],
            link=row['link'],
            created_at=row['created_at'],
            updated_at=row['updated_at'],
            company_name=row['company_name']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to update offer")

@app.delete("/api/custom/offers/{offer_id}")
async def delete_offer(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Delete an offer"""
    try:
        
        async with DB_POOL.acquire() as connection:
            result = await connection.execute(
                "DELETE FROM offers WHERE id = $1 AND onyx_user_id = $2",
                offer_id, onyx_user_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(status_code=404, detail="Offer not found")
        
        return {"message": "Offer deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to delete offer")

@app.get("/api/custom/offers/{offer_id}/details")
async def get_offer_details(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Get detailed offer information for offer detail page"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offer basic info
            offer_row = await connection.fetchrow("""
                SELECT o.*, pf.name as company_name
                FROM offers o
                JOIN project_folders pf ON o.company_id = pf.id
                WHERE o.id = $1 AND o.onyx_user_id = $2
            """, offer_id, onyx_user_id)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Offer not found")
            
            offer = dict(offer_row)
            
            # Get projects in the company folder to build course modules
            projects_rows = await connection.fetch("""
                SELECT p.id, p.project_name, p.microproduct_content
                FROM projects p
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                AND p.microproduct_content IS NOT NULL
                AND p.microproduct_content->>'sections' IS NOT NULL
            """, offer['company_id'], onyx_user_id)
            
            course_modules = []
            total_lessons = 0
            total_learning_duration = 0
            total_production_time = 0
            
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    project_lessons = 0
                    project_modules = 0
                    project_completion_time = 0
                    project_hours = 0
                    
                    for section in content['sections']:
                        if section.get('lessons'):
                            project_modules += 1  # Count modules (sections)
                            for lesson in section['lessons']:
                                project_lessons += 1
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                    project_completion_time += completion_minutes
                                except (ValueError, AttributeError):
                                    project_completion_time += 5
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                project_hours += lesson_hours
                    
                    if project_lessons > 0:
                        learning_duration_hours = round(project_completion_time / 60.0, 1)
                        course_modules.append({
                            'title': project_dict['project_name'],
                            'modules': project_modules,
                            'lessons': project_lessons,
                            'learningDuration': f"{learning_duration_hours}h",
                            'productionTime': f"{project_hours}h"
                        })
                        
                        total_lessons += project_lessons
                        total_learning_duration += learning_duration_hours
                        total_production_time += project_hours
            
            # Calculate quality-level-specific totals by examining each lesson's quality tier
            quality_tier_totals = {
                'basic': {'learning_duration': 0, 'production_time': 0},
                'interactive': {'learning_duration': 0, 'production_time': 0},
                'advanced': {'learning_duration': 0, 'production_time': 0},
                'immersive': {'learning_duration': 0, 'production_time': 0}
            }
            
            # Re-process projects to calculate quality-specific totals
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    for section in content['sections']:
                        if section.get('lessons'):
                            for lesson in section['lessons']:
                                # Determine effective quality tier for this lesson
                                effective_quality_tier = (
                                    lesson.get('quality_tier') or 
                                    section.get('quality_tier') or 
                                    content.get('quality_tier') or
                                    'interactive'  # Default fallback
                                ).lower()
                                
                                # Map alternative names to standard tiers
                                tier_mapping = {
                                    'basic': 'basic',
                                    'interactive': 'interactive',
                                    'advanced': 'advanced',
                                    'immersive': 'immersive',
                                    'medium': 'interactive',  # Map medium to interactive
                                    'premium': 'advanced',    # Map premium to advanced
                                }
                                
                                standard_tier = tier_mapping.get(effective_quality_tier, 'interactive')
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                except (ValueError, AttributeError):
                                    completion_minutes = 5
                                
                                learning_duration_hours = completion_minutes / 60.0
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                
                                # Add to the appropriate quality tier totals
                                quality_tier_totals[standard_tier]['learning_duration'] += learning_duration_hours
                                quality_tier_totals[standard_tier]['production_time'] += lesson_hours
            
            # Generate quality levels data using quality-specific totals
            quality_levels = [
                {
                    'level': 'Level 1 - Basic',
                    'learningDuration': f"{round(quality_tier_totals['basic']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['basic']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 2 - Interactive',
                    'learningDuration': f"{round(quality_tier_totals['interactive']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['interactive']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 3 - Advanced',
                    'learningDuration': f"{round(quality_tier_totals['advanced']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['advanced']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 4 - Immersive',
                    'learningDuration': f"{round(quality_tier_totals['immersive']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['immersive']['production_time'], 1)}h"
                }
            ]
            
            return {
                'offer': offer,
                'courseModules': course_modules,
                'qualityLevels': quality_levels
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching offer details: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offer details")

@app.post("/api/custom/offers/migrate-links")
async def migrate_offer_links(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Update existing offers with auto-generated links"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offers without links or with empty links
            offers = await connection.fetch("""
                SELECT id FROM offers 
                WHERE onyx_user_id = $1 AND (link IS NULL OR link = '')
            """, onyx_user_id)
            
            updated_count = 0
            base_url = str(request.base_url).rstrip('/')
            
            # Log the base URL for debugging
            logger.info(f"Migration: Original base URL: {base_url}")
            
            # Remove /api/custom-projects-backend from the base URL if present
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            logger.info(f"Migration: Found {len(offers)} offers to update")
            
            for offer in offers:
                offer_id = offer['id']
                auto_link = f"{base_url}/custom-projects-ui/offer/{offer_id}"
                
                logger.info(f"Migration: Updating offer {offer_id} with link: {auto_link}")
                
                await connection.execute(
                    "UPDATE offers SET link = $1 WHERE id = $2",
                    auto_link, offer_id
                )
                updated_count += 1
            
            logger.info(f"Migration: Successfully updated {updated_count} offers")
            return {"message": f"Updated {updated_count} offers with auto-generated links"}
            
    except Exception as e:
        logger.error(f"Error migrating offer links: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to migrate offer links")

@app.post("/api/custom/offers/{offer_id}/generate-share-link")
async def generate_offer_share_link(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Generate a shareable link for an offer that doesn't require authentication"""
    try:
        async with DB_POOL.acquire() as connection:
            # Verify offer exists and belongs to user
            offer_row = await connection.fetchrow("""
                SELECT id, share_token FROM offers 
                WHERE id = $1 AND onyx_user_id = $2
            """, offer_id, onyx_user_id)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Offer not found")
            
            # Generate or use existing share token
            share_token = offer_row['share_token']
            if not share_token:
                share_token = str(uuid.uuid4())
                await connection.execute(
                    "UPDATE offers SET share_token = $1 WHERE id = $2",
                    share_token, offer_id
                )
            
            # Build shareable URL
            base_url = str(request.base_url).rstrip('/')
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            share_url = f"{base_url}/custom-projects-ui/offer/shared/{share_token}"
            
            return {
                "share_token": share_token,
                "share_url": share_url
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating share link: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to generate share link")

@app.get("/api/custom/offers/shared/{share_token}/details")
async def get_shared_offer_details(share_token: str):
    """Get offer details using share token - no authentication required"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offer basic info using share token
            offer_row = await connection.fetchrow("""
                SELECT o.*, pf.name as company_name
                FROM offers o
                JOIN project_folders pf ON o.company_id = pf.id
                WHERE o.share_token = $1
            """, share_token)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Shared offer not found")
            
            offer = dict(offer_row)
            
            # Get projects in the company folder to build course modules
            projects_rows = await connection.fetch("""
                SELECT p.id, p.project_name, p.microproduct_content
                FROM projects p
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                AND p.microproduct_content IS NOT NULL
                AND p.microproduct_content->>'sections' IS NOT NULL
            """, offer['company_id'], offer['onyx_user_id'])
            
            course_modules = []
            total_lessons = 0
            total_learning_duration = 0
            total_production_time = 0
            
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    project_lessons = 0
                    project_modules = 0
                    project_completion_time = 0
                    project_hours = 0
                    
                    for section in content['sections']:
                        if section.get('lessons'):
                            project_modules += 1  # Count modules (sections)
                            for lesson in section['lessons']:
                                project_lessons += 1
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                    project_completion_time += completion_minutes
                                except (ValueError, AttributeError):
                                    project_completion_time += 5
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                project_hours += lesson_hours
                    
                    if project_lessons > 0:
                        learning_duration_hours = round(project_completion_time / 60.0, 1)
                        course_modules.append({
                            'title': project_dict['project_name'],
                            'modules': project_modules,
                            'lessons': project_lessons,
                            'learningDuration': f"{learning_duration_hours}h",
                            'productionTime': f"{project_hours}h"
                        })
                        
                        total_lessons += project_lessons
                        total_learning_duration += learning_duration_hours
                        total_production_time += project_hours
            
            # Generate quality levels data - using actual totals from all courses
            quality_levels = [
                {
                    'level': 'Level 1 - Basic',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{total_production_time}h"
                },
                {
                    'level': 'Level 2 - Interactive',
                    'learningDuration': f"{total_learning_duration}h", 
                    'productionTime': f"{int(total_production_time * 1.5)}h"  # 50% more for interactive
                },
                {
                    'level': 'Level 3 - Advanced',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{int(total_production_time * 2)}h"  # 2x for advanced
                },
                {
                    'level': 'Level 4 - Immersive',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{int(total_production_time * 3)}h"  # 3x for immersive
                }
            ]
            
            # Remove sensitive information for shared view
            offer_public = {
                'id': offer['id'],
                'offer_name': offer['offer_name'],
                'company_name': offer['company_name'],
                'manager': offer['manager'],
                'created_on': offer['created_on'],
                'status': offer['status'],
                'total_hours': offer['total_hours']
            }
            
            return {
                'offer': offer_public,
                'courseModules': course_modules,
                'qualityLevels': quality_levels
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching shared offer details: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch shared offer details")

# ============================================================================
# WORKSPACE MANAGEMENT API ENDPOINTS
# ============================================================================

# Dependency to get current user ID (placeholder - integrate with your auth system)
async def get_current_user_id_for_workspaces(request: Request) -> str:
    """Get current user ID for workspace operations - uses same logic as projects"""
    try:
        # Use the same user identification logic as projects
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # For workspace operations, we'll use email as the primary identifier
        # since workspace members are stored with emails
        return user_email if user_email else user_uuid
    except Exception as e:
        logger.error(f"Failed to get user ID for workspace operations: {e}")
        # Fallback for development
        return "current_user_123"

# Workspace Management Endpoints

@app.post("/api/custom/workspaces", response_model=Workspace)
async def create_workspace(workspace_data: WorkspaceCreate, request: Request):
    """Create a new workspace."""
    try:
        # Get user identifiers (same logic as get_workspaces endpoint)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # Use email for workspace operations since members are stored with emails
        current_user_id = user_email if user_email else user_uuid
        
        logger.info(f"🔍 [WORKSPACE CREATE] Creating workspace '{workspace_data.name}' for user: {current_user_id} (UUID: {user_uuid})")
        workspace = await WorkspaceService.create_workspace(workspace_data, current_user_id)
        logger.info(f"✅ [WORKSPACE CREATE] Successfully created workspace {workspace.id} for user {current_user_id}")
        return workspace
    except ValueError as e:
        logger.error(f"❌ [WORKSPACE CREATE] Validation error: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"❌ [WORKSPACE CREATE] Failed to create workspace: {e}")
        raise HTTPException(status_code=500, detail="Failed to create workspace")

@app.get("/api/custom/workspaces", response_model=List[Workspace])
async def get_workspaces(request: Request):
    """Get all workspaces where the current user is a member."""
    try:
        # Get user identifiers (same logic as projects endpoint)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # Use email for workspace operations since members are stored with emails
        current_user_id = user_email if user_email else user_uuid
        
        logger.info(f"🔍 [WORKSPACE LIST] Getting workspaces for user: {current_user_id} (UUID: {user_uuid})")
        workspaces = await WorkspaceService.get_user_workspaces(current_user_id)
        logger.info(f"🔍 [WORKSPACE LIST] Found {len(workspaces)} workspaces for user {current_user_id}")
        
        return workspaces
    except Exception as e:
        logger.error(f"Failed to retrieve workspaces: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve workspaces")

@app.get("/api/custom/workspaces/{workspace_id}", response_model=Workspace)
async def get_workspace(workspace_id: int, request: Request):
    """Get a specific workspace by ID."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        workspace = await WorkspaceService.get_workspace(workspace_id)
        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace")

@app.get("/api/custom/workspaces/{workspace_id}/full", response_model=WorkspaceWithMembers)
async def get_workspace_with_members(workspace_id: int, request: Request):
    """Get a workspace with all its members and roles."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        workspace_data = await WorkspaceService.get_workspace_with_members(workspace_id)
        if not workspace_data:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace_data
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace data")

@app.put("/api/custom/workspaces/{workspace_id}", response_model=Workspace)
async def update_workspace(workspace_data: WorkspaceUpdate, workspace_id: int, request: Request):
    """Update a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        workspace = await WorkspaceService.update_workspace(workspace_id, workspace_data, current_user_id)
        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update workspace")

@app.delete("/api/custom/workspaces/{workspace_id}")
async def delete_workspace(workspace_id: int, request: Request):
    """Delete a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        success = await WorkspaceService.delete_workspace(workspace_id, current_user_id)
        if not success:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return {"message": "Workspace deleted successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to delete workspace")

# Role Management Endpoints

@app.post("/api/custom/workspaces/{workspace_id}/roles", response_model=WorkspaceRole)
async def create_role(role_data: WorkspaceRoleCreate, workspace_id: int, request: Request):
    """Create a new custom role in a workspace."""
    try:
        # Ensure workspace_id matches path parameter
        role_data.workspace_id = workspace_id
        
        role = await RoleService.create_custom_role(role_data, "current_user_123")
        return role
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to create role")

@app.get("/api/custom/workspaces/{workspace_id}/roles", response_model=List[WorkspaceRole])
async def get_workspace_roles(workspace_id: int, request: Request):
    """Get all roles for a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        roles = await RoleService.get_workspace_roles(workspace_id)
        logger.info(f"✅ [WORKSPACE ROLES] Retrieved {len(roles)} roles for workspace {workspace_id}")
        return roles
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [WORKSPACE ROLES] Failed to get roles for workspace {workspace_id}: {e}")
        logger.error(f"❌ [WORKSPACE ROLES] Error type: {type(e).__name__}")
        import traceback
        logger.error(f"❌ [WORKSPACE ROLES] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve roles: {str(e)}")

@app.get("/api/custom/workspaces/{workspace_id}/roles/{role_id}", response_model=WorkspaceRole)
async def get_workspace_role(workspace_id: int, role_id: int):
    """Get a specific role from a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        role = await RoleService.get_workspace_role(role_id, workspace_id)
        if not role:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return role
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve role")

@app.put("/api/custom/workspaces/{workspace_id}/roles/{role_id}", response_model=WorkspaceRole)
async def update_role(role_data: WorkspaceRoleUpdate, workspace_id: int, role_id: int, request: Request):
    """Update a custom role in a workspace."""
    try:
        role = await RoleService.update_custom_role(role_id, workspace_id, role_data, "current_user_123")
        if not role:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return role
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update role")

@app.delete("/api/custom/workspaces/{workspace_id}/roles/{role_id}")
async def delete_role(workspace_id: int, role_id: int):
    """Delete a custom role from a workspace."""
    try:
        success = await RoleService.delete_custom_role(role_id, workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return {"message": "Role deleted successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to delete role")

# Member Management Endpoints

@app.post("/api/custom/workspaces/{workspace_id}/members", response_model=WorkspaceMember)
async def add_member(member_data: WorkspaceMemberCreate, workspace_id: int, request: Request):
    """Add a new member to a workspace."""
    try:
        # Ensure workspace_id matches path parameter
        member_data.workspace_id = workspace_id
        
        member = await WorkspaceService.add_member(workspace_id, member_data, "current_user_123")
        return member
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to add member")

@app.get("/api/custom/workspaces/{workspace_id}/members", response_model=List[WorkspaceMember])
async def get_workspace_members(workspace_id: int, request: Request):
    """Get all members of a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        members = await WorkspaceService.get_workspace_members(workspace_id)
        logger.info(f"✅ [WORKSPACE MEMBERS] Retrieved {len(members)} members for workspace {workspace_id}")
        return members
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [WORKSPACE MEMBERS] Failed to get members for workspace {workspace_id}: {e}")
        logger.error(f"❌ [WORKSPACE MEMBERS] Error type: {type(e).__name__}")
        import traceback
        logger.error(f"❌ [WORKSPACE MEMBERS] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve members: {str(e)}")

@app.put("/api/custom/workspaces/{workspace_id}/members/{user_id}", response_model=WorkspaceMember)
async def update_member(member_data: WorkspaceMemberUpdate, workspace_id: int, user_id: str):
    """Update a workspace member."""
    try:
        member = await WorkspaceService.update_member(workspace_id, user_id, member_data, "current_user_123")
        if not member:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return member
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update member")

@app.delete("/api/custom/workspaces/{workspace_id}/members/{user_id}")
async def remove_member(workspace_id: int, user_id: str):
    """Remove a member from a workspace."""
    try:
        success = await WorkspaceService.remove_member(workspace_id, user_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return {"message": "Member removed successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to remove member")

@app.post("/api/custom/workspaces/{workspace_id}/leave")
async def leave_workspace(workspace_id: int, request: Request):
    """Leave a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        success = await WorkspaceService.leave_workspace(workspace_id, current_user_id)
        if not success:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return {"message": "Successfully left workspace"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to leave workspace")

# Product Access Control Endpoints

@app.post("/api/custom/products/{product_id}/access", response_model=ProductAccess)
async def grant_product_access(
    access_data: ProductAccessCreate, 
    product_id: int,
    request: Request
):
    """Grant access to a product for a workspace, role, or individual."""
    try:
        logger.info(f"🔍 [PRODUCT ACCESS DEBUG] Grant access request for product {product_id}")
        # Get user identifiers (both UUID and email)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        logger.info(f"🔍 [PRODUCT ACCESS] Grant access request - User: {user_uuid} (email: {user_email})")
        logger.info(f"   - Product ID: {product_id}, Workspace ID: {access_data.workspace_id}")
        logger.info(f"   - Access type: {access_data.access_type}, Target ID: {access_data.target_id}")
        logger.info(f"🔍 [PRODUCT ACCESS] Grant access request - User: {user_uuid} (email: {user_email})")
        logger.info(f"   - Product ID: {product_id}")
        logger.info(f"   - Workspace ID: {access_data.workspace_id}")
        logger.info(f"   - Access type: {access_data.access_type}")
        logger.info(f"   - Target ID: {access_data.target_id}")
        
        # Ensure product_id matches path parameter
        access_data.product_id = product_id
        
        # Check if user is a member of the workspace (use email for membership check)
        member = await WorkspaceService.get_workspace_member(access_data.workspace_id, user_email)
        if not member:
            logger.error(f"❌ [PRODUCT ACCESS] User {user_email} is not a member of workspace {access_data.workspace_id}")
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        logger.info(f"✅ [PRODUCT ACCESS] User {user_email} is a member of workspace {access_data.workspace_id}")
        
        access = await ProductAccessService.grant_access(access_data, user_email)
        return access
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to grant product access")

@app.get("/api/custom/products/{product_id}/access", response_model=List[ProductAccess])
async def get_product_access_list(product_id: int):
    """Get all access records for a specific product."""
    try:
        # TODO: Check if user has permission to view product access
        # This might require checking if the user owns the product or has admin access
        
        access_list = await ProductAccessService.get_product_access_list(product_id)
        return access_list
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve product access list")

@app.delete("/api/custom/products/{product_id}/access/{access_id}")
async def revoke_product_access(product_id: int, access_id: int):
    """Revoke access to a product."""
    try:
        # Get the access record to find the workspace_id
        access = await ProductAccessService.get_product_access(access_id)
        if not access:
            raise HTTPException(status_code=404, detail="Access record not found")
        
        if access.product_id != product_id:
            raise HTTPException(status_code=400, detail="Access record does not match product")
        
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(access.workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        success = await ProductAccessService.revoke_access(access_id, access.workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Access record not found")
        
        return {"message": "Product access revoked successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to revoke product access")

@app.delete("/api/custom/products/{product_id}/access/remove")
async def remove_product_access(product_id: int, criteria: dict):
    """Remove product access based on criteria (access_type, target_id, workspace_id)."""
    try:
        access_type = criteria.get('access_type')
        target_id = criteria.get('target_id')
        workspace_id = criteria.get('workspace_id')
        
        if not access_type or not workspace_id:
            raise HTTPException(status_code=400, detail="access_type and workspace_id are required")
        
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        # Find and remove the matching access record
        access_list = await ProductAccessService.get_product_access_list(product_id)
        matching_access = None
        
        for access in access_list:
            if (access.access_type == access_type and 
                access.workspace_id == workspace_id and
                access.target_id == target_id):
                matching_access = access
                break
        
        if not matching_access:
            raise HTTPException(status_code=404, detail="No matching access record found")
        
        success = await ProductAccessService.revoke_access(matching_access.id, workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Failed to remove access record")
        
        return {"message": "Product access removed successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to remove product access")

@app.get("/api/custom/products/{product_id}/access/check")
async def check_user_product_access(product_id: int, workspace_id: int):
    """Check if the current user has access to a specific product in a workspace."""
    try:
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        has_access = await ProductAccessService.check_user_product_access(product_id, "current_user_123", workspace_id)
        
        return {
            "product_id": product_id,
            "workspace_id": workspace_id,
            "user_id": "current_user_123",
            "has_access": has_access
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to check product access")

@app.get("/api/custom/products/workspace/{workspace_id}/access")
async def get_workspace_product_access(workspace_id: int):
    """Get all product access records for a specific workspace."""
    try:
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        access_list = await ProductAccessService.get_workspace_product_access(workspace_id)
        
        return {
            "workspace_id": workspace_id,
            "access_records": access_list,
            "count": len(access_list)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace product access")


class LMSExportRequest(BaseModel):
    productIds: List[int]
    options: dict = {}
    token: Optional[str] = None


def validate_export_request(request: LMSExportRequest):
    if not request.productIds:
        raise HTTPException(status_code=400, detail="No products selected")
    if len(request.productIds) > 10:
        raise HTTPException(status_code=400, detail="Too many products selected for a single export")


@app.post("/api/custom/lms/export")
async def export_to_lms(
    request: LMSExportRequest,
    http_request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Export selected course outlines to LMS format with streaming keep-alive."""
    from app.services.lms_exporter import export_course_outline_to_lms_format

    logger.info(f"[API:LMS] Request start | productIds={request.productIds}")

    validate_export_request(request)

    user_uuid, user_email = await get_user_identifiers_for_workspace(http_request)
    onyx_user_id = user_uuid
    logger.info(f"[API:LMS] User resolved | onyx_user_id={onyx_user_id} email={user_email}")

    async with pool.acquire() as connection:
        accessible_products = await connection.fetch(
            """
            SELECT p.id
            FROM projects p
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id
            WHERE p.id = ANY($1::int[]) AND p.onyx_user_id = $2 AND dt.microproduct_type = 'Training Plan'
            """,
            request.productIds, onyx_user_id
        )
        accessible_ids = [p['id'] for p in accessible_products]
        logger.info(f"[API:LMS] Accessible IDs | {accessible_ids}")
        if not accessible_ids:
            logger.warning("[API:LMS] No accessible course outlines found")
            raise HTTPException(status_code=404, detail="No accessible course outlines found")

    async def streamer():
        last_send = asyncio.get_event_loop().time()
        results = []
        total = len(accessible_ids)
        completed = 0
        yield (json.dumps({"type": "start", "total": total}) + "\n").encode()

        for product_id in accessible_ids:
            try:
                start_ts = asyncio.get_event_loop().time()
                yield (json.dumps({"type": "progress", "message": f"Exporting course {product_id}...", "productId": product_id}) + "\n").encode()
                export_task = asyncio.create_task(export_course_outline_to_lms_format(product_id, onyx_user_id, user_email, request.token))
                while not export_task.done():
                    await asyncio.sleep(8)
                    elapsed = int(asyncio.get_event_loop().time() - start_ts)
                    # Heartbeat keep-alive + lightweight progress ping
                    yield b" "
                    yield (json.dumps({"type": "progress", "message": f"Working on course {product_id}... ({elapsed}s)", "productId": product_id}) + "\n").encode()
                course_structure = await export_task
                results.append(course_structure)
                completed += 1
                yield (json.dumps({
                    "type": "progress",
                    "message": f"Course {product_id} exported",
                    "productId": product_id,
                    "downloadLink": course_structure.get("downloadLink")
                }) + "\n").encode()
            except Exception as e:
                logger.error(f"[API:LMS] Course export failed | id={product_id} err={e}")
                results.append({
                    "courseTitle": f"Course {product_id}",
                    "error": str(e),
                    "downloadLink": None,
                    "structure": None
                })
                yield (json.dumps({
                    "type": "progress",
                    "message": f"Course {product_id} failed: {str(e)}",
                    "productId": product_id,
                    "error": True
                }) + "\n").encode()

            now = asyncio.get_event_loop().time()
            if now - last_send > 8:
                yield b" "
                last_send = now

        status = "completed" if all(r.get("downloadLink") for r in results) else "partial"
        final_payload = {
            "success": True,
            "message": "Export completed",
            "results": results,
            "status": status
        }
        user_msg = f"Your courses have been exported. You can find them in your SmartExpert account linked to {user_email}."
        yield (json.dumps({"type": "done", "payload": final_payload, "userMessage": user_msg}) + "\n").encode()
        return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "X-Accel-Buffering": "no"
        }
    )


@app.get("/api/custom/lms/export/{export_id}/status")
async def get_export_status(export_id: str):
    return {"exportId": export_id, "status": "completed", "progress": 100}

@app.get("/api/custom/lms/user-settings")
async def get_lms_user_settings(http_request: Request):
    try:
        user_uuid, _ = await get_user_identifiers_for_workspace(http_request)
        async with DB_POOL.acquire() as connection:
            row = await connection.fetchrow("SELECT lms_account_choice FROM lms_user_settings WHERE onyx_user_id = $1", user_uuid)
            return {"choice": row["lms_account_choice"] if row else None}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to load LMS user settings")

@app.post("/api/custom/lms/user-settings")
async def set_lms_user_settings(http_request: Request):
    try:
        body = await http_request.json()
        choice = (body or {}).get("choice")
        if choice not in ("yes", "no-success", "no-failed"):
            raise HTTPException(status_code=400, detail="Invalid choice")
        user_uuid, _ = await get_user_identifiers_for_workspace(http_request)
        async with DB_POOL.acquire() as connection:
            await connection.execute(
                """
                INSERT INTO lms_user_settings (onyx_user_id, lms_account_choice)
                VALUES ($1, $2)
                ON CONFLICT (onyx_user_id) DO UPDATE SET lms_account_choice = EXCLUDED.lms_account_choice, updated_at = NOW()
                """,
                user_uuid, choice
            )
            return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to save LMS user settings")

@app.post("/api/custom/lms/create-workspace-owner")
async def create_workspace_owner(http_request: Request):
    """Create SmartExpert Workspace Owner using user's email and provided or default token."""
    try:
        user_uuid, user_email = await get_user_identifiers_for_workspace(http_request)
        if not user_email:
            raise HTTPException(status_code=400, detail="Unable to resolve user email from session")
        name_part = user_email.split("@")[0]
        try:
            body = await http_request.json()
        except Exception:
            body = {}
        token = (body or {}).get("token")
        try:
            from app.services.lms_exporter import DEFAULT_SMARTEXPERT_TOKEN
        except Exception:
            DEFAULT_SMARTEXPERT_TOKEN = None
        if not token:
            token = DEFAULT_SMARTEXPERT_TOKEN
        params = {"name": name_part, "email": user_email, "token": token or ""}
        target_url = "https://dev.smartexpert.net/store-workspace-owner"
        logger.info(f"[API:LMS] Workspace owner create start | email={user_email} name={name_part}")
        import httpx
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.get(target_url, params=params, headers={"User-Agent": "Custom Extensions Backend"})
            ok = resp.status_code in (200, 201, 202, 204, 302, 303)
            redirect_url = resp.headers.get("location") or resp.headers.get("Location")
            logger.info(f"[API:LMS] Workspace owner create result | status={resp.status_code} redirect={redirect_url}")
            # Persist choice server-side if successful
            if ok:
                try:
                    async with DB_POOL.acquire() as connection:
                        await connection.execute(
                            """
                            INSERT INTO lms_user_settings (onyx_user_id, lms_account_choice, updated_at)
                            VALUES ($1, $2, NOW())
                            ON CONFLICT (onyx_user_id) DO UPDATE SET lms_account_choice = EXCLUDED.lms_account_choice, updated_at = NOW()
                            """,
                            user_uuid, 'no-success'
                        )
                except Exception as _:
                    pass
            return {"success": ok, "status": resp.status_code, "email": user_email, "redirectUrl": redirect_url, "data": resp.text}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API:LMS] Workspace owner create failed: {e}")
        raise HTTPException(status_code=500, detail=f"Workspace owner creation failed: {str(e)}")

@app.on_event("startup")
async def startup_event_lms_exports():
    try:
        async with DB_POOL.acquire() as connection:
            # Ensure lms_user_settings table exists (per-account persistence)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS lms_user_settings (
                    onyx_user_id VARCHAR(255) PRIMARY KEY,
                    lms_account_choice VARCHAR(32),
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS lms_exports (
                    id SERIAL PRIMARY KEY,
                    user_id VARCHAR(255) NOT NULL,
                    product_ids JSONB NOT NULL,
                    status VARCHAR(50) DEFAULT 'processing',
                    progress INTEGER DEFAULT 0,
                    result_data JSONB,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    completed_at TIMESTAMP WITH TIME ZONE
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_user_id ON lms_exports(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_status ON lms_exports(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_created_at ON lms_exports(created_at);")
            logger.info("'lms_exports' table ensured.")
    except Exception as e:
        logger.error(f"Failed to ensure lms_exports table: {e}")
