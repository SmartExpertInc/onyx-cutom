# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4
# NEW: PDF manipulation imports
try:
    from PyPDF2 import PdfMerger
except ImportError:
    PdfMerger = None

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_VIDEO_LESSON_PRESENTATION = "VideoLessonPresentationDisplay"  # New component for video lesson presentations
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

SERPAPI_KEY = "ef10e9f3a1c8f0c2cd5d9379e39c597b58b6d0628f465c3030cace4d70494df7"

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

def parse_id_list(id_string: str, context_name: str) -> List[int]:
    """
    Parse a comma-separated string of IDs, handling negative integers (like -1 for special cases).
    
    Args:
        id_string: Comma-separated string of IDs (e.g., "1,2,3" or "-1" or "42")
        context_name: Context name for logging (e.g., "folder" or "file")
    
    Returns:
        List of parsed integer IDs
    """
    if not id_string:
        return []
    
    id_list = []
    try:
        for id_part in id_string.split(','):
            id_stripped = id_part.strip()
            if id_stripped.lstrip('-').isdigit():  # Allow negative numbers
                id_list.append(int(id_stripped))
            elif id_stripped:  # Log non-empty invalid parts
                logger.warning(f"[ID_PARSING] Skipping invalid {context_name} ID: '{id_stripped}'")
        
        logger.debug(f"[ID_PARSING] Parsed {context_name} IDs from '{id_string}': {id_list}")
        return id_list
    except Exception as e:
        logger.error(f"[ID_PARSING] Failed to parse {context_name} IDs from '{id_string}': {e}")
        return []

def should_use_hybrid_approach(payload) -> bool:
    """
    Determine if we should use the hybrid approach (Onyx for context extraction + OpenAI for generation).
    Returns True when file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this also uses hybrid approach)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use hybrid approach when there's file context or text context
    use_hybrid = has_files or has_text_context
    
    logger.info(f"[HYBRID_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_hybrid={use_hybrid}")
    return use_hybrid

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

class ImageBlock(BaseContentBlock):
    type: str = "image"
    src: str
    alt: Optional[str] = None
    caption: Optional[str] = None
    width: Optional[Union[int, str]] = None
    height: Optional[Union[int, str]] = None
    alignment: Optional[str] = "center"
    borderRadius: Optional[str] = "8px"
    maxWidth: Optional[str] = "100%"
    # Layout mode fields for positioning
    layoutMode: Optional[str] = None  # 'standalone', 'inline-left', 'inline-right'
    layoutPartnerIndex: Optional[int] = None  # Index of the content block to pair with for side-by-side layouts
    layoutProportion: Optional[str] = None  # '50-50', '60-40', '40-60', '70-30', '30-70'
    float: Optional[str] = None  # Legacy field for backward compatibility: 'left', 'right'

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    },
    {
      "slideId": "slide_16_table_dark",
      "slideNumber": 16,
      "slideTitle": "Technology Comparison",
      "templateId": "table-dark",
      "props": {
        "title": "Technology Comparison",
        "tableData": {
          "headers": ["Technology", "Performance", "Security", "Cost"],
          "rows": [
            ["React", "High", "Good", "Free"],
            ["Vue.js", "Medium", "Excellent", "Free"],
            ["Angular", "High", "Excellent", "Free"]
          ]
        }
      }
    },
    {
      "slideId": "slide_17_table_light",
      "slideNumber": 17,
      "slideTitle": "Product Features",
      "templateId": "table-light",
      "props": {
        "title": "Product Features Comparison",
        "tableData": {
          "headers": ["Feature", "Basic Plan", "Pro Plan", "Enterprise"],
          "rows": [
            ["Storage", "10GB", "100GB", "Unlimited"],
            ["Users", "5", "25", "Unlimited"],
            ["Support", "Email", "Priority", "24/7"]
          ]
        }
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example Video Lesson with Voiceover",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "big-image-left",
      "voiceoverText": "Welcome to this comprehensive lesson. Today we'll explore the fundamentals of our topic, breaking down complex concepts into easy-to-understand segments. This introduction sets the stage for what you're about to learn.",
      "props": {
          "title": "Welcome to the Lesson",
          "subtitle": "This slide introduces the main topic.",
          "imageUrl": "https://via.placeholder.com/600x400?text=Your+Image",
          "imageAlt": "Descriptive alt text",
          "imagePrompt": "A high-quality illustration that visually represents the lesson introduction",
          "imageSize": "large"
      }
    },
    {
      "slideId": "slide_2_main",
      "slideNumber": 2,
      "slideTitle": "Main Concepts",
      "templateId": "content-slide",
      "voiceoverText": "Now let's dive into the core concepts. These fundamental ideas form the foundation of our understanding. We'll explore each concept in detail, ensuring you have a solid grasp before moving forward.",
      "props": {
        "title": "Core Ideas",
        "content": "These concepts form the foundation of understanding.\n\n• First important concept\n• Second important concept\n• Third important concept",
        "alignment": "left"
      }
    },
    {
      "slideId": "slide_3_bullets",
      "slideNumber": 3,
      "slideTitle": "Key Points",
      "templateId": "bullet-points",
      "voiceoverText": "Here are the key takeaways from our lesson. Each of these points represents a critical insight that you should remember. Let me walk you through each one to ensure you understand their significance.",
      "props": {
        "title": "Key Points",
        "bullets": [
          "First important point",
          "Second key insight",
          "Third critical element"
        ],
        "maxColumns": 2,
        "bulletStyle": "dot",
        "imagePrompt": "A relevant illustration for the bullet points, e.g. 'Checklist, modern flat style, purple and yellow accents'",
        "imageAlt": "Illustration for bullet points"
      }
    },
    {
      "slideId": "slide_4_two_column",
      "slideNumber": 4,
      "slideTitle": "Comparison Analysis",
      "templateId": "two-column",
      "voiceoverText": "Let's examine this topic from two different perspectives. On the left, we have one approach, and on the right, we have another. Both perspectives are valuable and complement each other to give you a complete understanding.",
      "props": {
        "title": "Two Column Layout",
        "leftTitle": "Left Column Title",
        "leftContent": "Content for the left side with detailed explanations",
        "rightTitle": "Right Column Title",
        "rightContent": "Content for the right side with detailed information",
        "columnRatio": "50-50"
      }
    },
    {
      "slideId": "slide_5_four_box",
      "slideNumber": 5,
      "slideTitle": "Four Key Areas",
      "templateId": "four-box-grid",
      "voiceoverText": "Now we'll explore four essential areas that are crucial to understanding this topic. Each box represents a different aspect, and together they provide a comprehensive overview of the subject matter.",
      "props": {
        "title": "Main Title for Four Boxes",
        "boxes": [
          { "heading": "Box 1 Heading", "text": "Detailed description for the first box" },
          { "heading": "Box 2 Heading", "text": "Comprehensive explanation for the second box" },
          { "heading": "Box 3 Heading", "text": "Thorough description for the third box" },
          { "heading": "Box 4 Heading", "text": "In-depth explanation for the fourth box" }
        ]
      }
    },
    {
      "slideId": "slide_6_challenges",
      "slideNumber": 6,
      "slideTitle": "Problem Solving",
      "templateId": "challenges-solutions",
      "voiceoverText": "Every field has its challenges, but for every challenge, there's a solution. Let's examine the common obstacles you might face and the proven strategies to overcome them effectively.",
      "props": {
        "title": "Challenges and Solutions",
        "challengesTitle": "Common Challenges",
        "solutionsTitle": "Effective Solutions",
        "challenges": [
          "Challenge 1 with detailed explanation of the problem",
          "Challenge 2 with comprehensive analysis of the issue"
        ],
        "solutions": [
          "Solution 1 with detailed approach and implementation strategy",
          "Solution 2 with comprehensive methodology and practical steps"
        ]
      }
    },
    {
      "slideId": "slide_7_process",
      "slideNumber": 7,
      "slideTitle": "Step-by-Step Process",
      "templateId": "process-steps",
      "voiceoverText": "Finally, let's look at the practical implementation. This step-by-step process shows you exactly how to apply what you've learned. Follow along carefully as we go through each step together.",
      "props": {
        "title": "Implementation Steps",
        "steps": [
          "Analyze the requirements carefully",
          "Design the solution architecture",
          "Implement core functionality",
          "Test and validate results"
        ]
      }
    },
    {
      "slideId": "slide_5_table_dark",
      "slideNumber": 5,
      "slideTitle": "Technology Comparison",
      "templateId": "table-dark",
      "voiceoverText": "Let's examine the technology comparison table. This table shows us the key differences between various technologies in terms of performance, security, and cost. Understanding these comparisons helps us make informed decisions.",
      "props": {
        "title": "Technology Comparison",
        "tableData": {
          "headers": ["Technology", "Performance", "Security", "Cost"],
          "rows": [
            ["React", "High", "Good", "Free"],
            ["Vue.js", "Medium", "Excellent", "Free"],
            ["Angular", "High", "Excellent", "Free"]
          ]
        }
      }
    },
    {
      "slideId": "slide_6_table_light",
      "slideNumber": 6,
      "slideTitle": "Product Features",
      "templateId": "table-light",
      "voiceoverText": "Now let's look at the product features comparison. This table clearly shows the differences between our various subscription plans, helping you understand what each tier offers.",
      "props": {
        "title": "Product Features Comparison",
        "tableData": {
          "headers": ["Feature", "Basic Plan", "Pro Plan", "Enterprise"],
          "rows": [
            ["Storage", "10GB", "100GB", "Unlimited"],
            ["Users", "5", "25", "Unlimited"],
            ["Support", "Email", "Priority", "24/7"]
          ]
        }
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en",
  "hasVoiceover": true
}
"""

def normalize_slide_props(slides: List[Dict], component_name: str = None) -> List[Dict]:
    """
    Normalize slide props to match frontend template schemas.
    
    This function fixes common prop mismatches between AI-generated JSON
    and the expected frontend template schemas. Invalid slides are automatically
    removed to prevent rendering errors.
    
    Args:
        slides: List of slide dictionaries to normalize
        component_name: Component type (e.g., COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
                       Used to determine if voiceoverText should be preserved
    """
    if not slides:
        return slides
        
    normalized_slides = []
    
    for slide_index, slide in enumerate(slides):
        if not isinstance(slide, dict) or 'templateId' not in slide or 'props' not in slide:
            normalized_slides.append(slide)
            continue
            
        template_id = slide.get('templateId')
        props = slide.get('props', {})
        
        # Create a copy to avoid modifying the original
        normalized_slide = slide.copy()
        normalized_props = props.copy()
        
        try:
            # Fix template ID mappings first
            if template_id == 'event-dates':
                # Map event-dates (AI instruction) to event-list (frontend registry)
                template_id = 'event-list'
                normalized_slide['templateId'] = template_id
                
            # Ensure critical props are preserved for all templates
            # Fix missing imagePrompt and other content issues
            if 'imagePrompt' not in normalized_props and 'imageAlt' in normalized_props:
                # If we have imageAlt but no imagePrompt, use imageAlt as the prompt
                normalized_props['imagePrompt'] = normalized_props['imageAlt']
            
            # Generate missing imagePrompts for templates that require them
            if template_id in ['bullet-points', 'bullet-points-right'] and not normalized_props.get('imagePrompt'):
                title = normalized_props.get('title', 'concept')
                bullets = normalized_props.get('bullets', [])
                
                # Create a contextual prompt based on title and content keywords
                title_lower = title.lower()
                content_sample = ' '.join(bullets[:2]) if bullets else ''
                content_lower = content_sample.lower()
                
                if 'tool' in title_lower or 'software' in content_lower or 'application' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a developer using programming tools at a clean workstation. The scene features a young Asian woman sitting at a modern desk with a single laptop displaying simple code interface elements (no readable text) and one external monitor showing basic geometric development tool mockups. A wireless keyboard and mouse are positioned on the desk alongside a coffee cup. The laptop screen and coding interface are [COLOR1], the external monitor and keyboard are [COLOR2], and the desk and accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'trend' in title_lower or 'future' in title_lower or 'innovation' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of futuristic technology concepts in a clean tech environment. The scene features a Hispanic male scientist in a white lab coat standing next to a single large holographic display showing simple geometric patterns and flowing data visualizations (no readable text). A modern desk with a tablet displaying basic technology interface elements sits nearby. The holographic display and data flows are [COLOR1], the scientist's lab coat and tablet are [COLOR2], and the desk and lab environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'learn' in title_lower or 'education' in title_lower or 'skill' in content_lower or 'training' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of modern learning in a clean educational environment. The scene features a young Black female student sitting at a modern desk using a tablet displaying simple educational interface elements and geometric learning modules (no readable text). A single interactive whiteboard in the background shows basic diagrams with simple shapes and connecting lines. Educational materials like a notebook and digital stylus are positioned on the desk. The tablet interface and learning modules are [COLOR1], the interactive whiteboard and educational tools are [COLOR2], and the desk and educational environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'business' in title_lower or 'strategy' in content_lower or 'management' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional business strategy and management in a modern corporate environment. The scene features a contemporary conference room with three business professionals engaged in strategic planning. A confident Latina businesswoman in a navy blazer stands at the left presenting to a large wall display showing simple business charts, process flows, and strategic diagrams with geometric shapes (no readable text). In the center, a Black male executive sits at a glass conference table reviewing documents and tablets displaying abstract business analytics as simple bar charts and pie segments. On the right, a Caucasian female manager takes notes while sitting in an ergonomic chair, with a laptop showing business interface mockups with geometric layouts. Business materials like documents, tablets, coffee cups, and strategic planning boards are arranged throughout the professional space. The presentation displays and business interfaces are [COLOR1], conference furniture and professional devices are [COLOR2], documents and planning materials are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                else:
                    # General professional/educational fallback
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional collaboration and knowledge sharing in a modern educational environment. The scene features three diverse professionals working together in a bright, contemporary workspace. A young Hispanic woman in business attire sits at a modern desk on the left, using a laptop displaying simple interface elements and geometric data visualizations (no readable text). In the center, a Black male professional stands presenting to a wall-mounted display showing abstract concepts as interconnected nodes, flowcharts, and simple diagrams with geometric shapes. On the right, a Caucasian female colleague sits in a comfortable chair reviewing materials on a tablet, with documents and notebooks arranged on a side table. Professional tools like laptops, tablets, notebooks, coffee cups, and presentation materials are positioned throughout the collaborative workspace. The digital displays and interface elements are [COLOR1], professional devices and presentation tools are [COLOR2], furniture and workspace accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                normalized_props['imageAlt'] = f"Professional illustration for {title}"
            
            # Ensure subtitle/content exists for templates that need it
            if template_id in ['big-image-left', 'big-image-top']:
                if 'subtitle' not in normalized_props and 'content' in normalized_props:
                    normalized_props['subtitle'] = normalized_props['content']
                # Ensure subtitle is different from title
                if (normalized_props.get('subtitle') == normalized_props.get('title') and 
                    len(normalized_props.get('subtitle', '')) > 50):
                    # If subtitle equals title and is long, use it as subtitle and create shorter title
                    full_text = normalized_props['subtitle']
                    # Extract first sentence as title
                    sentences = full_text.split('. ')
                    if len(sentences) > 1:
                        normalized_props['title'] = sentences[0]
                        normalized_props['subtitle'] = '. '.join(sentences[1:])
                        
            # Fix template selection for analytics/evaluation content
            if (template_id == 'metrics-analytics' and 
                'metrics' in normalized_props and 
                isinstance(normalized_props['metrics'], list) and 
                len(normalized_props['metrics']) <= 3):
                # If metrics-analytics has only bullet points, convert to bullet-points template
                logger.info(f"Converting slide {slide_index + 1} from metrics-analytics to bullet-points (better fit)")
                normalized_slide['templateId'] = 'bullet-points'
                template_id = 'bullet-points'
                normalized_props['bullets'] = normalized_props.pop('metrics')
                # Add image prompt for bullet-points
                if not normalized_props.get('imagePrompt'):
                    title = normalized_props.get('title', 'concepts')
                    title_lower = title.lower()
                    
                    # Generate contextual, detailed image prompts for metrics/analytics content
                    if 'metric' in title_lower or 'analytic' in title_lower or 'performance' in title_lower:
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern data analytics workspace. The scene features a professional data analyst sitting at a clean desk with a laptop displaying simple geometric charts and performance dashboards (no readable text). A large monitor shows flowing data visualizations with abstract patterns and trends. The workspace includes notebooks, a coffee cup, and modern office accessories. Natural light streams through large windows. The laptop charts and data visualizations are [COLOR1], the monitor and office equipment are [COLOR2], and the workspace environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'tracking' in title_lower or 'monitoring' in title_lower:
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern monitoring and tracking center. The scene features a professional analyst standing next to a large wall display showing flowing geometric patterns representing tracking systems and monitoring data. A clean workstation with a tablet displaying simple interface elements sits nearby. The environment is bright and contemporary with floor-to-ceiling windows. The wall display and tracking patterns are [COLOR1], the analyst's attire and tablet are [COLOR2], and the monitoring center environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # General professional data/analytics fallback
                        normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern professional workspace focused on {title.lower()}. The scene features a diverse professional in business attire working at a contemporary desk with a laptop displaying simple data interface elements and geometric visualizations (no readable text). Professional tools like a tablet, notebooks, and a coffee cup are positioned around the clean workspace. Large windows provide natural light to the modern office environment. The laptop interface and data displays are [COLOR1], the professional's attire and desk accessories are [COLOR2], and the office environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    
                    normalized_props['imageAlt'] = f"Professional illustration for {title}"
                    
            # This big-numbers conversion logic is moved to after big-numbers normalization below
                
            # Fix big-numbers template props
            if template_id == 'big-numbers':
                # Accept both 'items' (preferred) and 'numbers' (alternative) as the source array
                source_list = normalized_props.get('items')
                if not (isinstance(source_list, list) and source_list):
                    alt_list = normalized_props.get('numbers')
                    if isinstance(alt_list, list) and alt_list:
                        logger.info(f"Normalizing 'big-numbers' slide {slide_index + 1} from 'numbers' → 'items'")
                        source_list = alt_list
                    else:
                        source_list = []

                # Validate and coerce each item
                fixed_items = []
                for item in source_list:
                    if isinstance(item, dict):
                        fixed_item = {
                            'value': str(item.get('value') or item.get('number') or '').strip(),
                            'label': str(item.get('label') or item.get('title') or '').strip(),
                            'description': str(item.get('description') or item.get('desc') or item.get('text') or '').strip()
                        }
                        if fixed_item['value'] and fixed_item['label']:
                            fixed_items.append(fixed_item)

                # Pad/trim to exactly 3 items to preserve slide instead of skipping
                if len(fixed_items) != 3:
                    logger.warning(f"Coercing slide {slide_index + 1} with template 'big-numbers': Expected 3 items, got {len(fixed_items)}")
                    while len(fixed_items) < 3:
                        idx = len(fixed_items) + 1
                        fixed_items.append({'value': '0', 'label': f'Item {idx}', 'description': 'No description available'})
                    if len(fixed_items) > 3:
                        fixed_items = fixed_items[:3]

                # Frontend expects 'steps' not 'items' for big-numbers template
                normalized_props['steps'] = fixed_items
                # Drop legacy keys to unify shape
                if 'numbers' in normalized_props:
                    normalized_props.pop('numbers', None)
                if 'items' in normalized_props:
                    normalized_props.pop('items', None)
                
                # Check if we generated placeholder content and convert to bullet-points if so
                has_placeholder_content = all(
                    step.get('value', '').strip() in ['0', ''] and 
                    step.get('label', '').startswith('Item ') and
                    step.get('description', '') == 'No description available'
                    for step in fixed_items
                )
                
                if has_placeholder_content:
                    # Convert to bullet-points template since the content is conceptual, not numerical
                    logger.info(f"Converting slide {slide_index + 1} from big-numbers to bullet-points (no numerical data)")
                    normalized_slide['templateId'] = 'bullet-points'
                    template_id = 'bullet-points'
                    
                    # Generate appropriate bullet points based on the title
                    title = normalized_props.get('title', '').lower()
                    if 'assessment' in title:
                        normalized_props['bullets'] = [
                            "Automated Grading: AI can evaluate multiple-choice and short-answer questions instantly, providing immediate feedback to students.",
                            "Essay Analysis: Advanced AI tools can assess essay structure, grammar, and content quality, offering detailed feedback.",
                            "Adaptive Testing: AI-powered assessments adjust question difficulty based on student performance, providing personalized evaluation.",
                            "Plagiarism Detection: AI tools can identify potential plagiarism by comparing student work against vast databases of academic content.",
                            "Performance Analytics: AI systems provide detailed analytics on student performance patterns and learning gaps."
                        ]
                    elif 'future' in title or 'trends' in title:
                        normalized_props['bullets'] = [
                            "Personalized Learning Experiences: AI will create more sophisticated personalized learning paths tailored to individual student needs.",
                            "Virtual Reality Integration: AI combined with VR will create immersive educational experiences for complex subjects.",
                            "Natural Language Processing: Advanced chatbots will provide more human-like interactions for student support and tutoring.",
                            "Predictive Analytics: AI will better predict student success and identify at-risk students earlier in their academic journey.",
                            "Automated Content Creation: AI will generate educational materials and assessments customized to curriculum standards."
                        ]
                    elif 'technology' in title or 'tech' in title:
                        normalized_props['bullets'] = [
                            "Digital Learning Platforms: Technology provides interactive and engaging learning environments for students.",
                            "Personalized Learning Paths: Advanced algorithms adapt content delivery to individual learning styles and pace.",
                            "Real-time Feedback Systems: Immediate assessment and feedback help students track their progress effectively.",
                            "Collaborative Tools: Technology enables seamless collaboration between students and teachers across different locations.",
                            "Resource Accessibility: Digital platforms make learning materials available anytime, anywhere for enhanced flexibility."
                        ]
                    else:
                        # Generic bullets based on title
                        slide_title = normalized_props.get('title', 'this topic')
                        normalized_props['bullets'] = [
                            f"Key aspect of {slide_title} that enhances educational outcomes and student engagement.",
                            f"Important consideration for implementing {slide_title} effectively in educational settings.",
                            f"Significant benefit of using {slide_title} for improving student learning and performance.",
                            f"Challenge that educators should be aware of when adopting {slide_title} in their curriculum.",
                            f"Future implication of {slide_title} for educational institutions and learning methodologies."
                        ]
                    
                    # Remove big-numbers props and add bullet-points props
                    normalized_props.pop('steps', None)
                    
                    # Add image prompt for bullet-points
                    if not normalized_props.get('imagePrompt'):
                        slide_title = normalized_props.get('title', 'concepts')
                        title_lower = slide_title.lower()
                        
                        # Generate contextual, detailed image prompts based on content
                        if 'assessment' in title_lower or 'evaluation' in title_lower:
                            normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern educational assessment environment. The scene features a young female teacher sitting at a clean desk in a bright classroom, reviewing student assessments on a tablet displaying simple geometric grade analytics (no readable text). Behind her, students work quietly at individual desks taking a digital assessment on laptops. The classroom has large windows with natural light and a whiteboard showing basic geometric patterns. The tablet interface and assessment analytics are [COLOR1], the teacher's clothing and desk items are [COLOR2], and the classroom environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'future' in title_lower or 'trends' in title_lower:
                            normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a futuristic educational technology center. The scene features a Hispanic male educator standing next to a large interactive holographic display showing flowing geometric patterns representing future learning concepts. A modern curved desk with a sleek laptop and digital stylus sits in the foreground. Through floor-to-ceiling windows, a futuristic cityscape is visible. The holographic display and future tech patterns are [COLOR1], the educator's attire and laptop are [COLOR2], and the modern facility and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'technology' in title_lower or 'tech' in title_lower:
                            normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern educational technology lab. The scene features a young Asian female student sitting at a sleek workstation using a tablet for interactive learning, with a single large monitor displaying simple educational interface elements and geometric learning modules (no readable text). Modern educational equipment and a coffee cup are positioned on the clean desk. Natural light streams through large windows. The tablet interface and learning modules are [COLOR1], the monitor and educational technology are [COLOR2], and the lab environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            # General educational fallback
                            normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern educational environment related to {slide_title.lower()}. The scene features a diverse educator in professional attire working at a contemporary desk with a laptop displaying simple interface elements related to the topic (no readable text). Educational materials like notebooks and a tablet are positioned around the workspace. Large windows provide natural light to the clean, professional space. The laptop interface and educational displays are [COLOR1], the educator's attire and desk accessories are [COLOR2], and the educational environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        
                        normalized_props['imageAlt'] = f"Professional educational illustration for {slide_title}"
                    
            # Fix four-box-grid template props
            elif template_id == 'four-box-grid':
                boxes = normalized_props.get('boxes', [])
                if boxes and isinstance(boxes, list):
                    fixed_boxes = []
                    for box in boxes:
                        if isinstance(box, dict):
                            # Convert title/content to heading/text
                            fixed_box = {
                                'heading': box.get('heading') or box.get('title', ''),
                                'text': box.get('text') or box.get('content', '')
                            }
                            if fixed_box['heading']:  # Only add if heading exists
                                fixed_boxes.append(fixed_box)
                    normalized_props['boxes'] = fixed_boxes
                    
            # Fix two-column template props
            elif template_id == 'two-column':
                # Handle missing right column content by splitting existing content if it's substantial
                if (not normalized_props.get('rightContent') or normalized_props.get('rightContent') == '') and normalized_props.get('leftContent'):
                    left_content = normalized_props.get('leftContent', '')
                    
                    # If left content is substantial and right is empty, try to split content intelligently
                    if len(left_content) > 100:  # Only split if there's substantial content
                        lines = left_content.split('\n')
                        if len(lines) >= 2:
                            # Split content roughly in half
                            mid_point = len(lines) // 2
                            left_part = '\n'.join(lines[:mid_point]).strip()
                            right_part = '\n'.join(lines[mid_point:]).strip()
                            
                            if left_part and right_part:
                                normalized_props['leftContent'] = left_part
                                normalized_props['rightContent'] = right_part
                                logger.info(f"Split two-column content for slide {slide_index + 1}")
                
                # Generate appropriate titles if missing
                if not normalized_props.get('rightTitle') or normalized_props.get('rightTitle') == '':
                    title = normalized_props.get('title', '')
                    left_title = normalized_props.get('leftTitle', '')
                    
                    # Generate contextual right title based on slide content
                    if 'advantages' in left_title.lower() or 'benefits' in left_title.lower():
                        normalized_props['rightTitle'] = 'Disadvantages' if 'advantages' in left_title.lower() else 'Challenges'
                    elif 'pros' in left_title.lower():
                        normalized_props['rightTitle'] = 'Cons'
                    elif 'before' in left_title.lower():
                        normalized_props['rightTitle'] = 'After'
                    elif 'strategies' in title.lower() or 'methods' in title.lower():
                        normalized_props['rightTitle'] = 'Implementation' if left_title else 'Best Practices'
                    else:
                        # Generic fallback
                        normalized_props['rightTitle'] = 'Additional Details'
                
                # Ensure leftTitle exists
                if not normalized_props.get('leftTitle'):
                    normalized_props['leftTitle'] = 'Key Points'
                
                # Generate missing image prompts for two-column templates (check each side independently)
                title = normalized_props.get('title', 'comparison')
                left_title = normalized_props.get('leftTitle', 'concept')
                right_title = normalized_props.get('rightTitle', 'concept')
                
                # Generate left image prompt if missing using detailed format
                if not normalized_props.get('leftImagePrompt') and normalized_props.get('leftContent'):
                    left_content_sample = normalized_props.get('leftContent', '')[:100].lower()
                    if 'network' in left_content_sample or 'event' in left_content_sample or 'meeting' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing people networking and collaborating. The scene features a group of diverse professionals in business attire engaging in conversations, exchanging ideas, and building connections in a modern corporate environment. People are positioned throughout the scene in small groups, with some standing and others sitting around tables. Professional networking activities are highlighted with [COLOR1], conversation indicators in [COLOR2], and background elements in [COLOR3]. NO text, labels, or readable content on any surfaces or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in left_content_sample or 'digital' in left_content_sample or 'system' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} featuring modern technology and digital systems. The scene shows interconnected technological components, digital interfaces, and system architectures with detailed visual elements. Main technology components are [COLOR1], connecting elements are [COLOR2], and supporting details are [COLOR3]. All screens and displays show abstract geometric patterns with NO readable text or labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'process' in left_content_sample or 'step' in left_content_sample or 'method' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing a detailed process workflow. The scene features sequential steps connected by arrows, with clear visual indicators for each stage of the process. Process elements are rendered in [COLOR1], connecting arrows in [COLOR2], and step indicators in [COLOR3]. Use symbols and geometric shapes instead of text labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        left_content_lower = normalized_props.get('leftContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in left_content_lower or 'education' in left_content_lower or 'student' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of students learning in a modern classroom setting. The scene features diverse students sitting at desks with laptops, a teacher presenting at the front, and educational materials around the room. Students are engaged and taking notes, with some raising hands to ask questions. Student laptops and materials are [COLOR1], the teacher and presentation board are [COLOR2], and classroom furniture is [COLOR3]. No readable text on any surfaces. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in left_content_lower or 'meeting' in left_content_lower or 'team' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of professionals collaborating in a modern office meeting room. The scene features a diverse group of business people sitting around a conference table, engaged in discussion, with one person presenting ideas. Laptops and documents are on the table, and a presentation screen shows charts. Conference table and laptops are [COLOR1], people and presentation screen are [COLOR2], and office furniture is [COLOR3]. No readable text anywhere. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in left_content_lower or 'analysis' in left_content_lower or 'research' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of a data analyst working with information in a modern office. The scene features a focused professional at a desk with multiple monitors displaying charts and graphs, surrounded by organized workspace elements. The person is analyzing data patterns on screen while taking notes. Computer monitors and data visualizations are [COLOR1], the analyst and desk setup are [COLOR2], and office environment is [COLOR3]. All screens show abstract geometric patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of people working together in a professional environment related to {left_title.lower()}. The scene features diverse professionals engaged in relevant activities, using modern tools and technology. The setting shows a clean, organized workspace with people collaborating effectively. Primary work elements are [COLOR1], people and main activities are [COLOR2], and environmental details are [COLOR3]. No readable text on any surfaces or screens. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Generate right image prompt if missing using detailed format
                if not normalized_props.get('rightImagePrompt') and normalized_props.get('rightContent'):
                    right_content_sample = normalized_props.get('rightContent', '')[:100].lower()
                    if 'association' in right_content_sample or 'professional' in right_content_sample or 'group' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing professional associations and industry connections. The scene features business professionals in formal attire attending conferences, participating in panel discussions, and engaging in professional development activities. A large conference hall setting with speakers, audience members, and networking areas. Association activities are highlighted in [COLOR1], professional interactions in [COLOR2], and venue elements in [COLOR3]. NO visible text on presentations, banners, or displays - use abstract symbols instead. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in right_content_sample or 'digital' in right_content_sample or 'system' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} featuring advanced technology and digital innovation. The scene shows cutting-edge technological solutions, modern interfaces, and innovative systems with comprehensive visual details. Technology components are [COLOR1], interface elements are [COLOR2], and innovation indicators are [COLOR3]. All digital displays show abstract geometric patterns with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'strategy' in right_content_sample or 'approach' in right_content_sample or 'solution' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing strategic planning and solution implementation. The scene features strategic diagrams, planning documents, and implementation frameworks with detailed visual representations. Strategic elements are [COLOR1], planning components are [COLOR2], and implementation details are [COLOR3]. Documents and charts show abstract shapes and symbols with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        right_content_lower = normalized_props.get('rightContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in right_content_lower or 'education' in right_content_lower or 'student' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of students using technology for learning. The scene features diverse students in a modern computer lab, working on individual projects with tablets and laptops. Some students are collaborating on assignments while others focus independently. A teacher walks among them providing guidance. Student devices and work materials are [COLOR1], students and teacher are [COLOR2], and lab environment is [COLOR3]. No readable text on any screens or materials. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in right_content_lower or 'meeting' in right_content_lower or 'team' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of business professionals in a client presentation meeting. The scene features a confident presenter explaining concepts to seated clients, with visual aids displayed on a large screen. The atmosphere is professional and engaging, with participants actively listening and taking notes. Presentation equipment and materials are [COLOR1], presenter and clients are [COLOR2], and meeting room elements are [COLOR3]. No readable text on presentations or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in right_content_lower or 'analysis' in right_content_lower or 'research' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of researchers collaborating on data analysis in a modern lab setting. The scene features scientists and analysts working together around computer workstations, discussing findings and sharing insights. Multiple screens display data visualizations while team members point to specific patterns. Computer equipment and data displays are [COLOR1], researchers and team members are [COLOR2], and lab environment is [COLOR3]. All screens show abstract patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of professionals implementing solutions in a modern workplace related to {right_title.lower()}. The scene features a diverse team actively working on practical applications, using contemporary tools and methods. The environment shows organized, efficient operations with people engaged in meaningful work. Implementation tools and equipment are [COLOR1], team members and activities are [COLOR2], and workplace environment is [COLOR3]. No readable text on any surfaces or equipment. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Handle case where AI used leftContent/rightContent but missing titles
                if normalized_props.get('leftContent') and not normalized_props.get('leftTitle'):
                    # Try to extract title from content
                    left_content = normalized_props.get('leftContent', '')
                    if '\n' in left_content:
                        lines = left_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['leftTitle'] = lines[0].strip()
                            normalized_props['leftContent'] = '\n'.join(lines[1:]).strip()
                
                if normalized_props.get('rightContent') and not normalized_props.get('rightTitle'):
                    # Try to extract title from content
                    right_content = normalized_props.get('rightContent', '')
                    if '\n' in right_content:
                        lines = right_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['rightTitle'] = lines[0].strip()
                            normalized_props['rightContent'] = '\n'.join(lines[1:]).strip()
                            
            # Fix challenges-solutions template props
            elif template_id == 'challenges-solutions':
                # Convert leftContent/rightContent to challenges/solutions arrays
                if 'leftContent' in normalized_props and 'challenges' not in normalized_props:
                    left_content = normalized_props.get('leftContent', '')
                    # Parse content into challenges array
                    challenges = []
                    for line in left_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('common challenges'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                challenges.append(clean_line)
                    
                    if challenges:
                        normalized_props['challenges'] = challenges
                        del normalized_props['leftContent']
                        
                if 'rightContent' in normalized_props and 'solutions' not in normalized_props:
                    right_content = normalized_props.get('rightContent', '')
                    # Parse content into solutions array
                    solutions = []
                    for line in right_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('recommended resources') and not line.lower().startswith('solutions'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                solutions.append(clean_line)
                    
                    if solutions:
                        normalized_props['solutions'] = solutions
                        del normalized_props['rightContent']
                
                # Ensure required titles exist
                if 'challengesTitle' not in normalized_props:
                    normalized_props['challengesTitle'] = 'Challenges'
                if 'solutionsTitle' not in normalized_props:
                    normalized_props['solutionsTitle'] = 'Solutions'
            
            # Fix timeline template props
            elif template_id == 'timeline':
                # Convert "events" to "steps" and restructure data
                events = normalized_props.get('events', [])
                if events and isinstance(events, list):
                    steps = []
                    for event in events:
                        if isinstance(event, dict):
                            # Convert event structure to step structure
                            heading = (event.get('title') or event.get('heading') or event.get('date') or '').strip()
                            description = (event.get('description') or '').strip()
                            if heading or description:
                                steps.append({
                                    'heading': heading or 'Timeline Step',
                                    'description': description or 'No description available'
                                })
                    normalized_props['steps'] = steps
                    normalized_props.pop('events', None)  # Remove the old structure
                elif 'steps' not in normalized_props:
                    # Create default steps if no events or steps exist
                    normalized_props['steps'] = [
                        {'heading': 'Step 1', 'description': 'First milestone'},
                        {'heading': 'Step 2', 'description': 'Second milestone'},
                        {'heading': 'Step 3', 'description': 'Third milestone'},
                        {'heading': 'Step 4', 'description': 'Final milestone'}
                    ]
            
            # Fix pyramid template props
            elif template_id == 'pyramid':
                # Ensure 'steps' array exists by parsing from common inputs
                steps = normalized_props.get('steps', []) or normalized_props.get('items', [])
                levels = normalized_props.get('levels', [])
                
                # If we have levels data, use it to create proper steps
                if levels and isinstance(levels, list) and len(levels) >= 3:
                    parsed_items = []
                    for i, level in enumerate(levels[:3], start=1):
                        if isinstance(level, dict):
                            heading = level.get('text', f'Level {i}')
                            description = level.get('description', '')
                            parsed_items.append({'heading': heading, 'description': description})
                        else:
                            parsed_items.append({'heading': f'Level {i}', 'description': str(level) if level else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                
                # If no levels but we have steps, validate and fix them
                elif steps and isinstance(steps, list):
                    fixed_steps = []
                    for i, step in enumerate(steps[:3], start=1):
                        if isinstance(step, dict):
                            heading = step.get('heading', f'Level {i}')
                            description = step.get('description', '')
                            fixed_steps.append({'heading': heading, 'description': description})
                        else:
                            fixed_steps.append({'heading': f'Level {i}', 'description': str(step) if step else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(fixed_steps) < 3:
                        idx = len(fixed_steps) + 1
                        fixed_steps.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = fixed_steps[:3]
                
                # If no structured data, try to parse from content
                else:
                    import re
                    text = (normalized_props.get('content') or '').strip()
                    parsed_items = []
                    if text:
                        # Try to extract segments like **Heading**: description ... up to next **Heading**
                        pattern = re.compile(r"\*\*([^*]+)\*\*\s*:?[\s\-–—]*([^*]+?)(?=\s*\*\*[^*]+\*\*|$)", re.S)
                        for m in pattern.finditer(text):
                            heading = m.group(1).strip()
                            desc = m.group(2).strip().replace('\n', ' ')
                            if heading and desc:
                                parsed_items.append({'heading': heading, 'description': desc})
                    
                    if not parsed_items and text:
                        # Fallback: split into up to 3 sentence-like chunks
                        # First try double newlines, then periods.
                        chunks = [c.strip() for c in re.split(r"\n\n+", text) if c.strip()]
                        if not chunks:
                            chunks = [c.strip() for c in re.split(r"(?<=[.!?])\s+", text) if c.strip()]
                        for i, ch in enumerate(chunks[:3], start=1):
                            parsed_items.append({'heading': f'Level {i}', 'description': ch})
                    
                    # Ensure we have exactly 3 levels, but don't add "No description available"
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                # Clean up: pyramid does not use a long 'content' blob when steps are present
                if normalized_props.get('steps') and 'content' in normalized_props:
                    pass  # keep content for now as optional; frontend ignores it
                    
            # Fix event-list template props
            elif template_id == 'event-list':
                # Ensure events array exists
                events = normalized_props.get('events', [])
                if not (isinstance(events, list) and events):
                    # Create default events if none exist
                    normalized_props['events'] = [
                        {'date': 'Event 1', 'description': 'Event description'},
                        {'date': 'Event 2', 'description': 'Event description'},
                        {'date': 'Event 3', 'description': 'Event description'}
                    ]
                else:
                    # Ensure each event has required fields
                    fixed_events = []
                    for event in events:
                        if isinstance(event, dict):
                            fixed_event = {
                                'date': str(event.get('date') or event.get('title') or 'Event Date'),
                                'description': str(event.get('description') or event.get('desc') or 'Event description')
                            }
                            fixed_events.append(fixed_event)
                    if fixed_events:
                        normalized_props['events'] = fixed_events
        
            # Fix bullet-points template props
            elif template_id in ['bullet-points', 'bullet-points-right']:
                bullets = normalized_props.get('bullets', [])
                if bullets and isinstance(bullets, list):
                    # Ensure bullets are strings and not empty
                    fixed_bullets = [str(bullet).strip() for bullet in bullets if str(bullet).strip()]
                    if fixed_bullets:
                        normalized_props['bullets'] = fixed_bullets
                    else:
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': No valid bullet points, adding placeholder")
                        normalized_props['bullets'] = ['No content available']
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid or missing bullets array, adding placeholder")
                    normalized_props['bullets'] = ['No content available']
            
            # Fix comparison-slide template props
            elif template_id == 'comparison-slide':
                table_data = normalized_props.get('tableData', {})
                if isinstance(table_data, dict):
                    # Ensure headers exist
                    if 'headers' not in table_data or not isinstance(table_data['headers'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid headers")
                        table_data['headers'] = ['Feature', 'Option A', 'Option B']
                    
                    # Ensure rows exist
                    if 'rows' not in table_data or not isinstance(table_data['rows'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid rows")
                        table_data['rows'] = [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    
                    # Ensure all rows have the correct number of columns
                    expected_cols = len(table_data['headers'])
                    fixed_rows = []
                    for row_idx, row in enumerate(table_data['rows']):
                        if isinstance(row, list):
                            # Pad or trim row to match header count
                            if len(row) < expected_cols:
                                # Pad with empty strings
                                row = row + [''] * (expected_cols - len(row))
                            elif len(row) > expected_cols:
                                # Trim to match headers
                                row = row[:expected_cols]
                            fixed_rows.append([str(cell) for cell in row])
                        else:
                            logger.warning(f"Coercing slide {slide_index + 1}: Invalid row format at index {row_idx}")
                            # Create a placeholder row
                            fixed_rows.append([f'Row {row_idx + 1} Col {i + 1}' for i in range(expected_cols)])
                    
                    table_data['rows'] = fixed_rows
                    normalized_props['tableData'] = table_data
                    
                    logger.info(f"Fixed comparison table data for slide {slide_index + 1}: {len(table_data['headers'])} columns, {len(table_data['rows'])} rows")
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid tableData, adding default comparison")
                    normalized_props['tableData'] = {
                        'headers': ['Feature', 'Option A', 'Option B'],
                        'rows': [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    }
        
            normalized_slide['props'] = normalized_props
            
            # Remove voiceoverText for non-video presentations
            if (component_name == COMPONENT_NAME_SLIDE_DECK and 
                'voiceoverText' in normalized_slide):
                logger.info(f"Removing voiceoverText from slide {slide_index + 1} for regular slide deck")
                normalized_slide.pop('voiceoverText', None)
            
            normalized_slides.append(normalized_slide)
            
        except Exception as e:
            logger.error(f"Error normalizing slide {slide_index + 1} with template '{template_id}': {e}")
            logger.warning(f"Removing problematic slide {slide_index + 1}")
            continue  # Skip this slide
    
    logger.info(f"Slide normalization complete: {len(slides)} -> {len(normalized_slides)} slides (removed {len(slides) - len(normalized_slides)} invalid slides)")
    return normalized_slides

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

BING_API_KEY = os.getenv("BING_API_KEY")
BING_API_URL = "https://api.bing.microsoft.com/v7.0/search"

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}

# In-memory job status store (for demo; use Redis for production)
AI_AUDIT_PROGRESS = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AiAuditQuestionnaireRequest(BaseModel):
    companyWebsite: str
    language: str = "ru"  # Default to Russian

class AiAuditScrapedData(BaseModel):
    companyName: str
    companyDesc: str
    employees: str
    franchise: str
    onboardingProblems: str
    documents: list[str]
    documentsOther: str = ""
    priorities: list[str]
    priorityOther: str = ""

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
        {
          "type": "numbered_list",
          "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Digital Marketing Strategy: A Complete Guide",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Introduction",
      "templateId": "hero-title-slide",
      "props": {
        "title": "Digital Marketing Strategy",
        "subtitle": "A comprehensive guide to building effective online presence and driving business growth",
        "author": "Marketing Excellence Team",
        "date": "2024",
        "backgroundColor": "#1e40af",
        "titleColor": "#ffffff",
        "subtitleColor": "#bfdbfe"
      }
    },
    {
      "slideId": "slide_2_agenda",
      "slideNumber": 2,
      "slideTitle": "Learning Agenda",
      "templateId": "bullet-points",
      "props": {
        "title": "What We'll Cover Today",
        "bullets": [
          "Understanding digital marketing fundamentals",
          "Market research and target audience analysis",
          "Content strategy development",
          "Social media marketing tactics",
          "Email marketing best practices",
          "SEO and search marketing"
        ],
        "maxColumns": 2,
        "bulletStyle": "number",
        "imagePrompt": "A roadmap or pathway illustration showing the learning journey, modern flat design with blue and purple accents",
        "imageAlt": "Learning roadmap illustration"
      }
    },
    {
      "slideId": "slide_3_stats",
      "slideNumber": 3,
      "slideTitle": "Digital Marketing by the Numbers",
      "templateId": "big-numbers",
      "props": {
        "title": "Digital Marketing Impact",
        "numbers": [
          {
            "value": "4.8B",
            "label": "Internet Users Worldwide",
            "color": "#3b82f6"
          },
          {
            "value": "68%",
            "label": "Of Online Experiences Start with Search",
            "color": "#8b5cf6"
          },
          {
            "value": "$42",
            "label": "ROI for Every $1 Spent on Email Marketing",
            "color": "#10b981"
          }
        ]
      }
    },
    {
      "slideId": "slide_4_ecosystem",
      "slideNumber": 4,
      "slideTitle": "Digital Marketing Ecosystem",
      "templateId": "big-image-top",
      "props": {
        "title": "The Digital Marketing Landscape",
        "content": "Understanding the interconnected nature of digital marketing channels and how they work together to create a cohesive customer experience across all touchpoints.",
        "imageUrl": "https://via.placeholder.com/800x400?text=Digital+Ecosystem",
        "imageAlt": "Digital marketing ecosystem diagram",
        "imagePrompt": "A comprehensive diagram showing interconnected digital marketing channels including social media, email, SEO, PPC, content marketing, and analytics in a modern network visualization",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_5_audience_vs_market",
      "slideNumber": 5,
      "slideTitle": "Audience vs Market Research",
      "templateId": "two-column",
      "props": {
        "title": "Understanding the Difference",
        "leftTitle": "Market Research",
        "leftContent": "• Industry trends and size\n• Competitive landscape\n• Market opportunities\n• Overall demand patterns\n• Economic factors",
        "rightTitle": "Audience Research",
        "rightContent": "• Customer demographics\n• Behavioral patterns\n• Pain points and needs\n• Communication preferences\n• Decision-making process"
      }
    },
    {
      "slideId": "slide_6_personas",
      "slideNumber": 6,
      "slideTitle": "Buyer Persona Development",
      "templateId": "process-steps",
      "props": {
        "title": "Creating Effective Buyer Personas",
        "steps": [
          "Collect demographic and psychographic data",
          "Conduct customer interviews and surveys",
          "Analyze behavioral patterns and preferences",
          "Identify goals, challenges, and pain points",
          "Map the customer journey and touchpoints",
          "Validate personas with real customer data"
        ]
      }
    },
    {
      "slideId": "slide_7_content_strategy",
      "slideNumber": 7,
      "slideTitle": "Content Strategy Foundation",
      "templateId": "pyramid",
      "props": {
        "title": "Content Strategy Pyramid",
        "levels": [
          {
            "text": "Content Distribution & Promotion",
            "description": "Multi-channel amplification strategy"
          },
          {
            "text": "Content Creation & Production",
            "description": "High-quality, engaging content development"
          },
          {
            "text": "Content Planning & Calendar",
            "description": "Strategic planning and scheduling"
          },
          {
            "text": "Content Audit & Analysis",
            "description": "Understanding current content performance"
          },
          {
            "text": "Goals, Audience & Brand Foundation",
            "description": "Strategic foundation and core objectives"
          }
        ]
      }
    },
    {
      "slideId": "slide_8_content_types",
      "slideNumber": 8,
      "slideTitle": "Content Format Matrix",
      "templateId": "four-box-grid",
      "props": {
        "title": "Content Formats for Different Goals",
        "boxes": [
          {
            "title": "Educational Content",
            "content": "Blog posts, tutorials, webinars, how-to guides",
            "icon": "📚"
          },
          {
            "title": "Engagement Content", 
            "content": "Social media posts, polls, user-generated content",
            "icon": "💬"
          },
          {
            "title": "Conversion Content",
            "content": "Case studies, testimonials, product demos",
            "icon": "🎯"
          },
          {
            "title": "Entertainment Content",
            "content": "Videos, memes, interactive content, stories",
            "icon": "🎭"
          }
        ]
      }
    },
    {
      "slideId": "slide_9_social_challenges",
      "slideNumber": 9,
      "slideTitle": "Social Media Challenges & Solutions",
      "templateId": "challenges-solutions",
      "props": {
        "title": "Overcoming Social Media Obstacles",
        "challenges": [
          "Low organic reach and engagement",
          "Creating consistent, quality content",
          "Managing multiple platform requirements"
        ],
        "solutions": [
          "Focus on community building and authentic interactions",
          "Develop content pillars and batch creation workflows", 
          "Use scheduling tools and platform-specific strategies"
        ]
      }
    },
    {
      "slideId": "slide_10_email_timeline",
      "slideNumber": 10,
      "slideTitle": "Email Marketing Campaign Timeline",
      "templateId": "timeline",
      "props": {
        "title": "Building Your Email Marketing Program",
        "events": [
          {
            "date": "Week 1-2",
            "title": "Foundation Setup",
            "description": "Choose platform, design templates, set up automation"
          },
          {
            "date": "Week 3-4", 
            "title": "List Building",
            "description": "Create lead magnets, optimize signup forms"
          },
          {
            "date": "Week 5-8",
            "title": "Content Creation",
            "description": "Develop welcome series, newsletters, promotional campaigns"
          },
          {
            "date": "Week 9-12",
            "title": "Optimization",
            "description": "A/B testing, segmentation, performance analysis"
          }
        ]
      }
    },
    {
      "slideId": "slide_11_seo_quote",
      "slideNumber": 11,
      "slideTitle": "SEO Philosophy",
      "templateId": "quote-center",
      "props": {
        "quote": "The best place to hide a dead body is page 2 of Google search results.",
        "author": "Digital Marketing Wisdom",
        "context": "This humorous quote highlights the critical importance of ranking on the first page of search results for visibility and traffic."
      }
    },
    {
      "slideId": "slide_12_seo_factors",
      "slideNumber": 12,
      "slideTitle": "SEO Success Factors",
      "templateId": "bullet-points-right",
      "props": {
        "title": "Key SEO Elements",
        "bullets": [
          "Keyword research and strategic implementation",
          "High-quality, original content creation",
          "Technical SEO and site speed optimization",
          "Mobile-first design and user experience",
          "Authority building through quality backlinks",
          "Local SEO for geographic targeting"
        ],
        "bulletStyle": "dot",
        "imagePrompt": "SEO optimization illustration with search elements, website structure, and ranking factors in a modern, clean style",
        "imageAlt": "SEO optimization visual guide"
      }
    },
    {
      "slideId": "slide_13_paid_advertising",
      "slideNumber": 13,
      "slideTitle": "Paid Advertising Strategy",
      "templateId": "big-image-left",
      "props": {
        "title": "Maximizing Paid Campaign ROI",
        "subtitle": "Strategic paid advertising accelerates reach and drives targeted traffic when organic efforts need support.",
        "imageUrl": "https://via.placeholder.com/600x400?text=Paid+Advertising",
        "imageAlt": "Digital advertising dashboard",
        "imagePrompt": "A modern advertising dashboard showing campaign performance metrics, targeting options, and ROI indicators across multiple platforms",
        "imageSize": "large"
      }
    },
    {
      "slideId": "slide_14_implementation",
      "slideNumber": 14,
      "slideTitle": "90-Day Implementation Plan",
      "templateId": "process-steps",
      "props": {
        "title": "Your Digital Marketing Roadmap",
        "steps": [
          "Month 1: Foundation - Research, audit, and strategy development",
          "Month 2: Launch - Implement core channels and begin content creation",
          "Month 3: Optimize - Analyze data, refine approach, and scale success"
        ]
      }
    },
    {
      "slideId": "slide_15_conclusion",
      "slideNumber": 15,
      "slideTitle": "Success Principles",
      "templateId": "title-slide",
      "props": {
        "title": "Your Digital Marketing Success Formula",
        "subtitle": "Strategy + Consistency + Measurement = Growth",
        "author": "Remember: Digital marketing is a marathon, not a sprint",
        "backgroundColor": "#059669",
        "titleColor": "#ffffff",
        "subtitleColor": "#d1fae5"
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

@app.on_event("startup")
async def startup_event():
    global DB_POOL
    logger.info("Custom Backend starting...")
    if not CUSTOM_PROJECTS_DATABASE_URL:
        logger.critical("CRITICAL: CUSTOM_PROJECTS_DATABASE_URL env var not set.")
        return
    try:
        DB_POOL = await asyncpg.create_pool(dsn=CUSTOM_PROJECTS_DATABASE_URL, min_size=1, max_size=10,
                                            init=lambda conn: conn.set_type_codec(
                                                'jsonb',
                                                encoder=lambda value: json.dumps(value) if value is not None else None,
                                                decoder=lambda value: json.loads(value) if value is not None else None,
                                                schema='pg_catalog',
                                                format='text'
                                            ))
        async with DB_POOL.acquire() as connection:
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS design_templates (
                    id SERIAL PRIMARY KEY,
                    template_name TEXT NOT NULL UNIQUE,
                    template_structuring_prompt TEXT NOT NULL,
                    design_image_path TEXT,
                    microproduct_type TEXT,
                    component_name TEXT NOT NULL,
                    date_created TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    project_name TEXT NOT NULL,
                    product_type TEXT,
                    microproduct_type TEXT,
                    microproduct_name TEXT,
                    microproduct_content JSONB,
                    design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_name TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS product_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_onyx_user_id ON projects(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_design_template_id ON projects(design_template_id);")
            logger.info("'projects' table ensured and updated.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS microproduct_pipelines (
                    id SERIAL PRIMARY KEY,
                    pipeline_name TEXT NOT NULL,
                    pipeline_description TEXT,
                    is_prompts_data_collection BOOLEAN DEFAULT FALSE,
                    is_prompts_data_formating BOOLEAN DEFAULT FALSE,
                    prompts_data_collection JSONB,
                    prompts_data_formating JSONB,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_pipelines_name ON microproduct_pipelines(pipeline_name);")
            logger.info("'microproduct_pipelines' table ensured.")

            col_type_row = await connection.fetchrow(
                "SELECT data_type FROM information_schema.columns "
                "WHERE table_name = 'projects' AND column_name = 'microproduct_content';"
            )
            if col_type_row and col_type_row['data_type'] != 'jsonb':
                logger.info("Attempting to alter 'microproduct_content' column type to JSONB...")
                await connection.execute("ALTER TABLE projects ALTER COLUMN microproduct_content TYPE JSONB USING microproduct_content::text::jsonb;")
                logger.info("Successfully altered 'microproduct_content' to JSONB.")

            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            logger.info("'projects' table ensured and updated with 'source_chat_session_id'.")

            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_name ON design_templates(template_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_mptype ON design_templates(microproduct_type);")
            logger.info("'design_templates' table ensured.")

            # --- Ensure a soft-delete trash table for projects ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS trashed_projects (LIKE projects INCLUDING ALL);
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_trashed_projects_user ON trashed_projects(onyx_user_id);")
            logger.info("'trashed_projects' table ensured (soft-delete).")

            # --- Ensure user credits table ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_credits (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL UNIQUE,
                    name TEXT NOT NULL,
                    credits_balance INTEGER NOT NULL DEFAULT 0,
                    total_credits_used INTEGER NOT NULL DEFAULT 0,
                    credits_purchased INTEGER NOT NULL DEFAULT 0,
                    last_purchase_date TIMESTAMP WITH TIME ZONE,
                    subscription_tier TEXT DEFAULT 'basic',
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_credits_onyx_user_id ON user_credits(onyx_user_id);")
            logger.info("'user_credits' table ensured.")

            # NEW: Ensure credit transactions table for analytics/timeline
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS credit_transactions (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    user_credits_id INTEGER REFERENCES user_credits(id) ON DELETE CASCADE,
                    type TEXT NOT NULL CHECK (type IN ('purchase','product_generation','admin_removal')),
                    title TEXT,
                    product_type TEXT,
                    credits INTEGER NOT NULL CHECK (credits >= 0),
                    delta INTEGER NOT NULL,
                    reason TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_user ON credit_transactions(onyx_user_id, created_at DESC);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_type ON credit_transactions(type);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_product ON credit_transactions(product_type);")
            
            # Migration: Update credit_transactions table to support admin_removal type
            try:
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    DROP CONSTRAINT IF EXISTS credit_transactions_type_check;
                """)
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    ADD CONSTRAINT credit_transactions_type_check 
                    CHECK (type IN ('purchase','product_generation','admin_removal'));
                """)
                logger.info("Updated credit_transactions table to support admin_removal type")
            except Exception as e:
                logger.warning(f"Could not update credit_transactions constraint: {e}")
            
            logger.info("'credit_transactions' table ensured.")

            # Migration: Populate user_credits table with existing Onyx users
            try:
                migrated_count = await migrate_onyx_users_to_credits_table()
                logger.info(f"Populated user_credits table with {migrated_count} existing Onyx users (100 credits each).")
            except Exception as e:
                logger.error(f"Failed to migrate Onyx users to credits table: {e}")
                logger.info("Migration will be available manually via admin interface.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS project_folders (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    "order" INTEGER DEFAULT 0,
                    parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_onyx_user_id ON project_folders(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_folder_id ON projects(folder_id);")
            
            # Add parent_id column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for parent_id column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_parent_id ON project_folders(parent_id);")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add order column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for order column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_order ON project_folders(\"order\");")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add quality_tier column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS quality_tier TEXT DEFAULT 'medium';")
                logger.info("Added quality_tier column to project_folders table")
                
                # Update existing folders to have 'medium' tier if they don't have one
                await connection.execute("UPDATE project_folders SET quality_tier = 'medium' WHERE quality_tier IS NULL;")
                logger.info("Updated existing folders with default 'medium' tier")
                
                # Create index for quality_tier column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_quality_tier ON project_folders(quality_tier);")
                logger.info("Created index for quality_tier column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding quality_tier column: {e}")
                    raise e
            
            # Add custom_rate column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS custom_rate INTEGER DEFAULT 200;")
                logger.info("Added custom_rate column to project_folders table")
                
                # Update existing folders to have default custom_rate if they don't have one
                await connection.execute("UPDATE project_folders SET custom_rate = 200 WHERE custom_rate IS NULL;")
                logger.info("Updated existing folders with default custom_rate")
                
                # Create index for custom_rate column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_custom_rate ON project_folders(custom_rate);")
                logger.info("Created index for custom_rate column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding custom_rate column: {e}")
                    raise e
            
            # Add order column for project sorting
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_order ON projects(\"order\");")

            # Add completionTime column to projects table (for the new completion time feature)
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level custom rate and quality tier columns
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to projects table")
                
                # Create indexes for project-level custom rate and quality tier
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_custom_rate ON projects(custom_rate);")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_quality_tier ON projects(quality_tier);")
                logger.info("Created indexes for project-level custom_rate and quality_tier columns")
                
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level custom_rate/quality_tier columns: {e}")
                    raise e
            
            # Add completionTime column to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add other missing columns to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level columns to trashed_projects table
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to trashed_projects table")
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level columns to trashed_projects: {e}")
                    raise e

            # CRITICAL FIX: Ensure order and completion_time columns are TEXT type to prevent casting errors
            try:
                logger.info("Applying critical fix: Ensuring order and completion_time columns are TEXT type")
                
                # Fix projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set projects.order and projects.completion_time to TEXT type")
                
                # Fix trashed_projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE trashed_projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set trashed_projects.order and trashed_projects.completion_time to TEXT type")
                
                # Set default values for empty strings
                await connection.execute("""
                    UPDATE projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                logger.info("Successfully set default values for empty order and completion_time fields")
                
            except Exception as e:
                logger.error(f"Error applying critical TEXT type fix: {e}")

            # Final verification - ensure all required columns exist with correct types
            try:
                # Verify projects table schema
                projects_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Projects table schema verification:")
                for row in projects_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
                # Verify trashed_projects table schema
                trashed_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'trashed_projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Trashed_projects table schema verification:")
                for row in trashed_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
            except Exception as e:
                logger.error(f"Error during schema verification: {e}")

            # Create request analytics table
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS request_analytics (
                    id TEXT PRIMARY KEY,
                    endpoint TEXT NOT NULL,
                    method TEXT NOT NULL,
                    user_id TEXT,
                    status_code INTEGER NOT NULL,
                    response_time_ms INTEGER NOT NULL,
                    request_size_bytes INTEGER,
                    response_size_bytes INTEGER,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_created_at ON request_analytics(created_at);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_endpoint ON request_analytics(endpoint);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_user_id ON request_analytics(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_status_code ON request_analytics(status_code);")
            logger.info("'request_analytics' table ensured.")

            # Add AI parser tracking columns to request_analytics table
            try:
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS is_ai_parser_request BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_tokens INTEGER;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_model TEXT;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_project_name TEXT;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_ai_parser ON request_analytics(is_ai_parser_request);")
                logger.info("AI parser tracking columns added to request_analytics table.")
            except Exception as e:
                logger.warning(f"Error adding AI parser columns (may already exist): {e}")

            # Add is_standalone field to projects table to track standalone vs outline-based products
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_standalone ON projects(is_standalone);")
                logger.info("Added is_standalone column to projects table.")
                
                # Add same field to trashed_projects table to match schema
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                logger.info("Added is_standalone column to trashed_projects table.")
                
                # For legacy support: Set is_standalone = NULL for all existing products
                # This allows the frontend filtering logic to handle legacy products gracefully
                # New products will have this field explicitly set during creation
                logger.info("Legacy support: is_standalone field defaults to NULL for existing products.")
                
            except Exception as e:
                logger.warning(f"Error adding is_standalone column (may already exist): {e}")

            logger.info("Database schema migration completed successfully.")
    except Exception as e:
        logger.critical(f"Failed to initialize custom DB pool or ensure tables: {e}", exc_info=not IS_PRODUCTION)
        DB_POOL = None

@app.on_event("shutdown")
async def shutdown_event():
    if DB_POOL:
        await DB_POOL.close()
        logger.info("Custom projects DB pool closed.")

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

# NEW: Analytics/timeline models
class ProductUsage(BaseModel):
    product_type: str
    credits_used: int

class CreditUsageAnalyticsResponse(BaseModel):
    usage_by_product: List[ProductUsage]
    total_credits_used: int

class TimelineActivity(BaseModel):
    id: str
    type: Literal['purchase', 'product_generation', 'admin_removal']
    title: str
    credits: int
    timestamp: datetime
    product_type: Optional[str] = None

class UserTransactionHistoryResponse(BaseModel):
    user_id: int
    user_email: str
    user_name: str
    transactions: List[TimelineActivity]

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: str
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

# --- End: Add New Quiz Models ---

# +++ NEW MODEL FOR TEXT PRESENTATION +++
class TextPresentationDetails(BaseModel):
    textTitle: str
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, None]

class DesignTemplateBase(BaseModel):
    template_name: str
    template_structuring_prompt: str
    microproduct_type: str
    component_name: str
    model_config = {"from_attributes": True}

class DesignTemplateCreate(DesignTemplateBase):
    design_image_path: Optional[str] = None

class DesignTemplateUpdate(BaseModel):
    template_name: Optional[str] = None
    template_structuring_prompt: Optional[str] = None
    microproduct_type: Optional[str] = None
    component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class DesignTemplateResponse(DesignTemplateBase):
    id: int
    design_image_path: Optional[str] = None
    date_created: datetime

class ProjectCreateRequest(BaseModel):
    projectName: str
    design_template_id: int
    microProductName: Optional[str] = None
    aiResponse: str
    chatSessionId: Optional[uuid.UUID] = None
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    folder_id: Optional[int] = None  # Add folder_id for automatic folder assignment
    theme: Optional[str] = None      # Selected theme for presentations
    model_config = {"from_attributes": True}

class ProjectDB(BaseModel):
    id: int
    onyx_user_id: str
    project_name: str
    product_type: Optional[str] = None
    microproduct_type: Optional[str] = None
    microproduct_name: Optional[str] = None
    microproduct_content: Optional[MicroProductContentType] = None
    design_template_id: Optional[int] = None
    created_at: datetime
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    model_config = {"from_attributes": True}

class MicroProductApiResponse(BaseModel):
    name: str
    slug: str
    project_id: int
    design_template_id: int
    component_name: str
    parentProjectName: Optional[str] = None
    webLinkPath: Optional[str] = None
    pdfLinkPath: Optional[str] = None
    details: Optional[MicroProductContentType] = None
    sourceChatSessionId: Optional[uuid.UUID] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectApiResponse(BaseModel):
    id: int
    projectName: str
    projectSlug: str
    microproduct_name: Optional[str] = None
    design_template_name: Optional[str] = None
    design_microproduct_type: Optional[str] = None
    created_at: datetime
    design_template_id: Optional[int] = None
    folder_id: Optional[int] = None
    order: Optional[int] = None
    source_chat_session_id: Optional[str] = None
    is_standalone: Optional[bool] = None  # Track whether this is standalone or part of an outline
    model_config = {"from_attributes": True}

class ProjectDetailForEditResponse(BaseModel):
    id: int
    projectName: str
    microProductName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductContent: Optional[MicroProductContentType] = None
    createdAt: Optional[datetime] = None
    design_template_name: Optional[str] = None
    design_component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectUpdateRequest(BaseModel):
    projectName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductName: Optional[str] = None
    microProductContent: Optional[MicroProductContentType] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int

BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

class ErrorDetail(BaseModel):
    detail: str

class RequestAnalytics(BaseModel):
    id: str
    endpoint: str
    method: str
    user_id: Optional[str] = None
    status_code: int
    response_time_ms: int
    request_size_bytes: Optional[int] = None
    response_size_bytes: Optional[int] = None
    error_message: Optional[str] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class ProjectsDeleteRequest(BaseModel):
    project_ids: List[int]
    scope: Optional[str] = 'self'

class MicroproductPipelineBase(BaseModel):
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool = Field(False, alias="is_prompts_data_collection")
    is_structuring_prompts: bool = Field(False, alias="is_prompts_data_formating")
    discovery_prompts_list: Optional[List[str]] = Field(default_factory=list)
    structuring_prompts_list: Optional[List[str]] = Field(default_factory=list)
    model_config = {"from_attributes": True, "populate_by_name": True}

class MicroproductPipelineCreateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineUpdateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineDBRaw(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_prompts_data_collection: bool
    is_prompts_data_formating: bool
    prompts_data_collection: Optional[Dict[str, str]] = None
    prompts_data_formating: Optional[Dict[str, str]] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class MicroproductPipelineGetResponse(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool
    is_structuring_prompts: bool
    discovery_prompts_list: List[str] = Field(default_factory=list)
    structuring_prompts_list: List[str] = Field(default_factory=list)
    created_at: datetime
    model_config = {"from_attributes": True}

class DuplicatedProductInfo(BaseModel):
    original_id: int
    new_id: int
    type: str
    name: str

class ProjectDuplicationResponse(BaseModel):
    id: int
    name: str
    type: str
    connected_products: Optional[List[DuplicatedProductInfo]] = None
    total_products_duplicated: int
    model_config = {"from_attributes": True}

    @classmethod
    def from_db_model(cls, db_model: MicroproductPipelineDBRaw) -> "MicroproductPipelineGetResponse":
        discovery_list = [db_model.prompts_data_collection[key] for key in sorted(db_model.prompts_data_collection.keys(), key=int)] if db_model.prompts_data_collection else []
        structuring_list = [db_model.prompts_data_formating[key] for key in sorted(db_model.prompts_data_formating.keys(), key=int)] if db_model.prompts_data_formating else []
        return cls(
            id=db_model.id,
            pipeline_name=db_model.pipeline_name,
            pipeline_description=db_model.pipeline_description,
            is_discovery_prompts=db_model.is_prompts_data_collection,
            is_structuring_prompts=db_model.is_prompts_data_formating,
            discovery_prompts_list=discovery_list,
            structuring_prompts_list=structuring_list,
            created_at=db_model.created_at
        )

# --- Authentication and Utility Functions ---
# async def bing_company_research(company_name: str, company_desc: str) -> str:
#     if not BING_API_KEY:
#         return "(Bing API key not configured)"
#     query = f"{company_name} {company_desc} официальный сайт отзывы новости"
#     headers = {"Ocp-Apim-Subscription-Key": BING_API_KEY}
#     params = {"q": query, "mkt": "ru-RU"}
#     async with httpx.AsyncClient(timeout=15.0) as client:
#         resp = await client.get(BING_API_URL, headers=headers, params=params)
#         resp.raise_for_status()
#         data = resp.json()
#     # Extract summary: snippet, knowledge panel, news, etc.
#     snippets = []
#     if "webPages" in data:
#         for item in data["webPages"].get("value", [])[:3]:
#             snippets.append(item.get("snippet", ""))
#     if "entities" in data and data["entities"].get("value"):
#         for ent in data["entities"]["value"]:
#             if ent.get("description"):
#                 snippets.append(ent["description"])
#     if "news" in data and data["news"].get("value"):
#         for news in data["news"]["value"][:2]:
#             snippets.append(f"Новость: {news.get('name', '')} — {news.get('description', '')}")
#     return "\n".join(snippets)

async def serpapi_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    """
    Uses SerpAPI to gather:
    - General company info (snippets, knowledge panel, about, etc.)
    - Website-specific info (site: queries)
    - Open job listings (site:company_website jobs/careers, and generic queries)
    Returns a structured string with all findings.
    """
    url = "https://serpapi.com/search.json"
    async with httpx.AsyncClient(timeout=20.0) as client:
        # 1. General company info
        search_query = company_name
        if company_desc and company_desc.strip():
            search_query = f"{company_name} {company_desc}"
        
        params_general = {
            "q": search_query,
            "engine": "google",
            "api_key": SERPAPI_KEY,
            "hl": "ru"
        }
        try:
            resp = await client.get(url, params=params_general)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            logger.error(f"❌ [SERPAPI] Error in general search: {e}")
            # If general search fails, try with just the company name
            params_general["q"] = company_name
        resp = await client.get(url, params=params_general)
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if "organic_results" in data:
            for item in data["organic_results"][:3]:
                if "snippet" in item:
                    general_snippets.append(item["snippet"])
        if "knowledge_graph" in data:
            kg = data["knowledge_graph"]
            if "description" in kg:
                general_snippets.append(kg["description"])
            if "title" in kg:
                general_snippets.append(f"Название: {kg['title']}")
            if "type" in kg:
                general_snippets.append(f"Тип: {kg['type']}")
            if "website" in kg:
                general_snippets.append(f"Сайт: {kg['website']}")
            if "address" in kg:
                general_snippets.append(f"Адрес: {kg['address']}")
            if "phone" in kg:
                general_snippets.append(f"Телефон: {kg['phone']}")
        general_info = "\n".join(general_snippets) or "(Нет релевантных данных SerpAPI)"

        # 2. Website-specific info
        website_info = ""
        if company_website:
            params_site = {
                "q": f"site:{company_website} о компании информация контакты",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp2 = await client.get(url, params=params_site)
            resp2.raise_for_status()
            data2 = resp2.json()
            site_snippets = []
            if "organic_results" in data2:
                for item in data2["organic_results"][:3]:
                    if "snippet" in item:
                        site_snippets.append(item["snippet"])
            website_info = "\n".join(site_snippets) or "(Нет информации по сайту)"

        # 3. Open job listings (site:company_website + jobs/careers)
        jobs_info = ""
        jobs_snippets = []
        if company_website:
            # Try site:company_website jobs/careers
            params_jobs_site = {
                "q": f"site:{company_website} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp3 = await client.get(url, params=params_jobs_site)
            resp3.raise_for_status()
            data3 = resp3.json()
            if "organic_results" in data3:
                for item in data3["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        # If not enough, try generic company_name + jobs
        if len(jobs_snippets) < 2:
            params_jobs_generic = {
                "q": f"{company_name} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp4 = await client.get(url, params=params_jobs_generic)
            resp4.raise_for_status()
            data4 = resp4.json()
            if "organic_results" in data4:
                for item in data4["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        jobs_info = "\n\n".join(jobs_snippets) or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[SerpAPI General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def duckduckgo_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    # Step 1: General info
    general_query = f"{company_name} {company_desc} официальный сайт отзывы новости"
    url = "https://api.duckduckgo.com/"
    params = {
        "format": "json",
        "no_redirect": 1,
        "no_html": 1,
        "skip_disambig": 1,
    }
    async with httpx.AsyncClient(timeout=10.0) as client:
        # General info
        resp = await client.get(url, params={**params, "q": general_query})
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if data.get("AbstractText"):
            general_snippets.append(data["AbstractText"])
        if data.get("RelatedTopics"):
            for topic in data["RelatedTopics"]:
                if isinstance(topic, dict) and topic.get("Text"):
                    general_snippets.append(topic["Text"])
        if data.get("Answer"):
            general_snippets.append(data["Answer"])
        if data.get("Definition"):
            general_snippets.append(data["Definition"])
        general_info = "\n".join([s for s in general_snippets if s]).strip() or "(Нет релевантных данных DuckDuckGo)"

        # Step 2: Info about the company website
        website_info = ""
        if company_website:
            website_query = f"site:{company_website} о компании информация контакты"
            resp2 = await client.get(url, params={**params, "q": website_query})
            resp2.raise_for_status()
            data2 = resp2.json()
            website_snippets = []
            if data2.get("AbstractText"):
                website_snippets.append(data2["AbstractText"])
            if data2.get("RelatedTopics"):
                for topic in data2["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        website_snippets.append(topic["Text"])
            if data2.get("Answer"):
                website_snippets.append(data2["Answer"])
            if data2.get("Definition"):
                website_snippets.append(data2["Definition"])
            website_info = "\n".join([s for s in website_snippets if s]).strip() or "(Нет информации по сайту)"

        # Step 3: Open positions
        jobs_info = ""
        if company_website:
            jobs_query = f"site:{company_website} вакансии OR careers OR jobs"
            resp3 = await client.get(url, params={**params, "q": jobs_query})
            resp3.raise_for_status()
            data3 = resp3.json()
            jobs_snippets = []
            if data3.get("AbstractText"):
                jobs_snippets.append(data3["AbstractText"])
            if data3.get("RelatedTopics"):
                for topic in data3["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        jobs_snippets.append(topic["Text"])
            if data3.get("Answer"):
                jobs_snippets.append(data3["Answer"])
            if data3.get("Definition"):
                jobs_snippets.append(data3["Definition"])
            jobs_info = "\n".join([s for s in jobs_snippets if s]).strip() or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[DuckDuckGo General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def get_current_onyx_user_id(request: Request) -> str:
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: return dev_user_id
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id)
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_id: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def get_current_onyx_user_with_email(request: Request) -> tuple[str, str]:
    """Get both user ID and email from Onyx"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id), user_email
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_with_email: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def verify_admin_user(request: Request) -> tuple[str, str]:
    """Verify that the current user is an admin using Onyx's built-in role system"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id and not IS_PRODUCTION: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            user_role = user_data.get("role", "")
            
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
            # Check if user has admin role in Onyx
            if user_role != "admin":
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN, 
                    detail="Access denied. Admin privileges required."
                )
            
            return str(onyx_user_id), user_email
            
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in verify_admin_user: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

def create_slug(text: Optional[str]) -> str:
    if not text: return "default-slug"
    text_processed = str(text).lower()
    text_processed = re.sub(r'\s+', '-', text_processed)
    text_processed = re.sub(r'[^\wа-яёa-z0-9\-]+', '', text_processed, flags=re.UNICODE | re.IGNORECASE)
    return text_processed or "generated-slug"

def get_tier_ratio(tier: str) -> int:
    """Get the creation hours ratio for a given tier (legacy support)"""
    ratios = {
        'starter': 120,
        'medium': 200,
        'advanced': 320,
        'professional': 450,
        'basic': 100,
        'interactive': 200,
        'immersive': 700
    }
    return ratios.get(tier, 200)  # Default to medium (200) if tier not found

def calculate_creation_hours(completion_time_minutes: int, custom_rate: int) -> int:
    """Calculate creation hours based on completion time and custom rate, rounded to nearest integer"""
    if completion_time_minutes <= 0:
        return 0
    
    # Convert completion time from minutes to hours, then multiply by custom rate
    completion_hours = completion_time_minutes / 60.0
    creation_hours = completion_hours * custom_rate
    return round(creation_hours)

def round_hours_in_content(content: Any) -> Any:
    """Recursively round all hours fields to integers in content structure"""
    if isinstance(content, dict):
        result = {}
        for key, value in content.items():
            if key == 'hours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif key == 'totalHours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif isinstance(value, (dict, list)):
                result[key] = round_hours_in_content(value)
            else:
                result[key] = value
        return result
    elif isinstance(content, list):
        return [round_hours_in_content(item) for item in content]
    else:
        return content

async def get_folder_tier(folder_id: int, pool: asyncpg.Pool) -> str:
    """Get the tier of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its tier
        folder = await conn.fetchrow(
            "SELECT quality_tier, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 'interactive'  # Default tier
        
        # If folder has its own tier, use it
        if folder['quality_tier']:
            return folder['quality_tier']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_tier(folder['parent_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

async def get_folder_custom_rate(folder_id: int, pool: asyncpg.Pool) -> int:
    """Get the custom rate of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its custom rate
        folder = await conn.fetchrow(
            "SELECT custom_rate, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 200  # Default custom rate
        
        # If folder has its own custom rate, use it
        if folder['custom_rate']:
            return folder['custom_rate']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_custom_rate(folder['parent_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_custom_rate(project_id: int, pool: asyncpg.Pool) -> int:
    """Get the effective custom rate for a project, falling back to folder rate if not set"""
    async with pool.acquire() as conn:
        # Get the project's custom rate and folder_id
        project = await conn.fetchrow(
            "SELECT custom_rate, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 200  # Default custom rate
        
        # If project has its own custom rate, use it
        if project['custom_rate']:
            return project['custom_rate']
        
        # Otherwise, get the folder's custom rate
        if project['folder_id']:
            return await get_folder_custom_rate(project['folder_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_quality_tier(project_id: int, pool: asyncpg.Pool) -> str:
    """Get the effective quality tier for a project, falling back to folder tier if not set"""
    async with pool.acquire() as conn:
        # Get the project's quality tier and folder_id
        project = await conn.fetchrow(
            "SELECT quality_tier, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 'interactive'  # Default tier
        
        # If project has its own quality tier, use it
        if project['quality_tier']:
            return project['quality_tier']
        
        # Otherwise, get the folder's quality tier
        if project['folder_id']:
            return await get_folder_tier(project['folder_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

def get_lesson_effective_custom_rate(lesson: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a lesson, falling back to project rate if not set"""
    if lesson.get('custom_rate'):
        return lesson['custom_rate']
    return project_custom_rate

def get_lesson_effective_quality_tier(lesson: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a lesson, falling back to project tier if not set"""
    if lesson.get('quality_tier'):
        return lesson['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours(lesson: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a specific lesson using its tier settings"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        effective_custom_rate = get_lesson_effective_custom_rate(lesson, project_custom_rate)
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

def get_module_effective_custom_rate(section: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a module, falling back to project rate if not set"""
    if section.get('custom_rate'):
        return section['custom_rate']
    return project_custom_rate

def get_module_effective_quality_tier(section: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a module, falling back to project tier if not set"""
    if section.get('quality_tier'):
        return section['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours_with_module_fallback(lesson: dict, section: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a lesson with module-level fallback"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        
        # Check lesson-level tier first, then module-level, then project-level
        if lesson.get('custom_rate'):
            effective_custom_rate = lesson['custom_rate']
        elif section.get('custom_rate'):
            effective_custom_rate = section['custom_rate']
        else:
            effective_custom_rate = project_custom_rate
            
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

async def get_or_create_user_credits(onyx_user_id: str, user_name: str, pool: asyncpg.Pool) -> UserCredits:
    """Get user credits or create if doesn't exist"""
    async with pool.acquire() as conn:
        # Try to get existing credits
        credits_row = await conn.fetchrow(
            "SELECT * FROM user_credits WHERE onyx_user_id = $1",
            onyx_user_id
        )
        
        if credits_row:
            return UserCredits(**dict(credits_row))
        
        # Create new user credits entry with default values
        new_credits_row = await conn.fetchrow("""
            INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
            VALUES ($1, $2, $3, $3)
            RETURNING *
        """, onyx_user_id, user_name, 100, 100)  # Default 100 credits for new users
        
        logger.info(f"Auto-migrated new user {onyx_user_id} ({user_name}) with 100 credits")
        return UserCredits(**dict(new_credits_row))

def calculate_product_credits(product_type: str, content_data: dict = None) -> int:
    """Calculate credit cost for product creation"""
    if product_type == "course_outline":
        return 5  # Course outline finalization costs 5 credits
    elif product_type == "lesson_presentation":
        # Calculate based on slide count
        if content_data and isinstance(content_data, dict):
            slides = content_data.get("slides", [])
            slide_count = len(slides) if slides else 0
            
            if slide_count <= 5:
                return 3
            elif slide_count <= 10:
                return 5
            else:
                return 10
        return 5  # Default if we can't determine slide count
    elif product_type in ["quiz", "one_pager"]:
        return 5  # Quiz and one-pager both cost 5 credits
    else:
        return 0  # Unknown product type, no cost

# Helper: reason -> product type fallback
def infer_product_type_from_reason(reason: str) -> str:
    r = (reason or "").lower()
    if "course outline" in r:
        return "Course Outline"
    if "lesson presentation" in r or "text presentation" in r or "presentation" in r:
        return "Presentation"
    if "quiz" in r:
        return "Quiz"
    if "one-pager" in r or "one pager" in r:
        return "One-Pager"
    if "video" in r:
        return "Video Lesson"
    return "Unknown"

# Helper: write a credit transaction row
async def log_credit_transaction(
    conn,
    *,
    onyx_user_id: str,
    user_credits_id: Optional[int],
    type_: Literal['purchase','product_generation','admin_removal'],
    amount: int,
    delta: int,
    title: Optional[str] = None,
    product_type: Optional[str] = None,
    reason: Optional[str] = None,
) -> None:
    tx_id = str(uuid.uuid4())
    await conn.execute(
        """
        INSERT INTO credit_transactions
            (id, onyx_user_id, user_credits_id, type, title, product_type, credits, delta, reason, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
        """,
        tx_id, onyx_user_id, user_credits_id, type_, title, product_type, abs(amount), delta, reason
    )

async def deduct_credits(
    onyx_user_id: str,
    amount: int,
    pool: asyncpg.Pool,
    reason: str = "Product creation",
    product_type: Optional[str] = None,
    title: Optional[str] = None,
) -> UserCredits:
    """Deduct credits from user balance with transaction safety"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Check current balance
            current_balance = await conn.fetchval(
                "SELECT credits_balance FROM user_credits WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if current_balance is None:
                raise HTTPException(status_code=404, detail="User credits not found")
            
            if current_balance < amount:
                raise HTTPException(
                    status_code=402, 
                    detail=f"Insufficient credits. Current balance: {current_balance}, Required: {amount}"
                )
            
            # Deduct credits and update usage
            updated_row = await conn.fetchrow("""
                UPDATE user_credits 
                SET credits_balance = credits_balance - $1,
                    total_credits_used = total_credits_used + $1,
                    updated_at = NOW()
                WHERE onyx_user_id = $2
                RETURNING *
            """, amount, onyx_user_id)

            # Log product_generation transaction
            user_row = await conn.fetchrow("SELECT id FROM user_credits WHERE onyx_user_id = $1", onyx_user_id)
            await log_credit_transaction(
                conn,
                onyx_user_id=onyx_user_id,
                user_credits_id=int(user_row["id"]) if user_row else None,
                type_='product_generation',
                amount=amount,
                delta=-abs(amount),
                title=title or reason,
                product_type=product_type or infer_product_type_from_reason(reason),
                reason=reason,
            )
            
            return UserCredits(**dict(updated_row))

async def modify_user_credits_by_email(user_email: str, amount: int, action: str, pool: asyncpg.Pool, reason: str = "Admin adjustment") -> UserCredits:
    """Modify user credits by email (for admin use)"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            if action == "add":
                # Add credits (can be new user or existing)
                # Use email as onyx_user_id for simplicity in admin interface
                credits_row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) 
                    DO UPDATE SET 
                        credits_balance = user_credits.credits_balance + $3,
                        credits_purchased = user_credits.credits_purchased + $3,
                        last_purchase_date = NOW(),
                        updated_at = NOW()
                    RETURNING *
                """, user_email, user_email.split('@')[0], amount, amount)

                # Log purchase
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='purchase',
                    amount=amount,
                    delta=abs(amount),
                    title="Credits purchase",
                    product_type=None,
                    reason=reason or "Credits purchase",
                )
                
            elif action == "remove":
                # Remove credits (must exist)
                credits_row = await conn.fetchrow("""
                    UPDATE user_credits 
                    SET credits_balance = GREATEST(0, credits_balance - $1),
                        updated_at = NOW()
                    WHERE onyx_user_id = $2
                    RETURNING *
                """, amount, user_email)
                
                if not credits_row:
                    raise HTTPException(status_code=404, detail="User not found")

                # Log admin removal as 'admin_removal'
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='admin_removal',
                    amount=amount,
                    delta=-abs(amount),
                    title="Admin adjustment",
                    product_type=None,
                    reason=reason or "Admin adjustment",
                )
            
            else:
                raise HTTPException(status_code=400, detail="Invalid action. Must be 'add' or 'remove'")
            
            return UserCredits(**dict(credits_row))

async def migrate_onyx_users_to_credits_table() -> int:
    """Migrate users from Onyx database to credits table"""
    if not ONYX_DATABASE_URL:
        raise Exception("ONYX_DATABASE_URL not configured")
    
    logger.info(f"Attempting to connect to Onyx database: {ONYX_DATABASE_URL}")
    
    # Try multiple possible database names
    possible_db_urls = [
        ONYX_DATABASE_URL,
        ONYX_DATABASE_URL.replace('/onyx_db', '/postgres'),  # Try postgres DB name
        ONYX_DATABASE_URL.replace('/postgres', '/onyx_db')   # Try onyx_db name
    ]
    
    # Remove duplicates while preserving order
    db_urls_to_try = []
    for url in possible_db_urls:
        if url not in db_urls_to_try:
            db_urls_to_try.append(url)
    
    onyx_pool = None
    last_error = None
    
    for db_url in db_urls_to_try:
        try:
            logger.info(f"Trying to connect to: {db_url}")
            # Connect to Onyx database
            onyx_pool = await asyncpg.create_pool(dsn=db_url, min_size=1, max_size=5)
        
            async with onyx_pool.acquire() as onyx_conn:
                # Get users from Onyx database
                onyx_users = await onyx_conn.fetch("""
                    SELECT 
                        id::text as onyx_user_id,
                        COALESCE(email, 'Unknown User') as name
                    FROM "user" 
                    WHERE is_active = true
                    AND role != 'ext_perm_user'
                    AND role != 'slack_user'
                    AND email NOT LIKE '%@danswer-api-key.invalid'
                """)
            
            if not onyx_users:
                logger.info("No Onyx users found to migrate")
                return 0
            
            logger.info(f"Found {len(onyx_users)} Onyx users to migrate")
            
            # Insert into custom database
            async with DB_POOL.acquire() as custom_conn:
                migrated_count = 0
                for user in onyx_users:
                    try:
                        await custom_conn.execute("""
                            INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                            VALUES ($1, $2, 100)
                            ON CONFLICT (onyx_user_id) DO NOTHING
                        """, user['onyx_user_id'], user['name'])
                        migrated_count += 1
                    except Exception as e:
                        logger.warning(f"Failed to migrate user {user['onyx_user_id']}: {e}")
                
                return migrated_count
                
        except Exception as e:
            last_error = e
            logger.warning(f"Failed to connect to {db_url}: {e}")
            if onyx_pool:
                await onyx_pool.close()
                onyx_pool = None
            continue  # Try next database URL
        
        # If we got here, the connection worked, so break out of the loop
        break
    
    # If we tried all URLs and none worked, raise the last error
    if onyx_pool is None:
        raise Exception(f"Could not connect to any Onyx database. Last error: {last_error}")
    
    # This should never be reached, but just in case
    return 0

VIDEO_SCRIPT_LANG_STRINGS = {
    'ru': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Видео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Отображаемый текст:',
        'DISPLAYED_IMAGE_LABEL': 'Отображаемая картинка:',
        'DISPLAYED_VIDEO_LABEL': 'Отображаемое видео:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучки:',
        'NO_SLIDES_TEXT': 'Нет слайдов для отображения.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Название теста',
            'question': 'Вопрос',
            'correctAnswer': 'Правильный ответ',
            'correctAnswers': 'Правильные ответы',
            'acceptableAnswers': 'Допустимые ответы',
            'prompts': 'Элементы',
            'options': 'Варианты',
            'correctMatches': 'Правильные соответствия',
            'itemsToSort': 'Элементы для сортировки',
            'explanation': 'Объяснение',
            'multipleChoice': 'Один правильный ответ',
            'multiSelect': 'Несколько правильных ответов',
            'matching': 'Соответствие',
            'sorting': 'Сортировка',
            'openAnswer': 'Свободный ответ',
            'answerKey': 'Ключ ответов',
            'correctOrder': 'Правильный порядок',
            'emptyContent': '...',
        }
    },
    'en': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Video Lesson Script',
        'SLIDE_NUMBER_PREFIX': 'SLIDE №',
        'DISPLAYED_TEXT_LABEL': 'Displayed Text:',
        'DISPLAYED_IMAGE_LABEL': 'Displayed Image:',
        'DISPLAYED_VIDEO_LABEL': 'Displayed Video:',
        'VOICEOVER_TEXT_LABEL': 'Voiceover Text:',
        'NO_SLIDES_TEXT': 'No slides to display.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'COURSE',
        'lessonLabel': 'LESSON',
        'quiz': {
            'quizTitle': 'Quiz Title',
            'question': 'Question',
            'correctAnswer': 'Correct Answer',
            'correctAnswers': 'Correct Answers',
            'acceptableAnswers': 'Acceptable Answers',
            'prompts': 'Items',
            'options': 'Options',
            'correctMatches': 'Correct Matches',
            'itemsToSort': 'Items to Sort',
            'explanation': 'Explanation',
            'multipleChoice': 'Multiple Choice',
            'multiSelect': 'Multi-Select',
            'matching': 'Matching',
            'sorting': 'Sorting',
            'openAnswer': 'Open Answer',
            'answerKey': 'Answer Key',
            'correctOrder': 'Correct Order',
            'emptyContent': '...',
        }
    },
    'uk': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Відео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Текст, що відображається:',
        'DISPLAYED_IMAGE_LABEL': 'Зображення, що відображається:',
        'DISPLAYED_VIDEO_LABEL': 'Відео, що відображається:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучення:',
        'NO_SLIDES_TEXT': 'Немає слайдів для відображення.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Назва тесту',
            'question': 'Питання',
            'correctAnswer': 'Правильна відповідь',
            'correctAnswers': 'Правильні відповіді',
            'acceptableAnswers': 'Допустимі відповіді',
            'prompts': 'Елементи',
            'options': 'Варіанти',
            'correctMatches': 'Правильні відповідності',
            'itemsToSort': 'Елементи для сортування',
            'explanation': 'Пояснення',
            'multipleChoice': 'Одна правильна відповідь',
            'multiSelect': 'Декілька правильних відповідей',
            'matching': 'Відповідність',
            'sorting': 'Сортування',
            'openAnswer': 'Вільна відповідь',
            'answerKey': 'Ключ відповідей',
            'correctOrder': 'Правильний порядок',
            'emptyContent': '...',
        }
    },
    'es': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Guión de la lección en video',
        'SLIDE_NUMBER_PREFIX': 'DIAPOSITIVA №',
        'DISPLAYED_TEXT_LABEL': 'Texto mostrado:',
        'DISPLAYED_IMAGE_LABEL': 'Imagen mostrada:',
        'DISPLAYED_VIDEO_LABEL': 'Video mostrado:',
        'VOICEOVER_TEXT_LABEL': 'Texto de voz en off:',
        'NO_SLIDES_TEXT': 'No hay diapositivas para mostrar.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'CURSO',
        'lessonLabel': 'LECCIÓN',
        'quiz': {
            'quizTitle': 'Título del cuestionario',
            'question': 'Pregunta',
            'correctAnswer': 'Respuesta correcta',
            'correctAnswers': 'Respuestas correctas',
            'acceptableAnswers': 'Respuestas aceptables',
            'prompts': 'Elementos',
            'options': 'Opciones',
            'correctMatches': 'Correspondencias correctas',
            'itemsToSort': 'Elementos para ordenar',
            'explanation': 'Explicación',
            'multipleChoice': 'Opción múltiple',
            'multiSelect': 'Selección múltiple',
            'matching': 'Correspondencia',
            'sorting': 'Ordenamiento',
            'openAnswer': 'Respuesta abierta',
            'answerKey': 'Clave de respuestas',
            'correctOrder': 'Orden correcto',
            'emptyContent': '...',
        }
    }
}

def detect_language(text: str, configs: Dict[str, Dict[str, str]] = LANG_CONFIG) -> str:
    en_score = 0; ru_score = 0; uk_score = 0
    en_config = configs.get('en', {})
    ru_config = configs.get('ru', {})
    uk_config = configs.get('uk', {})

    if en_config.get('MODULE_KEYWORD') and en_config.get('LESSONS_HEADER_KEYWORD') and en_config.get('TOTAL_TIME_KEYWORD'):
        if en_config['MODULE_KEYWORD'] in text and \
           en_config['LESSONS_HEADER_KEYWORD'] in text and \
           en_config['TOTAL_TIME_KEYWORD'] in text:
            en_score += 3
    if ru_config.get('MODULE_KEYWORD') and ru_config.get('LESSONS_HEADER_KEYWORD') and ru_config.get('TOTAL_TIME_KEYWORD'):
        if ru_config['MODULE_KEYWORD'] in text and \
           ru_config['LESSONS_HEADER_KEYWORD'] in text and \
           ru_config['TOTAL_TIME_KEYWORD'] in text:
            ru_score += 3
    if uk_config.get('MODULE_KEYWORD') and uk_config.get('LESSONS_HEADER_KEYWORD') and uk_config.get('TOTAL_TIME_KEYWORD'):
        if uk_config['MODULE_KEYWORD'] in text and \
           uk_config['LESSONS_HEADER_KEYWORD'] in text and \
           uk_config['TOTAL_TIME_KEYWORD'] in text:
            uk_score += 3
    if en_score == 0 and ru_score == 0 and uk_score == 0:
        if en_config.get('MODULE_KEYWORD') and en_config['MODULE_KEYWORD'] in text: en_score +=1
        if ru_config.get('MODULE_KEYWORD') and ru_config['MODULE_KEYWORD'] in text: ru_score +=1
        if uk_config.get('MODULE_KEYWORD') and uk_config['MODULE_KEYWORD'] in text: uk_score +=1
        if en_config.get('TIME_KEYWORD') and en_config['TIME_KEYWORD'] in text: en_score +=1
        if ru_config.get('TIME_KEYWORD') and ru_config['TIME_KEYWORD'] in text: ru_score +=1
        if uk_config.get('TIME_KEYWORD') and uk_config['TIME_KEYWORD'] in text: uk_score +=1
        if en_score == 0 and ru_score == 0 and uk_score == 0:
            en_chars = sum(1 for char_ in text if 'a' <= char_.lower() <= 'z')
            cyrillic_chars = sum(1 for char_ in text if 'а' <= char_.lower() <= 'я' or char_.lower() in ['і', 'ї', 'є', 'ґ'])
            if en_chars > cyrillic_chars and en_chars > 10 :
                 en_score += 0.1
            elif cyrillic_chars > en_chars and cyrillic_chars > 10:
                if uk_score == 0: uk_score += 0.05
                if ru_score == 0: ru_score += 0.05
                ukrainian_specific_chars = sum(1 for char_ in text if char_.lower() in ['і', 'ї', 'є', 'ґ'])
                if ukrainian_specific_chars > 0:
                    uk_score += 0.05 * ukrainian_specific_chars
    if en_score > ru_score and en_score > uk_score: return 'en'
    if ru_score > en_score and ru_score > uk_score: return 'ru'
    if uk_score > en_score and uk_score > ru_score: return 'uk'
    if uk_score > 0 and uk_score >= ru_score and uk_score >= en_score: return 'uk'
    if ru_score > 0 and ru_score >= en_score : return 'ru'
    if en_score > 0 : return 'en'
    logger.warning("detect_language could not reliably determine language. Defaulting to 'en'.")
    return 'en'

def parse_training_plan_from_string(original_content_str: str, main_table_title: str) -> Optional[TrainingPlanDetails]:
    logger.warning("Old 'parse_training_plan_from_string' called. Ensure this is intended for legacy data.")
    return TrainingPlanDetails(mainTitle=f"Content for {main_table_title} (Old Parser)", sections=[], detectedLanguage='ru')

async def parse_ai_response_with_llm(
    ai_response: str,
    project_name: str,
    target_model: Type[BaseModel],
    default_error_model_instance: BaseModel,
    dynamic_instructions: str,
    target_json_example: str
) -> BaseModel:
    # Start timing for analytics
    start_time = time.time()
    
    # DEBUG: Log that the function was called
    logger.info(f"=== AI PARSER FUNCTION CALLED ===")
    logger.info(f"Project: {project_name}")
    logger.info(f"Target model: {target_model.__name__}")
    logger.info(f"AI response length: {len(ai_response)}")
    logger.info(f"DB_POOL available: {DB_POOL is not None}")
    logger.info(f"Call stack: {len(inspect.stack())} frames")
    logger.info(f"=== END FUNCTION CALL DEBUG ===")
    
    # Create a list of API keys to try, filtering out any that are not set
    api_keys_to_try = [key for key in [LLM_API_KEY, LLM_API_KEY_FALLBACK] if key]

    if not api_keys_to_try:
        logger.error(f"LLM_API_KEY not configured for {project_name}. Cannot parse AI response with LLM.")
        return default_error_model_instance

    prompt_message = f"""
You are a highly accurate text-to-JSON parsing assistant. Your task is to convert the *entirety* of the following unstructured text into a single, structured JSON object.
Ensure *all* relevant information from the "Raw text to parse" is included in your JSON output.
Pay close attention to data types: strings should be quoted, numerical values should be numbers, and lists should be arrays. Null values are not permitted for string fields; use an empty string "" instead if text is absent but the field is required according to the example structure.
Maintain the original language of the input text for all textual content in the JSON.

🚨 SPECIAL INSTRUCTION FOR VIDEO LESSONS: If the target model is SlideDeckDetails and the JSON example contains "voiceoverText" fields, you MUST generate voiceover text for every slide object. Look at the example JSON structure and ensure your output matches it exactly, including all voiceoverText fields and hasVoiceover flag.

Specific Instructions for this Content Type ({target_model.__name__}):
---
{dynamic_instructions}
---

The desired JSON output format is exemplified below. This example is CRUCIAL and your output MUST strictly follow this JSON format and structure.
---
{target_json_example}
---

Raw text to parse:
---
{ai_response}
---

Return ONLY the JSON object corresponding to the parsed text. Do not include any other explanatory text or markdown formatting (like ```json ... ```) around the JSON.
The entire output must be a single, valid JSON object and must include all relevant data found in the input, with textual content in the original language.
    """
    # OpenAI Chat API expects a list of chat messages
    system_msg = {"role": "system", "content": "You are a JSON parsing expert. You must output ONLY valid JSON in the exact format specified. Do not include any explanations, markdown formatting, or additional text. Your response must be a single, complete JSON object. CRITICAL: If the example JSON contains voiceoverText fields, your output MUST include them for every slide. Match the example structure exactly."}
    user_msg = {"role": "user", "content": prompt_message}
    base_payload: Dict[str, Any] = {"model": LLM_DEFAULT_MODEL, "messages": [system_msg, user_msg], "temperature": 0.1}
    # Ask the model to output pure JSON
    base_payload_with_rf = {**base_payload, "response_format": {"type": "json_object"}}
    detected_lang_by_rules = detect_language(ai_response)
    last_exception = None

    for i, api_key in enumerate(api_keys_to_try):
        attempt_number = i + 1
        logger.info(f"Attempting LLM call for '{project_name}' using API key #{attempt_number}.")
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        try:
            # Try with response_format first, then without if Cohere rejects it
            for pf_idx, payload_variant in enumerate([base_payload_with_rf, base_payload]):
                try:
                    # Remove per-request timeout so long parses are not cut off (backend will rely on upstream timeouts)
                    async with httpx.AsyncClient(timeout=None) as client:
                        response = await client.post(LLM_API_URL, headers=headers, json=payload_variant)
                        response.raise_for_status()
                    break  # success
                except httpx.HTTPStatusError as he:
                    # OpenAI returns 400 when response_format isn't supported for a given model
                    if he.response.status_code in (400, 422) and pf_idx == 0:
                        logger.info("LLM rejected response_format – retrying without it.")
                        continue
                    raise

            llm_api_response_data = response.json()

            # --- Process the Response ---
            json_text_output = None
            if "text" in llm_api_response_data: json_text_output = llm_api_response_data["text"]
            elif "chatHistory" in llm_api_response_data and llm_api_response_data["chatHistory"]:
                last_message = next((msg for msg in reversed(llm_api_response_data["chatHistory"]) if msg.get("role") == "CHATBOT"), None)
                if last_message and "message" in last_message: json_text_output = last_message["message"]
            elif llm_api_response_data.get("generations") and isinstance(llm_api_response_data["generations"], list) and llm_api_response_data["generations"][0].get("text"):
                json_text_output = llm_api_response_data["generations"][0]["text"]
            elif "choices" in llm_api_response_data and isinstance(llm_api_response_data["choices"], list) and llm_api_response_data["choices"]:
                # Support for OpenAI Chat Completions format: choices[0].message.content (or .delta.content in streaming)
                first_choice = llm_api_response_data["choices"][0]
                if isinstance(first_choice, dict):
                    # Standard chat completion response
                    if "message" in first_choice and isinstance(first_choice["message"], dict):
                        json_text_output = first_choice["message"].get("content")
                    # If using the delta format (streaming-style response aggregated by OpenAI)
                    if not json_text_output and "delta" in first_choice and isinstance(first_choice["delta"], dict):
                        json_text_output = first_choice["delta"].get("content")
                    # Fallback to any direct text field
                    if not json_text_output:
                        json_text_output = first_choice.get("text")

            if json_text_output is None:
                # Log the raw keys of the response for easier debugging
                logger.warning(
                    "No 'content' field found in LLM response. Raw keys: %s" % list(llm_api_response_data.keys())
                )
                logger.debug("Full LLM response: %s" % json.dumps(llm_api_response_data)[:1000])
                raise ValueError("LLM response did not contain an expected text field.")

            json_text_output = re.sub(r"^```json\s*|\s*```$", "", json_text_output.strip(), flags=re.MULTILINE)

            cleaned_json_str = json_text_output.strip()
            cleaned_json_str = re.sub(r"^```(?:json)?\s*|\s*```$", "", cleaned_json_str, flags=re.IGNORECASE | re.MULTILINE).strip()
            first_brace = cleaned_json_str.find('{')
            last_brace = cleaned_json_str.rfind('}')
            if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
                cleaned_json_str = cleaned_json_str[first_brace:last_brace + 1]
            if not cleaned_json_str.startswith('{'):
                cleaned_json_str = json_text_output.strip()
            json_text_output = cleaned_json_str

            try:
                parsed_json_data = json.loads(json_text_output)
            except json.JSONDecodeError:
                fixed_str = _clean_loose_json(json_text_output)
                try:
                    parsed_json_data = json.loads(fixed_str)
                except json.JSONDecodeError:
                    import ast
                    try:
                        parsed_json_data = ast.literal_eval(fixed_str)
                    except (ValueError, SyntaxError):
                        # Last-ditch heuristic: convert single quotes to double and
                        # Python constants to JSON equivalents, then try json.loads again.
                        brute = fixed_str
                        brute = re.sub(r"'([^']*)'", lambda m: '"' + m.group(1).replace('"', '\\"') + '"', brute)
                        brute = brute.replace("True", "true").replace("False", "false").replace("None", "null")
                        parsed_json_data = json.loads(brute)

            logger.debug(f'Cohere response: {parsed_json_data}')

            if 'detectedLanguage' not in parsed_json_data or not parsed_json_data['detectedLanguage']:
                parsed_json_data['detectedLanguage'] = detected_lang_by_rules

            if target_model == TrainingPlanDetails and ('mainTitle' not in parsed_json_data or not parsed_json_data['mainTitle']):
                parsed_json_data['mainTitle'] = project_name
            elif target_model == PdfLessonDetails and ('lessonTitle' not in parsed_json_data or not parsed_json_data['lessonTitle']):
                parsed_json_data['lessonTitle'] = project_name
            
            # Round hours to integers before validation to prevent float validation errors
            if target_model == TrainingPlanDetails:
                parsed_json_data = round_hours_in_content(parsed_json_data)
            
            validated_model = target_model.model_validate(parsed_json_data)
            logger.info(f"LLM parsing for '{project_name}' succeeded on attempt #{attempt_number}.")

            # Log AI parser usage
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    response_tokens = len(encoding.encode(json.dumps(parsed_json_data)))
                    total_tokens = prompt_tokens + response_tokens
                    
                    logger.info(f"=== AI PARSER LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Response tokens: {response_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"=== END AI PARSER LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 200, int((time.time() - start_time) * 1000), len(ai_response), len(json.dumps(parsed_json_data)), None, True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Database insert result: {result}")
                            logger.info(f"Successfully logged AI parser usage for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Database insert failed: {db_error}")
                            logger.error(f"Insert parameters: endpoint='/ai/parse', method='POST', status_code=200, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as e:
                    logger.warning(f"Failed to log AI parser usage: {e}")
                    logger.error(f"AI Parser logging error details: {str(e)}")

            return validated_model

        except Exception as e:
            last_exception = e
            logger.warning(
                f"LLM parsing attempt #{attempt_number} for '{project_name}' failed with {type(e).__name__}. "
                f"Details: {str(e)[:250]}. Trying next key if available."
            )
            
            # Log failed AI parser attempt
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    total_tokens = prompt_tokens  # No response tokens for failed attempts
                    
                    logger.info(f"=== AI PARSER FAILED LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"Error: {str(e)[:200]}")
                    logger.info(f"=== END AI PARSER FAILED LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert failed AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 500, int((time.time() - start_time) * 1000), len(ai_response), 0, str(e)[:500], True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Failed attempt database insert result: {result}")
                            logger.info(f"Successfully logged failed AI parser attempt for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Failed attempt database insert failed: {db_error}")
                            logger.error(f"Failed attempt insert parameters: endpoint='/ai/parse', method='POST', status_code=500, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as log_error:
                    logger.warning(f"Failed to log AI parser error: {log_error}")
                    logger.error(f"AI Parser failed logging error details: {str(log_error)}")
            
            continue

    # --- Handle Final Failure ---
    # This block is reached only if the loop completes without a successful return.
    logger.error(f"All LLM API call attempts failed for '{project_name}'. Last error: {last_exception}")
    if hasattr(default_error_model_instance, 'detectedLanguage'):
        default_error_model_instance.detectedLanguage = detected_lang_by_rules
    return default_error_model_instance

def _clean_loose_json(text: str) -> str:
    """Attempt to fix common minor JSON issues produced by LLMs: trailing commas, smart quotes, etc."""
    # remove common markdown code fences again (defensive)
    text = re.sub(r"^```(?:json)?\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE | re.MULTILINE)
    # replace smart quotes with standard quotes
    for sq in ('\u201c', '\u201d', '\u2018', '\u2019'):
        text = text.replace(sq, '"')
    # strip trailing commas before object/array close
    text = re.sub(r",\s*(\}|\])", r"\1", text)
    # remove escaped newlines that are unnecessary
    text = text.replace('\\n', '\n')
    return text

# --- API Endpoints ---
@app.post("/api/custom/pipelines/add", response_model=MicroproductPipelineDBRaw, status_code=status.HTTP_201_CREATED)
async def add_pipeline(pipeline_data: MicroproductPipelineCreateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        INSERT INTO microproduct_pipelines (pipeline_name, pipeline_description, {db_is_discovery}, {db_is_structuring}, prompts_data_collection, prompts_data_formating, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create pipeline.")
        return MicroproductPipelineDBRaw(**dict(row))
    except Exception as e:
        logger.error(f"Error inserting pipeline: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding the pipeline." if IS_PRODUCTION else f"DB error on pipeline insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines", response_model=List[MicroproductPipelineGetResponse])
async def get_pipelines(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines ORDER BY created_at DESC;"
    try:
        async with pool.acquire() as conn: rows = await conn.fetch(query)
        pipelines_list = [MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row))) for row in rows]
        return pipelines_list
    except Exception as e:
        logger.error(f"Error fetching pipelines: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching pipelines." if IS_PRODUCTION else f"DB error fetching pipelines: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines/{pipeline_id}", response_model=MicroproductPipelineGetResponse)
async def get_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines WHERE id = $1;"
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row)))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching the pipeline." if IS_PRODUCTION else f"DB error fetching pipeline: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/pipelines/update/{pipeline_id}", response_model=MicroproductPipelineDBRaw)
async def update_pipeline(pipeline_id: int, pipeline_data: MicroproductPipelineUpdateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        UPDATE microproduct_pipelines SET pipeline_name = $1, pipeline_description = $2, {db_is_discovery} = $3, {db_is_structuring} = $4, prompts_data_collection = $5, prompts_data_formating = $6
        WHERE id = $7 RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found or update failed.")
        return MicroproductPipelineDBRaw(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating the pipeline." if IS_PRODUCTION else f"DB error on pipeline update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/pipelines/delete/{pipeline_id}", status_code=status.HTTP_200_OK)
async def delete_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "DELETE FROM microproduct_pipelines WHERE id = $1 RETURNING id;"
    try:
        async with pool.acquire() as conn: deleted_id = await conn.fetchval(query, pipeline_id)
        if deleted_id is None:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return {"detail": f"Successfully deleted pipeline with ID {pipeline_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the pipeline." if IS_PRODUCTION else f"DB error on pipeline deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_design_template_image(file: UploadFile = File(...)):
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 5 * 1024 * 1024
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving design image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/onepager/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_onepager_image(file: UploadFile = File(...)):
    """Upload an image for use in one-pagers"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for one-pager images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"onepager_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving one-pager image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/presentation/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_presentation_image(file: UploadFile = File(...)):
    """Upload an image for use in presentations"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for presentation images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"presentation_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving presentation image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

# NEW: AI Image Generation Endpoint
class AIImageGenerationRequest(BaseModel):
    prompt: str = Field(..., description="Text prompt for image generation")
    width: int = Field(..., description="Image width in pixels", ge=256, le=1792)
    height: int = Field(..., description="Image height in pixels", ge=256, le=1792)
    quality: str = Field(default="standard", description="Image quality: standard or hd")
    style: str = Field(default="vivid", description="Image style: vivid or natural")
    model: str = Field(default="dall-e-3", description="DALL-E model to use")

@app.post("/api/custom/presentation/generate_image", responses={
    200: {"description": "Image generated successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/ai_generated_image.png"}}}},
    400: {"description": "Invalid request parameters", "model": ErrorDetail},
    500: {"description": "AI generation failed", "model": ErrorDetail}
})
async def generate_ai_image(request: AIImageGenerationRequest):
    """Generate an image using DALL-E AI"""
    try:
        logger.info(f"[AI_IMAGE_GENERATION] Starting generation with prompt: '{request.prompt[:50]}...'")
        logger.info(f"[AI_IMAGE_GENERATION] Dimensions: {request.width}x{request.height}, Quality: {request.quality}, Style: {request.style}")
        
        # Validate dimensions (DALL-E 3 requirements)
        valid_sizes = [(1024, 1024), (1792, 1024), (1024, 1792)]
        current_size = (request.width, request.height)
        
        if current_size not in valid_sizes:
            # Find the closest valid size based on aspect ratio
            aspect_ratio = request.width / request.height
            
            if aspect_ratio > 1.5:  # Landscape
                request.width, request.height = 1792, 1024
            elif aspect_ratio < 0.7:  # Portrait
                request.width, request.height = 1024, 1792
            else:  # Square-ish
                request.width, request.height = 1024, 1024
                
            logger.info(f"[AI_IMAGE_GENERATION] Adjusted dimensions from {current_size} to {request.width}x{request.height}")
        
        # Get OpenAI client
        client = get_openai_client()
        
        # Generate image using DALL-E
        response = await client.images.generate(
            model=request.model,
            prompt=request.prompt,
            size=f"{request.width}x{request.height}",
            quality=request.quality,
            style=request.style,
            n=1
        )
        
        if not response.data or len(response.data) == 0:
            raise Exception("No image data received from DALL-E")
        
        # Get the generated image URL
        image_url = response.data[0].url
        if not image_url:
            raise Exception("No image URL received from DALL-E")
        
        logger.info(f"[AI_IMAGE_GENERATION] Image generated successfully, downloading from: {image_url[:50]}...")
        
        # Download the image
        async with httpx.AsyncClient() as http_client:
            image_response = await http_client.get(image_url)
            image_response.raise_for_status()
            image_data = image_response.content
        
        # Save the image to disk
        safe_filename_base = str(uuid.uuid4())
        unique_filename = f"ai_generated_{safe_filename_base}.png"
        file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
        
        try:
            with open(file_path_on_disk, "wb") as buffer:
                buffer.write(image_data)
            
            web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
            
            logger.info(f"[AI_IMAGE_GENERATION] Image saved successfully: {web_accessible_path}")
            
            return {
                "file_path": web_accessible_path,
                "prompt": request.prompt,
                "dimensions": {"width": request.width, "height": request.height},
                "quality": request.quality,
                "style": request.style
            }
            
        except Exception as e:
            logger.error(f"[AI_IMAGE_GENERATION] Error saving image to disk: {e}", exc_info=not IS_PRODUCTION)
            detail_msg = "Could not save generated image." if IS_PRODUCTION else f"Could not save generated image: {str(e)}"
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[AI_IMAGE_GENERATION] Error generating image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "AI image generation failed." if IS_PRODUCTION else f"AI image generation failed: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/add", response_model=DesignTemplateResponse, status_code=status.HTTP_201_CREATED)
async def add_design_template(template_data: DesignTemplateCreate, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "INSERT INTO design_templates (template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created) VALUES ($1, $2, $3, $4, $5, $6) RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, template_data.template_name, template_data.template_structuring_prompt, template_data.design_image_path, template_data.microproduct_type, template_data.component_name, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Design template with this name already exists." if IS_PRODUCTION else f"Design template with name '{template_data.template_name}' already exists."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except Exception as e:
        logger.error(f"Error inserting design template: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding design template." if IS_PRODUCTION else f"DB error on design template insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates", response_model=List[DesignTemplateResponse])
async def get_design_templates_list(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates ORDER BY date_created DESC;"
    try:
        async with pool.acquire() as conn: rows = await conn.fetch(query)
        return [DesignTemplateResponse(**dict(row)) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching design templates: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design templates." if IS_PRODUCTION else f"DB error fetching design templates: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates/{template_id}", response_model=DesignTemplateResponse)
async def get_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates WHERE id = $1;"
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, template_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")
        return DesignTemplateResponse(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design template." if IS_PRODUCTION else f"DB error fetching design template: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/design_templates/update/{template_id}", response_model=DesignTemplateResponse)
async def update_design_template(template_id: int, template_data: DesignTemplateUpdate, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            existing_template_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", template_id)
            if not existing_template_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")

        update_fields = template_data.model_dump(exclude_unset=True)
        if not update_fields:
            return DesignTemplateResponse(**dict(existing_template_row))

        set_clauses = []; update_values = []; i = 1
        for key, value in update_fields.items(): set_clauses.append(f"{key} = ${i}"); update_values.append(value); i += 1
        update_values.append(template_id)
        query = f"UPDATE design_templates SET {', '.join(set_clauses)} WHERE id = ${i} RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"

        async with pool.acquire() as conn: row = await conn.fetchrow(query, *update_values)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Update would violate a unique constraint (e.g., template name)."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating design template." if IS_PRODUCTION else f"DB error on design template update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/design_templates/delete/{template_id}", status_code=status.HTTP_200_OK)
async def delete_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            template_to_delete = await conn.fetchrow("SELECT design_image_path FROM design_templates WHERE id = $1", template_id)
            if not template_to_delete:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found.")

            if template_to_delete["design_image_path"]:
                filename_only = os.path.basename(template_to_delete["design_image_path"])
                full_image_path = os.path.join(STATIC_DESIGN_IMAGES_DIR, filename_only)
                if os.path.exists(full_image_path):
                    try:
                        os.remove(full_image_path)
                        logger.info(f"Successfully deleted image file: {full_image_path}")
                    except OSError as e_img:
                        logger.warning(f"Error deleting image file {full_image_path}: {e_img}. Continuing with DB record deletion.", exc_info=not IS_PRODUCTION)
                else:
                    logger.warning(f"Image file not found for deletion: {full_image_path}")
            deleted_count_status = await conn.execute("DELETE FROM design_templates WHERE id = $1", template_id)
        if deleted_count_status == "DELETE 0":
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found during delete, or already deleted.")
        return {"detail": f"Successfully initiated deletion for design template with ID {template_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during design template deletion." if IS_PRODUCTION else f"DB error on design template deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS = [
    "Training Plan", "PDF Lesson", "Slide Deck", "Text Presentation"
]

# Constants for text size thresholds - Aggressive thresholds to prevent AI memory issues
TEXT_SIZE_THRESHOLD = 1500  # Characters - switch to compression for texts larger than this
LARGE_TEXT_THRESHOLD = 3000  # Characters - use virtual file system to prevent AI memory issues
VIRTUAL_TEXT_FILE_PREFIX = "paste_text_"

# Cache for virtual text files to prevent duplicate uploads
VIRTUAL_TEXT_FILE_CACHE: Dict[str, int] = {}

def compress_text(text_content: str) -> str:
    """
    Compress large text content using gzip and encode as base64.
    This reduces the payload size for large texts.
    """
    try:
        # Compress the text
        text_bytes = text_content.encode('utf-8')
        compressed = gzip.compress(text_bytes)
        # Encode as base64 for JSON transmission
        compressed_b64 = base64.b64encode(compressed).decode('utf-8')
        logger.info(f"Compressed text from {len(text_content)} chars to {len(compressed_b64)} chars (reduction: {(1 - len(compressed_b64)/len(text_content))*100:.1f}%)")
        return compressed_b64
    except Exception as e:
        logger.error(f"Error compressing text: {e}")
        # Return original text if compression fails
        return text_content

def decompress_text(compressed_b64: str) -> str:
    """
    Decompress base64-encoded gzipped text content.
    """
    try:
        # Decode from base64
        compressed = base64.b64decode(compressed_b64.encode('utf-8'))
        # Decompress
        text_bytes = gzip.decompress(compressed)
        return text_bytes.decode('utf-8')
    except Exception as e:
        logger.error(f"Error decompressing text: {e}")
        # Return original if decompression fails (assume it wasn't compressed)
        return compressed_b64

def chunk_text(text_content: str, max_chunk_size: int = 2000) -> List[str]:
    """
    Split large text into manageable chunks while preserving sentence boundaries.
    """
    if len(text_content) <= max_chunk_size:
        return [text_content]
    
    chunks = []
    current_chunk = ""
    
    # Split by sentences first, then by words if needed
    sentences = text_content.replace('\n', ' ').split('. ')
    
    for sentence in sentences:
        # Add period back if it's not the last sentence
        if not sentence.endswith('.') and sentence != sentences[-1]:
            sentence += '.'
        
        # If adding this sentence would exceed chunk size
        if len(current_chunk) + len(sentence) + 1 > max_chunk_size:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                # Single sentence is too long, split by words
                words = sentence.split()
                for word in words:
                    if len(current_chunk) + len(word) + 1 > max_chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            current_chunk = word
                        else:
                            # Single word is too long, truncate
                            chunks.append(word[:max_chunk_size])
                            current_chunk = ""
                    else:
                        current_chunk += " " + word if current_chunk else word
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    logger.info(f"Split text into {len(chunks)} chunks (original: {len(text_content)} chars)")
    return chunks



async def create_virtual_text_file(text_content: str, cookies: Dict[str, str]) -> int:
    """
    Create a virtual text file for large text content and return the file ID.
    Uses caching to prevent duplicate uploads of the same text.
    """
    try:
        # Create a hash of the text content for caching
        import hashlib
        text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
        
        # Check if we already have this text cached
        if text_hash in VIRTUAL_TEXT_FILE_CACHE:
            cached_file_id = VIRTUAL_TEXT_FILE_CACHE[text_hash]
            logger.info(f"Using cached virtual file for text hash {text_hash[:8]}... -> file ID: {cached_file_id}")
            return cached_file_id
        
        # Create a temporary file-like object with the text content
        text_bytes = text_content.encode('utf-8')
        text_file = io.BytesIO(text_bytes)
        
        # Create a filename with timestamp for uniqueness
        timestamp = int(asyncio.get_event_loop().time())
        filename = f"{VIRTUAL_TEXT_FILE_PREFIX}{timestamp}.txt"
        
        # Create FormData for file upload
        files = {
            'files': (filename, text_file, 'text/plain')
        }
        
        # Upload file to Onyx file system
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files
            logger.info(f"Uploading virtual text file: {filename} ({len(text_content)} chars)")
            
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/user/file/upload",
                files=files,
                cookies=cookies
            )
            response.raise_for_status()
            
            # Parse response to get file ID
            upload_result = response.json()
            if not upload_result or len(upload_result) == 0:
                raise HTTPException(status_code=500, detail="No file ID returned from upload response")
            
            file_id = upload_result[0].get('id')
            if not file_id:
                raise HTTPException(status_code=500, detail="Invalid file ID in upload response")
            
            logger.info(f"File uploaded successfully with ID: {file_id}")
            
            # Cache the file ID for this text content
            VIRTUAL_TEXT_FILE_CACHE[text_hash] = file_id
            
            # For text files, we don't need to wait for processing - they're immediately available
            # The 405 error suggests the status endpoint doesn't exist for simple text files
            logger.info(f"Virtual text file ready for use: {file_id}")
            return file_id
                    
    except Exception as e:
        logger.error(f"Error creating virtual text file: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail=f"Failed to create virtual text file: {str(e)}")

# --- Enhanced Hybrid Approach Functions ---

# Cache for file contexts to avoid repeated extraction
FILE_CONTEXT_CACHE: Dict[str, Dict[str, Any]] = {}
FILE_CONTEXT_CACHE_TTL = 3600  # 1 hour cache

async def extract_file_context_from_onyx(file_ids: List[int], folder_ids: List[int], cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract relevant context from files and folders using Onyx's capabilities.
    Returns structured context that can be used with OpenAI.
    """
    try:
        # Create cache key
        cache_key = f"{hash(tuple(sorted(file_ids)))}_{hash(tuple(sorted(folder_ids)))}"
        
        # Check cache first
        if cache_key in FILE_CONTEXT_CACHE:
            cached_data = FILE_CONTEXT_CACHE[cache_key]
            if time.time() - cached_data["timestamp"] < FILE_CONTEXT_CACHE_TTL:
                logger.info(f"[FILE_CONTEXT] Using cached context for key: {cache_key[:16]}...")
                return cached_data["context"]
        
        logger.info(f"[FILE_CONTEXT] Extracting context from {len(file_ids)} files and {len(folder_ids)} folders")
        
        extracted_context = {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {
                "total_files": len(file_ids),
                "total_folders": len(folder_ids),
                "extraction_time": time.time()
            }
        }
        
        # Extract file contexts with enhanced retry mechanism
        successful_extractions = 0
        for file_id in file_ids:
            file_context = None
            for retry_attempt in range(3):  # Up to 3 attempts per file
                try:
                    file_context = await extract_single_file_context(file_id, cookies)
                    if file_context and (file_context.get("summary") or file_context.get("content")):
                        # Check if this was a successful extraction (not a generic response or error)
                        content = file_context.get("content", "")
                        if any(phrase in content.lower() for phrase in ["file access issue", "not indexed", "could not access", "file_access_error"]):
                            logger.warning(f"[FILE_CONTEXT] File {file_id} has access issues (attempt {retry_attempt + 1})")
                            if retry_attempt < 2:  # Don't sleep on the last attempt
                                await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                                continue
                        
                        # Success - add to context
                        extracted_context["file_summaries"].append(file_context["summary"])
                        extracted_context["file_contents"].append(file_context["content"])
                        extracted_context["key_topics"].extend(file_context.get("topics", []))
                        successful_extractions += 1
                        logger.info(f"[FILE_CONTEXT] Successfully extracted context from file {file_id} (attempt {retry_attempt + 1})")
                        break  # Success, no need for more retries
                    else:
                        logger.warning(f"[FILE_CONTEXT] No valid context extracted from file {file_id} (attempt {retry_attempt + 1})")
                        if retry_attempt < 2:  # Don't sleep on the last attempt
                            await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                except Exception as e:
                    logger.warning(f"[FILE_CONTEXT] Failed to extract context from file {file_id} (attempt {retry_attempt + 1}): {e}")
                    if retry_attempt < 2:  # Don't sleep on the last attempt
                        await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
            
            if not file_context or not (file_context.get("summary") or file_context.get("content")):
                logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
        
        # Extract folder contexts
        for folder_id in folder_ids:
            try:
                folder_context = await extract_folder_context(folder_id, cookies)
                if folder_context and folder_context.get("summary"):
                    extracted_context["folder_contexts"].append(folder_context)
                    extracted_context["key_topics"].extend(folder_context.get("topics", []))
                    successful_extractions += 1
                    logger.info(f"[FILE_CONTEXT] Successfully extracted context from folder {folder_id}")
                else:
                    logger.warning(f"[FILE_CONTEXT] No valid context extracted from folder {folder_id}")
            except Exception as e:
                logger.warning(f"[FILE_CONTEXT] Failed to extract context from folder {folder_id}: {e}")
        
        # If no context was extracted successfully, provide a fallback
        if successful_extractions == 0:
            logger.warning(f"[FILE_CONTEXT] No context extracted successfully, providing fallback context")
            extracted_context["file_summaries"] = [f"File(s) provided for content creation (IDs: {file_ids + folder_ids})"]
            extracted_context["key_topics"] = ["content creation", "educational materials"]
            extracted_context["metadata"]["fallback_used"] = True
        
        # Remove duplicate topics
        extracted_context["key_topics"] = list(set(extracted_context["key_topics"]))
        
        # Cache the result
        FILE_CONTEXT_CACHE[cache_key] = {
            "context": extracted_context,
            "timestamp": time.time()
        }
        
        logger.info(f"[FILE_CONTEXT] Successfully extracted context: {len(extracted_context['file_summaries'])} file summaries, {len(extracted_context['key_topics'])} key topics")
        
        return extracted_context
        
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting file context: {e}", exc_info=True)
        return {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {"error": str(e)}
        }

async def extract_single_file_context(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a single file using Onyx's chat API with 100% file attachment guarantee.
    """
    try:
        # Step 1: Verify file exists and is accessible
        file_info = await verify_file_accessibility(file_id, cookies)
        if not file_info:
            return {
                "file_id": file_id,
                "summary": f"File {file_id} is not accessible or does not exist",
                "topics": ["file access error"],
                "key_info": "File may need to be re-uploaded",
                "content": f"File {file_id} access verification failed"
            }
        
        # Step 2: Create a temporary chat session with forced file attachment
        persona_id = await get_contentbuilder_persona_id(cookies)
        temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
        
        # Step 3: Enhanced analysis prompt with explicit file reference
        analysis_prompt = f"""
        I have provided you with file ID {file_id}. This file should be directly attached to this message and available for analysis.
        
        Please analyze this specific file and provide:
        1. A concise summary of the main content (max 200 words)
        2. Key topics and concepts covered
        3. The most important information that would be relevant for content creation
        
        IMPORTANT: 
        - The file is attached to this message with ID {file_id}
        - Do not ask for the file content - it should already be available to you
        - If you cannot see the file content, respond with "FILE_ACCESS_ERROR"
        - If you can see the file content, proceed with the analysis
        
        Format your response as:
        SUMMARY: [summary here]
        TOPICS: [comma-separated topics]
        KEY_INFO: [most important information]
        """
        
        # Step 4: Multiple retry attempts with different strategies
        for attempt in range(3):
            try:
                result = await attempt_file_analysis_with_retry(
                    temp_chat_id, file_id, analysis_prompt, cookies, attempt
                )
                if result and not is_generic_response(result):
                    return parse_analysis_result(file_id, result)
                elif attempt < 2:
                    logger.warning(f"[FILE_CONTEXT] Attempt {attempt + 1} failed for file {file_id}, retrying...")
                    await asyncio.sleep(1)  # Brief delay before retry
                else:
                    logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
                    break
            except Exception as e:
                logger.error(f"[FILE_CONTEXT] Attempt {attempt + 1} error for file {file_id}: {e}")
                if attempt < 2:
                    await asyncio.sleep(1)
                else:
                    raise
        
        # Step 5: Fallback response if all attempts fail
        return {
            "file_id": file_id,
            "summary": f"File analysis failed after multiple attempts (ID: {file_id})",
            "topics": ["analysis error", "file processing"],
            "key_info": "File may need manual review or re-upload",
            "content": f"Analysis failed for file {file_id} ({file_info.get('name', 'Unknown')})"
        }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting single file context for file {file_id}: {e}")
        return {
            "file_id": file_id,
            "summary": f"Error processing file {file_id}: {str(e)}",
            "topics": ["processing error"],
            "key_info": "File processing encountered an error",
            "content": f"Error: {str(e)}"
        }

async def verify_file_accessibility(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Verify that a file exists and is accessible before attempting analysis.
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Check file indexing status using the correct endpoint
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/file/indexing-status?file_ids={file_id}", 
                cookies=cookies
            )
            response.raise_for_status()
            status_data = response.json()
            
            is_indexed = status_data.get(str(file_id), False)
            file_status = "INDEXED" if is_indexed else "NOT_INDEXED"
            
            logger.info(f"[FILE_CONTEXT] File {file_id} indexing status: {file_status}")
            
            # For now, assume the file is accessible if we can check its status
            # The actual file content will be verified during the analysis phase
            return {
                "id": file_id,
                "name": f"file_{file_id}",  # We'll get the real name during analysis
                "status": file_status,
                "accessible": True  # Assume accessible, let the analysis phase handle actual access
            }
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] File accessibility check failed for {file_id}: {e}")
        # Return a basic info structure even if check fails
        return {
            "id": file_id,
            "name": f"file_{file_id}",
            "status": "UNKNOWN",
            "accessible": True  # Let the analysis phase determine actual accessibility
        }

async def attempt_file_analysis_with_retry(
    chat_id: str, 
    file_id: int, 
    prompt: str, 
    cookies: Dict[str, str], 
    attempt: int
) -> str:
    """
    Attempt file analysis with different strategies based on attempt number.
    """
    # Different strategies for each attempt
    strategies = [
        # Attempt 1: Standard approach with user_file_ids
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "never", "real_time": False},
            "force_direct_attachment": True
        },
        # Attempt 2: Force search tool with file-specific query
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Analyze the content of file ID {file_id}"
        },
        # Attempt 3: Use file_descriptors as fallback with search
        {
            "file_descriptors": [{"id": str(file_id), "type": "USER_KNOWLEDGE", "name": f"file_{file_id}"}],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Find and analyze the content of file {file_id}"
        }
    ]
    
    strategy = strategies[attempt]
    
    async with httpx.AsyncClient(timeout=180.0) as client:
        payload = {
            "chat_session_id": chat_id,
            "message": prompt,
            "parent_message_id": None,
            "file_descriptors": strategy.get("file_descriptors", []),
            "user_file_ids": strategy.get("user_file_ids", []),
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": strategy["retrieval_options"],
            "stream_response": True,
            "query_override": strategy.get("query_override")
        }
        
        logger.info(f"[FILE_CONTEXT] Attempt {attempt + 1} for file {file_id} with strategy: {list(strategy.keys())}")
        
        try:
            # Try simple API first
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                json=payload,
                cookies=cookies
            )
            response.raise_for_status()
            result = response.json()
            return result.get("answer", "")
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                # Fallback to streaming endpoint
                return await stream_file_analysis(client, payload, cookies, file_id)
            else:
                raise

async def stream_file_analysis(
    client: httpx.AsyncClient, 
    payload: Dict[str, Any], 
    cookies: Dict[str, str], 
    file_id: int
) -> str:
    """
    Stream file analysis response with enhanced error handling.
    """
    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
        resp.raise_for_status()
        analysis_text = ""
        line_count = 0
        file_mentioned = False
        
        async for raw_line in resp.aiter_lines():
            line_count += 1
            if not raw_line:
                continue
                
            line = raw_line.strip()
            if line.startswith("data:"):
                line = line.split("data:", 1)[1].strip()
                
            if line == "[DONE]":
                logger.info(f"[FILE_CONTEXT] Stream completed for file {file_id} after {line_count} lines")
                break
                
            try:
                pkt = json.loads(line)
                if "answer_piece" in pkt:
                    piece = pkt["answer_piece"].replace("\\n", "\n")
                    analysis_text += piece
                    
                    # Check if file is mentioned in the response
                    if str(file_id) in piece or "file" in piece.lower():
                        file_mentioned = True
                        
            except json.JSONDecodeError:
                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                continue
        
        logger.info(f"[FILE_CONTEXT] Stream processing completed for file {file_id}, "
                   f"total text length: {len(analysis_text)}, file mentioned: {file_mentioned}")
        
        return analysis_text

def is_generic_response(text: str) -> bool:
    """
    Check if the AI response is generic (indicating file access issues).
    """
    generic_phrases = [
        "could you please share the file",
        "please share the file",
        "paste its content",
        "upload the file",
        "provide the file",
        "share the document",
        "i don't see any file",
        "no file was provided",
        "file content is not available",
        "file_access_error",
        "i cannot access",
        "i don't have access to",
        "please provide the content",
        "i wasn't able to access the file",
        "it might be in a format i don't support",
        "could be password-protected",
        "try a different file",
        "proceed based on a general topic",
        "using my knowledge"
    ]
    
    text_lower = text.lower()
    return any(phrase in text_lower for phrase in generic_phrases)

def parse_analysis_result(file_id: int, analysis_text: str) -> Dict[str, Any]:
    """
    Parse the analysis result and extract structured information.
    """
    summary = ""
    topics = []
    key_info = ""
    
    # Log the raw response for debugging
    logger.info(f"[FILE_CONTEXT] Raw analysis response for file {file_id} (length: {len(analysis_text)}): "
               f"{analysis_text[:500]}{'...' if len(analysis_text) > 500 else ''}")
    
    lines = analysis_text.split('\n')
    for line in lines:
        if line.startswith("SUMMARY:"):
            summary = line.replace("SUMMARY:", "").strip()
        elif line.startswith("TOPICS:"):
            topics_text = line.replace("TOPICS:", "").strip()
            topics = [t.strip() for t in topics_text.split(',') if t.strip()]
        elif line.startswith("KEY_INFO:"):
            key_info = line.replace("KEY_INFO:", "").strip()
    
    # If no structured response, try to extract meaningful content
    if not summary and analysis_text.strip():
        # Take first 200 characters as summary if no structured response
        summary = analysis_text.strip()[:200]
        if len(analysis_text) > 200:
            summary += "..."
        logger.info(f"[FILE_CONTEXT] No structured SUMMARY found, using first 200 chars as summary for file {file_id}")
    
    # If still no summary, use a fallback
    if not summary:
        summary = f"File content analyzed successfully (ID: {file_id})"
        logger.warning(f"[FILE_CONTEXT] No summary could be extracted for file {file_id}, using fallback")
    
    return {
        "file_id": file_id,
        "summary": summary,
        "topics": topics,
        "key_info": key_info,
        "content": analysis_text
    }

async def extract_folder_context(folder_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a folder by analyzing its files.
    """
    try:
        # Get folder files
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files like 200-page PDFs
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/folder/{folder_id}",
                cookies=cookies
            )
            response.raise_for_status()
            
            folder_data = response.json()
            files = folder_data.get("files", [])
            
            if not files:
                return {"folder_id": folder_id, "summary": "Empty folder", "topics": []}
            
            # Create a temporary chat session to analyze folder content
            persona_id = await get_contentbuilder_persona_id(cookies)
            temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
            
            # Analyze folder content
            analysis_prompt = f"""
            This folder contains {len(files)} files. Please analyze the overall theme and provide:
            1. A summary of what this folder is about (max 150 words)
            2. Key topics that are covered across all files
            3. The main purpose or theme of this collection
            
            Format your response as:
            SUMMARY: [summary here]
            TOPICS: [comma-separated topics]
            THEME: [main theme or purpose]
            """
            
            file_ids = [f["id"] for f in files if f.get("status") == "INDEXED"]
            
            if not file_ids:
                return {"folder_id": folder_id, "summary": "No indexed files in folder", "topics": []}
            
            payload = {
                "chat_session_id": temp_chat_id,
                "message": analysis_prompt,
                "parent_message_id": None,
                "file_descriptors": [],
                "user_file_ids": file_ids,
                "user_folder_ids": [],
                "prompt_id": None,
                "search_doc_ids": None,
                "retrieval_options": {"run_search": "never", "real_time": False},
                "stream_response": True,
            }
            
            # Try the simple API first, fallback to regular streaming endpoint
            try:
                response = await client.post(
                    f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                    json=payload,
                    cookies=cookies
                )
                response.raise_for_status()
                result = response.json()
                analysis_text = result.get("answer", "")
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404:
                    logger.info(f"[FILE_CONTEXT] Simple API not available, using streaming endpoint for folder {folder_id}")
                    # Fallback to streaming endpoint
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
                        resp.raise_for_status()
                        analysis_text = ""
                        line_count = 0
                        async for raw_line in resp.aiter_lines():
                            line_count += 1
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info(f"[FILE_CONTEXT] Stream completed for folder {folder_id} after {line_count} lines")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    analysis_text += pkt["answer_piece"].replace("\\n", "\n")
                            except json.JSONDecodeError:
                                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                                continue
                        logger.info(f"[FILE_CONTEXT] Stream processing completed for folder {folder_id}, total text length: {len(analysis_text)}")
                else:
                    raise
            
            # Parse the analysis
            summary = ""
            topics = []
            theme = ""
            
            lines = analysis_text.split('\n')
            for line in lines:
                if line.startswith("SUMMARY:"):
                    summary = line.replace("SUMMARY:", "").strip()
                elif line.startswith("TOPICS:"):
                    topics_text = line.replace("TOPICS:", "").strip()
                    topics = [t.strip() for t in topics_text.split(',') if t.strip()]
                elif line.startswith("THEME:"):
                    theme = line.replace("THEME:", "").strip()
            
            return {
                "folder_id": folder_id,
                "folder_name": folder_data.get("name", ""),
                "summary": summary,
                "topics": topics,
                "theme": theme,
                "file_count": len(files)
            }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting folder context for folder {folder_id}: {e}")
        return None

def build_enhanced_prompt_with_context(original_prompt: str, file_context: Dict[str, Any], product_type: str) -> str:
    """
    Build an enhanced prompt that includes the extracted file context for OpenAI.
    """
    enhanced_prompt = f"""
{original_prompt}

--- CONTEXT FROM UPLOADED FILES ---

"""
    
    # Check if fallback was used
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += "NOTE: File context extraction was limited, but files were provided for content creation.\n\n"
    
    # Add file summaries
    if file_context.get("file_summaries"):
        enhanced_prompt += "FILE SUMMARIES:\n"
        for i, summary in enumerate(file_context["file_summaries"], 1):
            enhanced_prompt += f"{i}. {summary}\n"
        enhanced_prompt += "\n"
    
    # Add folder contexts
    if file_context.get("folder_contexts"):
        enhanced_prompt += "FOLDER CONTEXTS:\n"
        for folder_ctx in file_context["folder_contexts"]:
            enhanced_prompt += f"• {folder_ctx.get('folder_name', 'Unknown')}: {folder_ctx.get('summary', '')}\n"
        enhanced_prompt += "\n"
    
    # Add key topics
    if file_context.get("key_topics"):
        enhanced_prompt += f"KEY TOPICS COVERED: {', '.join(file_context['key_topics'])}\n\n"
    
    # Add specific instructions for the product type with enhanced formatting guidance
    if product_type == "Course Outline":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR COURSE OUTLINE:
1. Use exactly this structure: ## Module [Number]: [Module Title]
2. Each module must be a separate H2 header starting with ##
3. Lessons must be numbered list items (1. 2. 3.) under each module

ENSURE: Create the requested number of modules, not a single module with all lessons.
"""
    elif product_type == "Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Lesson Presentation** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly.
"""
    elif product_type == "Video Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]**
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly for proper video lesson processing.
"""
    
    # Add specific instructions for the product type
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += f"""
IMPORTANT: Files were provided for content creation. Create a {product_type} that is relevant to the uploaded materials.
If specific content details are not available, focus on creating high-quality educational content that would be appropriate for the file types provided.
"""
    else:
        enhanced_prompt += f"""
IMPORTANT: Use the context from the uploaded files to create a {product_type} that is relevant and accurate to the source materials. 
Ensure that the content aligns with the topics and information provided in the file summaries and folder contexts.
"""
    
    return enhanced_prompt

async def stream_hybrid_response(prompt: str, file_context: Dict[str, Any], product_type: str, model: str = None):
    """
    Stream response using OpenAI with enhanced context from Onyx file extraction.
    """
    try:
        # Build enhanced prompt with file context
        enhanced_prompt = build_enhanced_prompt_with_context(prompt, file_context, product_type)
        
        logger.info(f"[HYBRID_STREAM] Starting hybrid streaming with enhanced context")
        logger.info(f"[HYBRID_STREAM] Original prompt length: {len(prompt)} chars")
        logger.info(f"[HYBRID_STREAM] Enhanced prompt length: {len(enhanced_prompt)} chars")
        logger.info(f"[HYBRID_STREAM] File context: {len(file_context.get('file_summaries', []))} summaries, {len(file_context.get('key_topics', []))} topics")
        
        # Use OpenAI with enhanced prompt
        async for chunk_data in stream_openai_response(enhanced_prompt, model):
            yield chunk_data
            
    except Exception as e:
        logger.error(f"[HYBRID_STREAM] Error in hybrid streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"Hybrid streaming error: {str(e)}"}

@app.get("/api/custom/microproduct_types", response_model=List[str])
async def get_allowed_microproduct_types_list_for_design_templates():
    return ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS

# --- Project and MicroProduct Endpoints ---
@app.post("/api/custom/projects/add", response_model=ProjectDB, status_code=status.HTTP_201_CREATED)
async def add_project_to_custom_db(project_data: ProjectCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # ---- Guard against duplicate concurrent submissions (same user+project name) ----
    lock_key = f"{onyx_user_id}:{project_data.projectName.strip().lower()}"
    if lock_key in ACTIVE_PROJECT_CREATE_KEYS:
        raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="Project creation already in progress.")
    
    ACTIVE_PROJECT_CREATE_KEYS.add(lock_key)
    
    # Auto-cleanup lock after maximum processing time to prevent deadlocks
    async def cleanup_lock_after_timeout():
        await asyncio.sleep(300)  # 5 minutes max processing time
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)
        logger.warning(f"Auto-cleaned stuck project creation lock: {lock_key}")
    
    # Start cleanup task in background
    asyncio.create_task(cleanup_lock_after_timeout())
    try:
        selected_design_template: Optional[DesignTemplateResponse] = None
        async with pool.acquire() as conn:
            design_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", project_data.design_template_id)
            if not design_row:
                detail_msg = "Design template not found." if IS_PRODUCTION else f"Design Template with ID {project_data.design_template_id} not found."
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=detail_msg)
            selected_design_template = DesignTemplateResponse(**dict(design_row))

        db_microproduct_name_to_store = project_data.microProductName if project_data.microProductName and project_data.microProductName.strip() else selected_design_template.template_name

        target_content_model: Type[BaseModel]
        default_error_instance: BaseModel
        llm_json_example: str
        component_specific_instructions: str

        # Using the long specific instructions from the original user prompt
        if selected_design_template.component_name == COMPONENT_NAME_PDF_LESSON:
            target_content_model = PdfLessonDetails
            default_error_instance = PdfLessonDetails(lessonTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'PDF Lesson' content.
    Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

    **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Capture all information and hierarchical relationships. Maintain original language.

    **Global Fields:**
    1.  `lessonTitle` (string): Main lesson title for the document.
       - Look for patterns like "**Course Name** : **Lesson** : **Lesson Title**" or "**Lesson** : **Lesson Title**"
       - Extract ONLY the lesson title part (the last part after the last "**")
       - For example: "**Code Optimization Course** : **Lesson** : **Introduction to Optimization**" → extract "Introduction to Optimization"
       - For example: "**Lesson** : **JavaScript Basics**" → extract "JavaScript Basics"
       - Do NOT include the course name or "Lesson" label in the title
       - If no clear pattern is found, use the first meaningful title or heading
    2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
    3.  `detectedLanguage` (string): e.g., "en", "ru".

    **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

    1.  **`type: "headline"`**
        * `level` (integer):
            * `1`: Reserved for the main title of a document, usually handled by `lessonTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
            * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
            * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
            * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
        * `text` (string): Headline text.
        * `iconName` (string, optional): Based on level and context as described above.
        * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.


    2.  **`type: "paragraph"`**
        * `text` (string): Full paragraph text.
        * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thoght in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендация', 'Рекомендація' — or their localized equivalents, and isn't a part of the buller list.

    3.  **`type: "bullet_list"`**
        * `items` (array of `ListItem`): Can be strings or other nested content blocks.
        * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

    4.  **`type: "numbered_list"`**
        * `items` (array of `ListItem`):
            * Can be simple strings for basic numbered points.
            * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                * The `items` of this *inner* `bullet_list` should then be:
                    1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                    2. A `paragraph` block (for the main descriptive text).
                    3. Optionally, another `paragraph` block with `isRecommendation: true`.
            * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

    **General Parsing Rules & Icon Names:**
    * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
    * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
    * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
    * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
    * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
    * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

    Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
      • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
      • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
      • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

    Return ONLY the JSON object. 
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            target_content_model = TextPresentationDetails
            default_error_instance = TextPresentationDetails(textTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM # Can reuse this example structure
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string, optional): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):**

            1.  **`type: "headline"`**
                * `level` (integer): `2`, `3`, or `4`.
                * `text` (string): Headline text.
                * `iconName` (string, optional): If the raw text includes an icon name like `{iconName}`, extract it. Permissible Icon Names: `info`, `goal`, `star`, `apple`, `award`, `boxes`, `calendar`, `chart`, `clock`, `globe`.
                * `isImportant` (boolean, optional): If the raw text includes `{isImportant}`, set this to `true`. If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): Set to `true` if this paragraph should be styled as a recommendation (e.g., with a side border).

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be simple strings. Nested lists are supported; you can place a `bullet_list` or `numbered_list` inside another list's `items` array.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`): Can be simple strings or other blocks, including a `bullet_list` for nested content.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.

            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **Key Parsing Rules:**
            *   Parse `{isImportant}` on headlines to the `isImportant` boolean field.
            *   Parse `{iconName}` on headlines to the `iconName` string field.
            *   After extracting `iconName` and `isImportant` values, you MUST remove their corresponding `{...}` tags from the final headline `text` field. The user should not see these tags in the output text.
            *   If a paragraph starts with `**Recommendation:**` (or a localized translation like `**Рекомендация:**`, `**Рекомендація:**`), you MUST set the `isRecommendation` field on that paragraph block to `true` and remove the keyword itself from the final `text` field.
            *   Do NOT remove the `**` from the text for any other purpose; treat it as part of the text. It is critical that you preserve the double-asterisk (`**`) markdown for bold text within all `text` fields.
            *   You are encouraged to use a diverse range of the available `iconName` values to make the presentation visually engaging.
            *   If the raw text starts with `# Title`, this becomes the `textTitle`. The `contentBlocks` should not include this Level 1 headline. All other headlines (`##`, `###`, `####`) are content blocks.
            *   **If the source text contains a Markdown table or tabular data, and the 'tables' style is selected, you MUST output a `table` block as described above. Do NOT output Markdown tables or represent tables as lists or paragraphs.**

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TRAINING_PLAN:
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Parsing Error for {project_data.projectName}", sections=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): Sequential number formatted as "№1", "№2", … Always use this exact format; never "Module 1".
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
+                - IMPORTANT: When the raw text explicitly names the assessment (for example just "Test"), copy that word *exactly*—do not expand it to phrases such as "Knowledge Test", "Proficiency Test", or similar, and do not spell-correct it.
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}. DO NOT use "Content missing" or "Content Coverage:" or similar phrases in the text.

            **Parsing Rules & Constraints:**
+            • Except where explicit transformations are required by these instructions, reproduce every extracted text fragment verbatim — preserving spelling, punctuation, capitalisation, and line breaks. Absolutely do NOT paraphrase, translate, or autocorrect the source text.
            • Detect modules and lessons from headings, tables, or enumerations in the source text. Preserve their original order.
            • Always use dot as decimal separator for `hours` (e.g., 2.5).
            • If `hours` is written as "2 h 30 min", convert to 2.5.
            • Do not create empty arrays; if a module has no lessons, set `lessons: []`.
            • Never output null values for required string fields; use an empty string instead.
            • Ensure that every lesson belongs to a module; do not leave stray lessons.
            • Preserve bold (`**`) or italic (`*`) markdown that exists inside titles or texts.
            • Auxiliary keywords like "Goal", "Outcome", "Assessment" must be translated to the language of the content using the same localization rules described earlier.

            **Validation Checklist BEFORE returning JSON:**
            □ Each module id follows the "№X" pattern.
            □ No lesson titles start with "Lesson X.Y" or similar numbering patterns.
            □ Sum of `hours` in lessons equals `totalHours` if `autoCalculateHours` is false.
            □ Every `check.type` other than "none" has non-empty `text`.
            □ `detectedLanguage` is filled with a 2-letter code.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM  # Force use of new template format
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has 2-3 numerical metrics/statistics with clear values → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - If slide has event dates → use "event-list"
            - If slide has 6 numbered ideas → use "six-ideas-list"
            - If slide has challenges vs solutions → use "challenges-solutions"
            - If slide has analytics metrics in bullet points → use "metrics-analytics"
            - For standard content → use "content-slide"
            
            **CRITICAL TEMPLATE SELECTION RULES:**
            - NEVER use "big-numbers" unless content has exactly 2-3 clear numerical metrics with values, labels, and descriptions
            - NEVER use "metrics-analytics" unless content specifically mentions analytics/performance metrics  
            - If content has bullet points about concepts (not metrics), use "bullet-points" NOT "metrics-analytics"
            - If content mentions "evaluation", "analysis", or has bullet points about tracking/measuring, consider "bullet-points" first
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "steps" array with value/label/description
            - For timeline: parse chronological content into "steps" array
            - For pyramid: parse hierarchical content into "steps" array
            
            **CRITICAL IMAGE PROMPT EXTRACTION - PRESENTATION ILLUSTRATIONS:**
            - ALWAYS extract image prompts from [IMAGE_PLACEHOLDER] sections
            - Format: [IMAGE_PLACEHOLDER: SIZE | POSITION | DESCRIPTION]
            - Map DESCRIPTION to "imagePrompt" and "imageAlt" fields
            - **CRITICAL: Generate extremely detailed, descriptive prompts with specific visual elements**
            - **DETAILED PROMPT FORMAT REQUIREMENTS:**
              - Start with "Minimalist flat design illustration of [detailed subject/scene description]"
              - Include SPECIFIC visual elements: exact objects, people, layouts, arrangements
              - Describe COMPOSITION: positioning, spatial relationships, perspective
              - Detail CHARACTER descriptions: gender, age, clothing, poses, actions
              - Specify OBJECT details: shapes, sizes, orientations, interactions
              - Include ENVIRONMENTAL elements: setting, context, atmosphere
              - Use color placeholders: [COLOR1], [COLOR2], [COLOR3], [BACKGROUND]
              - End with style and background specifications
              - NO separate color descriptions or presentation context
            - **VISUAL ELEMENT REQUIREMENTS:**
              - **People**: Describe gender, ethnicity, age range, specific clothing, poses, facial expressions, interactions
              - **Objects**: Detail size, shape, orientation, material appearance, positioning relative to other elements
              - **Technology**: Specify device types, screen content, interface elements, connection indicators
              - **Architecture**: Describe building styles, structural elements, spatial relationships, interior/exterior details
              - **Data/Charts**: Detail chart types, data representation methods, axis labels, trend indicators
              - **Nature/Abstract**: Specify shapes, patterns, flow directions, organic vs geometric elements
            - **COMPOSITION REQUIREMENTS:**
              - Describe exact positioning: "person sitting on the left side", "laptop positioned at center-right"
              - Include spatial relationships: "behind", "in front of", "surrounding", "connected by"
              - Specify viewing angles: "front view", "three-quarter perspective", "top-down view"
              - Detail background/foreground layering: "foreground elements", "middle ground", "background context"
            - **COLOR PLACEHOLDER USAGE:**
              - [COLOR1] or [PRIMARY]: Main accent color for primary focal elements
              - [COLOR2] or [SECONDARY]: Secondary color for supporting elements, borders, text
              - [COLOR3] or [TERTIARY]: Accent color for details, highlights, subtle elements
              - [BACKGROUND]: Background color for the entire illustration
            - **ENHANCED PROMPT STRUCTURE:**
              - "Minimalist flat design illustration of [comprehensive scene description with 3-5 specific visual elements]. The scene features [detailed character/object descriptions with exact positioning]. [Additional environmental and compositional details]. [Specific color assignments for each visual element using placeholders]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
            - **MANDATORY SCENE STRUCTURING:**
              - ALWAYS describe WHO is in the scene (specific people with demographics, clothing, poses)
              - ALWAYS describe WHERE they are positioned (left, center, right, foreground, background)
              - ALWAYS describe WHAT they are doing (specific actions, interactions, activities)
              - ALWAYS describe the SETTING details (furniture, equipment, environment specifics)
              - ALWAYS describe the LAYOUT (how elements are arranged spatially)
              - NEVER use vague terms like "featuring visual representations" or "playful design"
              - REPLACE abstract descriptions with concrete, observable elements
            - **SIMPLICITY REQUIREMENTS:**
              - LIMIT to 1-2 people maximum per illustration (never 3+ people)
              - SHOW 1-3 main visual elements only (avoid complex multi-panel setups)
              - FOCUS on clean, uncluttered compositions with plenty of white space
              - AVOID crowded scenes with multiple monitors, workstations, or complex layouts
              - PREFER single focal points rather than busy multi-element arrangements
            - **VISUAL ILLUSTRATION REQUIREMENTS:**
              - CREATE scenic illustrations NOT infographics or charts
              - AVOID "featuring icons representing" or "with clear labels" language
              - GENERATE actual scenes with objects, environments, and atmospheres
              - PREFER realistic scenarios over abstract concept representations
              - FOCUS on visual storytelling rather than information display
              - REPLACE "infographic" prompts with "illustration of [actual scene/environment]"
            - **DETAILED SCENE EXAMPLES (SIMPLE COMPOSITIONS):**
              - TEAM COLLABORATION: "two professionals at a clean desk: a Black woman in a blue blazer presenting to an Asian man in glasses who is taking notes on a single laptop, with one simple whiteboard showing basic geometric shapes in the background"
              - TECHNOLOGY SETUP: "a single modern workstation with one large monitor displaying simple geometric charts, a wireless keyboard, and minimal desk accessories"
              - DATA FLOW: "a simple network diagram with three circular nodes connected by clean arrow lines, showing data flow between connected points"
              - EDUCATIONAL SCENE: "one Hispanic female teacher standing next to a single wall chart with simple pictographic elements, facing two students sitting at clean desks"
              - LANGUAGE LEARNING: "one student at a modern desk using a tablet for language learning, with simple educational materials nearby"
            - **TEXT AND LABELING RESTRICTIONS:**
              - MINIMIZE text elements in illustrations - use symbols, icons, and visual indicators instead
              - AVOID readable text, labels, signs, or written content on screens, documents, or displays
              - USE abstract geometric shapes, simple icons, and visual patterns instead of text
              - REPLACE charts with text labels with simple bar charts, pie segments, or geometric data representations
              - AVOID books, documents, or papers with visible text - use blank documents or simple geometric patterns
              - USE color coding and visual hierarchy instead of text labels for differentiation
            - **MANDATORY REQUIREMENTS:**
              - NEVER include "presentation slide" or "for presentations" in the prompt
              - NEVER add separate color descriptions after the main scene description
              - ALWAYS describe at least 3-5 specific visual elements in detail
              - ALWAYS specify exact positioning and spatial relationships
              - ALWAYS include character demographics and specific object details
              - ALWAYS assign colors to specific elements using placeholders
              - MINIMIZE or eliminate text elements - focus on visual symbols and icons
              - NEVER leave imagePrompt fields empty - generate comprehensive, detailed prompts
            - **FORBIDDEN VAGUE LANGUAGE:**
              - NEVER use "featuring visual representations" - describe specific objects instead
              - NEVER use "playful design" - describe specific arrangement and visual elements
              - NEVER use "colorful illustration" - specify who, what, where, and how they're positioned
              - NEVER use "depicting [concept]" - describe the actual scene with people and objects
              - REPLACE abstract concepts with concrete scenes showing people engaged in specific activities
              - REPLACE "showing [topic]" with "scene features [specific people] doing [specific actions] in [specific setting]"
            - **FORBIDDEN INFOGRAPHIC LANGUAGE:**
              - NEVER use "infographic illustrating" - create actual scenic illustrations instead
              - NEVER use "featuring icons representing" - describe real objects and environments
              - NEVER use "with clear labels" or "arranged in a layout" - focus on natural scenes
              - NEVER use "icons for [concepts]" - create realistic environments where concepts occur
              - REPLACE "infographic of [topic]" with "illustration of [people doing topic-related activities in specific environment]"
              - REPLACE "featuring icons" with "scene showing [specific objects, people, and activities]"
            
            **TEMPLATE-SPECIFIC PROPS REQUIREMENTS:**
            
            For "big-image-left" and "big-image-top":
            - "title": Main slide heading
            - "subtitle": Descriptive content (NOT same as title)  
            - "imagePrompt": Detailed description for AI image generation
            - "imageAlt": Alt text for the image
            
            For "bullet-points" and "bullet-points-right":
            - "title": Main heading
            - "bullets": Array of bullet point strings
            - "imagePrompt": Description for supporting image (REQUIRED) - MUST be scenic illustration showing people in real environments, NOT infographics or icons. NEVER leave this field empty or the slide will use generic fallback prompts.
            - "imageAlt": Alt text for image
            
            For "two-column":
            - "title": Main slide title
            - "leftTitle": Left column heading  
            - "rightTitle": Right column heading (NEVER leave empty - always provide meaningful title)
            - "leftContent": Left column text content
            - "rightContent": Right column text content (NEVER leave empty - split slide content between columns)
            - "leftImagePrompt": Image prompt for left column (if applicable)
            - "rightImagePrompt": Image prompt for right column (if applicable)
            - **CRITICAL**: Two-column slides MUST have content in BOTH columns. Split slide content intelligently between left and right sections.
            
            For "big-numbers":
            - "title": Main heading
            - "steps": Array with exactly 3 items, each having:
              - "value": Numerical value or short metric (e.g., "25%", "3x", "$42")
              - "label": Short descriptive label (e.g., "Performance Improvement") 
              - "description": Detailed explanation of the metric
            - **CRITICAL**: ALWAYS provide exactly 3 steps. If slide content has less, expand into 3 logical points. If more than 3, group into 3 main categories.
            
            For "metrics-analytics":
            - "title": Main heading
            - "metrics": Array of metric descriptions (strings)

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title
            - NEVER create duplicate content for title and subtitle - extract different content
            - ALWAYS generate imagePrompt for templates that support images - NEVER leave imagePrompt fields empty
            - CRITICAL: bullet-points and bullet-points-right templates MUST include detailed imagePrompt fields

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM  # Use video lesson template with voiceover
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.
            
            **VIDEO LESSON MODE: You are creating a Video Lesson Presentation with voiceover.**
            - This is NOT a regular slide deck - it's a Video Lesson that requires voiceover for every slide
            - You MUST generate voiceover text for each slide regardless of the input content
            - The voiceover is essential for the video lesson functionality
            - FAILURE TO INCLUDE VOICEOVER WILL RESULT IN AN INVALID OUTPUT
            
            🚨 CRITICAL REQUIREMENT: Every slide object MUST have a "voiceoverText" field with 2-4 sentences of conversational explanation. The root object MUST have "hasVoiceover": true. This is NON-NEGOTIABLE for Video Lesson Presentations.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.
            
            **VIDEO LESSON VOICEOVER REQUIREMENTS:**
            When parsing a Video Lesson Presentation, you MUST include voiceover text for each slide. The voiceover should:
            - Be conversational and engaging, as if speaking directly to the learner
            - Explain the slide content in detail, expanding on what's visually presented
            - Use natural transitions between concepts
            - Be approximately 30-60 seconds of speaking time per slide
            - Include clear explanations of complex concepts
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide
            
            **CRITICAL: You MUST generate voiceover text for EVERY slide in Video Lesson Presentations.**
            - Each slide object MUST include a "voiceoverText" field
            - The voiceover should be 2-4 sentences explaining the slide content
            - Set "hasVoiceover": true in the root object
            - If you don't see voiceover text in the input, GENERATE it based on the slide content
            
            **MANDATORY VOICEOVER GENERATION:**
            - For Video Lesson Presentations, you MUST create voiceover text for EVERY slide
            - Do NOT skip voiceover generation under any circumstances
            - Generate conversational, engaging voiceover that explains the slide content
            - Each voiceover should be 2-4 sentences (approximately 30-60 seconds of speaking time)
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".
            6.  `hasVoiceover` (boolean, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST set this to true since every slide will have voiceover text.

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.
            * `voiceoverText` (string, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST include conversational voiceover text that explains the slide content in detail. This field is REQUIRED for every slide in video lessons.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column" or "two-column-diversity"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has metrics/statistics → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - For standard content → use "content-slide"
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Available Template IDs and their Props (must match exactly):**

            1. **`hero-title-slide`** - Hero opening slides:
            ```json
            "props": {
              "title": "Main slide title",
              "subtitle": "Detailed subtitle explaining the overview",
              "showAccent": true,
              "accentPosition": "left",
              "textAlign": "center",
              "titleSize": "xlarge",
              "subtitleSize": "medium"
            }
            ```

            2. **`title-slide`** - Simple title slides:
            ```json
            "props": {
              "title": "Presentation Title",
              "subtitle": "Compelling subtitle that captures attention",
              "author": "Author name",
              "date": "Date"
            }
            ```

            3. **`content-slide`** - Standard content slides:
            ```json
            "props": {
              "title": "Slide title",
              "content": "Main content with bullet points:\\n\\n• Point 1\\n• Point 2\\n• Point 3",
              "alignment": "left"
            }
            ```

            4. **`bullet-points`** - Formatted bullet point lists:
            ```json
            "props": {
              "title": "Key Points",
              "bullets": [
                "First important point with detailed explanation",
                "Second key insight with comprehensive analysis",
                "Third critical element with thorough examination",
                "Fourth essential consideration with strategic importance",
                "Fifth valuable perspective with actionable recommendations",
                "Sixth valuable perspective with actionable recommendations",
                "Seventh valuable perspective with actionable recommendations"
              ],
              "maxColumns": 2,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            5. **`two-column`** - Split layout:
            ```json
            "props": {
                "title": "Two Column Layout",
                "leftTitle": "Left Column",
                "leftContent": "Content for the left side with detailed explanations",
                "leftImageUrl": "https://via.placeholder.com/320x200?text=Left+Image",
                "leftImageAlt": "Description of left image",
                "leftImagePrompt": "Prompt for left image",
                "rightTitle": "Right Column",
                "rightContent": "Content for the right side with detailed information",
                "rightImageUrl": "https://via.placeholder.com/320x200?text=Right+Image",
                "rightImageAlt": "Description of right image",
                "rightImagePrompt": "Prompt for right image",
                "columnRatio": "50-50"
            }
            ```

            6. **`process-steps`** - Numbered process steps:
            ```json
            "props": {
              "title": "Process Steps",
              "steps": [
                "Step 1 with detailed description explaining what to do",
                "Step 2 with comprehensive explanation covering the process",
                "Step 3 with thorough description of the methodology",
                "Step 4 with in-depth explanation covering the final phase"
              ],
              "layout": "horizontal"
            }
            ```

            

            7. **`challenges-solutions`** - Problems vs solutions:
            ```json
            "props": {
              "title": "Challenges and Solutions",
              "challengesTitle": "Challenges",
              "solutionsTitle": "Solutions",
              "challenges": [
                "Challenge 1 with detailed explanation of the problem",
                "Challenge 2 with comprehensive analysis of the issue"
              ],
              "solutions": [
                "Solution 1 with detailed approach and implementation strategy",
                "Solution 2 with comprehensive methodology and practical steps"
              ]
            }
            ```

            8. **`big-image-left`** - Large image on left:
            ```json
            "props": {
              "title": "Slide Title",
              "subtitle": "Subtitle or detailed description for the slide",
              "imageUrl": "https://via.placeholder.com/600x400?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration that visually represents the slide title",
              "imageSize": "large"
            }
            ```

            9. **`bullet-points-right`** - Title, subtitle, bullet points with image:
            ```json
            "props": {
              "title": "Key Points",
              "subtitle": "Short intro or context before the list",
              "bullets": [
                "First important point",
                "Second key insight",
                "Third critical element",
                "Fourth essential consideration",
                "Fifth valuable perspective"
              ],
              "maxColumns": 1,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            10. **`big-image-top`** - Large image on top:
            ```json
            "props": {
              "title": "Main Title",
              "subtitle": "Subtitle or content goes here",
              "imageUrl": "https://via.placeholder.com/700x350?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration for the topic",
              "imageSize": "large"
            }
            ```

            11. **`four-box-grid`** - Title and 4 boxes in 2x2 grid:
            ```json
            "props": {
              "title": "Main Title",
              "boxes": [
                { "heading": "Box 1", "text": "Detailed description with comprehensive explanations" },
                { "heading": "Box 2", "text": "Comprehensive explanation covering detailed insights" },
                { "heading": "Box 3", "text": "Thorough description spanning multiple sentences" },
                { "heading": "Box 4", "text": "In-depth explanation with actionable insights" }
              ]
            }
            ```

            12. **`timeline`** - Horizontal timeline with 4 steps:
            ```json
            "props": {
              "title": "History and Evolution",
              "steps": [
                { "heading": "Step 1", "description": "Detailed description of the first phase" },
                { "heading": "Step 2", "description": "Comprehensive explanation of the second phase" },
                { "heading": "Step 3", "description": "Thorough description of the third phase" },
                { "heading": "Step 4", "description": "In-depth explanation of the final phase" }
              ]
            }
            ```

            13. **`big-numbers`** - Three-column layout for metrics:
            ```json
            "props": {
              "title": "Key Metrics",
              "steps": [
                { "value": "25%", "label": "Performance Improvement", "description": "System performance improved by 25% after optimization" },
                { "value": "3x", "label": "Speed Increase", "description": "Processing speed increased 3 times faster than before" },
                { "value": "50%", "label": "Cost Reduction", "description": "Operating costs reduced by 50% through efficient design" }
              ]
            }
            ```

            14. **`pyramid`** - Pyramid diagram with 3 levels:
            ```json
            "props": {
              "title": "Hierarchical Structure",
              "subtitle": "Explanation of the hierarchical relationship between elements",
              "steps": [
                { "heading": "Top Level", "description": "Description of the highest level" },
                { "heading": "Middle Level", "description": "Description of the intermediate level" },
                { "heading": "Base Level", "description": "Description of the foundational level" }
              ]
            }
            ```

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "items" array with value/label/description
            - For timeline: parse chronological content into "steps" array
            - For pyramid: parse hierarchical content into "steps" array

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON:
            target_content_model = VideoLessonData
            default_error_instance = VideoLessonData(
                mainPresentationTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
            {
  "mainPresentationTitle": "Курс: Обучение для рекрутера",
  "slides": [
    {
      "slideId": "slide_1_znakomstvo",
      "slideNumber": 1,
      "slideTitle": "Знакомство",
      "displayedText": "Знакомимся с основами рекрутинга и ключевыми обязанностями.",
      "displayedPictureDescription": "Улыбающиеся профессионалы в современном офисе.",
      "displayedVideoDescription": "Анимация воронки рекрутинга: поиск, отбор, интервью, оффер.",
      "voiceoverText": "Приветствую вас на курсе 'Обучение для рекрутера'! Начнем с основ. Этот модуль посвящен ключевым аспектам профессии."
    },
    {
      "slideId": "slide_2_instrumenty",
      "slideNumber": 2,
      "slideTitle": "Инструменты Рекрутера",
      "displayedText": "Рассматриваем основные инструменты для современного рекрутера.",
      "displayedPictureDescription": "Коллаж логотипов: LinkedIn, ATS, GitHub, поиск, календарь.",
      "displayedVideoDescription": "Анимация кликов по иконкам инструментов с краткими пояснениями их функций.",
      "voiceoverText": "Для успеха рекрутеру нужен арсенал инструментов. Рассмотрим основные категории и их назначение. Эффективное использование повысит вашу производительность."
    }
  ],
  "detectedLanguage": "ru"
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant. Your task is to convert the provided presentation slide content, which is in a specific structured text format, into a perfectly structured JSON object.

Your output MUST be a single, valid JSON object, and it must strictly adhere to the exact structure provided in the example JSON you have been given separately. Do not include any additional text, explanations, or conversational fillers outside the JSON object.

Input Text Structure and Extraction Rules:
The input text will describe a presentation or video lesson. The content within the fields (like slide titles, descriptions) can be in various languages (e.g., Ukrainian, Russian, English). You must extract the content exactly as it appears, preserving its original language, including any original formatting like line breaks within the content where present (e.g., in "Відображуваний Текст").

Overall Presentation Title:

This will be identified by a header like "Загальний Заголовок Курсу:" (or its equivalent in other languages like "ОБЩИЙ ЗАГОЛОВОК КУРСА:" or "Overall Course Title:").
Extract the text that immediately follows this bolded header as the value for the mainPresentationTitle field.
Individual Slides:

Each slide's information is clearly marked by consistently bolded headers.
slideNumber (integer): Look for "Номер Слайда:" (or equivalent, e.g., "Номер Слайда:", "Slide Number:"). Extract the numerical value that immediately follows this bolded header.
slideTitle (string): Look for "Заголовок Слайда:" (or equivalent, e.g., "Заголовок Слайда:", "Slide Title:"). Extract the text that immediately follows this bolded header.
displayedText (string): Look for "Відображуваний Текст:" (or equivalent, e.g., "Відображуваний Текст:", "Displayed Text:"). Extract all text that immediately follows this bolded header, up until the next bolded header. Preserve any internal line breaks or numbering.
displayedPictureDescription (string): Look for "Опис Зображення:" (or equivalent, e.g., "Опис Зображення:", "Image Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
displayedVideoDescription (string): Look for "Опис Відео:" (or equivalent, e.g., "Опис Відео:", "Video Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
voiceoverText (string): Look for "Текст Озвучення:" (or equivalent, e.g., "Текст Озвучення:", "Voiceover Text:"). Extract the text that immediately follows this bolded header, up until the next bolded header or the end of the slide's content block.
slideId Generation:

For each slide, you must generate a unique slideId.
This ID should be a concatenation of the literal string "slide_", the slideNumber, and a simplified, lowercase version of the slideTitle.
To simplify the slideTitle for the ID, convert it to lowercase and replace all spaces with underscores (_). Remove any punctuation or special characters from the simplified title part of the ID. If the title is very long, consider using only the first few words to keep the ID concise, but ensure uniqueness. For example:
slideNumber: 1, slideTitle: "Вступ" -> slideId: "slide_1_вступ"
slideNumber: 2, slideTitle: "Питання 1" -> slideId: "slide_2_питання_1"
slideNumber: 3, slideTitle: "Варіанти відповіді" -> slideId: "slide_3_варіанти_відповіді"
slideNumber: 4, slideTitle: "Пояснення до Питання 1" -> slideId: "slide_4_пояснення_до_питання_1"
detectedLanguage (string):

This will be identified by a header like "Language of Content:" (or its equivalent, e.g., "Язык Контента:", "Мова Контенту:").
Extract the two-letter ISO 639-1 language code (e.g., "uk", "ru", "en") that immediately follows this label.
If this "Language of Content:" label is missing from the input, infer the primary language from the majority of the content (specifically the mainPresentationTitle and slideTitle fields) and use the appropriate two-letter ISO 639-1 code.
Key Parsing Rules & Constraints for 100% Reliability:

Header Recognition: Always identify fields by their bolded headers (e.g., "Номер Слайда:", "Заголовок Слайда:"). These bolded headers consistently precede the data you need to extract.
Exact Text Extraction: All extracted text content must be preserved exactly as it appears in the input, including its original capitalization, punctuation, and line breaks within the content block for a given field.
Field Presence: If a field's bolded header is present in the input but the text following it is empty before the next header, the corresponding JSON field should be an empty string (""). Do not use null or omit fields that are defined as strings in the target schema if their labels are present in the input.
Sequential Parsing: Process the text sequentially, extracting content associated with each bolded header until the next bolded header is encountered.
Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_QUIZ:
            target_content_model = QuizData
            default_error_instance = QuizData(
                quizTitle=f"LLM Parsing Error for {project_data.projectName}",
                questions=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
{
"quizTitle": "Advanced Sales Techniques Quiz",
"detectedLanguage": "en",
"questions": [
{
"question_type": "multiple-choice",
"question_text": "Which technique involves assuming the sale is made?",
"options": [
{"id": "A", "text": "The 'Question Close'"},
{"id": "B", "text": "The 'Presumptive Close'"}
],
"correct_option_id": "B",
"explanation": "A presumptive close assumes the sale is made."
},
{
"question_type": "multi-select",
"question_text": "Which of the following are primary colors? (Select all that apply)",
"options": [
{"id": "A", "text": "Red"},
{"id": "B", "text": "Green"},
{"id": "C", "text": "Orange"},
{"id": "D", "text": "Blue"}
],
"correct_option_ids": ["A", "D"],
"explanation": "In the traditional subtractive model, the primary colors are Red, Yellow, and Blue."
},
{
"question_type": "matching",
"question_text": "Match each sales technique with its description:",
"prompts": [
{"id": "A", "text": "The 'Alternative Close'"},
{"id": "B", "text": "The 'Summary Close"}
],
"options": [
{"id": "1", "text": "Presenting two options to the customer"},
{"id": "2", "text": "Recapping key benefits before asking for the sale"}
],
"correct_matches": {"A": "1", "B": "2"},
"explanation": "The Alternative Close gives customers a choice between options, while the Summary Close reinforces value before closing."
},
{
"question_type": "sorting",
"question_text": "Arrange these steps in the correct order for a successful sales call:",
"items_to_sort": [
{"id": "step1", "text": "Identify customer needs"},
{"id": "step2", "text": "Present solution"},
{"id": "step3", "text": "Handle objections"},
{"id": "step4", "text": "Close the sale"}
],
"correct_order": ["step1", "step2", "step3", "step4"],
"explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
},
{
"question_type": "open-answer",
"question_text": "What are the three key elements of an effective elevator pitch?",
"acceptable_answers": [
"Problem, Solution, Call to Action",
"Problem statement, Your solution, What you want them to do next",
"The issue, How you solve it, What action to take"
],
"explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
}
]
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Quiz' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the provided quiz content into a structured JSON object that captures all questions, their types, options, correct answers, and explanations.

            **Global Fields:**
            1. `quizTitle` (string): The main title of the quiz.
            2. `questions` (array): An array of question objects.
            3. `detectedLanguage` (string): e.g., "en", "ru".

            **Question Types and Their Structures:**

            1. **Multiple Choice (`question_type: "multiple-choice"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_id` (string): The ID of the correct option.
               * `explanation` (string, optional): Explanation of the correct answer.

            2. **Multi-Select (`question_type: "multi-select"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_ids` (array): Array of IDs of all correct options.
               * `explanation` (string, optional): Explanation of the correct answers.

            3. **Matching (`question_type: "matching"`)**
               * `question_text` (string): The question text.
               * `prompts` (array): List of `MatchingPrompt` objects with `id` and `text`.
               * `options` (array): List of `MatchingOption` objects with `id` and `text`.
               * `correct_matches` (object): Maps prompt IDs to option IDs.
               * `explanation` (string, optional): Explanation of the correct matches.

            4. **Sorting (`question_type: "sorting"`)**
               * `question_text` (string): The question text.
               * `items_to_sort` (array): List of `SortableItem` objects with `id` and `text`.
               * `correct_order` (array): Array of item IDs in the correct sequence.
               * `explanation` (string, optional): Explanation of the correct order.

            5. **Open Answer (`question_type: "open-answer"`)**
               * `question_text` (string): The question text.
               * `acceptable_answers` (array): List of acceptable answer strings.
               * `explanation` (string, optional): Explanation or additional context.

            **Key Parsing Rules:**
            1. Each question must have a unique type and appropriate fields for that type.
            2. Option IDs should be consistent (e.g., "A", "B", "C" for multiple choice).
            3. Maintain original language and formatting in all text fields.
            4. Include explanations where available to help users understand correct answers.
            5. Ensure all required fields are present for each question type.
            6. Validate that correct answers reference valid option IDs.

            Return ONLY the JSON object.
            """
        else:
            logger.warning(f"Unknown component_name '{selected_design_template.component_name}' for DT ID {selected_design_template.id}. Defaulting to TrainingPlanDetails for parsing.")
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Config Error for {project_data.projectName}", sections=[])
            llm_json_example = DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = "Parse the content according to the JSON example provided."


        if hasattr(default_error_instance, 'detectedLanguage'):
                default_error_instance.detectedLanguage = detect_language(project_data.aiResponse)

        parsed_content_model_instance = await parse_ai_response_with_llm(
            ai_response=project_data.aiResponse,
            project_name=project_data.projectName,
            target_model=target_content_model,
            default_error_model_instance=default_error_instance,
            dynamic_instructions=component_specific_instructions,
            target_json_example=llm_json_example
        )

        logger.info(f"LLM Parsing Result Type: {type(parsed_content_model_instance).__name__}")
        logger.info(f"LLM Parsed Content (first 200 chars): {str(parsed_content_model_instance.model_dump_json())[:200]}") # Use model_dump_json()

        # Inject theme for slide decks from the finalize request
        if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
            hasattr(parsed_content_model_instance, 'theme') and 
            hasattr(project_data, 'theme') and 
            project_data.theme):
            parsed_content_model_instance.theme = project_data.theme
            logger.info(f"Injected theme '{project_data.theme}' into slide deck")

        # Post-process module IDs for training plans to ensure № character is preserved
        if hasattr(parsed_content_model_instance, 'sections') and parsed_content_model_instance.sections:
            for section in parsed_content_model_instance.sections:
                if hasattr(section, 'id') and section.id:
                    # Fix module IDs that lost the № character
                    if section.id.isdigit():
                        # Plain number like "2" -> "№2"
                        section.id = f"№{section.id}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                    elif section.id.startswith("#"):
                        # Hash format like "#2" -> "№2"
                        number = section.id[1:]
                        section.id = f"№{number}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                    elif not section.id.startswith("№"):
                        # Other formats without № - try to extract number and format correctly
                        import re
                        number_match = re.search(r'\d+', section.id)
                        if number_match:
                            number = number_match.group()
                            section.id = f"№{number}"
                            logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed ID format to '{section.id}'")

        # Apply slide prop normalization for slide decks and video lesson presentations
        if (selected_design_template.component_name in [COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION] and 
            hasattr(parsed_content_model_instance, 'slides') and 
            parsed_content_model_instance.slides):
            
            # Normalize slide props to fix schema mismatches
            slides_dict = [slide.model_dump() if hasattr(slide, 'model_dump') else dict(slide) for slide in parsed_content_model_instance.slides]
            normalized_slides = normalize_slide_props(slides_dict, selected_design_template.component_name)
            
            # Update the content with normalized slides
            content_dict = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            content_dict['slides'] = normalized_slides
            
            # Remove hasVoiceover flag for regular slide decks
            if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
                'hasVoiceover' in content_dict):
                logger.info("Removing hasVoiceover flag for regular slide deck")
                content_dict.pop('hasVoiceover', None)
            
            content_to_store_for_db = content_dict
            
            logger.info(f"Applied slide prop normalization for {len(normalized_slides)} slides")
        else:
            content_to_store_for_db = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            
        derived_product_type = selected_design_template.microproduct_type
        derived_microproduct_type = selected_design_template.template_name

        logger.info(f"Content prepared for DB storage (first 200 chars of JSON): {str(content_to_store_for_db)[:200]}")

        # Determine if this is a standalone product (default to True for general project creation)
        # For specific products like quizzes, this will be overridden in their dedicated endpoints
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_product = project_data.outlineId is None
        
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id;
    """

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                project_data.projectName,
                derived_product_type,
                derived_microproduct_type,
                db_microproduct_name_to_store,
                content_to_store_for_db,
                project_data.design_template_id,
                project_data.chatSessionId,
                is_standalone_product,
                project_data.folder_id
            )
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create project entry.")

        db_content_dict = row["microproduct_content"]
        final_content_for_response: Optional[MicroProductContentType] = None
        if db_content_dict and isinstance(db_content_dict, dict):
            component_name_from_db = selected_design_template.component_name
            try:
                if component_name_from_db == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_response = PdfLessonDetails(**db_content_dict)
                    logger.info("Re-parsed as PdfLessonDetails.")
                elif component_name_from_db == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_response = TextPresentationDetails(**db_content_dict)
                    logger.info("Re-parsed as TextPresentationDetails.")
                elif component_name_from_db == COMPONENT_NAME_TRAINING_PLAN:
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    final_content_for_response = TrainingPlanDetails(**db_content_dict)
                    logger.info("Re-parsed as TrainingPlanDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_response = VideoLessonData(**db_content_dict)
                    logger.info("Re-parsed as VideoLessonData.")
                elif component_name_from_db == COMPONENT_NAME_QUIZ:
                    final_content_for_response = QuizData(**db_content_dict)
                    logger.info("Re-parsed as QuizData.")
                elif component_name_from_db == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails (Video Lesson Presentation).")
                else:
                    logger.warning(f"Unknown component_name '{component_name_from_db}' when re-parsing content from DB on add. Attempting generic TrainingPlanDetails fallback.")
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    final_content_for_response = TrainingPlanDetails(**db_content_dict)
            except Exception as e_parse:
                logger.error(f"Error parsing content from DB on add (proj ID {row['id']}): {e_parse}", exc_info=not IS_PRODUCTION)

        return ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_response,
            design_template_id=row["design_template_id"], created_at=row["created_at"]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error inserting project: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding project." if IS_PRODUCTION else f"DB error on project insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        # Always release the in-flight lock
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)


@app.get("/api/custom/projects/names", response_model=List[str], summary="Get unique project names for the user")
async def get_distinct_project_names_for_user(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT DISTINCT project_name
        FROM projects
        WHERE onyx_user_id = $1
        ORDER BY project_name ASC;
        """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        project_names: List[str] = [str(row["project_name"]) for row in rows if row["project_name"] is not None]
        return project_names
    except Exception as e:
        logger.error(f"Error fetching distinct project names for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project names." if IS_PRODUCTION else f"Database error while fetching project names: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/projects/{project_id}/edit", response_model=ProjectDetailForEditResponse)
async def get_project_details_for_edit(project_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT
            p.id, p.project_name, p.microproduct_name, p.microproduct_content, p.created_at,
            p.design_template_id, dt.template_name as design_template_name,
            dt.component_name as design_component_name,
            dt.design_image_path as design_image_path,
            p.product_type, p.microproduct_type
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND p.onyx_user_id = $2;
    """
    try:
        async with pool.acquire() as conn: row = await conn.fetchrow(query, project_id, onyx_user_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found.")

        row_dict = dict(row)
        db_content_json = row_dict.get("microproduct_content")
        parsed_content_for_response: Optional[MicroProductContentType] = None
        component_name = row_dict.get("design_component_name")

        # Round hours to integers in the content before parsing
        if db_content_json and isinstance(db_content_json, dict):
            db_content_json = round_hours_in_content(db_content_json)
            
        if db_content_json and isinstance(db_content_json, dict):
            try:
                if component_name == COMPONENT_NAME_PDF_LESSON:
                    parsed_content_for_response = PdfLessonDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    parsed_content_for_response = TextPresentationDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TRAINING_PLAN:
                    parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON:
                    parsed_content_for_response = VideoLessonData(**db_content_json)
                elif component_name == COMPONENT_NAME_QUIZ:
                    parsed_content_for_response = QuizData(**db_content_json)
                elif component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                else:
                    logger.warning(f"Unknown component_name '{component_name}' for project {project_id}. Trying fallbacks.", exc_info=not IS_PRODUCTION)
                    try: parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                    except:
                        try: parsed_content_for_response = PdfLessonDetails(**db_content_json)
                        except Exception as e_parse_fallback: logger.error(f"Fallback parsing failed for project {project_id}: {e_parse_fallback}", exc_info=not IS_PRODUCTION)
            except Exception as e_main_parse:
                logger.error(f"Pydantic validation error for DB JSON (project {project_id}, component {component_name}): {e_main_parse}", exc_info=not IS_PRODUCTION)
        elif isinstance(db_content_json, str) and component_name == COMPONENT_NAME_TRAINING_PLAN:
                parsed_content_for_response = parse_training_plan_from_string(db_content_json, row_dict["project_name"])

        return ProjectDetailForEditResponse(
            id=row_dict["id"], projectName=row_dict["project_name"], microProductName=row_dict.get("microproduct_name"),
            design_template_id=row_dict.get("design_template_id"), microProductContent=parsed_content_for_response,
            createdAt=row_dict.get("created_at"), design_template_name=row_dict.get("design_template_name"),
            design_component_name=component_name, design_image_path=row_dict.get("design_image_path")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching project {project_id} for edit: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project details." if IS_PRODUCTION else f"DB error fetching project details for edit: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/projects", response_model=List[ProjectApiResponse])
async def get_user_projects_list_from_db(
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool),
    folder_id: Optional[int] = None
):
    select_query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type,
               p.folder_id, p."order", p.microproduct_content, p.source_chat_session_id, p.is_standalone
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 {folder_filter}
        ORDER BY p."order" ASC, p.created_at DESC;
    """
    folder_filter = ""
    params = [onyx_user_id]
    if folder_id is not None:
        folder_filter = "AND p.folder_id = $2"
        params.append(folder_id)
    query = select_query.format(folder_filter=folder_filter)
    async with pool.acquire() as conn:
        db_rows = await conn.fetch(query, *params)
    projects_list: List[ProjectApiResponse] = []
    for row_data in db_rows:
        row_dict = dict(row_data)
        project_slug = create_slug(row_dict.get('project_name'))
        # Convert UUID to string if it exists
        source_chat_session_id = row_dict.get("source_chat_session_id")
        if source_chat_session_id:
            source_chat_session_id = str(source_chat_session_id)
        
        projects_list.append(ProjectApiResponse(
            id=row_dict["id"], projectName=row_dict["project_name"], projectSlug=project_slug,
            microproduct_name=row_dict.get("microproduct_name"),
            design_template_name=row_dict.get("design_template_name"),
            design_microproduct_type=row_dict.get("design_microproduct_type"),
            created_at=row_dict["created_at"], design_template_id=row_dict.get("design_template_id"),
            folder_id=row_dict.get("folder_id"), order=row_dict.get("order"),
            microproduct_content=row_dict.get("microproduct_content"),
            source_chat_session_id=source_chat_session_id,
            is_standalone=row_dict.get("is_standalone")
        ))
    return projects_list

@app.get("/api/custom/projects/view/{project_id}", response_model=MicroProductApiResponse, responses={404: {"model": ErrorDetail}})
async def get_project_instance_detail(project_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    select_query = """
        SELECT p.*, dt.template_name as design_template_name, dt.microproduct_type as design_microproduct_type, dt.component_name
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND p.onyx_user_id = $2
    """
    async with pool.acquire() as conn:
        row = await conn.fetchrow(select_query, project_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Project not found")
    row_dict = dict(row)
    project_instance_name = row_dict.get("microproduct_name") or row_dict.get("project_name")
    project_slug = create_slug(project_instance_name)
    component_name = row_dict.get("component_name")
    details_data = row_dict.get("microproduct_content")
    
    # 🔍 BACKEND VIEW LOGGING: What we retrieved from database for view
    logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Raw details_data type: {type(details_data)}")
    
    # Parse the details_data if it's a JSON string
    parsed_details = None
    if details_data:
        if isinstance(details_data, str):
            try:
                # Parse JSON string to dict
                details_dict = json.loads(details_data)
                # Round hours to integers before returning
                details_dict = round_hours_in_content(details_dict)
                parsed_details = details_dict
                logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Parsed from JSON string: {json.dumps(parsed_details, indent=2)}")
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Failed to parse microproduct_content JSON for project {project_id}: {e}")
                parsed_details = None
        else:
            # Already a dict, just round hours
            parsed_details = round_hours_in_content(details_data)
            logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Already dict, after round_hours: {json.dumps(parsed_details, indent=2)}")
    
    # 🔍 BACKEND VIEW RESULT LOGGING
    if parsed_details and 'contentBlocks' in parsed_details:
        image_blocks = [block for block in parsed_details['contentBlocks'] if block.get('type') == 'image']
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Final image blocks for frontend: {json.dumps(image_blocks, indent=2)}")
    else:
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - No contentBlocks in parsed_details or parsed_details is None")
    
    web_link_path = None
    pdf_link_path = None
    return MicroProductApiResponse(
        name=project_instance_name, slug=project_slug, project_id=project_id,
        design_template_id=row_dict["design_template_id"], component_name=component_name,
        webLinkPath=web_link_path, pdfLinkPath=pdf_link_path, details=parsed_details,
        sourceChatSessionId=row_dict.get("source_chat_session_id"),
        parentProjectName=row_dict.get('project_name'),
        custom_rate=row_dict.get("custom_rate"),
        quality_tier=row_dict.get("quality_tier")
        # folder_id is not in MicroProductApiResponse, but can be added if needed
    )

@app.get("/api/custom/pdf/folder/{folder_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_folder_as_pdf(
    folder_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download all products in a folder as a single PDF, ordered by type and creation date."""
    try:
        # First, verify the folder exists and belongs to the user
        async with pool.acquire() as conn:
            folder_row = await conn.fetchrow(
                """
                SELECT name FROM project_folders 
                WHERE id = $1 AND onyx_user_id = $2;
                """,
                folder_id, onyx_user_id
            )
        if not folder_row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Folder not found for user.")

        folder_name = folder_row['name']
        
        # Get all projects in the folder, ordered by their position in the folder
        async with pool.acquire() as conn:
            projects = await conn.fetch(
                """
                SELECT p.id, p.project_name, p.microproduct_name, p.microproduct_content,
                       p.created_at, dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                ORDER BY p."order" ASC, p.created_at ASC;
                """,
                folder_id, onyx_user_id
            )
        
        if not projects:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="No projects found in folder.")
        
        # Generate individual PDFs for each project
        pdf_paths = []
        project_names = []
        
        for project in projects:
            project_id = project['id']
            project_name = project['microproduct_name'] or project['project_name']
            content_json = project['microproduct_content']
            component_name = project['design_component_name']
            
            # Skip unsupported project types
            if component_name not in ['TextPresentationDisplay', 'TrainingPlanTable']:
                continue
            
            try:
                # Generate PDF for this project using existing logic
                mp_name_for_pdf_context = project_name
                content_json = project['microproduct_content']
                component_name = project['design_component_name']
                data_for_template_render: Optional[Dict[str, Any]] = None
                pdf_template_file: str

                detected_lang_for_pdf = 'ru'  # Default language
                if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
                    detected_lang_for_pdf = content_json.get('detectedLanguage')
                elif mp_name_for_pdf_context:
                    detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
                
                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

                if component_name == 'TextPresentationDisplay':
                    pdf_template_file = "text_presentation_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        data_for_template_render = json.loads(json.dumps(content_json))
                        if not data_for_template_render.get('detectedLanguage'):
                            data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                        
                        # Log content blocks for debugging image issues
                        content_blocks = data_for_template_render.get('contentBlocks', [])
                        image_blocks = [block for block in content_blocks if block.get('type') == 'image']
                        
                        logger.info(f"🖼️ [PDF GEN] Processing {len(content_blocks)} content blocks, {len(image_blocks)} image blocks")
                        for i, block in enumerate(image_blocks):
                            logger.info(f"🖼️ [PDF GEN] Image block {i}: {json.dumps(block, indent=2)}")
                            if hasattr(block, 'keys'):
                                logger.info(f"🖼️ [PDF GEN] Image block {i} keys: {list(block.keys())}")
                            if 'src' in block:
                                logger.info(f"🖼️ [PDF GEN] Image block {i} src: '{block['src']}' (type: {type(block['src'])})")
                            else:
                                logger.info(f"🚨 [PDF GEN] Image block {i} missing 'src' property!")
                                
                    else:
                        data_for_template_render = {
                            "title": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "contentBlocks": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                elif component_name == 'TrainingPlanTable':
                    pdf_template_file = "training_plan_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        try:
                            content_json = round_hours_in_content(content_json)
                            parsed_model = TrainingPlanDetails(**content_json)
                            if parsed_model.detectedLanguage:
                                detected_lang_for_pdf = parsed_model.detectedLanguage
                                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                            
                            data_for_template_render = {
                                'mainTitle': parsed_model.mainTitle,
                                'sections': parsed_model.sections,
                                'detectedLanguage': detected_lang_for_pdf
                            }
                        except Exception as e:
                            logger.error(f"Error parsing training plan for project {project_id}: {e}")
                            data_for_template_render = {
                                "mainTitle": f"Error: {mp_name_for_pdf_context}",
                                "sections": [],
                                "detectedLanguage": detected_lang_for_pdf
                            }
                    else:
                        data_for_template_render = {
                            "mainTitle": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "sections": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                else:
                    continue  # Skip unsupported types
                
                if not isinstance(data_for_template_render, dict):
                    data_for_template_render = {"title": "Error", "contentBlocks": [], "detectedLanguage": "en"}
                
                unique_output_filename = f"folder_export_{folder_id}_project_{project_id}_{uuid.uuid4().hex[:8]}.pdf"
                
                context_for_jinja = {
                    'details': data_for_template_render,
                    'locale': current_pdf_locale_strings,
                    'pdf_context': {
                        'static_images_path': os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
                    }
                }
                
                pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
                if os.path.exists(pdf_path):
                    pdf_paths.append(pdf_path)
                    project_names.append(project_name)
                
            except Exception as e:
                logger.error(f"Error generating PDF for project {project_id}: {e}")
                continue
        
        if not pdf_paths:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs could be generated for projects in folder.")
        
        # Combine PDFs into a single file
        try:
            if PdfMerger is None:
                # If PyPDF2 is not available, return the first PDF as a fallback
                logger.warning("PyPDF2 not available, returning first PDF as fallback")
                if pdf_paths:
                    user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    return FileResponse(
                        path=pdf_paths[0],
                        filename=user_friendly_filename,
                        media_type='application/pdf',
                        headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
                    )
                else:
                    raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs generated and PyPDF2 not available")
            
            merger = PdfMerger()
            
            for pdf_path in pdf_paths:
                merger.append(pdf_path)
            
            combined_pdf_path = f"/tmp/folder_export_{folder_id}_{uuid.uuid4().hex[:8]}.pdf"
            merger.write(combined_pdf_path)
            merger.close()
            
            # Clean up individual PDF files
            for pdf_path in pdf_paths:
                try:
                    os.remove(pdf_path)
                except:
                    pass
            
            user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            return FileResponse(
                path=combined_pdf_path,
                filename=user_friendly_filename,
                media_type='application/pdf',
                headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
            )
            
        except Exception as e:
            logger.error(f"Error combining PDFs for folder {folder_id}: {e}")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to combine PDFs")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating folder PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate folder PDF: {str(e)[:200]}")
    

# Streaming slide deck PDF generation with progress updates
@app.get("/api/custom/pdf/slide-deck/{project_id}/stream")
async def stream_slide_deck_pdf_generation(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Stream slide deck PDF generation with progress updates"""
    from fastapi.responses import StreamingResponse
    import json
    
    async def generate_with_progress():
        try:
            # Get project data (same as the existing endpoint)
            async with pool.acquire() as conn:
                target_row_dict = await conn.fetchrow(
                    """
                    SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                           dt.component_name as design_component_name
                    FROM projects p
                    LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                    WHERE p.id = $1 AND p.onyx_user_id = $2;
                    """,
                    project_id, onyx_user_id
                )
            
            if not target_row_dict:
                yield f"data: {json.dumps({'error': 'Project not found'})}\n\n"
                return

            component_name = target_row_dict.get("design_component_name")
            if component_name != COMPONENT_NAME_SLIDE_DECK:
                yield f"data: {json.dumps({'error': 'This endpoint is only for slide deck projects'})}\n\n"
                return

            content_json = target_row_dict.get('microproduct_content')
            if not content_json or not isinstance(content_json, dict):
                yield f"data: {json.dumps({'error': 'Invalid slide deck content'})}\n\n"
                return

            # Prepare slide deck data
            slide_deck_data = {
                'slides': content_json.get('slides', []),
                'theme': theme or 'dark-purple'
            }

            total_slides = len(slide_deck_data['slides'])
            yield f"data: {json.dumps({'type': 'progress', 'message': f'Starting PDF generation for {total_slides} slides...', 'current': 0, 'total': total_slides})}\n\n"

            mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
            unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
            
            # Generate PDF with regular function and send progress updates
            from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
            
            # Send intermediate progress messages
            yield f"data: {json.dumps({'type': 'progress', 'message': 'Calculating slide dimensions...', 'current': 1, 'total': total_slides})}\n\n"
            
            # Simulate progress for user feedback during long operation
            import asyncio
            
            # Start PDF generation in background and send periodic updates
            pdf_task = asyncio.create_task(generate_slide_deck_pdf_with_dynamic_height(
                slides_data=slide_deck_data['slides'],
                theme=theme,
                output_filename=unique_output_filename,
                use_cache=True
            ))
            
            # Send progress updates while PDF is generating
            progress_step = 0
            max_steps = total_slides * 2  # Simulate steps for dimension calc + generation
            
            while not pdf_task.done():
                await asyncio.sleep(2)  # Update every 2 seconds
                progress_step += 1
                current_progress = min(progress_step, max_steps - 1)
                
                if progress_step <= total_slides:
                    message = f"Calculating dimensions for slide {progress_step}..."
                else:
                    slide_num = progress_step - total_slides
                    message = f"Generating slide {slide_num}..."
                
                yield f"data: {json.dumps({'type': 'progress', 'message': message, 'current': current_progress, 'total': max_steps})}\n\n"
            
            # Wait for PDF generation to complete
            pdf_path = await pdf_task
            
            # Send final progress update
            yield f"data: {json.dumps({'type': 'progress', 'message': 'PDF generation completed!', 'current': max_steps, 'total': max_steps})}\n\n"
                
            # Final success message with download info - FIXED: Return filename instead of URL that triggers regeneration
            user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
            final_message = {
                'type': 'complete',
                'message': 'PDF generation completed successfully!',
                'download_url': f'/pdf/slide-deck/{project_id}/download/{os.path.basename(pdf_path)}?theme={theme}',
                'filename': user_friendly_pdf_filename
            }
            yield f"data: {json.dumps(final_message)}\n\n"
            
        except Exception as e:
            logger.error(f"Error in streaming PDF generation: {e}", exc_info=True)
            error_message = {
                'type': 'error',
                'message': f'PDF generation failed: {str(e)[:200]}'
            }
            yield f"data: {json.dumps(error_message)}\n\n"

    return StreamingResponse(
        generate_with_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

# New endpoint to serve cached PDFs without regeneration
@app.get("/api/custom/pdf/slide-deck/{project_id}/download/{pdf_filename}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_cached_slide_deck_pdf(
    project_id: int,
    pdf_filename: str,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download cached slide deck PDF without regeneration"""
    try:
        # Verify the project exists and user has access
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        # Construct the path to the cached PDF
        from app.services.pdf_generator import PDF_CACHE_DIR
        pdf_path = PDF_CACHE_DIR / pdf_filename
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="PDF file not found. It may have expired or been deleted.")
        
        # Create user-friendly filename
        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
        
        return FileResponse(
            path=str(pdf_path), 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error serving cached slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to serve PDF: {str(e)[:200]}")

# Move slide deck route BEFORE the general route to avoid path conflicts
@app.get("/api/custom/pdf/slide-deck/{project_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_slide_deck_pdf(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download slide deck as multi-page PDF"""
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        if not content_json or not isinstance(content_json, dict):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slide deck content.")

        # Prepare slide deck data for PDF generation
        slide_deck_data = {
            'lessonTitle': content_json.get('lessonTitle', mp_name_for_pdf_context),
            'slides': content_json.get('slides', []),
            'theme': theme,
            'detectedLanguage': content_json.get('detectedLanguage', 'en')
        }

        # Validate slides structure
        if not isinstance(slide_deck_data['slides'], list):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slides structure.")

        logger.info(f"Slide Deck PDF Gen (Project {project_id}): Generating PDF with {len(slide_deck_data['slides'])} slides, theme: {theme}")

        # ✅ NEW: Detailed logging for slide data before PDF generation
        logger.info(f"=== SLIDE DATA ANALYSIS BEFORE PDF GENERATION ===")
        logger.info(f"Project ID: {project_id}")
        logger.info(f"Total slides: {len(slide_deck_data['slides'])}")
        logger.info(f"Theme: {theme}")
        
        # Analyze each slide for big-image-left template
        big_image_left_slides = []
        for i, slide in enumerate(slide_deck_data['slides']):
            if slide.get('templateId') == 'big-image-left':
                big_image_left_slides.append((i, slide))
                logger.info(f"Found big-image-left slide at index {i}")
                
                # Log slide structure
                logger.info(f"  Slide {i} structure:")
                logger.info(f"    templateId: {slide.get('templateId')}")
                logger.info(f"    slideId: {slide.get('slideId')}")
                logger.info(f"    props keys: {list(slide.get('props', {}).keys())}")
                logger.info(f"    metadata keys: {list(slide.get('metadata', {}).keys()) if slide.get('metadata') else 'None'}")
                
                # Log text content
                props = slide.get('props', {})
                logger.info(f"    title: '{props.get('title', 'NOT SET')}'")
                logger.info(f"    subtitle: '{props.get('subtitle', 'NOT SET')}'")
                
                # Log image info without base64 data
                image_path = props.get('imagePath', '')
                if image_path:
                    if image_path.startswith('data:'):
                        logger.info(f"    imagePath: [BASE64 DATA URL - {len(image_path)} characters]")
                    else:
                        logger.info(f"    imagePath: {image_path}")
                else:
                    logger.info(f"    imagePath: NOT SET")
                
                # Log positioning data
                metadata = slide.get('metadata', {})
                element_positions = metadata.get('elementPositions', {})
                logger.info(f"    elementPositions exists: {bool(element_positions)}")
                if element_positions:
                    logger.info(f"    elementPositions keys: {list(element_positions.keys())}")
                    
                    # Check for title and subtitle positions
                    slide_id = slide.get('slideId', 'unknown')
                    title_id = f'draggable-{slide_id}-0'
                    subtitle_id = f'draggable-{slide_id}-1'
                    
                    title_pos = element_positions.get(title_id)
                    subtitle_pos = element_positions.get(subtitle_id)
                    
                    logger.info(f"    title element ID: {title_id}")
                    logger.info(f"    title position: {title_pos}")
                    logger.info(f"    subtitle element ID: {subtitle_id}")
                    logger.info(f"    subtitle position: {subtitle_pos}")
        
        logger.info(f"Total big-image-left slides found: {len(big_image_left_slides)}")
        logger.info(f"=== END SLIDE DATA ANALYSIS ===")

        # Prepare template context
        context_for_jinja = {
            'details': slide_deck_data
        }

        unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Generate PDF using the new dynamic height slide deck generation
        from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
        
        pdf_path = await generate_slide_deck_pdf_with_dynamic_height(
            slides_data=slide_deck_data['slides'],
            theme=theme,
            output_filename=unique_output_filename,
            use_cache=True
        )
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate slide deck PDF: {str(e)[:200]}")


@app.post("/api/custom/pdf/debug/slides", response_class=JSONResponse)
async def debug_slide_generation(
    slides_data: List[Dict[str, Any]],
    theme: Optional[str] = Query("light-modern"),
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Debug endpoint to test individual slide generation and identify problematic slides."""
    try:
        from app.services.pdf_generator import test_all_slides_individually
        
        logger.info(f"Debug slide generation: Testing {len(slides_data)} slides with theme: {theme}")
        
        # Test all slides individually
        summary = await test_all_slides_individually(slides_data, theme)
        
        return JSONResponse(content=summary)
        
    except Exception as e:
        logger.error(f"Error in debug slide generation: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Debug failed: {str(e)[:200]}")


@app.get("/api/custom/pdf/{project_id}/", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf_no_slug(
    project_id: int,
    # all other parameters as in the main function
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    # Just call the main function with a default slug
    return await download_project_instance_pdf(
        project_id=project_id,
        document_name_slug="-",  # or any default value
        parentProjectName=parentProjectName,
        lessonNumber=lessonNumber,
        knowledgeCheck=knowledgeCheck,
        contentAvailability=contentAvailability,
        informationSource=informationSource,
        time=time,
        estCompletionTime=estCompletionTime,
        qualityTier=qualityTier,
        onyx_user_id=onyx_user_id,
        pool=pool,
    )    


@app.get("/api/custom/pdf/{project_id}/{document_name_slug}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf(
    project_id: int,
    document_name_slug: str,
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    print("OPTIONAL DATA:", parentProjectName, lessonNumber)
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        component_name = target_row_dict.get("design_component_name")
        data_for_template_render: Optional[Dict[str, Any]] = None
        pdf_template_file: str

        detected_lang_for_pdf = 'ru'  # Default language
        if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
            detected_lang_for_pdf = content_json.get('detectedLanguage')
        elif mp_name_for_pdf_context: # Fallback if not in content_json
            detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
        
        # Get the locale strings for the detected language, defaulting to 'en' if not found
        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

        logger.info(f"Project {project_id} PDF Gen: Raw content_json from DB (type: {type(content_json)}). First 1000 chars: {str(content_json)[:1000]}")

        if component_name == COMPONENT_NAME_PDF_LESSON:
            pdf_template_file = "pdf_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                logger.info(f"Project {project_id} PDF Gen (PDF LESSON): Using raw content_json directly for template.")
                data_for_template_render = json.loads(json.dumps(content_json)) 
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model_for_fallback_lang = PdfLessonDetails(**content_json)
                        if parsed_model_for_fallback_lang and parsed_model_for_fallback_lang.detectedLanguage:
                            detected_lang_for_pdf = parsed_model_for_fallback_lang.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                logger.warning(f"Project {project_id} PDF Gen (PDF LESSON): content_json is not a valid dict or is None. Using fallback structure.")
                data_for_template_render = {
                    "lessonTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf}
        elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            pdf_template_file = "text_presentation_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                data_for_template_render = {
                    "textTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_TRAINING_PLAN:
            pdf_template_file = "training_plan_pdf_template.html"
            temp_dumped_dict = None
            if content_json and isinstance(content_json, dict):
                try:
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json type: {type(content_json)}")
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json keys: {list(content_json.keys()) if isinstance(content_json, dict) else 'Not a dict'}")
                    if 'sections' in content_json:
                        logger.info(f"PDF Gen (Proj {project_id}): sections type: {type(content_json['sections'])}, length: {len(content_json['sections']) if isinstance(content_json['sections'], list) else 'Not a list'}")
                    
                    # Round hours to integers before parsing to prevent float validation errors
                    content_json = round_hours_in_content(content_json)
                    
                    parsed_model = TrainingPlanDetails(**content_json)
                    logger.info(f"PDF Gen (Proj {project_id}): Parsed model sections length: {len(parsed_model.sections)}")
                    
                    if parsed_model.detectedLanguage: 
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    
                    # Calculate completion time for each section
                    for section in parsed_model.sections:
                        total_completion_minutes = 0
                        for lesson in section.lessons:
                            if lesson.completionTime:
                                time_str = str(lesson.completionTime).strip()
                                if time_str and time_str != '':
                                    # Extract numeric part using regex to handle all language units (m, м, хв)
                                    import re
                                    numbers = re.findall(r'\d+', time_str)
                                    if numbers:
                                        try:
                                            # If it contains 'h' (hour indicator), convert to minutes
                                            if 'h' in time_str.lower():
                                                total_completion_minutes += int(numbers[0]) * 60
                                            else:
                                                # For minutes (m, м, хв), just use the number
                                                total_completion_minutes += int(numbers[0])
                                        except (ValueError, IndexError):
                                            total_completion_minutes += 5  # Fallback to 5 minutes
                        
                        # Add the calculated completion time to the section
                        section.totalCompletionTime = total_completion_minutes
                    
                    temp_dumped_dict = parsed_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"PDF Gen (Proj {project_id}): Dumped dict sections length: {len(temp_dumped_dict.get('sections', []))}")
                    data_for_template_render = json.loads(json.dumps(temp_dumped_dict))
                    logger.info(f"PDF Gen (Proj {project_id}): Final data sections length: {len(data_for_template_render.get('sections', []))}")
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for TrainingPlan (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
            if data_for_template_render is None:
                 logger.warning(f"Project {project_id} PDF Gen (TRAINING PLAN): data_for_template_render is None. Using fallback.")
                 data_for_template_render = {"mainTitle": f"Content Error: {mp_name_for_pdf_context}", "sections": [], "detectedLanguage": detected_lang_for_pdf}
            
            current_lang_cfg_main = LANG_CONFIG.get(detected_lang_for_pdf, LANG_CONFIG['ru']) # Using main LANG_CONFIG for units
            data_for_template_render['time_unit_singular'] = current_lang_cfg_main.get('TIME_UNIT_SINGULAR', 'h')
            data_for_template_render['time_unit_decimal_plural'] = current_lang_cfg_main.get('TIME_UNIT_DECIMAL_PLURAL', 'h')
            data_for_template_render['time_unit_general_plural'] = current_lang_cfg_main.get('TIME_UNIT_GENERAL_PLURAL', 'h')
        elif component_name == COMPONENT_NAME_VIDEO_LESSON: # Updated logic for Video Lesson
            pdf_template_file = "video_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model = VideoLessonData(**content_json)
                        if parsed_model.detectedLanguage:
                            detected_lang_for_pdf = parsed_model.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass 
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                else: # If language IS in content_json, ensure locale strings match
                    detected_lang_for_pdf = data_for_template_render.get('detectedLanguage', detected_lang_for_pdf)
                    current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

            else:
                data_for_template_render = {
                    "mainPresentationTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "slides": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_QUIZ: # Quiz handling
            pdf_template_file = "quiz_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                try:
                    parsed_model = QuizData(**content_json)
                    if parsed_model.detectedLanguage:
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    data_for_template_render = parsed_model.model_dump(mode='json', exclude_none=True)
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for Quiz (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
                    data_for_template_render = {
                        "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                        "questions": [],
                        "detectedLanguage": detected_lang_for_pdf
                    }
            else:
                data_for_template_render = {
                    "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "questions": [],
                    "detectedLanguage": detected_lang_for_pdf
                }
        else:
            logger.warning(f"PDF: Unknown component_name '{component_name}' for project {project_id}. Defaulting to simple PDF Lesson structure.")
            pdf_template_file = "pdf_lesson_pdf_template.html" # Or a generic template
            data_for_template_render = {
                "lessonTitle": f"Unknown Content Type: {mp_name_for_pdf_context}",
                "contentBlocks": [{"type":"paragraph", "text":"The content type of this project is not configured for PDF export."}],
                "detectedLanguage": detected_lang_for_pdf
            }

        if not isinstance(data_for_template_render, dict):
             logger.critical(f"Project {project_id} PDF Gen: data_for_template_render is NOT A DICT ({type(data_for_template_render)}) before final context prep.")
             data_for_template_render = {"lessonTitle": "Critical Data Preparation Error", "contentBlocks": [], "detectedLanguage": "en"}
             # Ensure locale is set for critical error case
             current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS['en']


        if isinstance(data_for_template_render, dict):
            logger.info(f"Project {project_id} PDF Gen: Starting deep inspection of data_for_template_render (to be passed as 'details' in template context)...")
            inspect_list_items_recursively(data_for_template_render.get('contentBlocks', []), "data_for_template_render.contentBlocks")

        unique_output_filename = f"{project_id}_{document_name_slug}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Pass the locale strings to the template context
        static_images_abs_path = os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Static images path: {static_images_abs_path}")
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Path exists: {os.path.exists(static_images_abs_path.rstrip('/'))}")
        
        context_for_jinja = {
            'details': data_for_template_render, 
            'locale': current_pdf_locale_strings,
            'parentProjectName': parentProjectName,
            'lessonNumber': lessonNumber,
            'pdf_context': {
                'static_images_path': static_images_abs_path
            }
        }
        
        # 🔍 PDF CONTEXT LOGGING: What we're passing to the template
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Full context keys: {list(context_for_jinja.keys())}")
        
        # Log image blocks without transforming them (let PDF generator handle the transformation)
        if 'details' in context_for_jinja and isinstance(context_for_jinja['details'], dict) and 'contentBlocks' in context_for_jinja['details']:
            content_blocks = context_for_jinja['details']['contentBlocks']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Processing {len(content_blocks)} content blocks")
            
            image_blocks = [block for block in content_blocks if block.get('type') == 'image']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Found {len(image_blocks)} image blocks")
            for img_block in image_blocks:
                original_src = img_block.get('src', '')
                logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Image block src (before PDF generation): {original_src}")
                if original_src.startswith('/static_design_images/'):
                    filename = original_src.replace('/static_design_images/', '')
                    full_path = static_images_abs_path + filename
                    logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Expected file path: {full_path} (exists: {os.path.exists(full_path)})")
        else:
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - No contentBlocks found in details")
        
        # Add column visibility settings for Training Plan PDFs
        if component_name == COMPONENT_NAME_TRAINING_PLAN:
            column_visibility = {
                'knowledgeCheck': knowledgeCheck == '1' if knowledgeCheck else True,
                'contentAvailability': contentAvailability == '1' if contentAvailability else True,
                'informationSource': informationSource == '1' if informationSource else True,
                'time': time == '1' if time else True,
                'estCompletionTime': estCompletionTime == '1' if estCompletionTime else True,
                'qualityTier': qualityTier == '1' if qualityTier else False,  # Hidden by default
            }
            context_for_jinja['columnVisibility'] = column_visibility

        logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']: {type(context_for_jinja.get('details'))}")
        if isinstance(context_for_jinja.get('details'), dict) and isinstance(context_for_jinja['details'].get('details'), dict):
            final_cb_source = context_for_jinja['details']['details']
            final_cb_type = type(final_cb_source.get('contentBlocks'))
            logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']['details']['contentBlocks']: {final_cb_type}")
            if isinstance(final_cb_source.get('contentBlocks'), list):
                 for block_idx, block_item_final_check in enumerate(final_cb_source.get('contentBlocks', [])):
                    if isinstance(block_item_final_check, dict) and block_item_final_check.get('type') in ('bullet_list', 'numbered_list'):
                        items_final_check_type = type(block_item_final_check.get('items'))
                        if not isinstance(block_item_final_check.get('items'), list):
                            logger.error(f"Project {project_id} PDF Gen: CRITICAL - 'items' in block_item_final_check for block #{block_idx} is STILL NOT A LIST (type: {items_final_check_type}) just before Jinja render.")
            elif final_cb_type is not None: # if it's not None and not a list
                logger.error(f"Project {project_id} PDF Gen: CRITICAL - context_for_jinja['details']['details']['contentBlocks'] is NOT A LIST (type: {final_cb_type}) just before Jinja render.")

        pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        return FileResponse(path=pdf_path, filename=user_friendly_pdf_filename, media_type='application/pdf', headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"})
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in PDF endpoint for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during PDF generation." if IS_PRODUCTION else f"Error during PDF generation: {str(e)[:200]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/projects/delete-multiple", status_code=status.HTTP_200_OK)
async def delete_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided."})

    project_ids_to_trash = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # If scope is 'all', find all associated lesson projects for any Training Plans
            if delete_request.scope == 'all':
                for project_id in delete_request.project_ids:
                    # Fetch outline project name
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM projects WHERE id=$1 AND onyx_user_id=$2",
                        project_id, onyx_user_id
                    )
                    if not row:
                        continue
                    outline_name: str = row["project_name"]
                    # Treat both 'Training Plan' and 'Course Outline' as outline types
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        # Not an outline – nothing extra to move
                        continue

                    # Select IDs of all projects whose name equals outline_name OR starts with outline_name + ': '
                    pattern = outline_name + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM projects WHERE onyx_user_id=$1 AND (project_name = $2 OR project_name LIKE $3)",
                        onyx_user_id, outline_name, pattern
                    )
                    for lr in lesson_rows:
                        project_ids_to_trash.add(lr["id"])

            if not project_ids_to_trash:
                 return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            # First, fetch all the data we need to move to trash
            projects_to_trash = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(project_ids_to_trash), onyx_user_id)

            if not projects_to_trash:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_trash:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into trashed_projects with safe values
                    await conn.execute("""
                        INSERT INTO trashed_projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from projects table
                result_status = await conn.execute(
                    "DELETE FROM projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(project_ids_to_trash), onyx_user_id
                )
        
        deleted_count_match = re.search(r"DELETE\s+(\d+)", result_status)
        deleted_count = int(deleted_count_match.group(1)) if deleted_count_match else 0
        
        logger.info(f"User {onyx_user_id} moved IDs {list(project_ids_to_trash)} to trash. Count: {deleted_count}.")
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully moved {deleted_count} project(s) to trash."})

    except Exception as e:
        logger.error(f"Error moving projects to trash for user {onyx_user_id}, IDs {delete_request.project_ids}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while sending projects to trash." if IS_PRODUCTION else f"Database error during trash operation: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Analytics Endpoints ---

@app.get("/api/custom/analytics/dashboard", response_model=Dict[str, Any])
async def get_analytics_dashboard(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get comprehensive analytics dashboard data"""
    import json
    
    print(f"=== DASHBOARD DEBUG: Incoming parameters ===")
    print(f"date_from: {date_from}")
    print(f"date_to: {date_to}")
    print(f"=== END PARAMETERS ===")

    try:
        # DEBUG: Print the latest 10 rows from request_analytics
        async with pool.acquire() as conn:
            debug_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, created_at FROM request_analytics ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: Latest 10 rows from request_analytics ===")
            for row in debug_rows:
                print(dict(row))
            print("=== END DEBUG ===")
            
            # DEBUG: Check specifically for AI parser records
            ai_parser_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at FROM request_analytics WHERE is_ai_parser_request = true ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: AI Parser records from request_analytics ===")
            for row in ai_parser_rows:
                print(dict(row))
            print(f"Total AI parser records found: {len(ai_parser_rows)}")
            print("=== END AI PARSER DEBUG ===")
            
            # DEBUG: Check if the columns exist and have any data
            column_check = await conn.fetch(
                "SELECT column_name, data_type, is_nullable, column_default FROM information_schema.columns WHERE table_name = 'request_analytics' ORDER BY ordinal_position"
            )
            print("=== DEBUG: All request_analytics columns check ===")
            for row in column_check:
                print(dict(row))
            print("=== END COLUMN CHECK ===")
            
            # DEBUG: Check for any records with non-null ai_parser fields
            any_ai_parser_data = await conn.fetch(
                "SELECT id, endpoint, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name FROM request_analytics WHERE is_ai_parser_request IS NOT NULL OR ai_parser_tokens IS NOT NULL OR ai_parser_model IS NOT NULL OR ai_parser_project_name IS NOT NULL ORDER BY created_at DESC LIMIT 5"
            )
            print("=== DEBUG: Any AI parser data ===")
            for row in any_ai_parser_data:
                print(dict(row))
            print(f"Total records with any AI parser data: {len(any_ai_parser_data)}")
            print("=== END ANY AI PARSER DATA ===")
    except Exception as e:
        print(f"DEBUG ERROR: Could not fetch request_analytics: {e}")

    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        print(f"=== DASHBOARD DEBUG: Filter and params ===")
        print(f"where_clause: {where_clause}")
        print(f"params: {params}")
        print(f"=== END FILTER ===")

        async with pool.acquire() as conn:
            # Overall statistics
            stats_query = f"""
                SELECT 
                    COUNT(*) as total_requests,
                    COUNT(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 END) as successful_requests,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as failed_requests,
                    COUNT(CASE WHEN error_message IS NOT NULL THEN 1 END) as error_requests,
                    AVG(response_time_ms) as avg_response_time,
                    MAX(response_time_ms) as max_response_time,
                    MIN(response_time_ms) as min_response_time,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data_transferred,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT endpoint) as unique_endpoints,
                    COUNT(CASE WHEN is_ai_parser_request THEN 1 END) as ai_parser_requests,
                    AVG(ai_parser_tokens) as avg_ai_parser_tokens,
                    MAX(ai_parser_tokens) as max_ai_parser_tokens,
                    MIN(ai_parser_tokens) as min_ai_parser_tokens,
                    SUM(ai_parser_tokens) as total_ai_parser_tokens
                FROM request_analytics
                {where_clause}
            """
            print(f"=== DASHBOARD DEBUG: Stats query ===")
            print(f"Query: {stats_query}")
            print(f"Params: {params}")
            stats_row = await conn.fetchrow(stats_query, *params)
            print(f"Stats result: {dict(stats_row) if stats_row else 'None'}")
            
            # Debug AI parser specific data
            if stats_row:
                print(f"=== AI PARSER DEBUG ===")
                print(f"ai_parser_requests: {stats_row['ai_parser_requests']}")
                print(f"avg_ai_parser_tokens: {stats_row['avg_ai_parser_tokens']}")
                print(f"max_ai_parser_tokens: {stats_row['max_ai_parser_tokens']}")
                print(f"min_ai_parser_tokens: {stats_row['min_ai_parser_tokens']}")
                print(f"total_ai_parser_tokens: {stats_row['total_ai_parser_tokens']}")
                print(f"=== END AI PARSER DEBUG ===")
            
            print(f"=== END STATS ===")
            
            # Status code distribution
            status_query = f"""
                SELECT 
                    status_code,
                    COUNT(*) as count,
                    AVG(response_time_ms) as avg_time
                FROM request_analytics
                {where_clause}
                GROUP BY status_code
                ORDER BY count DESC
            """
            print(f"=== DASHBOARD DEBUG: Status query ===")
            print(f"Query: {status_query}")
            print(f"Params: {params}")
            status_rows = await conn.fetch(status_query, *params)
            print(f"Status rows count: {len(status_rows)}")
            print(f"Status results: {[dict(row) for row in status_rows]}")
            print(f"=== END STATUS ===")
            
            # Top endpoints by request count
            endpoints_query = f"""
                SELECT 
                    endpoint,
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data
                FROM request_analytics
                {where_clause}
                GROUP BY endpoint, method
                ORDER BY request_count DESC
                LIMIT 20
            """
            endpoints_rows = await conn.fetch(endpoints_query, *params)
            
            # Top users by request count
            users_query = f"""
                SELECT 
                    user_id,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    MAX(created_at) as last_request
                FROM request_analytics
                {where_clause}
                {"AND user_id IS NOT NULL" if where_clause else "WHERE user_id IS NOT NULL"}
                GROUP BY user_id
                ORDER BY request_count DESC
                LIMIT 20
            """
            users_rows = await conn.fetch(users_query, *params)
            
            # Hourly distribution
            hourly_query = f"""
                SELECT 
                    EXTRACT(HOUR FROM created_at) as hour,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time
                FROM request_analytics
                {where_clause}
                GROUP BY EXTRACT(HOUR FROM created_at)
                ORDER BY hour
            """
            hourly_rows = await conn.fetch(hourly_query, *params)
            
            # Daily distribution
            daily_query = f"""
                SELECT 
                    DATE(created_at) as date,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY DATE(created_at)
                ORDER BY date DESC
                LIMIT 30
            """
            daily_rows = await conn.fetch(daily_query, *params)
            
            # Method distribution
            method_query = f"""
                SELECT 
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY method
                ORDER BY request_count DESC
            """
            method_rows = await conn.fetch(method_query, *params)
            
            # Recent errors
            errors_query = f"""
                SELECT 
                    id,
                    endpoint,
                    method,
                    status_code,
                    response_time_ms,
                    error_message,
                    user_id,
                    created_at
                FROM request_analytics
                {where_clause}
                {"AND (error_message IS NOT NULL OR status_code >= 400)" if where_clause else "WHERE error_message IS NOT NULL OR status_code >= 400"}
                ORDER BY created_at DESC
                LIMIT 50
            """
            errors_rows = await conn.fetch(errors_query, *params)
            
            # Performance percentiles
            percentile_query = f"""
                SELECT 
                    percentile_cont(0.5) WITHIN GROUP (ORDER BY response_time_ms) as p50,
                    percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95,
                    percentile_cont(0.99) WITHIN GROUP (ORDER BY response_time_ms) as p99
                FROM request_analytics
                {where_clause}
            """
            percentile_row = await conn.fetchrow(percentile_query, *params)

        response_data = {
            "overview": {
                "total_requests": stats_row["total_requests"],
                "successful_requests": stats_row["successful_requests"],
                "failed_requests": stats_row["failed_requests"],
                "error_requests": stats_row["error_requests"],
                "success_rate": round((stats_row["successful_requests"] / stats_row["total_requests"]) * 100, 2) if stats_row["total_requests"] > 0 else 0,
                "avg_response_time": round(stats_row["avg_response_time"], 2) if stats_row["avg_response_time"] else 0,
                "max_response_time": stats_row["max_response_time"],
                "min_response_time": stats_row["min_response_time"],
                "total_data_transferred": stats_row["total_data_transferred"],
                "unique_users": stats_row["unique_users"],
                "unique_endpoints": stats_row["unique_endpoints"],
                "ai_parser_requests": stats_row["ai_parser_requests"] or 0,
                "avg_ai_parser_tokens": round(stats_row["avg_ai_parser_tokens"], 2) if stats_row["avg_ai_parser_tokens"] else 0,
                "max_ai_parser_tokens": stats_row["max_ai_parser_tokens"] or 0,
                "min_ai_parser_tokens": stats_row["min_ai_parser_tokens"] or 0,
                "total_ai_parser_tokens": stats_row["total_ai_parser_tokens"] or 0
            },
            "status_distribution": [{"status_code": row["status_code"], "count": row["count"], "avg_time": round(row["avg_time"], 2) if row["avg_time"] else 0} for row in status_rows],
            "top_endpoints": [{
                "endpoint": row["endpoint"],
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "error_rate": round((row["error_count"] / row["request_count"]) * 100, 2) if row["request_count"] > 0 else 0,
                "total_data": row["total_data"]
            } for row in endpoints_rows],
            "top_users": [{
                "user_id": row["user_id"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "last_request": row["last_request"].isoformat() if row["last_request"] else None
            } for row in users_rows],
            "hourly_distribution": [{"hour": int(row["hour"]), "request_count": row["request_count"], "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0} for row in hourly_rows],
            "daily_distribution": [{
                "date": row["date"].isoformat(),
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in daily_rows],
            "method_distribution": [{
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in method_rows],
            "recent_errors": [{
                "id": row["id"],
                "endpoint": row["endpoint"],
                "method": row["method"],
                "status_code": row["status_code"],
                "response_time_ms": row["response_time_ms"],
                "error_message": row["error_message"],
                "user_id": row["user_id"],
                "created_at": row["created_at"].isoformat()
            } for row in errors_rows],
            "performance_percentiles": {
                "p50": round(percentile_row["p50"], 2) if percentile_row["p50"] else 0,
                "p95": round(percentile_row["p95"], 2) if percentile_row["p95"] else 0,
                "p99": round(percentile_row["p99"], 2) if percentile_row["p99"] else 0
            }
        }
        
        print(f"=== DASHBOARD DEBUG: Final response ===")
        print(f"Response overview: {response_data['overview']}")
        print(f"Status distribution count: {len(response_data['status_distribution'])}")
        print(f"Top endpoints count: {len(response_data['top_endpoints'])}")
        print(f"=== END FINAL RESPONSE ===")
        
        return response_data
    except Exception as e:
        logger.error(f"Error fetching analytics dashboard: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch analytics data")


@app.get("/api/custom/analytics/requests", response_model=List[RequestAnalytics])
async def get_analytics_requests(
    page: int = Query(1, ge=1, description="Page number"),
    limit: int = Query(50, ge=1, le=1000, description="Items per page"),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    user_id: Optional[str] = Query(None, description="Filter by user ID"),
    min_response_time: Optional[int] = Query(None, description="Minimum response time in ms"),
    max_response_time: Optional[int] = Query(None, description="Maximum response time in ms"),
    has_error: Optional[bool] = Query(None, description="Filter by error status"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get paginated request analytics with filters"""
    try:
        # Build WHERE clause
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if user_id:
            param_count += 1
            conditions.append(f"user_id = ${param_count}")
            params.append(user_id)
        
        if min_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms >= ${param_count}")
            params.append(min_response_time)
        
        if max_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms <= ${param_count}")
            params.append(max_response_time)
        
        if has_error is not None:
            if has_error:
                conditions.append("(error_message IS NOT NULL OR status_code >= 400)")
            else:
                conditions.append("(error_message IS NULL AND status_code < 400)")
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        # Calculate offset
        offset = (page - 1) * limit
        
        # Build query
        query = f"""
            SELECT 
                id, endpoint, method, user_id, status_code, 
                response_time_ms, request_size_bytes, response_size_bytes,
                error_message, created_at
            FROM request_analytics
            {where_clause}
            ORDER BY created_at DESC
            LIMIT ${param_count + 1} OFFSET ${param_count + 2}
        """
        params.extend([limit, offset])
        
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
            
            return [RequestAnalytics(**dict(row)) for row in rows]
            
    except Exception as e:
        logger.error(f"Error fetching analytics requests: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch request data")


@app.get("/api/custom/analytics/export")
async def export_analytics_data(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    format: str = Query("csv", description="Export format (csv or json)"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Export analytics data as CSV or JSON"""
    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""

        async with pool.acquire() as conn:
            query = f"""
                SELECT 
                    id, endpoint, method, user_id, status_code, 
                    response_time_ms, request_size_bytes, response_size_bytes,
                    error_message, created_at
                FROM request_analytics
                {where_clause}
                ORDER BY created_at DESC
            """
            rows = await conn.fetch(query, *params)
            
            if format.lower() == "csv":
                import csv
                import io
                
                output = io.StringIO()
                writer = csv.writer(output)
                
                # Write header
                writer.writerow([
                    "ID", "Endpoint", "Method", "User ID", "Status Code",
                    "Response Time (ms)", "Request Size (bytes)", "Response Size (bytes)",
                    "Error Message", "Created At"
                ])
                
                # Write data
                for row in rows:
                    writer.writerow([
                        row["id"], row["endpoint"], row["method"], row["user_id"],
                        row["status_code"], row["response_time_ms"],
                        row["request_size_bytes"], row["response_size_bytes"],
                        row["error_message"], row["created_at"].isoformat()
                    ])
                
                return StreamingResponse(
                    io.BytesIO(output.getvalue().encode()),
                    media_type="text/csv",
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"}
                )
            else:
                # JSON format
                data = [{
                    "id": row["id"],
                    "endpoint": row["endpoint"],
                    "method": row["method"],
                    "user_id": row["user_id"],
                    "status_code": row["status_code"],
                    "response_time_ms": row["response_time_ms"],
                    "request_size_bytes": row["request_size_bytes"],
                    "response_size_bytes": row["response_size_bytes"],
                    "error_message": row["error_message"],
                    "created_at": row["created_at"].isoformat()
                } for row in rows]
                
                return JSONResponse(
                    content=data,
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"}
                )
                
    except Exception as e:
        logger.error(f"Error exporting analytics data: {e}")
        raise HTTPException(status_code=500, detail="Failed to export analytics data")


@app.get("/api/custom/health")
async def health_check():
    return {"status": "healthy"}

HeadlineBlock.model_rebuild()
ParagraphBlock.model_rebuild()
AlertBlock.model_rebuild()
SectionBreakBlock.model_rebuild()
BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

# ========================= Wizard Course Outline Endpoints =========================

class OutlineWizardPreview(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: full markdown string of the current outline so the assistant can apply
    # targeted changes when the user sends an incremental "edit" prompt.
    originalOutline: Optional[str] = None
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    theme: Optional[str] = None  # Selected theme from frontend

class OutlineWizardFinalize(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    editedOutline: Dict[str, Any]
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    theme: Optional[str] = None  # Selected theme from frontend
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder

_CONTENTBUILDER_PERSONA_CACHE: Optional[int] = None

async def get_contentbuilder_persona_id(cookies: Dict[str, str]) -> int:
    """Return persona id of the default ContentBuilder assistant (cached)."""
    global _CONTENTBUILDER_PERSONA_CACHE
    if _CONTENTBUILDER_PERSONA_CACHE is not None:
        return _CONTENTBUILDER_PERSONA_CACHE
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.get(f"{ONYX_API_SERVER_URL}/persona", cookies=cookies)
        resp.raise_for_status()
        personas = resp.json()
        # naive: first persona marked is_default_persona and has name 'ContentBuilder'
        for p in personas:
            if p.get("is_default_persona") or "contentbuilder" in p.get("name", "").lower():
                _CONTENTBUILDER_PERSONA_CACHE = p["id"]
                return _CONTENTBUILDER_PERSONA_CACHE
    raise HTTPException(status_code=500, detail="Could not locate ContentBuilder persona")

async def create_onyx_chat_session(persona_id: int, cookies: Dict[str, str]) -> str:
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/create-chat-session",
            json={"persona_id": persona_id, "description": None},
            cookies=cookies,
        )
        resp.raise_for_status()
        data = resp.json()
        return data.get("chat_session_id") or data.get("chatSessionId")

async def stream_chat_message(chat_session_id: str, message: str, cookies: Dict[str, str]) -> str:
    """Send message via Onyx non-streaming simple API and return the full answer."""
    logger.debug(f"[stream_chat_message] chat_id={chat_session_id} len(message)={len(message)}")

    async with httpx.AsyncClient(timeout=300.0) as client:
        minimal_retrieval = {
            "run_search": "never",
            "real_time": False,
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": minimal_retrieval,
            "stream_response": False,
        }
        # Prefer the non-streaming simplified endpoint if available (much faster and avoids nginx timeouts)
        simple_url = f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api"
        logger.debug(f"[stream_chat_message] POST {simple_url} (preferred) ...")
        try:
            resp = await client.post(simple_url, json=payload, cookies=cookies)
            if resp.status_code == 404:
                raise HTTPStatusError("simple api not found", request=resp.request, response=resp)
        except HTTPStatusError:
            logger.debug("[stream_chat_message] simple-api not available, falling back to generic endpoint")
            # Fallback to the generic endpoint (may stream)
            resp = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message",
                json=payload,
                cookies=cookies,
            )
        logger.debug(f"[stream_chat_message] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        # Depending on deployment, Onyx may return SSE stream or JSON.
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            full_answer = ""
            async for line in resp.aiter_lines():
                if not line or not line.startswith("data: "):
                    continue
                payload = line.removeprefix("data: ").strip()
                if payload == "[DONE]":
                    break
                try:
                    packet = json.loads(payload)
                except Exception:
                    continue
                if packet.get("answer_piece"):
                    full_answer += packet["answer_piece"]
            return full_answer
        # Fallback JSON response
        try:
            data = resp.json()
            return data.get("answer") or data.get("answer_citationless") or ""
        except Exception:
            return resp.text.strip()

# ------------ utility to parse markdown outline (very simple) -------------

def _parse_outline_markdown(md: str) -> List[Dict[str, Any]]:
    """Parse the markdown outline produced by the assistant into a lightweight
    list-of-modules representation expected by the wizard UI.

    Enhanced to handle various markdown formats and create intelligent module divisions.
    """
    logger.info(f"[PARSE_OUTLINE] Starting parse with input length: {len(md)}")
    logger.info(f"[PARSE_OUTLINE] Input preview: {md[:200]}{'...' if len(md) > 200 else ''}")
    
    modules: List[Dict[str, Any]] = []
    current: Optional[Dict[str, Any]] = None

    list_item_regex = re.compile(r"^(?:- |\* |\d+\.)")
    _buf: List[str] = []  # buffer for current lesson lines

    def flush_current_lesson(buf: List[str]) -> Optional[str]:
        """Combine buffered lines into a single lesson string."""
        if not buf:
            return None
        return "\n".join(buf)

    lines_processed = 0
    for raw_line in md.splitlines():
        lines_processed += 1
        if not raw_line.strip():
            continue  # skip empty lines

        indent = len(raw_line) - len(raw_line.lstrip())
        line = raw_line.lstrip()

        # Enhanced module detection - look for ## headers OR ### headers OR "Module" patterns
        is_module_header = (
            line.startswith("## ") or 
            (line.startswith("# ") and "module" in line.lower()) or
            line.startswith("**Module") or
            re.match(r"^Module\s+\d+", line, re.IGNORECASE)
        )
        
        if is_module_header:
            # flush any buffered lesson into previous module before switching
            if current:
                last_lesson = flush_current_lesson(_buf)
                if last_lesson:
                    current["lessons"].append(last_lesson)
                _buf = []

            # Extract title from various formats
            title_part = line.lstrip("#* ").strip()
            if ":" in title_part:
                title_part = title_part.split(":", 1)[-1].strip()
            if title_part.lower().startswith("module"):
                # Keep the "Module X:" format if present
                pass
            
            current = {
                "id": f"mod{len(modules) + 1}",
                "title": title_part,
                "totalHours": 0.0,
                "lessons": [],
            }
            modules.append(current)
            logger.debug(f"[PARSE_OUTLINE] Found module: {title_part}")
            continue

        # Lesson detection – only consider top-level list items (indent == 0)
        if current:
            # Note: Total Time lines are now auto-calculated from lesson creation times
            # We still capture them for backward compatibility but will recalculate
            m_time = re.match(r"(?:Total Time|Общее время|Загальний час)\s*:\s*([0-9]+(?:\.[0-9]+)?)", line, re.IGNORECASE)
            if m_time:
                try:
                    # Store the original value for backward compatibility, but we'll recalculate
                    current["originalTotalHours"] = float(m_time.group(1))
                except ValueError:
                    pass  # leave default 0.0 if parsing fails

            if indent == 0 and list_item_regex.match(line):
                # Starting a new top-level lesson → flush previous buffer
                ls_string = flush_current_lesson(_buf) if '_buf' in locals() else None
                if ls_string:
                    current["lessons"].append(ls_string)
                _buf = []  # reset buffer for new lesson

                lesson_title = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if lesson_title.startswith("**") and "**" in lesson_title[2:]:
                    lesson_title = lesson_title.split("**", 2)[1].strip()
                _buf.append(lesson_title)
                logger.debug(f"[PARSE_OUTLINE] Found lesson: {lesson_title}")
                continue
            elif current.get('lessons') is not None and '_buf' in locals():
                # inside a lesson details block (indented)
                if indent > 0:
                    _buf.append(line)
                continue

    # flush buffer after loop to whichever module is active
    if current:
        last_lesson = flush_current_lesson(_buf)
        if last_lesson:
            current["lessons"].append(last_lesson)

    logger.info(f"[PARSE_OUTLINE] After main parsing: {len(modules)} modules found, {lines_processed} lines processed")

    # Enhanced fallback when no module headings present
    if not modules:
        logger.warning(f"[PARSE_OUTLINE] No modules found, using intelligent fallback parsing")
        
        # Collect all lessons first
        all_lessons = []
        for raw_line in md.splitlines():
            if not raw_line.strip():
                continue
            indent = len(raw_line) - len(raw_line.lstrip())
            line = raw_line.lstrip()
            if indent == 0 and list_item_regex.match(line):
                txt = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if txt.startswith("**") and "**" in txt[2:]:
                    txt = txt.split("**", 2)[1].strip()
                all_lessons.append(txt)
        
        # If we have lessons, try to intelligently divide them into modules
        if all_lessons:
            logger.info(f"[PARSE_OUTLINE] Found {len(all_lessons)} lessons for intelligent division")
            
            # Try to determine intended module count from lesson separators or natural breaks
            separator_indices = []
            for i, lesson in enumerate(all_lessons):
                if "---" in lesson or lesson.strip() == "---":
                    separator_indices.append(i)
            
            if separator_indices:
                # Use separator-based division
                logger.info(f"[PARSE_OUTLINE] Using separator-based division with {len(separator_indices)} separators")
                start_idx = 0
                for module_num, sep_idx in enumerate(separator_indices + [len(all_lessons)], 1):
                    if start_idx < sep_idx:
                        module_lessons = [l for l in all_lessons[start_idx:sep_idx] if "---" not in l]
                        if module_lessons:  # Only create module if it has lessons
                            module = {
                                "id": f"mod{module_num}",
                                "title": f"Module {module_num}",
                                "totalHours": 0.0,
                                "lessons": module_lessons
                            }
                            modules.append(module)
                    start_idx = sep_idx + 1
            else:
                # Intelligently divide lessons into reasonable groups (3-5 lessons per module)
                target_lessons_per_module = 4
                num_modules = max(1, min(6, (len(all_lessons) + target_lessons_per_module - 1) // target_lessons_per_module))
                lessons_per_module = len(all_lessons) // num_modules
                remainder = len(all_lessons) % num_modules
                
                logger.info(f"[PARSE_OUTLINE] Dividing {len(all_lessons)} lessons into {num_modules} modules (~{lessons_per_module} lessons each)")
                
                start_idx = 0
                for module_num in range(1, num_modules + 1):
                    # Add one extra lesson to some modules to handle remainder
                    current_module_size = lessons_per_module + (1 if module_num <= remainder else 0)
                    end_idx = start_idx + current_module_size
                    
                    module_lessons = all_lessons[start_idx:end_idx]
                    if module_lessons:  # Only create module if it has lessons
                        module = {
                            "id": f"mod{module_num}",
                            "title": f"Module {module_num}",
                            "totalHours": 0.0,
                            "lessons": module_lessons
                        }
                        modules.append(module)
                    start_idx = end_idx
        
        # Last resort fallback - create single module with all content
        if not modules:
            logger.warning(f"[PARSE_OUTLINE] No lessons found, dumping all non-empty lines")
            tmp_module = {"id": "mod1", "title": "Course Content", "lessons": [], "totalHours": 0.0}
            tmp_module["lessons"] = [l.strip() for l in md.splitlines() if l.strip()]
            modules.append(tmp_module)
        
        logger.info(f"[PARSE_OUTLINE] Intelligent fallback created {len(modules)} modules")

    logger.info(f"[PARSE_OUTLINE] Final result: {len(modules)} modules")
    
    # Auto-calculate total creation time for each module by summing lesson creation times
    for i, module in enumerate(modules):
        logger.info(f"[PARSE_OUTLINE] Module {i+1}: '{module.get('title', 'No title')}' with {len(module.get('lessons', []))} lessons")
        
        # Calculate total creation time from lesson creation times
        total_creation_hours = 0.0
        lessons = module.get('lessons', [])
        
        for lesson in lessons:
            if isinstance(lesson, str):
                # Parse lesson details from markdown format
                lesson_lines = lesson.split('\n')
                for line in lesson_lines:
                    # Look for Time field in markdown format: "- **Time**: 17h" or "- **Время**: 17h" or "- **Час**: 17h"
                    time_match = re.search(r'^\s*-\s*\*\*(?:Time|Время|Час)\*\*:\s*([0-9]+(?:\.[0-9]+)?)h?\s*$', line.strip())
                    if time_match:
                        try:
                            hours = float(time_match.group(1))
                            total_creation_hours += hours
                            logger.debug(f"[PARSE_OUTLINE] Found lesson time: {hours}h")
                        except (ValueError, TypeError):
                            logger.warning(f"[PARSE_OUTLINE] Could not parse lesson time from line: {line}")
        
        # Set the auto-calculated total hours
        module['totalHours'] = total_creation_hours
        logger.info(f"[PARSE_OUTLINE] Module {i+1} auto-calculated totalHours: {total_creation_hours}")

    return modules

# ----------------------- ENDPOINTS ---------------------------------------

@app.post("/api/custom/course-outline/preview")
async def wizard_outline_preview(payload: OutlineWizardPreview, request: Request):
    logger.info(f"[PREVIEW_START] Course outline preview initiated")
    logger.info(f"[PREVIEW_PARAMS] prompt='{payload.prompt[:50]}...' modules={payload.modules} lessonsPerModule={payload.lessonsPerModule} lang={payload.language}")
    logger.info(f"[PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    logger.info(f"[PREVIEW_PARAMS] chatSessionId={payload.chatSessionId}")
    logger.info(f"[PREVIEW_PARAMS] originalOutline length={len(payload.originalOutline) if payload.originalOutline else 0}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")
    logger.info(f"[PREVIEW_AUTH] Cookie value: {cookies[ONYX_SESSION_COOKIE_NAME][:20] if cookies[ONYX_SESSION_COOKIE_NAME] else 'None'}...")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[PREVIEW_CHAT] Creating new chat session")
        try:
            logger.info(f"[PREVIEW_CHAT] Attempting to get contentbuilder persona ID")
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[PREVIEW_CHAT] Got persona ID: {persona_id}")
            logger.info(f"[PREVIEW_CHAT] Attempting to create Onyx chat session")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[PREVIEW_CHAT_ERROR] Failed to create chat session: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    logger.info(f"[PREVIEW_PAYLOAD] Building wizard payload")
    wiz_payload = {
        "product": "Course Outline",
        "prompt": payload.prompt,
        "language": payload.language,
    }
    logger.info(f"[PREVIEW_PAYLOAD] Base payload created with product={wiz_payload['product']}, language={wiz_payload['language']}")

    # Add file context if provided
    if payload.fromFiles:
        logger.info(f"[PREVIEW_PAYLOAD] Adding file context: fromFiles=True")
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
            logger.info(f"[PREVIEW_PAYLOAD] Added folderIds: {payload.folderIds}")
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds
            logger.info(f"[PREVIEW_PAYLOAD] Added fileIds: {payload.fileIds}")

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        logger.info(f"[PREVIEW_PAYLOAD] Adding text context: fromText=True, textMode={payload.textMode}")
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"[PREVIEW_PAYLOAD] Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"[PREVIEW_PAYLOAD] Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                logger.info(f"[PREVIEW_PAYLOAD] Attempting to create virtual file for large text")
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"[PREVIEW_PAYLOAD] Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"[PREVIEW_PAYLOAD] Failed to create virtual file for large text: {e}", exc_info=True)
                # Fallback to chunking if virtual file creation fails
                logger.info(f"[PREVIEW_PAYLOAD] Falling back to chunking for large text")
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    logger.info(f"[PREVIEW_PAYLOAD] Single chunk fallback: using compression")
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"[PREVIEW_PAYLOAD] Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    logger.info(f"[PREVIEW_PAYLOAD] Multiple chunks fallback: using first chunk with compression")
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"[PREVIEW_PAYLOAD] Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"[PREVIEW_PAYLOAD] Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"[PREVIEW_PAYLOAD] Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"[PREVIEW_PAYLOAD] Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    if payload.originalOutline:
        logger.info(f"[PREVIEW_PAYLOAD] Adding originalOutline ({len(payload.originalOutline)} chars)")
        wiz_payload["originalOutline"] = payload.originalOutline
    else:
        logger.info(f"[PREVIEW_PAYLOAD] Adding module configuration: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
        wiz_payload.update({
            "modules": payload.modules,
            "lessonsPerModule": payload.lessonsPerModule,
        })

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        logger.info(f"[PREVIEW_PAYLOAD] Decompressing text for assistant")
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"[PREVIEW_PAYLOAD] Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"[PREVIEW_PAYLOAD] Failed to decompress text: {e}", exc_info=True)
            # Continue with original text if decompression fails
    
    logger.info(f"[PREVIEW_PAYLOAD] Final payload keys: {list(wiz_payload.keys())}")
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
    logger.info(f"[PREVIEW_PAYLOAD] Created wizard message ({len(wizard_message)} chars)")

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[PREVIEW_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                # Step 1: Extract file context from Onyx
                folder_ids_list = []
                file_ids_list = []
                
                if payload.fromFiles and payload.folderIds:
                    folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                
                if payload.fromFiles and payload.fileIds:
                    file_ids_list = parse_id_list(payload.fileIds, "file")
                    logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                
                # Add virtual file ID if created for large text
                if wiz_payload.get("virtualFileId"):
                    file_ids_list.append(wiz_payload["virtualFileId"])
                    logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                
                # Extract context from Onyx
                logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Course Outline"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache full raw outline for later finalize step
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

                if not assistant_reply.strip():
                    logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
                    error_packet = {"type": "error", "message": "No content received from AI service"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    return

                logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
                try:
                    modules_preview = _parse_outline_markdown(assistant_reply)
                    logger.info(f"[PREVIEW_PARSING] Successfully parsed {len(modules_preview)} modules")
                    logger.info(f"[PREVIEW_PARSING] Module details: {[{'id': m.get('id'), 'title': m.get('title'), 'lessons_count': len(m.get('lessons', []))} for m in modules_preview]}")
                    
                    # Validate the parsed result meets basic requirements
                    validation_passed = True
                    validation_messages = []
                    
                    # Check if we have reasonable number of modules (not just 1 with many lessons)
                    if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                        validation_passed = False
                        validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
                    
                    # Check if we have expected module count (if specified in payload)
                    expected_modules = getattr(payload, 'modules', None)
                    if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                        validation_passed = False
                        validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
                    
                    if not validation_passed:
                        logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                        logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
                        # Continue anyway but log the issue - the intelligent fallback should have handled it
                    else:
                        logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
                    
                except Exception as e:
                    logger.error(f"[PREVIEW_PARSING] CRITICAL: Failed to parse outline markdown: {e}", exc_info=True)
                    logger.error(f"[PREVIEW_PARSING] Raw content preview: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
                    error_packet = {"type": "error", "message": f"Failed to parse generated outline: {str(e)}"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    return
                
                # Send completion packet with the parsed outline
                logger.info(f"[PREVIEW_DONE] Creating completion packet")
                done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
                yield (json.dumps(done_packet) + "\n").encode()
                logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[PREVIEW_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            # Enhance the wizard message with formatting requirements for course outlines
            enhanced_wizard_message = wizard_message
            if "Course Outline" in wizard_message:
                enhanced_wizard_message += """

CRITICAL FORMATTING REQUIREMENTS:
1. Use exactly this structure: ## Module [Number]: [Module Title]
2. Each module must be a separate H2 header starting with ##
3. Lessons must be numbered list items (1. 2. 3.) under each module

ENSURE: Create the requested number of modules, not a single module with all lessons.
"""
            
            try:
                async for chunk_data in stream_openai_response(enhanced_wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            except Exception as e:
                logger.error(f"[OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        if not assistant_reply.strip():
            logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
            error_packet = {"type": "error", "message": "No content received from AI service"}
            yield (json.dumps(error_packet) + "\n").encode()
            return

        logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
        try:
            modules_preview = _parse_outline_markdown(assistant_reply)
            logger.info(f"[PREVIEW_PARSING] Successfully parsed {len(modules_preview)} modules")
            logger.info(f"[PREVIEW_PARSING] Module details: {[{'id': m.get('id'), 'title': m.get('title'), 'lessons_count': len(m.get('lessons', []))} for m in modules_preview]}")
            
            # Validate the parsed result meets basic requirements
            validation_passed = True
            validation_messages = []
            
            # Check if we have reasonable number of modules (not just 1 with many lessons)
            if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                validation_passed = False
                validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
            
            # Check if we have expected module count (if specified in payload)
            expected_modules = getattr(payload, 'modules', None)
            if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                validation_passed = False
                validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
            
            if not validation_passed:
                logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
                # Continue anyway but log the issue - the intelligent fallback should have handled it
            else:
                logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
            
        except Exception as e:
            logger.error(f"[PREVIEW_PARSING] CRITICAL: Failed to parse outline markdown: {e}", exc_info=True)
            logger.error(f"[PREVIEW_PARSING] Raw content preview: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
            error_packet = {"type": "error", "message": f"Failed to parse generated outline: {str(e)}"}
            yield (json.dumps(error_packet) + "\n").encode()
            return
        
                # Send completion packet with the parsed outline
        logger.info(f"[PREVIEW_DONE] Creating completion packet")
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        yield (json.dumps(done_packet) + "\n").encode()
        logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
        return
                

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )

async def _ensure_training_plan_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_TRAINING_PLAN)
        if row:
            return row["id"]
        # create minimal template
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Training Plan", DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM, "Training Plan", COMPONENT_NAME_TRAINING_PLAN
        )
        return row["id"]

# After you get the parsed content from the AI parser, insert it like this:
async def insert_ai_audit_onepager_to_db(
    pool: asyncpg.Pool,
    onyx_user_id: str,
    project_name: str,
    microproduct_content: dict,
    chat_session_id: str = None
) -> int:
    """Insert AI-audit one-pager into database with correct template and component"""
    
    # First, ensure we have a Text Presentation template
    template_id = await _ensure_text_presentation_template(pool)
    
    insert_query = """
    INSERT INTO projects (
        onyx_user_id, project_name, product_type, microproduct_type,
        microproduct_name, microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, NOW(), $9)
    RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id;
    """
    
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            insert_query,
            onyx_user_id,
            project_name,
            "Text Presentation",  # product_type
            "Text Presentation",  # microproduct_type
            project_name,  # microproduct_name
            microproduct_content,  # parsed content from AI parser
            template_id,  # design_template_id (from _ensure_text_presentation_template)
            chat_session_id,  # source_chat_session_id
            None,  # folder_id - AI audit doesn't support folder assignment yet
        )
    
    if not row:
        raise HTTPException(status_code=500, detail="Failed to create AI-audit one-pager project entry.")
    
    return row["id"]


async def create_audit_folder(pool, onyx_user_id, company_name):
    async with pool.acquire() as conn:
        query = """
        INSERT INTO project_folders (onyx_user_id, name)
        VALUES ($1, $2)
        RETURNING id;
        """
        row = await conn.fetchrow(query, onyx_user_id, f"AI-Аудит: {company_name}")
        return row["id"]
    

async def assign_projects_to_folder(pool, folder_id, project_ids):
    async with pool.acquire() as conn:
        await conn.executemany(
            "UPDATE projects SET folder_id = $1 WHERE id = $2",
            [(folder_id, pid) for pid in project_ids]
        )


def set_progress(job_id, message):
    AI_AUDIT_PROGRESS.setdefault(job_id, []).append(message)


@app.get("/api/custom/ai-audit/progress")
async def get_audit_progress(jobId: str):
    return {"messages": AI_AUDIT_PROGRESS.get(jobId, [])}


async def scrape_company_data_from_website(company_website: str, language: str = "ru") -> AiAuditScrapedData:
    """
    Scrape company website to extract all necessary data for AI audit.
    Returns structured data that can be used in prompts.
    """
    try:
        logger.info(f"🌐 [WEBSITE SCRAPING] Starting to scrape: {company_website}")
        
        # Use the existing SERPAPI research function to get website content
        # For website-only scraping, we need to extract domain name for the search
        from urllib.parse import urlparse
        parsed_url = urlparse(company_website)
        domain_name = parsed_url.netloc.replace('www.', '')
        logger.info(f"🌐 [WEBSITE SCRAPING] Using domain name for search: {domain_name}")
        website_content = await serpapi_company_research(domain_name, "", company_website)
        logger.info(f"🌐 [WEBSITE SCRAPING] Received content length: {len(website_content)} characters")
        
        # Extract company name from website content
        company_name = await extract_company_name_from_website_content(website_content, company_website)
        
        # Extract company description from website content
        company_description = await extract_company_description_from_website_content(website_content, company_website, language)
        
        # Extract other company data using AI analysis
        company_data = await extract_company_metadata_from_website(website_content, company_website)
        
        scraped_data = AiAuditScrapedData(
            companyName=company_name,
            companyDesc=company_description,
            employees=company_data.get("employees", "Unknown"),
            franchise=company_data.get("franchise", "Unknown"),
            onboardingProblems=company_data.get("onboardingProblems", "To be analyzed from website content"),
            documents=company_data.get("documents", ["Other"]),
            documentsOther=company_data.get("documentsOther", "To be determined from website analysis"),
            priorities=company_data.get("priorities", ["Other"]),
            priorityOther=company_data.get("priorityOther", "To be determined from website analysis")
        )
        
        logger.info(f"🌐 [WEBSITE SCRAPING] Successfully scraped data for: {company_name}")
        return scraped_data
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error scraping website {company_website}: {e}")
        # Return fallback data if scraping fails
        return AiAuditScrapedData(
            companyName="Company Name",
            companyDesc="Company Description",
            employees="Unknown",
            franchise="Unknown",
            onboardingProblems="To be analyzed from website content",
            documents=["Other"],
            documentsOther="To be determined from website analysis",
            priorities=["Other"],
            priorityOther="To be determined from website analysis"
        )

async def extract_company_name_from_website_content(website_content: str, company_website: str) -> str:
    """Extract company name from website content using AI."""
    try:
        prompt = f"""
        Извлеки точное название компании из предоставленного контента веб-сайта.
        
        ВЕБ-САЙТ: {company_website}
        КОНТЕНТ ВЕБ-САЙТА:
        {website_content}
        
        ИНСТРУКЦИИ:
        - Найди официальное название компании
        - Верни только название компании, без дополнительной информации
        - Если не можешь определить название, верни "Company Name"
        
        ОТВЕТ (только название компании):
        """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        company_name = response_text.strip()
        if not company_name or company_name.lower() in ["unknown", "неизвестно", "not found"]:
            company_name = "Company Name"
            
        logger.info(f"🏢 [WEBSITE SCRAPING] Extracted company name: {company_name}")
        return company_name
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company name: {e}")
        return "Company Name"

async def extract_company_description_from_website_content(website_content: str, company_website: str, language: str = "ru") -> str:
    """Extract company description from website content using AI."""
    try:
        if language == "en":
            prompt = f"""
            Create a brief company description based on the website content.

            WEBSITE: {company_website}
            WEBSITE CONTENT:
            {website_content}

            INSTRUCTIONS:
            - Create description in style: "Company providing services in [main services]"
            - Use only information from the website
            - Description should be maximally brief (ONLY 1 sentence)
            - DO NOT add additional details or examples
            - Generate ALL content EXCLUSIVELY in English
            - If you cannot determine description, return "Company Description"

            RESPONSE (company description only):
            """
        elif language == "es":
            prompt = f"""
            Crea una breve descripción de la empresa basada en el contenido del sitio web.

            SITIO WEB: {company_website}
            CONTENIDO DEL SITIO WEB:
            {website_content}

            INSTRUCCIONES:
            - Crea descripción en estilo: "Empresa que proporciona servicios en [servicios principales]"
            - Usa solo información del sitio web
            - La descripción debe ser máxima breve (SOLO 1 oración)
            - NO agregues detalles adicionales o ejemplos
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            - Si no puedes determinar la descripción, devuelve "Descripción de la Empresa"

            RESPUESTA (solo descripción de la empresa):
            """
        elif language == "ua":
            prompt = f"""
            Створіть короткий опис компанії на основі вмісту веб-сайту.

            ВЕБ-САЙТ: {company_website}
            ВМІСТ ВЕБ-САЙТУ:
            {website_content}

            ІНСТРУКЦІЇ:
            - Створіть опис у стилі: "Компанія, що надає послуги в галузі [основні послуги]"
            - Використовуйте лише інформацію з веб-сайту
            - Опис має бути максимально коротким (ЛИШЕ 1 речення)
            - НЕ додавайте додаткові деталі або приклади
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            - Якщо не можете визначити опис, поверніть "Опис компанії"

            ВІДПОВІДЬ (лише опис компанії):
            """
        else:
            prompt = f"""
            Создай краткое описание компании на основе контента веб-сайта.

            ВЕБ-САЙТ: {company_website}
            КОНТЕНТ ВЕБ-САЙТА:
            {website_content}

            ИНСТРУКЦИИ:
            - Создай описание в стиле: "Компания, предоставляющая услуги по [основные услуги]"
            - Используй только информацию с веб-сайта
            - Описание должно быть максимально кратким (ТОЛЬКО 1 предложение)
            - НЕ добавляй дополнительные детали или примеры
            - Если не можешь определить описание, верни "Company Description"

            ОТВЕТ (только описание компании):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        company_description = response_text.strip()
        if not company_description or company_description.lower() in ["unknown", "неизвестно", "not found"]:
            company_description = "Company Description"
            
        logger.info(f"📝 [WEBSITE SCRAPING] Extracted company description: {company_description}")
        return company_description
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company description: {e}")
        return "Company Description"

async def extract_company_metadata_from_website(website_content: str, company_website: str) -> dict:
    """Extract additional company metadata from website content using AI."""
    try:
        prompt = f"""
        Проанализируй контент веб-сайта и извлеки следующую информацию о компании:
        
        ВЕБ-САЙТ: {company_website}
        КОНТЕНТ ВЕБ-САЙТА:
        {website_content}
        
        ИНСТРУКЦИИ:
        - Определи примерное количество сотрудников (если указано)
        - Определи, является ли компания франшизой или планирует открывать филиалы
        - Определи основные проблемы с онбордингом (если упоминаются)
        - Определи типы документов, которые использует компания
        - Определи приоритеты компании в области HR
        
        ФОРМАТ ОТВЕТА (только JSON):
        {{
            "employees": "количество сотрудников или Unknown",
            "franchise": "Yes/No/Unknown",
            "onboardingProblems": "основные проблемы или To be analyzed from website content",
            "documents": ["список типов документов или [\"Other\"]"],
            "documentsOther": "дополнительные документы или To be determined from website analysis",
            "priorities": ["список приоритетов или [\"Other\"]"],
            "priorityOther": "дополнительные приоритеты или To be determined from website analysis"
        }}
        
        ОТВЕТ (только JSON):
        """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response
        try:
            company_data = json.loads(response_text.strip())
            logger.info(f"📊 [WEBSITE SCRAPING] Extracted company metadata: {company_data}")
            return company_data
        except json.JSONDecodeError:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse JSON, using defaults")
            return {
                "employees": "Unknown",
                "franchise": "Unknown",
                "onboardingProblems": "To be analyzed from website content",
                "documents": ["Other"],
                "documentsOther": "To be determined from website analysis",
                "priorities": ["Other"],
                "priorityOther": "To be determined from website analysis"
            }
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company metadata: {e}")
        return {
            "employees": "Unknown",
            "franchise": "Unknown",
            "onboardingProblems": "To be analyzed from website content",
            "documents": ["Other"],
            "documentsOther": "To be determined from website analysis",
            "priorities": ["Other"],
            "priorityOther": "To be determined from website analysis"
        }

async def create_audit_onepager(duckduckgo_summary, example_text_path, payload, language="ru"):
    try:
        with open(example_text_path, encoding="utf-8") as f:
            example_text = f.read()
    except Exception as e:
        logger.error(f"[AI-Audit] Error reading example: {e}")
        example_text = "(Example not found)"
    if not duckduckgo_summary or duckduckgo_summary.strip() == "" or duckduckgo_summary.strip().startswith("(Нет релевантных данных"):
        duck_info = "(DuckDuckGo не дал информации. Используй только анкету.)"
    else:
        duck_info = duckduckgo_summary
    # Language-specific instructions
    if language == "en":
        language_instruction = """
    CRITICAL LANGUAGE REQUIREMENT:
    - Generate ALL content EXCLUSIVELY in English
    - Use English terminology and professional business language
    - Maintain the same structure and formatting as the example
    - Translate all section headers, labels, and text to English
    - Use English business terminology for all concepts
    """
        system_message = "You are a professional AI assistant for generating training one-pager documents in English. Strictly follow ContentBuilder.ai rules and generate content exclusively in English."
    else:
        language_instruction = ""
        system_message = "Ты профессиональный AI-ассистент для генерации обучающих one-pager документов. Строго следуй правилам ContentBuilder.ai."

    prompt = f"""
    Сгенерируй AI-аудит (one-pager) для компании, используя ВСЮ информацию из анкеты пользователя и результаты интернет-исследования (DuckDuckGo).
    {language_instruction}

    ТВОЯ ЗАДАЧА:
    - СКОПИРУЙ ПРИМЕР НИЖЕ МАКСИМАЛЬНО ТОЧНО, ДОСЛОВНО.
    - Используй те же секции, тот же порядок, ту же длину, те же заголовки, те же таблицы, те же списки, те же абзацы, то же форматирование.
    - Если в примере есть таблица — в твоём ответе тоже должна быть таблица с тем же количеством строк и столбцов.
    - Если в примере 5 секций — в твоём ответе тоже должно быть 5 секций с теми же названиями и в том же порядке.
    - ЗАМЕНИ только данные, относящиеся к компании, на новые из анкеты и поиска.
    - НЕ сокращай, НЕ добавляй новых секций, НЕ меняй структуру, НЕ меняй форматирование, НЕ меняй количество строк, НЕ меняй количество столбцов в таблицах.
    - Если не уверен — лучше скопируй больше из примера, чем меньше.
    - Твой ответ должен быть на 90% буквальной копией примера, только с новыми данными.
    - Если DuckDuckGo не дал информации, используй только анкету.
    - Если в примере есть таблица, твоя таблица должна быть с тем же количеством строк и столбцов, только с новыми данными.
    - Если в примере есть абзац, твой ответ должен содержать такой же абзац на том же месте.
    - Если в примере есть список, твой ответ должен содержать такой же список с тем же количеством пунктов.
    - Не меняй ни одну структуру, даже если кажется, что это не подходит — просто замени данные.

    ---
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {payload.companyName}
    - Описание компании: {payload.companyDesc}
    - Сайт компании: {payload.companyWebsite}
    - Количество сотрудников: {payload.employees}
    - Франшиза: {payload.franchise}
    - Проблемы онбординга: {payload.onboardingProblems}
    - Документы: {', '.join(payload.documents)} {payload.documentsOther}
    - Приоритеты: {', '.join(payload.priorities)} {payload.priorityOther}

    ---
    РЕЗУЛЬТАТЫ ИНТЕРНЕТ-ИССЛЕДОВАНИЯ (DuckDuckGo):
    {duck_info}

    ---
    СКОПИРУЙ ПРИМЕР НИЖЕ, ЗАМЕНИВ ТОЛЬКО ДАННЫЕ О КОМПАНИИ:
    {example_text}

    Ответь только текстом one-pager по этим правилам, без пояснений.
    """
    logger.info(f"[AI-Audit] Final prompt (first 500 chars): {prompt[:500]}")
    client = get_openai_client()
    try:
        # Set a longer timeout for the OpenAI call
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4096,
            temperature=0.2,
            timeout=httpx.Timeout(180.0)  # 180 seconds
        )
    except Exception as e:
        logger.error(f"[AI-Audit] OpenAI generation error: {e}", exc_info=True)
        return {"error": f"Ошибка генерации AI-аудита: {e}"}
    
    result = response.choices[0].message.content
    logger.info(f"[AI-Audit] OpenAI result (first 500 chars): {result[:500]}")

    with open("custom_assistants/content_builder_ai.txt", encoding="utf-8") as f:
        assistant_instructions = f.read()

    # Compose the parsing prompt
    parsing_prompt = (
        f"{assistant_instructions}\n\n"
        "WIZARD_REQUEST\n"
        + json.dumps({
            "product": "Text Presentation",
            "microproduct": "One-Pager",
            "prompt": "Приведи этот текст к нужному формату one-pager для ContentBuilder.ai",
            "language": "ru",
            "fromText": True,
            "textMode": "context",
            "userText": result,
            "strict": True,
            "parseMode": "onepager"
        }, ensure_ascii=False)
    )

    # Call OpenAI again (use gpt-4o-mini or your preferred model)
    parsed_response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Ты профессиональный AI-ассистент для парсинга продуктов ContentBuilder.ai."},
            {"role": "user", "content": parsing_prompt}
        ],
        max_tokens=4096,
        temperature=0.1,
        timeout=httpx.Timeout(120.0)
    )
    parsed_markdown = parsed_response.choices[0].message.content

    parsed_json = await parse_ai_response_with_llm(
        ai_response=parsed_markdown,
        project_name=payload.companyName,
        target_model=TextPresentationDetails,  # or your one-pager model
        default_error_model_instance=TextPresentationDetails(textTitle="Parse error", contentBlocks=[]),
        dynamic_instructions=f"""
        You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
        This product is for general text like introductions, goal descriptions, etc.
        Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

        **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

        **Global Fields:**
        1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
            - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
            - Extract ONLY the title part (the last part after the last "**")
            - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
            - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
            - Do NOT include the course name or "Text Presentation" label in the title
            - If no clear pattern is found, use the first meaningful title or heading
        2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
        3.  `detectedLanguage` (string): e.g., "en", "ru".

        **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

        1.  **`type: "headline"`**
            * `level` (integer):
                * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
            * `text` (string): Headline text.
            * `iconName` (string, optional): Based on level and context as described above.
            * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

        2.  **`type: "paragraph"`**
            * `text` (string): Full paragraph text.
            * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

        3.  **`type: "bullet_list"`**
            * `items` (array of `ListItem`): Can be strings or other nested content blocks.
            * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

        4.  **`type: "numbered_list"`**
            * `items` (array of `ListItem`):
                * Can be simple strings for basic numbered points.
                * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                    * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                    * The `items` of this *inner* `bullet_list` should then be:
                        1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                        2. A `paragraph` block (for the main descriptive text).
                        3. Optionally, another `paragraph` block with `isRecommendation: true`.
                * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

        5.  **`type: "table"`**
            * `headers` (array of strings): The column headers for the table.
            * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
            * `caption` (string, optional): A short description or title for the table, if present in the source text.
            * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


        6.  **`type: "alert"`**
            *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
            *   `title` (string, optional): The title of the alert.
            *   `text` (string): The body text of the alert.
            *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

        7.  **`type: "section_break"`**
            * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

        **General Parsing Rules & Icon Names:**
        * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
        * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
        * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
        * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
        * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
        * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

        Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
            • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
            • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
            • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

        Return ONLY the JSON object.
        """,
        target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
    )
    return parsed_json


@app.post("/api/custom/ai-audit/generate")
async def generate_ai_audit_onepager(payload: AiAuditQuestionnaireRequest, request: Request, background_tasks: BackgroundTasks, pool: asyncpg.Pool = Depends(get_db_pool)):
    job_id = str(uuid.uuid4())
    set_progress(job_id, "Starting AI-Audit generation...")
    background_tasks.add_task(_run_audit_generation, payload, request, pool, job_id)
    return {"jobId": job_id}


@app.post("/api/custom/ai-audit/landing-page/generate")
async def generate_ai_audit_landing_page(payload: AiAuditQuestionnaireRequest, request: Request, background_tasks: BackgroundTasks, pool: asyncpg.Pool = Depends(get_db_pool)):
    job_id = str(uuid.uuid4())
    set_progress(job_id, "Starting AI-Audit landing page generation...")
    background_tasks.add_task(_run_landing_page_generation, payload, request, pool, job_id)
    return {"jobId": job_id}


@app.get("/api/custom/ai-audit/landing-page/{project_id}")
async def get_ai_audit_landing_page_data(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Get the dynamic landing page data for a specific AI audit project.
    """
    try:
        # 📊 LOG: Data retrieval request received
        logger.info(f"📥 [AUDIT DATA FLOW] Landing page data request for project ID: {project_id}")
        
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get the project data
        query = """
        SELECT microproduct_content, microproduct_name 
        FROM projects 
        WHERE id = $1 AND onyx_user_id = $2
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, project_id, onyx_user_id)
            
        if not row:
            logger.error(f"❌ [AUDIT DATA FLOW] Project {project_id} not found for user {onyx_user_id}")
            raise HTTPException(status_code=404, detail="Project not found")
        
        content = row["microproduct_content"]
        project_name = row["microproduct_name"]
        
        # 📊 DETAILED LOGGING: Language preference in retrieved data
        language_from_db = content.get("language", "NOT_FOUND") if content else "NO_CONTENT"
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Retrieved from database - language: '{language_from_db}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Retrieved from database - content type: {type(content)}")
        
        # 📊 LOG: Raw data retrieved from database
        logger.info(f"💾 [AUDIT DATA FLOW] Retrieved project data from database:")
        logger.info(f"💾 [AUDIT DATA FLOW] - Project name: '{project_name}'")
        logger.info(f"💾 [AUDIT DATA FLOW] - Content keys: {list(content.keys()) if content else 'None'}")
        
        # Extract the dynamic data
        company_name = content.get("companyName", "Unknown Company")
        company_description = content.get("companyDescription", "Company description not available")
        
        # 📊 LOG: Extracted dynamic data
        logger.info(f"🔍 [AUDIT DATA FLOW] Extracted dynamic data:")
        logger.info(f"🔍 [AUDIT DATA FLOW] - Company name: '{company_name}'")
        logger.info(f"🔍 [AUDIT DATA FLOW] - Company description: '{company_description}'")
        
        # Extract job positions from the landing page data
        job_positions = content.get("jobPositions", [])
        
        # 📊 LOG: Job positions extraction process
        logger.info(f"💼 [AUDIT DATA FLOW] Starting job positions extraction:")
        logger.info(f"💼 [AUDIT DATA FLOW] - Job positions in content: {len(job_positions)} positions")
        
        if job_positions:
            # 📊 LOG: Job positions found in landing page data
            logger.info(f"💼 [AUDIT DATA FLOW] Job positions found in landing page data:")
            for i, position in enumerate(job_positions):
                logger.info(f"💼 [AUDIT DATA FLOW] - Position {i+1}: {position}")
        else:
            logger.info(f"💼 [AUDIT DATA FLOW] No job positions in landing page data, using default positions")
            # Fallback to default positions if none found
            job_positions = [
                {"title": "HVAC Technician", "description": "Installation and maintenance of heating, ventilation, and air conditioning systems", "icon": "👷"},
                {"title": "Electrician", "description": "Installation and maintenance of electrical systems", "icon": "⚡"},
                {"title": "Project Manager", "description": "Overseeing projects and coordinating teams", "icon": "📋"}
            ]
        
        # Extract workforce crisis data from the landing page data
        workforce_crisis = content.get("workforceCrisis", {})
        
        # 📊 LOG: Workforce crisis data extraction
        logger.info(f"📊 [AUDIT DATA FLOW] Workforce crisis data extraction:")
        logger.info(f"📊 [AUDIT DATA FLOW] - Workforce crisis data: {workforce_crisis}")
        
        # Extract course outline modules from the landing page data
        course_outline_modules = content.get("courseOutlineModules", [])
        
        # 📊 LOG: Course outline modules extraction
        logger.info(f"📚 [AUDIT DATA FLOW] Course outline modules extraction:")
        logger.info(f"📚 [AUDIT DATA FLOW] - Course outline modules count: {len(course_outline_modules)}")
        for i, module_title in enumerate(course_outline_modules):
            logger.info(f"📚 [AUDIT DATA FLOW] - Module {i+1}: {module_title}")
        
        # Extract course templates from the landing page data
        course_templates = content.get("courseTemplates", [])
        
        # 📊 LOG: Course templates extraction
        logger.info(f"🎓 [AUDIT DATA FLOW] Course templates extraction:")
        logger.info(f"🎓 [AUDIT DATA FLOW] - Course templates count: {len(course_templates)}")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [AUDIT DATA FLOW] - Template {i+1}: {template.get('title', 'Unknown')}")
        
        # 📊 LOG: Final response data structure
        response_data = {
            "projectId": project_id,
            "projectName": project_name,
            "companyName": company_name,
            "companyDescription": company_description,
            "jobPositions": job_positions,
            "workforceCrisis": workforce_crisis,
            "courseOutlineModules": course_outline_modules,
            "courseTemplates": course_templates,
            "language": content.get("language", "ru")  # 🔧 FIX: Include language parameter in response
        }
        
        logger.info(f"📤 [AUDIT DATA FLOW] Final response data:")
        logger.info(f"📤 [AUDIT DATA FLOW] - Project ID: {response_data['projectId']}")
        logger.info(f"📤 [AUDIT DATA FLOW] - Project Name: '{response_data['projectName']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Company Name: '{response_data['companyName']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Company Description: '{response_data['companyDescription']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Job Positions Count: {len(response_data['jobPositions'])}")
        logger.info(f"📤 [AUDIT DATA FLOW] - Workforce Crisis Data: {response_data['workforceCrisis']}")
        
        # 📊 DETAILED LOGGING: Language parameter in response
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Response data - language: '{response_data['language']}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Response data keys: {list(response_data.keys())}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting landing page data: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


async def _run_audit_generation(payload, request, pool, job_id):
    try:
        set_progress(job_id, "Scraping company website...")
        # Scrape company data from website
        scraped_data = await scrape_company_data_from_website(payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit] Scraped company data: {scraped_data.companyName}")
        
        set_progress(job_id, "Researching additional company info...")
        # Get additional research data using scraped company name and description
        duckduckgo_summary = await serpapi_company_research(scraped_data.companyName, scraped_data.companyDesc, payload.companyWebsite)
        logger.info(f"[AI-Audit] SERPAPI summary: {duckduckgo_summary[:300]}")

        set_progress(job_id, "Generating first one-pager...")
        # Create a combined payload with scraped data for the prompt
        combined_payload = type('CombinedPayload', (), {
            'companyName': scraped_data.companyName,
            'companyDesc': scraped_data.companyDesc,
            'companyWebsite': payload.companyWebsite,
            'employees': scraped_data.employees,
            'franchise': scraped_data.franchise,
            'onboardingProblems': scraped_data.onboardingProblems,
            'documents': scraped_data.documents,
            'documentsOther': scraped_data.documentsOther,
            'priorities': scraped_data.priorities,
            'priorityOther': scraped_data.priorityOther
        })()
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/First-one-pager.txt", combined_payload, payload.language)

        onyx_user_id = await get_current_onyx_user_id(request)

        # After you get the parsed content from the AI parser:
        project_id = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {scraped_data.companyName}",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id}")

        set_progress(job_id, "Researching open positions...")
        positions = extract_open_positions_from_table(parsed_json)

        results = []
        for position in positions:
            set_progress(job_id, f"Generating onboarding for '{position.get('Позиция', 'New Position')}'")
            project = await generate_and_finalize_course_outline_for_position(
                scraped_data.companyName, position, onyx_user_id, pool, request
            )
            results.append(project)

        logger.info(f"[AI-Audit] Created {len(results)} course outlines for positions")

        set_progress(job_id, "Generating closing one-pager...")
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/Second-one-pager.txt", combined_payload)

        # After you get the parsed content from the AI parser:
        project_id_2 = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {scraped_data.companyName} (2)",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id_2}")

        set_progress(job_id, "Finalizing and saving to folder...")
        all_project_ids = [project_id] + [p.id for p in results] + [project_id_2]

        # 1. Create a new folder
        folder_id = await create_audit_folder(pool, onyx_user_id, scraped_data.companyName)

        # 2. Assign all projects to this folder
        await assign_projects_to_folder(pool, folder_id, all_project_ids)

        set_progress(job_id, "AI-Audit complete!")
        logger.info(f"[AI-Audit] Finished the AI-Audit Generation")
        return {
            "id": project_id,
            "id_2": project_id_2,
            "name": f"AI-Аудит: {scraped_data.companyName}",
            "folderId": folder_id
        }
    
    except Exception as e:
        set_progress(job_id, f"Error: {str(e)}")


async def extract_company_name_from_data(duckduckgo_summary: str, payload) -> str:
    """
    Extract the company name from scraped data using AI.
    Returns only the company name as a string.
    """
    prompt = f"""
    Извлеки ТОЛЬКО название компании из предоставленных данных.
    
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {getattr(payload, 'companyName', 'Company Name')}
    - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
    - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
    
    ДАННЫЕ ИЗ ИНТЕРНЕТА:
    {duckduckgo_summary}
    
    ТВОЯ ЗАДАЧА:
    - Верни ТОЛЬКО название компании
    - Используй наиболее точное и официальное название
    - Если есть несколько вариантов, выбери самый короткий и официальный
    - НЕ добавляй никаких дополнительных слов или объяснений
    - НЕ используй кавычки или другие символы
    
    ОТВЕТ (только название компании):
    """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        company_name = response_text.strip()
        if not company_name:
            company_name = getattr(payload, 'companyName', 'Company Name')  # Fallback to original name
        
        logger.info(f"[AI-Audit Landing Page] Extracted company name: {company_name}")
        return company_name
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting company name: {e}")
        return getattr(payload, 'companyName', 'Company Name')  # Fallback to original name


async def generate_company_description_from_data(duckduckgo_summary: str, payload) -> str:
    """
    Generate a company description from scraped data using AI.
    Returns a concise description similar to the original subtitle format.
    """
    prompt = f"""
    Создай краткое описание компании в стиле: "Компания предоставляющий услуги по [основные услуги]. [дополнительная информация о компании]"
    
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {getattr(payload, 'companyName', 'Company Name')}
    - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
    - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
    
    ДАННЫЕ ИЗ ИНТЕРНЕТА:
    {duckduckgo_summary}
    
    ТВОЯ ЗАДАЧА:
    - Создай описание в том же стиле, что и пример: "Компания предоставляющий услуги по установке и обслуживанию систем HVAC, электрики, солнечных панелей, а также бытовой и коммерческой техники. Обеспечивая полный цикл инженерных решений"
    - Используй информацию из интернета для определения основных услуг компании
    - Сделай описание кратким (1-2 предложения)
    - Начни с "Компания предоставляющий услуги по"
    - НЕ добавляй кавычки или другие символы
    - Пиши на русском языке
    
    ОТВЕТ (только описание компании):
    """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        company_description = response_text.strip()
        if not company_description:
            company_description = getattr(payload, 'companyDesc', 'Company Description')  # Fallback to original description
        
        logger.info(f"[AI-Audit Landing Page] Generated company description: {company_description}")
        return company_description
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating company description: {e}")
        return getattr(payload, 'companyDesc', 'Company Description')  # Fallback to original description


async def generate_ai_image_for_job_position(job_title: str, company_name: str) -> str:
    """
    Generate an AI image for a specific job position using DALL-E 3.
    """
    try:
        # Create a professional prompt for the job position with enhanced framing
        prompt = f"""A professional photograph of a {job_title} actively working at {company_name}. 
    
        SCENE: The person is engaged in their typical work activities in an authentic workplace environment appropriate for a {job_title}. Show them using professional tools, equipment, or technology relevant to their role. The composition should capture both the person (from waist up or full body) and their work environment.

        ACTIVITY: Include specific work processes - for example:
        - If barista: preparing coffee, operating espresso machine, arranging cups
        - If programmer: coding at multiple monitors, reviewing code, collaborating with team
        - If mechanic: working on equipment, using tools, diagnostic work
        - If teacher: conducting lesson, using whiteboard, interacting with materials
        - If sales representative: presenting products, meeting with clients, demonstrating features
        - If nurse: caring for patients, using medical equipment, documenting care

        ENVIRONMENT: Authentic workplace setting that matches the {job_title} role - not just a generic office. Include relevant background elements, tools, equipment, and work materials that tell the story of what this person does.

        STYLE: High-quality professional photography with good lighting that shows both the person and their work context. The person should be wearing appropriate work attire/uniform for their specific role.

        COMPOSITION: Environmental portrait style that captures the essence of the job, not just a headshot."""

        # Use wider dimensions for course template images to better fit the container
        width, height = 1792, 1024
        
        # Create the request
        request = AIImageGenerationRequest(
            prompt=prompt,
            width=width,
            height=height,
            quality="standard",
            style="vivid",
            model="dall-e-3"
        )
        
        # Generate the image
        result = await generate_ai_image(request)
        
        logger.info(f"🎨 [COURSE IMAGE] Generated image for {job_title}: {result['file_path']}")
        return result['file_path']
        
    except Exception as e:
        logger.error(f"❌ [COURSE IMAGE] Error generating image for {job_title}: {e}")
        # Return a fallback image path
        return f"/custom-projects-ui/images/audit-section-5-job-1-mobile.png"

async def generate_course_description_for_position(job_title: str, company_name: str, duckduckgo_summary: str, language: str = "ru") -> str:
    """
    Generate a concise course description for a specific job position.
    """
    try:
        if language == "en":
            prompt = f"""Create a brief course description for the position "{job_title}" at {company_name}.

COMPANY DATA:
{duckduckgo_summary}

CRITICAL REQUIREMENTS:
- Description must be VERY short - maximum 80 characters (not 100!)
- Use ONLY simple format: "Training in [skills] for [short position name]"
- Avoid long words and unnecessary details
- DO NOT use complex constructions

GOOD EXAMPLES (short):
- "Training in data analysis and visualization for analyst."
- "Training in system design for engineer."
- "Training in sales techniques for manager."

BAD EXAMPLES (too long):
- "Training in effective sales strategies and customer relationship management for sales manager"
- "Training in effective communication and problem solving for customer service specialists"

SHORTENING RULES:
- "sales manager" → "manager"
- "customer service specialist" → "consultant"
- "marketing specialist" → "marketer"
- "data analyst" → "analyst"

RESPONSE (course description only, maximum 80 characters):"""
        elif language == "es":
            prompt = f"""Crea una breve descripción del curso para la posición "{job_title}" en {company_name}.

DATOS DE LA EMPRESA:
{duckduckgo_summary}

REQUISITOS CRÍTICOS:
- La descripción debe ser MUY corta - máximo 80 caracteres (¡no 100!)
- Usa SOLO formato simple: "Capacitación en [habilidades] para [nombre corto de posición]"
- Evita palabras largas y detalles innecesarios
- NO uses construcciones complejas

BUENOS EJEMPLOS (cortos):
- "Capacitación en análisis de datos y visualización para analista."
- "Capacitación en diseño de sistemas para ingeniero."
- "Capacitación en técnicas de ventas para gerente."

MALOS EJEMPLOS (muy largos):
- "Capacitación en estrategias efectivas de ventas y gestión de relaciones con clientes para gerente de ventas"
- "Capacitación en comunicación efectiva y resolución de problemas para especialistas en atención al cliente"

REGLAS DE ABREVIACIÓN:
- "gerente de ventas" → "gerente"
- "especialista en atención al cliente" → "consultor"
- "especialista en marketing" → "marketero"
- "analista de datos" → "analista"

RESPUESTA (solo descripción del curso, máximo 80 caracteres):"""
        elif language == "ua":
            prompt = f"""Створіть короткий опис курсу для посади "{job_title}" в компанії {company_name}.

ДАНІ ПРО КОМПАНІЮ:
{duckduckgo_summary}

КРИТИЧНІ ВИМОГИ:
- Опис має бути ДУЖЕ коротким - максимум 80 символів (не 100!)
- Використовуйте ЛИШЕ простий формат: "Навчання [навичкам] для [скорочена назва посади]"
- Уникайте довгих слів та зайвих деталей
- НЕ використовуйте складні конструкції

ХОРОШІ ПРИКЛАДИ (короткі):
- "Навчання аналізу даних та візуалізації для аналітика."
- "Навчання проектуванню систем для інженера."
- "Навчання продажам для менеджера."

ПОГАНІ ПРИКЛАДИ (занадто довгі):
- "Навчання ефективним стратегіям продажів та управлінню клієнтськими відносинами для менеджера"
- "Навчання ефективному спілкуванню та вирішенню проблем для фахівців з обслуговування"

ПРАВИЛА СКОРОЧЕННЯ:
- "менеджер з продажів" → "менеджера"
- "фахівець з обслуговування клієнтів" → "консультанта"
- "спеціаліст з маркетингу" → "маркетолога"
- "аналізатор даних" → "аналітика"

ВІДПОВІДЬ (тільки опис курсу, максимум 80 символів):"""
        else:  # Russian
            prompt = f"""Создай краткое описание курса обучения для позиции "{job_title}" в компании {company_name}.

ДАННЫЕ О КОМПАНИИ:
{duckduckgo_summary}

КРИТИЧЕСКИЕ ТРЕБОВАНИЯ:
- Описание должно быть ОЧЕНЬ коротким - максимум 80 символов (не 100!)
- Используй ТОЛЬКО простой формат: "Обучение [навыкам] для [сокращенное название позиции]"
- Избегай длинных слов и лишних деталей
- НЕ используй сложные конструкции

ХОРОШИЕ ПРИМЕРЫ (короткие):
- "Обучение анализу данных и визуализации для аналитика."
- "Обучение проектированию систем для инженера."
- "Обучение продажам для менеджера."

ПЛОХИЕ ПРИМЕРЫ (слишком длинные):
- "Обучение эффективным стратегиям продаж и управлению клиентскими отношениями для менеджера"
- "Обучение эффективному общению и решению проблем для специалистов по обслуживанию"

ПРАВИЛА СОКРАЩЕНИЯ:
- "менеджер по продажам" → "менеджера"
- "специалист по обслуживанию клиентов" → "консультанта"
- "специалист по маркетингу" → "маркетолога"
- "аналитик данных" → "аналитика"

ОТВЕТ (только описание курса, максимум 80 символов):"""
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        description = response_text.strip()
        if len(description) > 80:
            description = description[:77] + "..."
            
        return description
        
    except Exception as e:
        logger.error(f"❌ [COURSE DESCRIPTION] Error generating course description for {job_title}: {e}")
        if language == "en":
            return f"Training in key skills for {job_title} position."
        elif language == "es":
            return f"Capacitación en habilidades clave para la posición {job_title}."
        elif language == "ua":
            return f"Навчання ключовим навичкам для посади {job_title}."
        else:  # Russian
            return f"Обучение ключевым навыкам для позиции {job_title}."

async def generate_course_outline_for_landing_page(duckduckgo_summary: str, job_positions: list, payload, language: str = "ru") -> list:
    """
    Generate course outline data for the landing page modules section.
    Returns a list of modules with titles and lessons extracted from the first job position's course outline.
    """
    try:
        if not job_positions:
            logger.warning("[COURSE OUTLINE] No job positions available for course outline generation")
            return []
        
        # Use the first job position for course outline generation
        first_position = job_positions[0]
        position_title = first_position.get('title', 'Сотрудник')
        
        logger.info(f"[COURSE OUTLINE] Generating course outline for position: {position_title}")
        
        # Build the prompt for course outline generation
        if language == "en":
            prompt = f"""Create a detailed course outline 'Onboarding for {position_title}' for new employees in this position at '{getattr(payload, 'companyName', 'Company Name')}'.

COMPANY CONTEXT:
- Company Name: {getattr(payload, 'companyName', 'Company Name')}
- Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
- Position: {position_title}
- Additional company information: {duckduckgo_summary}

COURSE REQUIREMENTS:
- The course should be specific to company {getattr(payload, 'companyName', 'Company Name')} and position {position_title}
- Content should reflect real tasks and responsibilities of this position in this company
- Consider industry specifics and corporate culture
- Create EXACTLY 4 modules with UNIQUE names
- Each module should have FROM 5 TO 7 lessons
- Module and lesson names should be CREATIVE and DIVERSE
- Avoid repetitive formulations
- Each lesson should be specific and practical for this position
- DO NOT add module numbers in titles (e.g., 'Module 1:', 'Module 2:', etc.)
- Use only descriptive module names without prefixes
- Generate ALL content EXCLUSIVELY in English

RESPONSE FORMAT (JSON only):
[
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}}
]

RESPONSE (JSON only):"""
        elif language == "es":
            prompt = f"""Crea un esquema detallado del curso 'Incorporación para {position_title}' para nuevos empleados en esta posición en '{getattr(payload, 'companyName', 'Company Name')}'.

CONTEXTO DE LA EMPRESA:
- Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
- Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
- Posición: {position_title}
- Información adicional de la empresa: {duckduckgo_summary}

REQUISITOS DEL CURSO:
- El curso debe ser específico para la empresa {getattr(payload, 'companyName', 'Company Name')} y la posición {position_title}
- El contenido debe reflejar las tareas y responsabilidades reales de esta posición en esta empresa
- Considera las especificidades de la industria y la cultura corporativa
- Crea EXACTAMENTE 4 módulos con nombres ÚNICOS
- Cada módulo debe tener DE 5 A 7 lecciones
- Los nombres de módulos y lecciones deben ser CREATIVOS y DIVERSOS
- Evita formulaciones repetitivas
- Cada lección debe ser específica y práctica para esta posición
- NO agregues números de módulos en los títulos (ej., 'Módulo 1:', 'Módulo 2:', etc.)
- Usa solo nombres descriptivos de módulos sin prefijos
- Genera TODO el contenido EXCLUSIVAMENTE en español

FORMATO DE RESPUESTA (solo JSON):
[
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}}
]

RESPUESTA (solo JSON):"""
        elif language == "ua":
            prompt = f"""Створіть детальний план курсу 'Онбординг для посади {position_title}' для нових співробітників на цій посаді в компанії '{getattr(payload, 'companyName', 'Company Name')}'.

КОНТЕКСТ КОМПАНІЇ:
- Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
- Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
- Посада: {position_title}
- Додаткова інформація про компанію: {duckduckgo_summary}

ВИМОГИ ДО КУРСУ:
- Курс повинен бути специфічним для компанії {getattr(payload, 'companyName', 'Company Name')} та посади {position_title}
- Зміст повинен відображати реальні завдання та обов'язки цієї посади в цій компанії
- Враховуйте специфіку галузі та корпоративну культуру
- Створіть РІВНО 4 модулі з УНІКАЛЬНИМИ назвами
- У кожному модулі має бути ВІД 5 ДО 7 уроків
- Назви модулів та уроків мають бути КРЕАТИВНИМИ та РІЗНОМАНІТНИМИ
- Уникайте повторюваних формулювань
- Кожен урок має бути конкретним та практичним для цієї посади
- НЕ додавайте номери модулів у назви (наприклад, 'Модуль 1:', 'Модуль 2:' тощо)
- Використовуйте лише описові назви модулів без префіксів
- Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою

ФОРМАТ ВІДПОВІДІ (тільки JSON):
[
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}}
]

ВІДПОВІДЬ (тільки JSON):"""
        else:
            wizard_request = {
                "product": "Course Outline",
                "prompt": (
                    f"Создай детальный курс аутлайн 'Онбординг для должности {position_title}' для новых сотрудников этой должности в компании '{getattr(payload, 'companyName', 'Company Name')}'. \n"
                    f"КОНТЕКСТ КОМПАНИИ:\n"
                    f"- Название компании: {getattr(payload, 'companyName', 'Company Name')}\n"
                    f"- Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}\n"
                    f"- Должность: {position_title}\n"
                    f"- Дополнительная информация о компании: {duckduckgo_summary}\n\n"
                    f"ТРЕБОВАНИЯ К КУРСУ:\n"
                    f"- Курс должен быть специфичным для компании {getattr(payload, 'companyName', 'Company Name')} и должности {position_title}\n"
                    f"- Содержание должно отражать реальные задачи и обязанности этой должности в данной компании\n"
                    f"- Учитывай специфику отрасли и корпоративную культуру компании\n"
                    f"- Создай РОВНО 4 модуля с УНИКАЛЬНЫМИ названиями\n"
                    f"- В каждом модуле должно быть ОТ 5 ДО 7 уроков\n"
                    f"- Названия модулей и уроков должны быть КРЕАТИВНЫМИ и РАЗНООБРАЗНЫМИ\n"
                    f"- Избегай повторяющихся формулировок\n"
                    f"- Каждый урок должен быть конкретным и практичным для данной должности\n"
                    f"- НЕ добавляй номера модулей в названия (например, 'Модуль 1:', 'Модуль 2:' и т.д.)\n"
                    f"- Используй только описательные названия модулей без префиксов\n"
                ),
                "modules": 4,
                "lessonsPerModule": "5-7",
                "language": language
            }
        
        # Generate the course outline
        outline_text = await stream_openai_response_direct(prompt, model=LLM_DEFAULT_MODEL)
        
        # Parse the outline text to extract modules with lessons
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = outline_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            parsed_outline = json.loads(cleaned_response)
            
            if not isinstance(parsed_outline, list):
                raise ValueError("Response is not a list")
                
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"[COURSE OUTLINE] Failed to parse JSON response: {e}")
            logger.warning(f"[COURSE OUTLINE] Raw response was: '{outline_text}'")
            # Fall back to default modules
            parsed_outline = []
        
        # Extract modules with lessons
        course_modules = []
        for i, module in enumerate(parsed_outline):
            if i < 4:  # Limit to 4 modules as per UI design
                module_data = {
                    "title": module.get('title', f'Модуль {i+1}'),
                    "lessons": module.get('lessons', [])
                }
                course_modules.append(module_data)
                logger.info(f"[COURSE OUTLINE] Module {i+1}: {module_data['title']} with {len(module_data['lessons'])} lessons")
                for j, lesson in enumerate(module_data['lessons']):
                    logger.info(f"[COURSE OUTLINE] - Lesson {j+1}: {lesson}")
        
        # Ensure we have exactly 4 modules (pad with default modules if needed)
        while len(course_modules) < 4:
            if language == "en":
                course_modules.append({
                    "title": f'Module {len(course_modules) + 1}',
                    "lessons": []
                })
            elif language == "es":
                course_modules.append({
                    "title": f'Módulo {len(course_modules) + 1}',
                    "lessons": []
                })
            elif language == "ua":
                course_modules.append({
                    "title": f'Модуль {len(course_modules) + 1}',
                    "lessons": []
                })
            else:  # Russian
                course_modules.append({
                    "title": f'Модуль {len(course_modules) + 1}',
                    "lessons": []
                })
        
        logger.info(f"[COURSE OUTLINE] Generated {len(course_modules)} modules with lessons for landing page")
        return course_modules
        
    except Exception as e:
        logger.error(f"[COURSE OUTLINE] Error generating course outline for landing page: {e}")
        # Return default modules as fallback
        if language == "en":
            return [
                {
                    "title": "Company Introduction and Corporate Culture",
                    "lessons": ["Company Overview", "Corporate Values and Standards", "Organizational Structure", "Policies and Procedures", "Communication Systems"]
                },
                {
                    "title": "Work Fundamentals and Professional Skills",
                    "lessons": ["Technical Job Requirements", "Work Processes and Procedures", "Tools and Systems", "Work Quality and Standards", "Safety and Compliance"]
                },
                {
                    "title": "Team and Customer Interaction",
                    "lessons": ["Teamwork", "Customer Service", "Conflict Management", "Effective Communication", "Feedback and Development"]
                },
                {
                    "title": "Development and Career Growth",
                    "lessons": ["Goal Setting", "Development Planning", "Performance Evaluation", "Growth Opportunities", "Continuous Learning"]
                }
            ]
        elif language == "es":
            return [
                {
                    "title": "Introducción a la Empresa y Cultura Corporativa",
                    "lessons": ["Visión General de la Empresa", "Valores y Estándares Corporativos", "Estructura Organizacional", "Políticas y Procedimientos", "Sistemas de Comunicación"]
                },
                {
                    "title": "Fundamentos del Trabajo y Habilidades Profesionales",
                    "lessons": ["Requisitos Técnicos del Puesto", "Procesos y Procedimientos de Trabajo", "Herramientas y Sistemas", "Calidad del Trabajo y Estándares", "Seguridad y Cumplimiento"]
                },
                {
                    "title": "Interacción con el Equipo y Clientes",
                    "lessons": ["Trabajo en Equipo", "Servicio al Cliente", "Gestión de Conflictos", "Comunicación Efectiva", "Retroalimentación y Desarrollo"]
                },
                {
                    "title": "Desarrollo y Crecimiento Profesional",
                    "lessons": ["Establecimiento de Objetivos", "Planificación del Desarrollo", "Evaluación del Rendimiento", "Oportunidades de Crecimiento", "Aprendizaje Continuo"]
                }
            ]
        elif language == "ua":
            return [
                {
                    "title": "Введення в компанію та корпоративну культуру",
                    "lessons": ["Огляд компанії", "Корпоративні цінності та стандарти", "Організаційна структура", "Політики та процедури", "Системи комунікації"]
                },
                {
                    "title": "Основи роботи та професійні навички",
                    "lessons": ["Технічні вимоги до посади", "Робочі процеси та процедури", "Інструменти та системи", "Якість роботи та стандарти", "Безпека та відповідність"]
                },
                {
                    "title": "Взаємодія з командою та клієнтами",
                    "lessons": ["Робота в команді", "Обслуговування клієнтів", "Управління конфліктами", "Ефективна комунікація", "Зворотний зв'язок та розвиток"]
                },
                {
                    "title": "Розвиток та кар'єрне зростання",
                    "lessons": ["Постановка цілей", "Планування розвитку", "Оцінка продуктивності", "Можливості зростання", "Безперервне навчання"]
                }
            ]
        else:
            return [
                {
                    "title": "Введение в компанию и корпоративную культуру",
                    "lessons": ["Знакомство с компанией", "Корпоративные ценности и стандарты", "Организационная структура", "Политики и процедуры", "Системы коммуникации"]
                },
                {
                    "title": "Основы работы и профессиональные навыки",
                    "lessons": ["Технические требования к должности", "Рабочие процессы и процедуры", "Инструменты и системы", "Качество работы и стандарты", "Безопасность и соответствие"]
                },
                {
                    "title": "Взаимодействие с командой и клиентами",
                    "lessons": ["Работа в команде", "Обслуживание клиентов", "Управление конфликтами", "Эффективная коммуникация", "Обратная связь и развитие"]
                },
                {
                    "title": "Развитие и карьерный рост",
                    "lessons": ["Постановка целей", "Планирование развития", "Оценка производительности", "Возможности роста", "Непрерывное обучение"]
                }
            ]


async def generate_course_templates(duckduckgo_summary: str, job_positions: list, payload, course_outline_modules: list = None, language: str = "ru") -> list:
    """
    Generate course templates by combining real job positions with AI-generated positions.
    Returns exactly 6 course templates with dynamic content.
    """
    try:
        logger.info(f"🎓 [COURSE TEMPLATES] Starting course templates generation")
        logger.info(f"🎓 [COURSE TEMPLATES] Real job positions: {len(job_positions)}")
        
        # Calculate total modules and lessons from course outline
        total_modules = 0
        total_lessons = 0
        if course_outline_modules:
            total_modules = len(course_outline_modules)
            total_lessons = sum(len(module.get('lessons', [])) for module in course_outline_modules)
            logger.info(f"🎓 [COURSE TEMPLATES] Course outline data: {total_modules} modules, {total_lessons} lessons")
        
        # Start with real job positions
        course_templates = []
        
        # Add real job positions first
        for i, position in enumerate(job_positions[:6]):  # Take up to 6 real positions
            job_title = position.get("title", f"Position {i+1}")
            
            # Generate proper course description for scraped positions
            course_description = await generate_course_description_for_position(
                job_title, 
                getattr(payload, 'companyName', 'Company Name'), 
                duckduckgo_summary,
                language
            )
            
            # Generate AI image for the job position
            logger.info(f"🎨 [COURSE TEMPLATES] Generating AI image for position: {job_title}")
            ai_image_path = await generate_ai_image_for_job_position(
                job_title,
                getattr(payload, 'companyName', 'Company Name')
            )
            logger.info(f"🎨 [COURSE TEMPLATES] Generated AI image path: {ai_image_path}")
            
            course_template = {
                "title": job_title,
                "description": course_description,
                "modules": total_modules if total_modules > 0 else random.randint(4, 6),
                "lessons": total_lessons if total_lessons > 0 else random.randint(15, 30),
                "rating": "5.0",
                "image": ai_image_path
            }
            course_templates.append(course_template)
        
        # If we need more positions to reach 6, generate them with AI
        if len(course_templates) < 6:
            needed_positions = 6 - len(course_templates)
            logger.info(f"🎓 [COURSE TEMPLATES] Generating {needed_positions} additional positions with AI")
            
            additional_positions = await generate_additional_positions(duckduckgo_summary, needed_positions, payload, getattr(payload, 'language', 'ru'))
            
            for i, position in enumerate(additional_positions):
                job_title = position.get("title", f"Generated Position {i+1}")
                
                # Generate AI image for the AI-generated position
                logger.info(f"🎨 [COURSE TEMPLATES] Generating AI image for AI-generated position: {job_title}")
                ai_image_path = await generate_ai_image_for_job_position(
                    job_title,
                    getattr(payload, 'companyName', 'Company Name')
                )
                logger.info(f"🎨 [COURSE TEMPLATES] Generated AI image path for AI-generated position: {ai_image_path}")
                
                course_template = {
                    "title": job_title,
                    "description": position.get("description", "Описание курса для данной позиции."),
                    "modules": total_modules if total_modules > 0 else random.randint(4, 6),
                    "lessons": total_lessons if total_lessons > 0 else random.randint(15, 30),
                    "rating": "5.0",
                    "image": ai_image_path
                }
                course_templates.append(course_template)
        
        logger.info(f"🎓 [COURSE TEMPLATES] Generated {len(course_templates)} course templates")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [COURSE TEMPLATES] - Template {i+1}: {template['title']}")
        
        return course_templates
        
    except Exception as e:
        logger.error(f"❌ [COURSE TEMPLATES] Error generating course templates: {e}")
        # Fallback to default templates
        return [
            {
                "title": "HVAC Installer",
                "description": "Обучение установке, обслуживанию и ремонту систем HVAC оборудования.",
                "modules": 5,
                "lessons": 25,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-1-mobile.png"
            },
            {
                "title": "Electrician", 
                "description": "Обучение монтажу, подключению и обслуживанию электрических систем.",
                "modules": 5,
                "lessons": 22,
                "rating": "4.6",
                "image": "/custom-projects-ui/images/audit-section-5-job-2-mobile.png"
            },
            {
                "title": "Service Technician",
                "description": "Обучение диагностике, техническому обслуживанию и проверке оборудования.",
                "modules": 5,
                "lessons": 18,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-3-mobile.png"
            },
            {
                "title": "Project Manager",
                "description": "Обучение планированию, организации и контролю проектов.",
                "modules": 5,
                "lessons": 14,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-4-mobile.png"
            },
            {
                "title": "Field Operations Manager",
                "description": "Обучение управлению процессами и координации полевых команд.",
                "modules": 5,
                "lessons": 22,
                "rating": "4.6",
                "image": "/custom-projects-ui/images/audit-section-5-job-5-desktop.png"
            },
            {
                "title": "Slide Deck Specialist",
                "description": "Обучение созданию презентаций и визуальных обучающих материалов.",
                "modules": 5,
                "lessons": 18,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-6-desktop.png"
            }
        ]


async def generate_additional_positions(duckduckgo_summary: str, count: int, payload, language: str = "ru") -> list:
    """
    Generate additional job positions using AI based on company industry and context.
    """
    try:
        # 📊 DETAILED LOGGING: Language parameter in additional positions generation
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - language: '{language}'")
        # Determine language for logging
        language_name = "English" if language == "en" else "Spanish" if language == "es" else "Ukrainian" if language == "ua" else "Russian"
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - will use {language_name} prompts")
        
        if language == "en":
            prompt = f"""
            Analyze the company data and generate {count} additional logical positions for training courses.
            
            QUESTIONNAIRE DATA:
            - Company name: {getattr(payload, 'companyName', 'Company Name')}
            - Company description: {getattr(payload, 'companyDesc', 'Company Description')}
            - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            INTERNET DATA:
            {duckduckgo_summary}
            
            INSTRUCTIONS:
            - Generate {count} logical positions that fit this company and industry
            - Each position should be realistic and suitable for training courses
            - Positions should complement existing vacancies
            - Course description should be BRIEF (maximum 100 characters)
            - Use format: "Training [key skills/processes] for [position]"
            - Return data in JSON format: [{{"title": "Position Title", "description": "Brief training course description"}}]
            - Generate ALL content EXCLUSIVELY in English
            
            EXAMPLES OF POSITIONS AND DESCRIPTIONS:
            - {{"title": "Customer Support", "description": "Training in customer service and problem solving."}}
            - {{"title": "Marketing Specialist", "description": "Training in marketing fundamentals and product promotion."}}
            - {{"title": "Logistics Coordinator", "description": "Training in supply chain management and logistics."}}
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Analiza los datos de la empresa y genera {count} posiciones lógicas adicionales para cursos de capacitación.
            
            DATOS DEL CUESTIONARIO:
            - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
            - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
            - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            DATOS DE INTERNET:
            {duckduckgo_summary}
            
            INSTRUCCIONES:
            - Genera {count} posiciones lógicas que se ajusten a esta empresa e industria
            - Cada posición debe ser realista y adecuada para cursos de capacitación
            - Las posiciones deben complementar las vacantes existentes
            - La descripción del curso debe ser BREVE (máximo 100 caracteres)
            - Usa el formato: "Capacitación en [habilidades/procesos clave] para [posición]"
            - Devuelve los datos en formato JSON: [{{"title": "Título de la Posición", "description": "Breve descripción del curso de capacitación"}}]
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            EJEMPLOS DE POSICIONES Y DESCRIPCIONES:
            - {{"title": "Atención al Cliente", "description": "Capacitación en servicio al cliente y resolución de problemas."}}
            - {{"title": "Especialista en Marketing", "description": "Capacitación en fundamentos de marketing y promoción de productos."}}
            - {{"title": "Coordinador de Logística", "description": "Capacitación en gestión de cadena de suministro y logística."}}
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Проаналізуйте дані компанії та згенеруйте {count} додаткових логічних позицій для курсів навчання.
            
            ДАНІ АНКЕТИ:
            - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
            - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
            - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            ДАНІ З ІНТЕРНЕТУ:
            {duckduckgo_summary}
            
            ІНСТРУКЦІЇ:
            - Згенеруйте {count} логічних позицій, які підходять для цієї компанії та галузі
            - Кожна позиція повинна бути реалістичною та підходящою для курсу навчання
            - Позиції повинні доповнювати існуючі вакансії
            - Опис курсу повинен бути КОРОТКИМ (максимум 100 символів)
            - Використовуйте формат: "Навчання [ключовим навичкам/процесам] для [позиції]"
            - Поверніть дані у форматі JSON: [{{"title": "Назва позиції", "description": "Короткий опис курсу навчання"}}]
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ПРИКЛАДИ ПОЗИЦІЙ ТА ОПИСІВ:
            - {{"title": "Спеціаліст з обслуговування клієнтів", "description": "Навчання роботі з клієнтами та вирішенню проблем."}}
            - {{"title": "Спеціаліст з маркетингу", "description": "Навчання основам маркетингу та просування товарів."}}
            - {{"title": "Координатор логістики", "description": "Навчання управлінню постачанням та логістикою."}}
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:
            prompt = f"""
            Проанализируй данные компании и сгенерируй {count} дополнительных логических позиций для курсов обучения.
            
            ДАННЫЕ АНКЕТЫ:
            - Название компании: {getattr(payload, 'companyName', 'Company Name')}
            - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
            - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            ДАННЫЕ ИЗ ИНТЕРНЕТА:
            {duckduckgo_summary}
            
            ИНСТРУКЦИИ:
            - Сгенерируй {count} логических позиций, которые подходят для данной компании и отрасли
            - Каждая позиция должна быть реалистичной и подходящей для курса обучения
            - Позиции должны дополнять уже существующие вакансии
            - Описание курса должно быть КРАТКИМ (максимум 100 символов)
            - Используй формат: "Обучение [ключевым навыкам/процессам] для [позиции]"
            - Верни данные в формате JSON: [{{"title": "Название позиции", "description": "Краткое описание курса обучения"}}]
            
            ПРИМЕРЫ ПОЗИЦИЙ И ОПИСАНИЙ:
            - {{"title": "Customer Support", "description": "Обучение работе с клиентами и решению проблем."}}
            - {{"title": "Marketing Specialist", "description": "Обучение основам маркетинга и продвижения товаров."}}
            - {{"title": "Logistics Coordinator", "description": "Обучение управлению поставками и логистикой."}}
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Log the raw response for debugging
        logger.info(f"[COURSE TEMPLATES] Raw additional positions response: '{response_text}'")
        
        # 📊 DETAILED LOGGING: Language parameter in response
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - raw response length: {len(response_text)}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - language used: {language_name}")
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            additional_positions = json.loads(cleaned_response)
            
            if not isinstance(additional_positions, list):
                raise ValueError("Response is not a list")
            
            logger.info(f"[COURSE TEMPLATES] Successfully parsed {len(additional_positions)} additional positions")
            return additional_positions
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"[COURSE TEMPLATES] JSON parsing error: {e}")
            logger.error(f"[COURSE TEMPLATES] Raw response was: '{response_text}'")
            # Fallback to default positions based on language
            if language == "en":
                fallback_positions = [
                    {"title": "Customer Support", "description": "Training in customer service and problem solving."},
                    {"title": "Marketing Specialist", "description": "Training in marketing strategies and promotion."},
                    {"title": "Logistics Coordinator", "description": "Training in logistics and supply chain management."},
                    {"title": "Quality Assurance", "description": "Training in quality control and testing."}
                ]
            elif language == "es":
                fallback_positions = [
                    {"title": "Atención al Cliente", "description": "Capacitación en servicio al cliente y resolución de problemas."},
                    {"title": "Especialista en Marketing", "description": "Capacitación en estrategias de marketing y promoción."},
                    {"title": "Coordinador de Logística", "description": "Capacitación en logística y gestión de cadena de suministro."},
                    {"title": "Control de Calidad", "description": "Capacitación en control de calidad y pruebas."}
                ]
            elif language == "ua":
                fallback_positions = [
                    {"title": "Спеціаліст з обслуговування клієнтів", "description": "Навчання роботі з клієнтами та вирішенню проблем."},
                    {"title": "Спеціаліст з маркетингу", "description": "Навчання маркетинговим стратегіям та просуванню."},
                    {"title": "Координатор логістики", "description": "Навчання логістиці та управлінню постачанням."},
                    {"title": "Контроль якості", "description": "Навчання контролю якості та тестуванню."}
                ]
            else:  # Russian
                fallback_positions = [
                    {"title": "Customer Support", "description": "Обучение работе с клиентами и решению их проблем."},
                    {"title": "Marketing Specialist", "description": "Обучение маркетинговым стратегиям и продвижению."},
                    {"title": "Logistics Coordinator", "description": "Обучение управлению логистическими процессами."},
                    {"title": "Quality Assurance", "description": "Обучение контролю качества и тестированию."}
                ]
            return fallback_positions[:count]
            
    except Exception as e:
        logger.error(f"❌ [COURSE TEMPLATES] Error generating additional positions: {e}")
        return []


async def generate_workforce_crisis_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate workforce crisis data including industry, burnout, turnover, losses, search time, and chart data.
    Returns a dictionary with all the dynamic values for the "Кадровый кризис" section.
    """
    try:
        # Generate all workforce crisis data in parallel for efficiency
        industry_task = extract_company_industry(duckduckgo_summary, payload, language)
        burnout_task = extract_burnout_data(duckduckgo_summary, payload, language)
        turnover_task = extract_turnover_data(duckduckgo_summary, payload, language)
        losses_task = extract_losses_data(duckduckgo_summary, payload, language)
        search_time_task = extract_search_time_data(duckduckgo_summary, payload, language)
        chart_data_task = extract_personnel_shortage_chart_data(duckduckgo_summary, payload, language)
        yearly_shortage_task = extract_yearly_shortage_data(duckduckgo_summary, payload, language)
        
        # Wait for all tasks to complete
        industry, burnout, turnover, losses, search_time, chart_data, yearly_shortage = await asyncio.gather(
            industry_task, burnout_task, turnover_task, losses_task, search_time_task, chart_data_task, yearly_shortage_task
        )
        
        # Get grammatically correct industry text variants
        industry_forms = get_industry_text_variants(industry)
        
        workforce_crisis_data = {
            "industry": industry,
            "industryForms": industry_forms,  # Add grammatically correct forms
            "burnout": burnout,
            "turnover": turnover,
            "losses": losses,
            "searchTime": search_time,
            "chartData": chart_data,
            "yearlyShortage": yearly_shortage
        }
        
        logger.info(f"[AI-Audit Landing Page] Generated workforce crisis data: {workforce_crisis_data}")
        return workforce_crisis_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating workforce crisis data: {e}")
        # Return default values as fallback with grammatically correct forms
        industry_forms = get_industry_text_variants("hvac")
        return {
            "industry": "hvac",
            "industryForms": industry_forms,
            "burnout": {"months": "14", "industryName": "HVAC-компаниях"},
            "turnover": {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}},
            "losses": {"amount": "$10К–$18К"},
            "searchTime": {"days": "30–60"},
            "chartData": {
                "industry": "hvac",
                "chartData": [
                    {"month": "Январь", "shortage": 150},
                    {"month": "Февраль", "shortage": 165},
                    {"month": "Март", "shortage": 180},
                    {"month": "Апрель", "shortage": 195},
                    {"month": "Май", "shortage": 210},
                    {"month": "Июнь", "shortage": 225},
                    {"month": "Июль", "shortage": 240},
                    {"month": "Август", "shortage": 255},
                    {"month": "Сентябрь", "shortage": 270},
                    {"month": "Октябрь", "shortage": 285},
                    {"month": "Ноябрь", "shortage": 300},
                    {"month": "Декабрь", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "рост",
                "description": f"Постоянный рост дефицита квалифицированных кадров {industry_forms['crisis_in']}"
            },
            "yearlyShortage": {
                "yearlyShortage": 80000,
                "industry": "hvac",
                "description": f"Типичный дефицит квалифицированных кадров {industry_forms['of_industry']}"
            }
        }


def get_industry_text_variants(industry_name: str) -> dict:
    """Generate grammatically correct industry references for Russian text"""
    
    # Normalize industry name to lowercase
    industry = industry_name.lower().strip()
    
    # Define proper grammatical forms for common industries
    industry_forms = {
        "автомобильная промышленность": {
            "in_sector": "в автомобильном секторе",
            "in_industry": "в автомобильной отрасли",
            "crisis_in": "в автомобильной отрасли",
            "shortage_in": "в автомобильном секторе",
            "of_industry": "автомобильной отрасли"
        },
        "информационные технологии": {
            "in_sector": "в IT-секторе", 
            "in_industry": "в IT-отрасли",
            "crisis_in": "в сфере информационных технологий",
            "shortage_in": "в IT-секторе",
            "of_industry": "IT-отрасли"
        },
        "it": {
            "in_sector": "в IT-секторе", 
            "in_industry": "в IT-отрасли",
            "crisis_in": "в сфере информационных технологий",
            "shortage_in": "в IT-секторе",
            "of_industry": "IT-отрасли"
        },
        "строительство": {
            "in_sector": "в строительном секторе",
            "in_industry": "в строительной отрасли", 
            "crisis_in": "в строительной отрасли",
            "shortage_in": "в строительном секторе",
            "of_industry": "строительной отрасли"
        },
        "медицина": {
            "in_sector": "в медицинском секторе",
            "in_industry": "в медицинской отрасли",
            "crisis_in": "в сфере здравоохранения", 
            "shortage_in": "в медицинском секторе",
            "of_industry": "медицинской отрасли"
        },
        "здравоохранение": {
            "in_sector": "в медицинском секторе",
            "in_industry": "в сфере здравоохранения",
            "crisis_in": "в сфере здравоохранения", 
            "shortage_in": "в медицинском секторе",
            "of_industry": "сферы здравоохранения"
        },
        "образование": {
            "in_sector": "в образовательном секторе",
            "in_industry": "в сфере образования",
            "crisis_in": "в сфере образования", 
            "shortage_in": "в образовательном секторе",
            "of_industry": "сферы образования"
        },
        "hvac": {
            "in_sector": "в HVAC-секторе",
            "in_industry": "в HVAC-отрасли",
            "crisis_in": "в HVAC-отрасли", 
            "shortage_in": "в HVAC-секторе",
            "of_industry": "HVAC-отрасли"
        },
        "производство": {
            "in_sector": "в производственном секторе",
            "in_industry": "в производственной отрасли",
            "crisis_in": "в производственной отрасли", 
            "shortage_in": "в производственном секторе",
            "of_industry": "производственной отрасли"
        },
        "торговля": {
            "in_sector": "в торговом секторе",
            "in_industry": "в торговой отрасли",
            "crisis_in": "в торговой отрасли", 
            "shortage_in": "в торговом секторе",
            "of_industry": "торговой отрасли"
        },
        "финансы": {
            "in_sector": "в финансовом секторе",
            "in_industry": "в финансовой отрасли",
            "crisis_in": "в финансовой отрасли", 
            "shortage_in": "в финансовом секторе",
            "of_industry": "финансовой отрасли"
        }
    }
    
    # Default fallback for unknown industries
    default_forms = {
        "in_sector": f"в {industry} секторе",
        "in_industry": f"в {industry} отрасли", 
        "crisis_in": f"в {industry} отрасли",
        "shortage_in": f"в {industry} секторе",
        "of_industry": f"{industry} отрасли"
    }
    
    return industry_forms.get(industry, default_forms)


async def extract_company_industry(duckduckgo_summary: str, payload, language: str = "ru") -> str:
    """
    Extract the company's primary industry from scraped data.
    """
    if language == "en":
        prompt = f"""
        Determine the company's primary industry based on the provided data.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Determine the company's primary industry
        - Return the industry name in lowercase
        - Use standard industry names from the list:
          * automotive industry
          * information technology (or IT)
          * construction
          * healthcare
          * education
          * manufacturing
          * retail
          * finance
          * HVAC
        - If you cannot determine, return "general services"
        - Generate ALL content EXCLUSIVELY in English
        
        RESPONSE (only industry name in lowercase):
        """
    elif language == "es":
        prompt = f"""
        Determina la industria principal de la empresa basándote en los datos proporcionados.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Determina la industria principal de la empresa
        - Devuelve el nombre de la industria en minúsculas
        - Usa nombres estándar de industrias de la lista:
          * industria automotriz
          * tecnología de la información (o TI)
          * construcción
          * salud
          * educación
          * manufactura
          * retail
          * finanzas
          * HVAC
        - Si no puedes determinar, devuelve "servicios generales"
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        RESPUESTA (solo nombre de la industria en minúsculas):
        """
    elif language == "ua":
        prompt = f"""
        Визначте основну галузь компанії на основі наданих даних.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Визначте основну галузь діяльності компанії
        - Поверніть назву галузі в називному відмінку, малими літерами
        - Використовуйте стандартні назви галузей зі списку:
          * автомобільна промисловість
          * інформаційні технології (або IT)
          * будівництво
          * охорона здоров'я
          * освіта
          * виробництво
          * торгівля
          * фінанси
          * HVAC
        - Якщо не можете визначити, поверніть "загальні послуги"
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ВІДПОВІДЬ (лише назва галузі в називному відмінку, малими літерами):
        """
    else:
        prompt = f"""
        Определи основную отрасль/индустрию компании на основе предоставленных данных.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Определи основную отрасль деятельности компании
        - Верни название отрасли в именительном падеже, строчными буквами
        - Используй стандартные названия отраслей из списка:
          * автомобильная промышленность
          * информационные технологии (или IT)
          * строительство
          * медицина (или здравоохранение)
          * образование
          * производство
          * торговля
          * финансы
          * HVAC
        - Если не можешь определить, верни "общие услуги"
        
        ОТВЕТ (только название отрасли в именительном падеже, строчными буквами):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        industry = response_text.strip().lower()
        if not industry:
            industry = "общие услуги"
        
        logger.info(f"[AI-Audit Landing Page] Extracted industry: {industry}")
        return industry
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting industry: {e}")
        return "HVAC"


async def extract_burnout_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract burnout statistics from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine employee burnout statistics in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Determine the company's industry based on the data
        - Find information about average employee tenure in this industry
        - If no data is available, use typical values for the industry
        - Return ONLY valid JSON without additional text
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - For IT companies: {{"months": "18", "industryName": "IT companies"}}
        - For e-commerce: {{"months": "16", "industryName": "e-commerce companies"}}
        - For construction: {{"months": "12", "industryName": "construction companies"}}
        - For HVAC: {{"months": "14", "industryName": "HVAC companies"}}
        
        IMPORTANT: Respond ONLY with a valid JSON object, without additional text or explanations.
        """
    elif language == "es":
        prompt = f"""
        Analiza los datos y determina las estadísticas de agotamiento de empleados en la industria de la empresa.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Determina la industria de la empresa basándote en los datos
        - Encuentra información sobre la duración promedio de empleados en esta industria
        - Si no hay datos disponibles, usa valores típicos para la industria
        - Devuelve SOLO JSON válido sin texto adicional
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        EJEMPLOS:
        - Para empresas IT: {{"months": "18", "industryName": "empresas de TI"}}
        - Para e-commerce: {{"months": "16", "industryName": "empresas de comercio electrónico"}}
        - Para construcción: {{"months": "12", "industryName": "empresas de construcción"}}
        - Para HVAC: {{"months": "14", "industryName": "empresas HVAC"}}
        
        IMPORTANTE: Responde SOLO con un objeto JSON válido, sin texto adicional o explicaciones.
        """
    elif language == "ua":
        prompt = f"""
        Проаналізуйте дані та визначте статистику вигорання співробітників у галузі компанії.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Визначте галузь компанії на основі даних
        - Знайдіть інформацію про середню тривалість роботи співробітників у цій галузі
        - Якщо даних немає, використовуйте типові значення для галузі
        - Поверніть ЛИШЕ валідний JSON без додаткового тексту
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ПРИКЛАДИ:
        - Для IT-компаній: {{"months": "18", "industryName": "IT-компаніях"}}
        - Для e-commerce: {{"months": "16", "industryName": "e-commerce-компаніях"}}
        - Для будівництва: {{"months": "12", "industryName": "будівельних компаніях"}}
        - Для HVAC: {{"months": "14", "industryName": "HVAC-компаніях"}}
        
        ВАЖЛИВО: Відповідайте ЛИШЕ валідним JSON об'єктом, без додаткового тексту або пояснень.
        """
    else:
        prompt = f"""
        Проанализируй данные и определи статистику выгорания сотрудников в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Определи отрасль компании на основе данных
        - Найди информацию о средней продолжительности работы сотрудников в этой отрасли
        - Если данных нет, используй типичные значения для отрасли
        - Верни ТОЛЬКО валидный JSON без дополнительного текста
        
        ПРИМЕРЫ:
        - Для IT-компании: {{"months": "18", "industryName": "IT-компаниях"}}
        - Для маркетплейса: {{"months": "16", "industryName": "e-commerce-компаниях"}}
        - Для строительства: {{"months": "12", "industryName": "строительных компаниях"}}
        - Для HVAC: {{"months": "14", "industryName": "HVAC-компаниях"}}
        
        ВАЖНО: Отвечай ТОЛЬКО валидным JSON объектом, без дополнительного текста или объяснений.
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Log the raw response for debugging
        logger.info(f"[AI-Audit Landing Page] Raw burnout response: '{response_text}'")
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            burnout_data = json.loads(cleaned_response)
            if "months" not in burnout_data or "industryName" not in burnout_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            burnout_data = {"months": "14", "industryName": "HVAC-компаниях"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted burnout data: {burnout_data}")
        return burnout_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting burnout data: {e}")
        return {"months": "14", "industryName": "HVAC-компаниях"}


async def extract_turnover_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract turnover statistics from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine employee turnover statistics in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about employee turnover in the industry (% of employees leaving per year)
        - Find information about early departures (% of employees leaving in the first months)
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"percentage": "percentage per year", "earlyExit": {{"percentage": "percentage", "months": "months"}}}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Construction: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        RESPONSE (JSON only):
        """
    elif language == "es":
        prompt = f"""
        Analiza los datos y determina las estadísticas de rotación de empleados en la industria de la empresa.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Encuentra información sobre la rotación de empleados en la industria (% de empleados que se van por año)
        - Encuentra información sobre salidas tempranas (% de empleados que se van en los primeros meses)
        - Si no hay datos disponibles, usa valores típicos para la industria
        - Devuelve datos en formato JSON: {{"percentage": "porcentaje por año", "earlyExit": {{"percentage": "porcentaje", "months": "meses"}}}}
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        EJEMPLOS:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Construcción: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        RESPUESTA (solo JSON):
        """
    elif language == "ua":
        prompt = f"""
        Проаналізуйте дані та визначте статистику плинності кадрів у галузі компанії.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Знайдіть інформацію про плинність кадрів у галузі (% звільнень на рік)
        - Знайдіть інформацію про ранні звільнення (% звільнень у перші місяці)
        - Якщо даних немає, використовуйте типові значення для галузі
        - Поверніть дані у форматі JSON: {{"percentage": "відсоток на рік", "earlyExit": {{"percentage": "відсоток", "months": "місяці"}}}}
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ПРИКЛАДИ:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Будівництво: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        ВІДПОВІДЬ (лише JSON):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи статистику текучести кадров в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о текучести кадров в отрасли (% увольнений в год)
        - Найди информацию о ранних увольнениях (% увольнений в первые месяцы)
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"percentage": "процент в год", "earlyExit": {{"percentage": "процент", "months": "месяцы"}}}}
        
        ПРИМЕРЫ:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Строительство: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            turnover_data = json.loads(cleaned_response)
            if "percentage" not in turnover_data or "earlyExit" not in turnover_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Turnover JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            turnover_data = {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}}
        
        logger.info(f"[AI-Audit Landing Page] Extracted turnover data: {turnover_data}")
        return turnover_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting turnover data: {e}")
        return {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}}


async def extract_losses_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract financial losses data from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the company's financial losses for unfilled positions.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about financial losses per year for unfilled positions
        - Consider lost profits, overtime, and downtime
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"amount": "amount in dollars"}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"amount": "$10K–$18K"}}
        - IT: {{"amount": "$15K–$25K"}}
        - Construction: {{"amount": "$8K–$15K"}}
        - Healthcare: {{"amount": "$20K–$35K"}}
        
        RESPONSE (JSON only):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи финансовые потери компании при незакрытой позиции.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о финансовых потерях при незакрытой позиции в год
        - Учитывай упущенную прибыль, переработки и простои
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"amount": "сумма в долларах"}}
        
        ПРИМЕРЫ:
        - HVAC: {{"amount": "$10К–$18К"}}
        - IT: {{"amount": "$15К–$25К"}}
        - Строительство: {{"amount": "$8К–$15К"}}
        - Медицина: {{"amount": "$20К–$35К"}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            losses_data = json.loads(cleaned_response)
            if "amount" not in losses_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Losses JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            losses_data = {"amount": "$10К–$18К"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted losses data: {losses_data}")
        return losses_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting losses data: {e}")
        return {"amount": "$10К–$18К"}


async def extract_search_time_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract candidate search time data from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the average candidate search time in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about average candidate search time in the industry
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"days": "day range"}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"days": "30–60"}}
        - IT: {{"days": "45–90"}}
        - Construction: {{"days": "20–45"}}
        - Healthcare: {{"days": "60–120"}}
        
        RESPONSE (JSON only):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи среднее время поиска кандидата в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о среднем времени поиска кандидата в отрасли
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"days": "диапазон дней"}}
        
        ПРИМЕРЫ:
        - HVAC: {{"days": "30–60"}}
        - IT: {{"days": "45–90"}}
        - Строительство: {{"days": "20–45"}}
        - Медицина: {{"days": "60–120"}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            search_time_data = json.loads(cleaned_response)
            if "days" not in search_time_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Search time JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            search_time_data = {"days": "30–60"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted search time data: {search_time_data}")
        return search_time_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting search time data: {e}")
        return {"days": "30–60"}


async def extract_personnel_shortage_chart_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate structured dataset for the "Shortage of qualified personnel" chart.
    Returns a dictionary with 12 months of personnel shortage data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and generate a structured dataset for the "Shortage of qualified personnel" chart for the last 12 months.

        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}

        INTERNET DATA:
        {duckduckgo_summary}

        INSTRUCTIONS:
        1. Determine the INDUSTRY as a whole (not the specific company) based on the provided data
        2. Analyze personnel shortage for the ENTIRE INDUSTRY, not just the specified company
        3. Generate realistic data with natural fluctuations (NOT linear growth)
        4. Consider industry specifics: seasonality, economic cycles, market events
        5. The shortage scale should match the industry size (large industries = large numbers)
        6. Generate ALL content EXCLUSIVELY in English

        REALISM REQUIREMENTS:
        - FORBIDDEN: perfectly linear increase every month
        - MANDATORY: include monthly fluctuations (some months may show decrease)
        - Seasonal factors: consider industry specifics (e.g., construction - peak in summer, automotive - decrease in August due to vacations)
        - Scale should reflect industry size (manufacturing, IT, finance = thousands of specialists)
        - Include 2-3 months with slight decrease in indicators

        RESPONSE FORMAT:
        Return ONLY a valid JSON object in the following format:
        {{
            "industry": "industry name (not company)",
            "chartData": [
                {{"month": "January", "shortage": 2800}},
                {{"month": "February", "shortage": 2650}},
                {{"month": "March", "shortage": 3100}},
                {{"month": "April", "shortage": 3450}},
                {{"month": "May", "shortage": 3200}},
                {{"month": "June", "shortage": 3800}},
                {{"month": "July", "shortage": 4100}},
                {{"month": "August", "shortage": 3600}},
                {{"month": "September", "shortage": 3900}},
                {{"month": "October", "shortage": 4200}},
                {{"month": "November", "shortage": 3950}},
                {{"month": "December", "shortage": 4300}}
            ],
            "totalShortage": [sum of all shortage values],
            "trend": "growth/stability/decline",
            "description": "Brief description of personnel shortage trend in the industry with mention of key factors"
        }}

        NEGATIVE EXAMPLE (DON'T do this):
        - Data: 100, 110, 120, 130, 140... (too linear and small scale)
        - Focus only on one company instead of industry

        MANDATORY CHECKS:
        - Use only English month names
        - Shortage values - whole numbers corresponding to industry size
        - Minimum 2 months should show decrease compared to previous
        - industry should be industry name, not company name
        - Consider real industry scale when generating numbers
        """
    else:
        prompt = f"""
        Проанализируй данные и сгенерируй структурированный набор данных для графика "Дефицит квалифицированных кадров" за последние 12 месяцев.

        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}

        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}

        ИНСТРУКЦИИ:
        1. Определи ОТРАСЛЬ в целом (не конкретную компанию) на основе предоставленных данных
        2. Анализируй дефицит кадров для ВСЕЙ ОТРАСЛИ, а не только для указанной компании
        3. Сгенерируй реалистичные данные с естественными колебаниями (НЕ линейный рост)
        4. Учти отраслевую специфику: сезонность, экономические циклы, рыночные события
        5. Масштаб дефицита должен соответствовать размеру отрасли (крупные отрасли = большие числа)

        ТРЕБОВАНИЯ К РЕАЛИСТИЧНОСТИ:
        - ЗАПРЕЩЕНО: идеально линейное увеличение каждый месяц
        - ОБЯЗАТЕЛЬНО: включи месячные колебания (некоторые месяцы могут показывать снижение)
        - Сезонные факторы: учти специфику отрасли (например, строительство - пик летом, автопром - снижение в августе из-за отпусков)
        - Масштаб должен отражать размер отрасли (машиностроение, IT, финансы = тысячи специалистов)
        - Включи 2-3 месяца с незначительным снижением показателей

        ФОРМАТ ОТВЕТА:
        Верни ТОЛЬКО валидный JSON объект в следующем формате:
        {{
            "industry": "название отрасли (не компании)",
            "chartData": [
                {{"month": "Январь", "shortage": 2800}},
                {{"month": "Февраль", "shortage": 2650}},
                {{"month": "Март", "shortage": 3100}},
                {{"month": "Апрель", "shortage": 3450}},
                {{"month": "Май", "shortage": 3200}},
                {{"month": "Июнь", "shortage": 3800}},
                {{"month": "Июль", "shortage": 4100}},
                {{"month": "Август", "shortage": 3600}},
                {{"month": "Сентябрь", "shortage": 3900}},
                {{"month": "Октябрь", "shortage": 4200}},
                {{"month": "Ноябрь", "shortage": 3950}},
                {{"month": "Декабрь", "shortage": 4300}}
            ],
            "totalShortage": [сумма всех shortage],
            "trend": "рост/стабильность/снижение",
            "description": "Краткое описание тренда дефицита кадров в отрасли с упоминанием ключевых факторов. Используй правильные падежи: 'в [отрасль] отрасли' или 'в [отрасль] секторе'"
        }}

        ОТРИЦАТЕЛЬНЫЙ ПРИМЕР (НЕ делай так):
        - Данные: 100, 110, 120, 130, 140... (слишком линейно и маленький масштаб)
        - Фокус только на одной компании вместо отрасли

        ОБЯЗАТЕЛЬНЫЕ ПРОВЕРКИ:
        - Используй только русские названия месяцев
        - Значения shortage - целые числа, соответствующие размеру отрасли
        - Минимум 2 месяца должны показывать снижение по сравнению с предыдущим
        - industry должно быть названием отрасли, а не компании
        - Учитывай реальный масштаб отрасли при генерации чисел
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean and parse the response
        cleaned_response = response_text.strip()
        if cleaned_response.startswith('```json'):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith('```'):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        
        # Parse JSON response
        chart_data = json.loads(cleaned_response)
        
        # Validate the structure
        if not isinstance(chart_data, dict) or 'chartData' not in chart_data:
            raise ValueError("Invalid chart data structure")
        
        if not isinstance(chart_data['chartData'], list) or len(chart_data['chartData']) != 12:
            raise ValueError("Chart data must contain exactly 12 months")
        
        # Log the generated data for verification
        logger.info(f"[AI-Audit Landing Page] Generated personnel shortage chart data:")
        logger.info(f"[AI-Audit Landing Page] - Industry: {chart_data.get('industry', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Total shortage: {chart_data.get('totalShortage', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Trend: {chart_data.get('trend', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Chart data points: {len(chart_data.get('chartData', []))}")
        
        # Log each month's data for detailed verification
        for i, month_data in enumerate(chart_data.get('chartData', [])):
            logger.info(f"[AI-Audit Landing Page] - Month {i+1}: {month_data.get('month', 'Unknown')} - {month_data.get('shortage', 'Unknown')} specialists")
        
        return chart_data
        
    except json.JSONDecodeError as e:
        logger.error(f"[AI-Audit Landing Page] Chart data JSON parsing error: {e}")
        logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
        logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
        # Fallback to default values
        return {
            "industry": "HVAC",
            "chartData": [
                {"month": "Январь", "shortage": 150},
                {"month": "Февраль", "shortage": 165},
                {"month": "Март", "shortage": 180},
                {"month": "Апрель", "shortage": 195},
                {"month": "Май", "shortage": 210},
                {"month": "Июнь", "shortage": 225},
                {"month": "Июль", "shortage": 240},
                {"month": "Август", "shortage": 255},
                {"month": "Сентябрь", "shortage": 270},
                {"month": "Октябрь", "shortage": 285},
                {"month": "Ноябрь", "shortage": 300},
                {"month": "Декабрь", "shortage": 315}
            ],
            "totalShortage": 2775,
            "trend": "рост",
            "description": "Постоянный рост дефицита квалифицированных кадров в HVAC-отрасли"
        }
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating chart data: {e}")
        return {
            "industry": "HVAC",
            "chartData": [
                {"month": "Январь", "shortage": 150},
                {"month": "Февраль", "shortage": 165},
                {"month": "Март", "shortage": 180},
                {"month": "Апрель", "shortage": 195},
                {"month": "Май", "shortage": 210},
                {"month": "Июнь", "shortage": 225},
                {"month": "Июль", "shortage": 240},
                {"month": "Август", "shortage": 255},
                {"month": "Сентябрь", "shortage": 270},
                {"month": "Октябрь", "shortage": 285},
                {"month": "Ноябрь", "shortage": 300},
                {"month": "Декабрь", "shortage": 315}
            ],
            "totalShortage": 2775,
            "trend": "рост",
            "description": "Постоянный рост дефицита квалифицированных кадров в HVAC-отрасли"
        }


async def extract_yearly_shortage_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate a single yearly shortage number for the company's specific industry.
    Returns a dictionary with the annual shortage count.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the exact number of missing qualified specialists per year for the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        1. Determine the company's industry based on the provided data
        2. Calculate a realistic number of missing specialists per year for this industry
        3. Consider industry size, growth rates, and current personnel shortage
        4. The number should be realistic and justified for this industry
        5. Consider regional characteristics and industry scale
        6. Generate ALL content EXCLUSIVELY in English
        
        REQUIREMENTS:
        - Return ONLY one number (number of missing specialists per year)
        - The number should be whole
        - The number should be realistic for the industry
        - Consider industry scale (local, regional, national)
        
        RESPONSE FORMAT:
        Return ONLY a valid JSON object in the following format:
        {{
            "yearlyShortage": 80000,
            "industry": "industry name",
            "description": "Brief justification of the number"
        }}
        
        EXAMPLES FOR DIFFERENT INDUSTRIES:
        - HVAC: 45000-80000 specialists per year
        - IT: 120000-200000 specialists per year  
        - Construction: 60000-100000 specialists per year
        - Healthcare: 80000-150000 specialists per year
        - Manufacturing: 70000-120000 specialists per year
        
        IMPORTANT: 
        - The number should be realistic for the industry
        - Consider industry size and scale
        - Include justification in description
        """
    else:
        prompt = f"""
        Проанализируй данные и определи точное количество недостающих квалифицированных специалистов в год для отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        1. Определи отрасль компании на основе предоставленных данных
        2. Рассчитай реалистичное количество недостающих специалистов в год для данной отрасли
        3. Учти размер отрасли, темпы роста и текущий дефицит кадров
        4. Число должно быть реалистичным и обоснованным для данной отрасли
        5. Учти региональные особенности и масштаб отрасли
        
        ТРЕБОВАНИЯ:
        - Верни ТОЛЬКО одно число (количество недостающих специалистов в год)
        - Число должно быть целым
        - Число должно быть реалистичным для отрасли
        - Учти масштаб отрасли (локальная, региональная, национальная)
        
        ФОРМАТ ОТВЕТА:
        Верни ТОЛЬКО валидный JSON объект в следующем формате:
        {{
            "yearlyShortage": 80000,
            "industry": "название отрасли",
            "description": "Краткое обоснование числа. Используй правильные падежи: 'в [отрасль] отрасли' или 'в [отрасль] секторе'"
        }}
        
        ПРИМЕРЫ ДЛЯ РАЗНЫХ ОТРАСЛЕЙ:
        - HVAC: 45000-80000 специалистов в год
        - IT: 120000-200000 специалистов в год  
        - Строительство: 60000-100000 специалистов в год
        - Медицина: 80000-150000 специалистов в год
        - Производство: 70000-120000 специалистов в год
        
        ВАЖНО: 
        - Число должно быть реалистичным для отрасли
        - Учти размер и масштаб отрасли
        - Включи обоснование в description
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean and parse the response
        cleaned_response = response_text.strip()
        if cleaned_response.startswith('```json'):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith('```'):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        
        # Parse JSON response
        yearly_data = json.loads(cleaned_response)
        
        # Validate the structure
        if not isinstance(yearly_data, dict) or 'yearlyShortage' not in yearly_data:
            raise ValueError("Invalid yearly shortage data structure")
        
        if not isinstance(yearly_data['yearlyShortage'], int) or yearly_data['yearlyShortage'] <= 0:
            raise ValueError("Yearly shortage must be a positive integer")
        
        # Log the generated data for verification
        logger.info(f"[AI-Audit Landing Page] Generated yearly shortage data:")
        logger.info(f"[AI-Audit Landing Page] - Industry: {yearly_data.get('industry', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Yearly Shortage: {yearly_data.get('yearlyShortage', 'Unknown')} specialists")
        logger.info(f"[AI-Audit Landing Page] - Description: {yearly_data.get('description', 'No description')}")
        
        return yearly_data
        
    except json.JSONDecodeError as e:
        logger.error(f"[AI-Audit Landing Page] Yearly shortage JSON parsing error: {e}")
        logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
        logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
        # Fallback to default values
        return {
            "yearlyShortage": 80000,
            "industry": "HVAC",
            "description": "Типичный дефицит квалифицированных кадров в HVAC-отрасли"
        }
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating yearly shortage data: {e}")
        return {
            "yearlyShortage": 80000,
            "industry": "HVAC", 
            "description": "Типичный дефицит квалифицированных кадров в HVAC-отрасли"
        }


async def _run_landing_page_generation(payload, request, pool, job_id):
    try:
        # 📊 DETAILED LOGGING: Language preference received in backend
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received payload: {payload.model_dump()}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received language: {payload.language}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received companyWebsite: {payload.companyWebsite}")
        
        # 📊 LOG: Initial payload received
        logger.info(f"🔍 [AUDIT DATA FLOW] Starting landing page generation for job {job_id}")
        logger.info(f"📥 [AUDIT DATA FLOW] Initial payload: {payload.model_dump()}")
        
        set_progress(job_id, "Scraping company website...")
        # Scrape company data from website
        scraped_data = await scrape_company_data_from_website(payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit Landing Page] Scraped company data: {scraped_data.companyName}")
        
        set_progress(job_id, "Researching additional company info...")
        # Get additional research data using scraped company name and description
        duckduckgo_summary = await serpapi_company_research(scraped_data.companyName, scraped_data.companyDesc, payload.companyWebsite)
        logger.info(f"[AI-Audit Landing Page] SERPAPI summary: {duckduckgo_summary[:300]}")
        
        # 📊 LOG: Scraped data received
        logger.info(f"🌐 [AUDIT DATA FLOW] Scraped data length: {len(duckduckgo_summary)} characters")
        logger.info(f"🌐 [AUDIT DATA FLOW] Scraped data preview: {duckduckgo_summary[:500]}...")

        set_progress(job_id, "Using scraped company name...")
        company_name = scraped_data.companyName
        
        # 📊 LOG: Company name generated
        logger.info(f"🏢 [AUDIT DATA FLOW] Generated company name: '{company_name}'")

        set_progress(job_id, "Using scraped company description...")
        company_description = scraped_data.companyDesc

        # 📊 LOG: Company description generated
        logger.info(f"📝 [AUDIT DATA FLOW] Generated company description: '{company_description}'")

        set_progress(job_id, "Generating job positions from scraped data...")
        # Create a combined payload with scraped data for job positions generation
        combined_payload = type('CombinedPayload', (), {
            'companyName': scraped_data.companyName,
            'companyDesc': scraped_data.companyDesc,
            'companyWebsite': payload.companyWebsite,
            'employees': scraped_data.employees,
            'franchise': scraped_data.franchise,
            'onboardingProblems': scraped_data.onboardingProblems,
            'documents': scraped_data.documents,
            'documentsOther': scraped_data.documentsOther,
            'priorities': scraped_data.priorities,
            'priorityOther': scraped_data.priorityOther
        })()
        # Generate job positions using the same logic as the old audit
        job_positions = await generate_job_positions_from_scraped_data(duckduckgo_summary, combined_payload, company_name, payload.language)
        
        # 📊 LOG: Job positions generated
        logger.info(f"💼 [AUDIT DATA FLOW] Generated {len(job_positions)} job positions")
        for i, position in enumerate(job_positions):
            logger.info(f"💼 [AUDIT DATA FLOW] - Position {i+1}: {position}")

        set_progress(job_id, "Generating workforce crisis data...")
        # Generate workforce crisis data for the "Кадровый кризис" section
        workforce_crisis_data = await generate_workforce_crisis_data(duckduckgo_summary, combined_payload, payload.language)
        
        # 📊 LOG: Workforce crisis data generated
        logger.info(f"📊 [AUDIT DATA FLOW] Generated workforce crisis data: {workforce_crisis_data}")

        set_progress(job_id, "Generating course outline...")
        # Generate course outline for the "План обучения" section
        course_outline_modules = await generate_course_outline_for_landing_page(duckduckgo_summary, job_positions, combined_payload, payload.language)
        
        # 📊 LOG: Course outline generated
        logger.info(f"📚 [AUDIT DATA FLOW] Generated course outline with {len(course_outline_modules)} modules")
        for i, module_title in enumerate(course_outline_modules):
            logger.info(f"📚 [AUDIT DATA FLOW] - Module {i+1}: {module_title}")

        set_progress(job_id, "Generating course templates...")
        # Generate course templates for the "Готовые шаблоны курсов" section
        course_templates = await generate_course_templates(duckduckgo_summary, job_positions, combined_payload, course_outline_modules, payload.language)
        
        # 📊 LOG: Course templates generated
        logger.info(f"🎓 [AUDIT DATA FLOW] Generated {len(course_templates)} course templates")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [AUDIT DATA FLOW] - Template {i+1}: {template['title']}")

        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Create the landing page content with dynamic data
        landing_page_data = {
            "companyName": company_name,
            "companyDescription": company_description,
            "jobPositions": job_positions,
            "workforceCrisis": workforce_crisis_data,
            "courseOutlineModules": course_outline_modules,
            "courseTemplates": course_templates,
            "language": payload.language,
            "originalPayload": payload.model_dump()
        }
        
        # 📊 DETAILED LOGGING: Language preference in landing page data
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Landing page data - language: '{landing_page_data['language']}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Landing page data - payload.language: '{payload.language}'")
        
        # 📊 LOG: Landing page data structure created
        logger.info(f"📦 [AUDIT DATA FLOW] Landing page data structure created:")
        logger.info(f"📦 [AUDIT DATA FLOW] - companyName: '{landing_page_data['companyName']}'")
        logger.info(f"📦 [AUDIT DATA FLOW] - companyDescription: '{landing_page_data['companyDescription']}'")
        logger.info(f"📦 [AUDIT DATA FLOW] - originalPayload keys: {list(landing_page_data['originalPayload'].keys())}")

        # Save as a product
        project_id = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит Landing Page: {company_name}",
            microproduct_content=landing_page_data,
            chat_session_id=None
        )

        logger.info(f"[AI-Audit Landing Page] Successfully created project with ID: {project_id}")
        
        # 📊 LOG: Project saved to database
        logger.info(f"💾 [AUDIT DATA FLOW] Project saved to database with ID: {project_id}")
        logger.info(f"💾 [AUDIT DATA FLOW] Project name: 'AI-Аудит Landing Page: {company_name}'")
        
        # 🔧 FIX: Assign landing page to existing audit folder or create new one
        # First, try to find an existing audit folder for this company
        async with pool.acquire() as conn:
            existing_folder_query = """
            SELECT pf.id 
            FROM project_folders pf
            JOIN projects p ON pf.id = p.folder_id
            WHERE pf.onyx_user_id = $1 
            AND p.microproduct_name LIKE 'AI-Аудит: %'
            AND p.microproduct_name LIKE $2
            LIMIT 1
            """
            existing_folder = await conn.fetchrow(existing_folder_query, onyx_user_id, f"%{company_name}%")
            
            if existing_folder:
                # Assign to existing folder
                folder_id = existing_folder["id"]
                await conn.execute("UPDATE projects SET folder_id = $1 WHERE id = $2", folder_id, project_id)
                logger.info(f"🔧 [AUDIT DATA FLOW] Assigned landing page to existing folder: {folder_id}")
            else:
                # Create new folder and assign
                folder_id = await create_audit_folder(pool, onyx_user_id, company_name)
                await conn.execute("UPDATE projects SET folder_id = $1 WHERE id = $2", folder_id, project_id)
                logger.info(f"🔧 [AUDIT DATA FLOW] Created new folder and assigned landing page: {folder_id}")

        set_progress(job_id, "Landing page complete!")
        logger.info(f"[AI-Audit Landing Page] Finished the Landing Page Generation")
        
        # 📊 LOG: Final response data
        final_response = {
            "id": project_id,
            "name": f"AI-Аудит Landing Page: {company_name}",
            "companyName": company_name,
            "companyDescription": company_description
        }
        logger.info(f"📤 [AUDIT DATA FLOW] Final response data: {final_response}")
    
        return final_response
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error: {e}")
        set_progress(job_id, f"Error: {str(e)}")


def extract_open_positions_from_table(parsed_json):
    """
    Extracts open positions from a TableBlock in parsed_json.contentBlocks.
    Returns a list of dicts, one per position, with keys matching the table headers.
    Removes trailing '*' from header keys.
    """
    def clean_key(key):
        # Remove all trailing and leading '*' and whitespace
        return key.strip().rstrip("*").lstrip("*").strip()

    for block in getattr(parsed_json, "contentBlocks", []):
        if getattr(block, "type", None) == "table":
            headers = getattr(block, "headers", [])
            rows = getattr(block, "rows", [])
            # Normalize header names for easier matching
            header_map = {clean_key(h).lower(): i for i, h in enumerate(headers)}
            print("HEADER MAP:", header_map)
            if "позиция" in header_map:
                positions = []
                for row in rows:
                    position = {clean_key(headers[i]): row[i] for i in range(min(len(headers), len(row)))}
                    positions.append(position)

                return positions
    return []


async def generate_company_specific_fallback_positions(company_name: str, language: str = "ru") -> list:
    """Generate company-specific fallback positions when no real positions are found."""
    try:
        if language == "en":
            prompt = f"""
            Create a list of 3-5 logical positions for the company {company_name}.
            
            INSTRUCTIONS:
            - Create positions that logically fit this company
            - Use realistic job titles
            - Add a brief description for each position
            - Generate ALL content EXCLUSIVELY in English
            
            RESPONSE FORMAT (JSON only):
            [
                {{"Position": "position title 1", "Description": "brief description"}},
                {{"Position": "position title 2", "Description": "brief description"}},
                ...
            ]
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Crea una lista de 3-5 posiciones lógicas para la empresa {company_name}.
            
            INSTRUCCIONES:
            - Crea posiciones que se ajusten lógicamente a esta empresa
            - Usa títulos de trabajo realistas
            - Agrega una descripción breve para cada posición
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            FORMATO DE RESPUESTA (solo JSON):
            [
                {{"Position": "título de posición 1", "Description": "descripción breve"}},
                {{"Position": "título de posición 2", "Description": "descripción breve"}},
                ...
            ]
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Створіть список з 3-5 логічних позицій для компанії {company_name}.
            
            ІНСТРУКЦІЇ:
            - Створіть позиції, які логічно підходять для цієї компанії
            - Використовуйте реалістичні назви посад
            - Додайте короткий опис для кожної позиції
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ФОРМАТ ВІДПОВІДІ (тільки JSON):
            [
                {{"Position": "назва позиції 1", "Description": "короткий опис"}},
                {{"Position": "назва позиції 2", "Description": "короткий опис"}},
                ...
            ]
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:  # Russian
            prompt = f"""
            Создай список из 3-5 логичных должностей для компании {company_name}.
            
            ИНСТРУКЦИИ:
            - Создай позиции, которые логично подходят для данной компании
            - Используй реалистичные названия должностей
            - Добавь краткое описание для каждой позиции
            
            ФОРМАТ ОТВЕТА (только JSON):
            [
                {{"Position": "название позиции 1", "Description": "краткое описание"}},
                {{"Position": "название позиции 2", "Description": "краткое описание"}},
                ...
            ]
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            positions = json.loads(cleaned_response)
            formatted_positions = []
            for position in positions:
                # Handle different field names based on language
                title = position.get("Position", position.get("Позиция", "Position"))
                description = position.get("Description", position.get("Описание", f"Open position at {company_name}"))
                formatted_positions.append({
                    "title": title,
                    "description": description,
                    "icon": "👷"
                })
            logger.info(f"💼 [WEBSITE SCRAPING] Generated {len(formatted_positions)} company-specific fallback positions")
            return formatted_positions
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse fallback positions JSON: {e}")
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Raw response was: '{response_text}'")
            # Language-specific generic fallback
            if language == "en":
                return [
                    {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
                    {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
                    {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"}
                ]
            elif language == "es":
                return [
                    {"title": "Representante de Ventas", "description": f"Ventas y desarrollo comercial en {company_name}", "icon": "💼"},
                    {"title": "Atención al Cliente", "description": f"Servicio al cliente y soporte en {company_name}", "icon": "🎧"},
                    {"title": "Gerente de Operaciones", "description": f"Gestión de operaciones y procesos en {company_name}", "icon": "⚙️"}
                ]
            elif language == "ua":
                return [
                    {"title": "Представник з продажів", "description": f"Продажі та розвиток бізнесу в {company_name}", "icon": "💼"},
                    {"title": "Служба підтримки клієнтів", "description": f"Обслуговування клієнтів та підтримка в {company_name}", "icon": "🎧"},
                    {"title": "Менеджер операцій", "description": f"Управління операціями та процесами в {company_name}", "icon": "⚙️"}
                ]
            else:  # Russian
                return [
                    {"title": "Менеджер по продажам", "description": f"Продажи и развитие бизнеса в {company_name}", "icon": "💼"},
                    {"title": "Служба поддержки", "description": f"Обслуживание клиентов и поддержка в {company_name}", "icon": "🎧"},
                    {"title": "Менеджер операций", "description": f"Управление операциями и процессами в {company_name}", "icon": "⚙️"}
                ]
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error generating fallback positions: {e}")
        return [
            {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
            {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
            {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"}
        ]

async def extract_job_positions_from_website_content(website_content: str, company_name: str, language: str = "ru") -> list:
    """Extract job positions directly from website content using AI."""
    try:
        if language == "en":
            prompt = f"""
            Analyze the website content and extract a list of open job positions for the company.
            
            COMPANY: {company_name}
            WEBSITE CONTENT:
            {website_content}
            
            INSTRUCTIONS:
            - Find all mentions of job openings, positions, career opportunities
            - Extract specific position titles (e.g., "Sales Manager", "Mechanical Engineer", "Marketing Specialist")
            - If no specific vacancies are found, create logical positions for this company
            - Return maximum 8 real positions
            - Generate ALL content EXCLUSIVELY in English
            
            RESPONSE FORMAT (JSON only):
            [
                {{"Position": "position title 1", "Description": "brief description"}},
                {{"Position": "position title 2", "Description": "brief description"}},
                ...
            ]
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Analiza el contenido del sitio web y extrae una lista de puestos de trabajo abiertos para la empresa.
            
            EMPRESA: {company_name}
            CONTENIDO DEL SITIO WEB:
            {website_content}
            
            INSTRUCCIONES:
            - Encuentra todas las menciones de ofertas de trabajo, posiciones, oportunidades de carrera
            - Extrae títulos de posiciones específicas (ej: "Gerente de Ventas", "Ingeniero Mecánico", "Especialista en Marketing")
            - Si no se encuentran vacantes específicas, crea posiciones lógicas para esta empresa
            - Devuelve máximo 8 posiciones reales
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            FORMATO DE RESPUESTA (solo JSON):
            [
                {{"Position": "título de posición 1", "Description": "descripción breve"}},
                {{"Position": "título de posición 2", "Description": "descripción breve"}},
                ...
            ]
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Проаналізуйте вміст веб-сайту та витягніть список відкритих вакансій для компанії.
            
            КОМПАНІЯ: {company_name}
            ВМІСТ ВЕБ-САЙТУ:
            {website_content}
            
            ІНСТРУКЦІЇ:
            - Знайдіть усі згадки про вакансії, посади, кар'єрні можливості
            - Витягніть назви конкретних посад (наприклад: "Менеджер з продажів", "Інженер-механік", "Спеціаліст з маркетингу")
            - Якщо конкретних вакансій не знайдено, створіть логічні позиції для цієї компанії
            - Поверніть максимум 8 реальних позицій
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ФОРМАТ ВІДПОВІДІ (тільки JSON):
            [
                {{"Position": "назва позиції 1", "Description": "короткий опис"}},
                {{"Position": "назва позиції 2", "Description": "короткий опис"}},
                ...
            ]
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:
            prompt = f"""
            Проанализируй контент веб-сайта и извлеки список открытых вакансий компании.
            
            КОМПАНИЯ: {company_name}
            КОНТЕНТ ВЕБ-САЙТА:
            {website_content}
            
            ИНСТРУКЦИИ:
            - Найди все упоминания вакансий, должностей, карьерных возможностей
            - Извлеки названия конкретных позиций (например: "Менеджер по продажам", "Инженер-механик", "Специалист по маркетингу")
            - Если конкретных вакансий нет, создай логичные позиции для данной компании
            - Верни максимум 8 реальных позиций
            
            ФОРМАТ ОТВЕТА (только JSON):
            [
                {{"Позиция": "название позиции 1", "Описание": "краткое описание"}},
                {{"Позиция": "название позиции 2", "Описание": "краткое описание"}},
                ...
            ]
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            job_positions = json.loads(cleaned_response)
            
            if not isinstance(job_positions, list):
                raise ValueError("Response is not a list")
            
            logger.info(f"💼 [WEBSITE SCRAPING] Extracted {len(job_positions)} job positions from website")
            return job_positions
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse job positions JSON: {e}")
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Raw response was: '{response_text}'")
            return []
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting job positions: {e}")
        return []

async def generate_job_positions_from_scraped_data(duckduckgo_summary: str, payload, company_name: str, language: str = "ru") -> list:
    """
    Generates job positions directly from scraped website content using AI.
    More efficient than generating a full audit one-pager.
    Ensures exactly 11 vacancies are returned by generating additional ones if needed.
    """
    try:
        # 📊 LOG: Starting job positions generation
        logger.info(f"🔍 [AUDIT DATA FLOW] generate_job_positions_from_scraped_data called")
        logger.info(f"🔍 [AUDIT DATA FLOW] Scraped data length: {len(duckduckgo_summary)} characters")
        
        # Extract job positions directly from scraped content using AI
        job_positions = await extract_job_positions_from_website_content(duckduckgo_summary, company_name, language)
        
        # Convert to the format expected by the frontend
        formatted_positions = []
        for position in job_positions:
            # Get the position title and description - handle different field names based on language
            position_title = position.get("Position", position.get("Позиция", "Position"))
            position_description = position.get("Description", position.get("Описание", f"Open position at {company_name}"))
            formatted_positions.append({
                "title": position_title,
                "description": position_description,
                "icon": "👷"  # Default icon
            })
        
        # 📊 LOG: Job positions extracted and formatted
        logger.info(f"🔍 [AUDIT DATA FLOW] Extracted {len(job_positions)} raw positions")
        logger.info(f"🔍 [AUDIT DATA FLOW] Formatted {len(formatted_positions)} positions for frontend")
        
        # If no positions found, use company-specific fallback
        if not formatted_positions:
            logger.info(f"🔍 [AUDIT DATA FLOW] No positions found, using company-specific fallback")
            # Generate company-specific fallback positions
            fallback_positions = await generate_company_specific_fallback_positions(company_name, language)
            formatted_positions = fallback_positions
        
        # Ensure exactly 11 vacancies by generating additional ones if needed
        target_count = 11
        if len(formatted_positions) < target_count:
            needed_positions = target_count - len(formatted_positions)
            logger.info(f"🔍 [AUDIT DATA FLOW] Need {needed_positions} additional positions to reach {target_count} total")
            
            # Generate additional positions using the same logic as course templates
            additional_positions = await generate_additional_positions(duckduckgo_summary, needed_positions, payload, language)
            
            # Convert additional positions to the expected format
            for position in additional_positions:
                formatted_positions.append({
                    "title": position.get("title", "Generated Position"),
                    "description": position.get("description", f"Open position at {company_name}"),
                    "icon": "👷"  # Default icon
                })
            
            logger.info(f"🔍 [AUDIT DATA FLOW] Added {len(additional_positions)} additional positions")
        
        # Ensure we don't exceed 11 positions
        if len(formatted_positions) > target_count:
            formatted_positions = formatted_positions[:target_count]
            logger.info(f"🔍 [AUDIT DATA FLOW] Trimmed positions to exactly {target_count}")
        
        logger.info(f"🔍 [AUDIT DATA FLOW] Final result: {len(formatted_positions)} positions")
        return formatted_positions
        
    except Exception as e:
        logger.error(f"❌ [AUDIT DATA FLOW] Error generating job positions: {e}")
        # Return company-specific fallback positions
        try:
            fallback_positions = await generate_company_specific_fallback_positions(company_name, language)
            # Ensure we have exactly 11 positions
            while len(fallback_positions) < 11:
                fallback_positions.append({
                    "title": f"Position {len(fallback_positions) + 1}",
                    "description": f"Open position at {company_name}",
                    "icon": "👷"
                })
            return fallback_positions[:11]
        except Exception as fallback_error:
            logger.error(f"❌ [AUDIT DATA FLOW] Error generating fallback positions: {fallback_error}")
            # Ultimate fallback - generic positions
            return [
                {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
                {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
                {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"},
                {"title": "Marketing Specialist", "description": f"Marketing strategies at {company_name}", "icon": "📢"},
                {"title": "Quality Assurance", "description": f"Quality control at {company_name}", "icon": "✅"},
                {"title": "Technical Support", "description": f"Technical assistance at {company_name}", "icon": "🔧"},
                {"title": "Project Manager", "description": f"Project coordination at {company_name}", "icon": "📋"},
                {"title": "Logistics Coordinator", "description": f"Supply chain management at {company_name}", "icon": "📦"},
                {"title": "HR Specialist", "description": f"Human resources at {company_name}", "icon": "👥"},
                {"title": "Finance Analyst", "description": f"Financial analysis at {company_name}", "icon": "💰"},
                {"title": "IT Administrator", "description": f"IT systems management at {company_name}", "icon": "💻"}
            ]


def extract_job_positions_from_content(content):
    """
    Extracts job positions from the AI audit content.
    Returns a list of job position objects with title and description.
    """
    # 📊 LOG: Job positions extraction function called
    logger.info(f"🔍 [AUDIT DATA FLOW] extract_job_positions_from_content called")
    logger.info(f"🔍 [AUDIT DATA FLOW] Content type: {type(content)}")
    logger.info(f"🔍 [AUDIT DATA FLOW] Content keys: {list(content.keys()) if isinstance(content, dict) else 'Not a dict'}")
    
    job_positions = []
    
    if not content or not isinstance(content, dict):
        logger.info(f"🔍 [AUDIT DATA FLOW] No valid content provided, returning empty list")
        return job_positions
    
    # Look for contentBlocks in the content
    content_blocks = content.get("contentBlocks", [])
    logger.info(f"🔍 [AUDIT DATA FLOW] Found {len(content_blocks)} content blocks")
    
    for i, block in enumerate(content_blocks):
        if block.get("type") == "table":
            headers = block.get("headers", [])
            rows = block.get("rows", [])
            
            logger.info(f"🔍 [AUDIT DATA FLOW] Table {i+1}: {len(headers)} headers, {len(rows)} rows")
            logger.info(f"🔍 [AUDIT DATA FLOW] Headers: {headers}")
            
            # Check if this is a job positions table
            if any("позиция" in str(header).lower() for header in headers):
                logger.info(f"🔍 [AUDIT DATA FLOW] Found job positions table!")
                for j, row in enumerate(rows):
                    if len(row) > 0:
                        position_title = str(row[0]).strip() if row[0] else "Position"
                        # Create a simple job position object
                        position = {
                            "title": position_title,
                            "description": f"Open position at the company",
                            "icon": "👷"  # Default icon
                        }
                        job_positions.append(position)
                        logger.info(f"🔍 [AUDIT DATA FLOW] Added position {j+1}: {position}")
    
    # If no positions found in tables, return some default positions
    if not job_positions:
        logger.info(f"🔍 [AUDIT DATA FLOW] No positions found in content, using default positions")
        job_positions = [
            {"title": "HVAC Technician", "description": "Installation and maintenance of heating, ventilation, and air conditioning systems", "icon": "👷"},
            {"title": "Electrician", "description": "Installation and maintenance of electrical systems", "icon": "⚡"},
            {"title": "Project Manager", "description": "Overseeing projects and coordinating teams", "icon": "📋"}
        ]
        logger.info(f"🔍 [AUDIT DATA FLOW] Using {len(job_positions)} default positions")
    
    logger.info(f"🔍 [AUDIT DATA FLOW] Returning {len(job_positions)} job positions")
    return job_positions


async def generate_and_finalize_course_outline_for_position(
    company_name: str,
    position: dict,
    onyx_user_id: str,
    pool,
    request: Request,
    language: str = "ru"):
    # 1. Build the prompt for the LLM
    wizard_request = {
        "product": "Course Outline",
        "prompt": (
            f"Создай курс аутлайн 'Онбординг для должности {position['Позиция']}' для новых сотрудников этой должности в компании '{company_name}'. \n"
            f"Структура должна охватывать все аспекты работы сотрудника на этой должности в данной среде. Не включай аспекты работы других должностей, только то, что касается должности '{position['Позиция']}'. \n"
        ),
        "modules": 4,
        "lessonsPerModule": "5-7",
        "language": language
    }
    # Convert to JSON string for the LLM
    prompt = json.dumps(wizard_request, ensure_ascii=False)

    outline_text = ""
    async for chunk_data in stream_openai_response(prompt):
        if chunk_data.get("type") == "delta":
            outline_text += chunk_data["text"]
        elif chunk_data.get("type") == "error":
            raise Exception(f"OpenAI error: {chunk_data['text']}")


    # 4. Finalize/save the project (reuse add_project_to_custom_db)
    template_id = await _ensure_training_plan_template(pool)

    project_data = ProjectCreateRequest(
        projectName=f"Онбординг: {position['Позиция']}",
        design_template_id=template_id,
        microProductName=f"Онбординг: {position['Позиция']}",
        aiResponse=outline_text,
        chatSessionId=None
    )
    project_db_candidate = await add_project_to_custom_db(
        project_data=project_data,
        onyx_user_id=onyx_user_id,
        pool=pool
    )

    try:
        async with pool.acquire() as conn:
            # Convert Pydantic model to dictionary for processing
            content = project_db_candidate.microproduct_content.model_dump(mode='json', exclude_none=True) if project_db_candidate.microproduct_content else {}
            
            if isinstance(content, dict) and content.get("sections"):
                sections = content["sections"]
                updated_sections = []
                
                for section in sections:
                    if isinstance(section, dict) and section.get("lessons"):
                        # Ensure each lesson has proper hours value and completionTime (default to 1 hour and 5m if missing)
                        updated_lessons = []
                        for lesson in section["lessons"]:
                            if isinstance(lesson, dict):
                                # Set default hours if missing or zero
                                if lesson.get("hours", 0) == 0:
                                    lesson["hours"] = 1
                                # Set default completionTime if missing
                                if not lesson.get("completionTime"):
                                    lesson["completionTime"] = "5m"
                                # Ensure all required lesson fields are present
                                lesson.setdefault("check", {"type": "none", "text": ""})
                                # Set content coverage based on source
                                source = lesson.get("source", "Create from scratch")
                                content_available = {"type": "yes", "text": "0%"} if source == "Create from scratch" else {"type": "yes", "text": "0%"}
                                lesson.setdefault("contentAvailable", content_available)
                                lesson.setdefault("source", source)
                                updated_lessons.append(lesson)
                            else:
                                # If lesson is just a string, convert to proper structure
                                updated_lessons.append({
                                    "title": str(lesson),
                                    "check": {"type": "none", "text": ""},
                                    "contentAvailable": {"type": "yes", "text": "0%"},
                                    "source": "Create from scratch",
                                    "hours": 1,
                                    "completionTime": "5m"
                                })
                        
                        # Calculate total hours from lesson hours
                        total_hours = sum(lesson.get("hours", 0) for lesson in updated_lessons)
                        
                        # Update section with calculated total hours and set autoCalculateHours to true
                        updated_section = {
                            **section,
                            "lessons": updated_lessons,
                            "totalHours": total_hours,
                            "autoCalculateHours": True
                        }
                        # Ensure section has proper ID if missing
                        if not updated_section.get("id"):
                            updated_section["id"] = f"№{len(updated_sections) + 1}"
                        updated_sections.append(updated_section)
                    else:
                        updated_sections.append(section)
                
                # Update the project with recalculated totals and ensure mainTitle and detectedLanguage
                if updated_sections:
                    updated_content = {
                        **content, 
                        "sections": updated_sections,
                        "mainTitle": content.get("mainTitle") or f"Онбординг: {position['Позиция']}",
                        "detectedLanguage": content.get("detectedLanguage") or language
                    }
                    await conn.execute(
                        """
                        UPDATE projects
                        SET microproduct_content = $1::jsonb
                        WHERE id = $2
                        """,
                        json.dumps(updated_content), project_db_candidate.id
                    )
                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
    except Exception as e:
        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")


    return project_db_candidate


@app.post("/api/custom/course-outline/finalize")
async def wizard_outline_finalize(payload: OutlineWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Get user ID and deduct credits for course outline finalization
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        credits_needed = calculate_product_credits("course_outline")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Course outline finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for course outline finalization")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for course outline: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    # Ensure we have a chat session id (needed both for cache lookup and possible assistant fallback)
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        persona_id = await get_contentbuilder_persona_id(cookies)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Helper: check whether the user made ANY changes (structure or content)
    def _any_changes_made(orig_modules: List[Dict[str, Any]], edited: Dict[str, Any]) -> bool:
        try:
            edited_sections = edited.get("sections") or edited.get("modules") or []
            
            # Debug logging to understand the data structures
            logger.info(f"Comparing changes: orig_modules count={len(orig_modules)}, edited_sections count={len(edited_sections)}")
            
            # Check structural changes first (modules/lessons added/removed)
            if len(orig_modules) != len(edited_sections):
                logger.info(f"Structural change detected: module count changed from {len(orig_modules)} to {len(edited_sections)}")
                return True
            
            # Check for content changes (titles modified)
            for i, (o, e) in enumerate(zip(orig_modules, edited_sections)):
                # Compare module titles
                orig_title = str(o.get("title", "")).strip()
                edited_title = str(e.get("title", "")).strip() if isinstance(e, dict) else str(e).strip()
                
                logger.debug(f"Module {i}: comparing titles '{orig_title}' vs '{edited_title}'")
                if orig_title != edited_title:
                    logger.info(f"Module title change detected at index {i}: '{orig_title}' -> '{edited_title}'")
                    return True
                
                # Compare lesson structure and content
                orig_lessons = o.get("lessons", [])
                edited_lessons = e.get("lessons", []) if isinstance(e, dict) else []
                
                if len(orig_lessons) != len(edited_lessons):
                    logger.info(f"Lesson count change detected in module {i}: {len(orig_lessons)} -> {len(edited_lessons)}")
                    return True
                
                # Compare individual lesson titles
                for j, (ol, el) in enumerate(zip(orig_lessons, edited_lessons)):
                    # Handle different lesson formats
                    if isinstance(ol, dict):
                        orig_lesson = str(ol.get("title", ol.get("name", ""))).strip()
                    else:
                        orig_lesson = str(ol).strip()
                    
                    if isinstance(el, dict):
                        edited_lesson = str(el.get("title", el.get("name", ""))).strip()
                    else:
                        edited_lesson = str(el).strip()
                    
                    logger.debug(f"Module {i}, Lesson {j}: comparing '{orig_lesson}' vs '{edited_lesson}'")
                    if orig_lesson != edited_lesson:
                        logger.info(f"Lesson change detected in module {i}, lesson {j}: '{orig_lesson}' -> '{edited_lesson}'")
                        return True
            
            logger.info("No changes detected - outline is identical")
            return False
        except Exception as e:
            # On any parsing issue assume changes were made so we use assistant
            logger.warning(f"Error during change detection (assuming changes made): {e}")
            return True



    # ---------- 1) Decide strategy ----------
    raw_outline_cached = OUTLINE_PREVIEW_CACHE.get(chat_id)
    
    # Debug cache lookup
    logger.info(f"DEBUG: Cache lookup for chat_id='{chat_id}', found cached outline: {bool(raw_outline_cached)}")
    if raw_outline_cached:
        logger.info(f"DEBUG: Cached outline preview (first 200 chars): {raw_outline_cached[:200]}...")
    else:
        logger.info(f"DEBUG: Available cache keys: {list(OUTLINE_PREVIEW_CACHE.keys())}")
    
    if raw_outline_cached:
        parsed_orig = _parse_outline_markdown(raw_outline_cached)
        
        # Debug: Log the data structures being compared
        logger.info(f"DEBUG: parsed_orig structure: {json.dumps(parsed_orig, indent=2)[:500]}...")
        logger.info(f"DEBUG: payload.editedOutline structure: {json.dumps(payload.editedOutline, indent=2)[:500]}...")
        
        any_changes = _any_changes_made(parsed_orig, payload.editedOutline)
        
        if not any_changes:
            # NO CHANGES: Use direct parser path (fastest)
            use_direct_parser = True
            use_assistant_then_parser = False
            logger.info("No changes detected - using direct parser path")
        else:
            # CHANGES DETECTED: Use assistant first, then parser
            use_direct_parser = False
            use_assistant_then_parser = True
            logger.info("Changes detected - using assistant + parser path")
    else:
        # No cached data available - use assistant + parser path
        use_direct_parser = False
        use_assistant_then_parser = True
        logger.info("No cached outline - using assistant + parser path")

    # ---------- 2) DIRECT PARSER PATH: No changes made, use cached data directly ----------
    if use_direct_parser:
        direct_path_project_id = None  # Track project ID for cleanup if needed
        try:
            # Use cached outline directly since no changes were made
            template_id = await _ensure_training_plan_template(pool)
            project_name_detected = _extract_project_name_from_markdown(raw_outline_cached) or payload.prompt
            
            logger.info(f"Direct parser path: Using cached outline with {len(raw_outline_cached)} characters")
            
            project_request = ProjectCreateRequest(
                projectName=project_name_detected,
                design_template_id=template_id,
                microProductName=None,
                aiResponse=raw_outline_cached,
                chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                folder_id=int(payload.folderId) if payload.folderId else None,
            )
            onyx_user_id = await get_current_onyx_user_id(request)

            project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
            direct_path_project_id = project_db_candidate.id  # Store for potential cleanup
            
            logger.info(f"Direct parser path: Created project {direct_path_project_id}")
            logger.info(f"Direct parser path: Project content type: {type(project_db_candidate.microproduct_content)}")
            
            # Check if content was parsed successfully
            content_valid = False
            if project_db_candidate.microproduct_content:
                if hasattr(project_db_candidate.microproduct_content, "sections"):
                    sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                    content_valid = len(sections) > 0
                    logger.info(f"Direct parser path: Found {len(sections)} sections in parsed content")
                else:
                    logger.warning(f"Direct parser path: Content does not have sections attribute")
            else:
                logger.warning(f"Direct parser path: microproduct_content is None")

            # --- Patch theme into DB if provided (only for TrainingPlan components) ---
            if payload.theme and content_valid:
                async with pool.acquire() as conn:
                    design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                    if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                        await conn.execute(
                            """
                            UPDATE projects
                            SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                            WHERE id = $2
                            """,
                            payload.theme, project_db_candidate.id
                        )
                        row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                        if row_patch and row_patch["microproduct_content"] is not None:
                            project_db_candidate.microproduct_content = row_patch["microproduct_content"]

            # --- Recalculate module total hours after creation ---
            if content_valid and project_db_candidate.microproduct_content:
                try:
                    async with pool.acquire() as conn:
                        content = project_db_candidate.microproduct_content
                        if isinstance(content, dict) and content.get("sections"):
                            sections = content["sections"]
                            updated_sections = []
                            
                            for section in sections:
                                if isinstance(section, dict) and section.get("lessons"):
                                    # Calculate total hours from lesson hours
                                    total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                    # Update section with calculated total hours and set autoCalculateHours to true
                                    updated_section = {
                                        **section,
                                        "totalHours": total_hours,
                                        "autoCalculateHours": True
                                    }
                                    updated_sections.append(updated_section)
                                else:
                                    updated_sections.append(section)
                            
                            # Update the project with recalculated totals
                            if updated_sections:
                                updated_content = {**content, "sections": updated_sections}
                                await conn.execute(
                                    """
                                    UPDATE projects
                                    SET microproduct_content = $1::jsonb
                                    WHERE id = $2
                                    """,
                                    json.dumps(updated_content), project_db_candidate.id
                                )
                                logger.info(f"Direct parser path: Recalculated module total hours for project {project_db_candidate.id}")
                except Exception as e:
                    logger.warning(f"Direct parser path: Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

            # Success when we have valid parsed content
            if content_valid:
                logger.info(f"Direct parser path successful for project {direct_path_project_id}")
                return JSONResponse(content={"type": "done", "id": project_db_candidate.id})
            else:
                # Direct parser path validation failed - clean up the created project and fall back to assistant
                logger.warning(f"Direct parser path validation failed for project {direct_path_project_id} - LLM parsing likely failed")
                logger.warning(f"Content details: {project_db_candidate.microproduct_content}")
                try:
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
                
                # Fall back to assistant path
                logger.info("Falling back to assistant + parser path due to direct parser failure")
                use_direct_parser = False
                use_assistant_then_parser = True
                
        except Exception as direct_e:
            # Clean up any project created during direct parser path failure
            if direct_path_project_id:
                logger.warning(f"Direct parser path failed with project {direct_path_project_id}, attempting cleanup...")
                try:
                    onyx_user_id = await get_current_onyx_user_id(request)
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
            
            logger.error(f"Direct parser path failed with error: {direct_e}")
            
            # If another concurrent request already started creation we patiently wait for it instead of kicking off assistant again
            if isinstance(direct_e, HTTPException) and direct_e.status_code == status.HTTP_429_TOO_MANY_REQUESTS:
                logger.info("wizard_outline_finalize detected in-progress creation. Waiting for completion…")
                max_wait_sec = 900  # 15 minutes
                poll_every_sec = 1
                waited = 0
                while waited < max_wait_sec:
                    async with pool.acquire() as conn:
                        if chat_id:
                            # Prefer locating the project by the wizard chat_session_id (unique identifier per outline wizard run)
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                                uuid.UUID(chat_id),
                            )
                        else:
                            # Fallback to the previous behaviour when we have no chat_id information available
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2 ORDER BY created_at DESC LIMIT 1",
                                onyx_user_id,
                                payload.prompt,
                            )
                    if row and row["microproduct_content"] is not None:
                        return JSONResponse(content={"type": "done", "id": row["id"]})
                    await asyncio.sleep(poll_every_sec)
                    waited += poll_every_sec
                logger.warning("wizard_outline_finalize waited too long for existing creation – giving up")
            else:
                logger.warning(f"wizard_outline_finalize direct parser path failed – will use assistant path. Details: {direct_e}")
            
            # Fall back to assistant path
            use_direct_parser = False
            use_assistant_then_parser = True

    # ---------- 3) ASSISTANT + PARSER PATH: Process changes with assistant, then parse ----------
    if use_assistant_then_parser:
        # Before starting assistant path, check if a project was already created successfully for this session
        if chat_id:
            try:
                async with pool.acquire() as conn:
                    existing_row = await conn.fetchrow(
                        "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                        uuid.UUID(chat_id),
                    )
                    if existing_row and existing_row["microproduct_content"] is not None:
                        # Check if the existing project has valid content
                        try:
                            content = existing_row["microproduct_content"]
                            if isinstance(content, dict) and content.get("sections"):
                                logger.info(f"Found existing valid project {existing_row['id']} for chat session, returning it")
                                return JSONResponse(content={"type": "done", "id": existing_row["id"]})
                        except Exception:
                            pass  # Continue with assistant path if content validation fails
            except Exception as e:
                logger.warning(f"Failed to check for existing project: {e}")
        
        # Build wizard payload for assistant path - different structure for finalization
        # CRITICAL: Don't send modules/lessonsPerModule during finalization as they conflict
        # with user edits and cause the AI to ignore the actual edited structure
        wiz_payload = {
            "product": "Course Outline",
            "action": "finalize",
            "prompt": payload.prompt,
            "language": payload.language,
            "editedOutline": payload.editedOutline,
        }
        
        # Only add structural parameters if no user edits exist (fallback case)
        edited_sections = payload.editedOutline.get("sections", payload.editedOutline.get("modules", [])) if payload.editedOutline else []
        user_edit_module_count = len(edited_sections)
        
        if not payload.editedOutline or user_edit_module_count == 0:
            logger.info(f"[FINALIZE_PAYLOAD] No user edits found, adding structural parameters as fallback: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
            wiz_payload["modules"] = payload.modules
            wiz_payload["lessonsPerModule"] = payload.lessonsPerModule
        else:
            logger.info(f"[FINALIZE_PAYLOAD] User edits present ({user_edit_module_count} modules) - omitting conflicting structural parameters (original: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}) to preserve user structure")
            # Log the first few modules to understand the structure
            for i, section in enumerate(edited_sections[:3]):
                if isinstance(section, dict):
                    section_title = section.get("title", "Unknown")
                    section_lessons = len(section.get("lessons", []))
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: '{section_title}' ({section_lessons} lessons)")
                else:
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: {type(section)} - {str(section)[:50]}...")
            if user_edit_module_count > 3:
                logger.info(f"[FINALIZE_PAYLOAD] ... and {user_edit_module_count - 3} more modules")

        # Add file context if provided
        if payload.fromFiles:
            wiz_payload["fromFiles"] = True
            if payload.folderIds:
                wiz_payload["folderIds"] = payload.folderIds
            if payload.fileIds:
                wiz_payload["fileIds"] = payload.fileIds

        # Add text context if provided
        if payload.fromText and payload.userText:
            wiz_payload["fromText"] = True
            wiz_payload["textMode"] = payload.textMode
            wiz_payload["userText"] = payload.userText

        wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
        logger.info(f"[FINALIZE_PAYLOAD] Final wizard message structure: {list(wiz_payload.keys())}")
        logger.info(f"[FINALIZE_PAYLOAD] Wizard message length: {len(wizard_message)} chars")

        async def streamer():
            assistant_reply: str = ""
            last_send = asyncio.get_event_loop().time()
            chunks_received = 0

            # Use longer timeout for large text processing to prevent AI memory issues
            timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
            logger.info(f"[FINALIZE_OPENAI_STREAM] Starting OpenAI finalization streamer with timeout: {timeout_duration} seconds")
            logger.info(f"[FINALIZE_OPENAI_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
            
            try:
                # Use OpenAI streaming for finalization instead of Onyx
                logger.info(f"[FINALIZE_OPENAI_STREAM] ✅ USING OPENAI DIRECT STREAMING for finalization")
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[FINALIZE_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[FINALIZE_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return

                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[FINALIZE_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[FINALIZE_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
            except Exception as e:
                logger.error(f"[FINALIZE_OPENAI_STREAM_ERROR] Error in OpenAI finalization streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

            # Cache full raw outline for later finalize step
            if chat_id:
                OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

            # Create the project using the assistant response
            try:
                template_id = await _ensure_training_plan_template(pool)
                project_name_detected = _extract_project_name_from_markdown(assistant_reply) or payload.prompt
                
                logger.info(f"Assistant + parser path: Creating project with {len(assistant_reply)} characters")
                
                project_request = ProjectCreateRequest(
                    projectName=project_name_detected,
                    design_template_id=template_id,
                    microProductName=None,
                    aiResponse=assistant_reply,
                    chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                    folder_id=int(payload.folderId) if payload.folderId else None,
                )
                onyx_user_id = await get_current_onyx_user_id(request)

                project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
                
                logger.info(f"Assistant + parser path: Created project {project_db_candidate.id}")
                
                # Check if content was parsed successfully
                content_valid = False
                if project_db_candidate.microproduct_content:
                    if hasattr(project_db_candidate.microproduct_content, "sections"):
                        sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                        content_valid = len(sections) > 0
                        logger.info(f"Assistant + parser path: Found {len(sections)} sections in parsed content")
                    else:
                        logger.warning(f"Assistant + parser path: Content does not have sections attribute")
                else:
                    logger.warning(f"Assistant + parser path: microproduct_content is None")

                # --- Patch theme into DB if provided (only for TrainingPlan components) ---
                if payload.theme and content_valid:
                    async with pool.acquire() as conn:
                        design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                        if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                            await conn.execute(
                                """
                                UPDATE projects
                                SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                                WHERE id = $2
                                """,
                                payload.theme, project_db_candidate.id
                            )
                            row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                            if row_patch and row_patch["microproduct_content"] is not None:
                                project_db_candidate.microproduct_content = row_patch["microproduct_content"]

                # --- Recalculate module total hours after creation ---
                if content_valid and project_db_candidate.microproduct_content:
                    try:
                        async with pool.acquire() as conn:
                            content = project_db_candidate.microproduct_content
                            if isinstance(content, dict) and content.get("sections"):
                                sections = content["sections"]
                                updated_sections = []
                                
                                for section in sections:
                                    if isinstance(section, dict) and section.get("lessons"):
                                        # Calculate total hours from lesson hours
                                        total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                        # Update section with calculated total hours and set autoCalculateHours to true
                                        updated_section = {
                                            **section,
                                            "totalHours": total_hours,
                                            "autoCalculateHours": True
                                        }
                                        updated_sections.append(updated_section)
                                    else:
                                        updated_sections.append(section)
                                
                                # Update the project with recalculated totals
                                if updated_sections:
                                    updated_content = {**content, "sections": updated_sections}
                                    await conn.execute(
                                        """
                                        UPDATE projects
                                        SET microproduct_content = $1::jsonb
                                        WHERE id = $2
                                        """,
                                        json.dumps(updated_content), project_db_candidate.id
                                    )
                                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
                    except Exception as e:
                        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

                if content_valid:
                    logger.info(f"Assistant + parser path successful for project {project_db_candidate.id}")
                    # Send completion packet with the project ID
                    done_packet = {"type": "done", "id": project_db_candidate.id}
                    yield (json.dumps(done_packet) + "\n").encode()
                else:
                    logger.error(f"Assistant + parser path: Project {project_db_candidate.id} created but content validation failed")
                    # Clean up the failed project
                    try:
                        async with pool.acquire() as conn:
                            await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", project_db_candidate.id, onyx_user_id)
                        logger.info(f"Successfully cleaned up failed assistant + parser project {project_db_candidate.id}")
                    except Exception as cleanup_e:
                        logger.error(f"Failed to cleanup assistant + parser project {project_db_candidate.id}: {cleanup_e}")
                    
                    # Send error packet
                    error_packet = {"type": "error", "message": "Failed to parse the generated outline"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    
            except Exception as create_e:
                logger.error(f"Assistant + parser path: Failed to create project: {create_e}")
                # Send error packet
                error_packet = {"type": "error", "message": f"Failed to create project: {str(create_e)}"}
                yield (json.dumps(error_packet) + "\n").encode()

        return StreamingResponse(streamer(), media_type="application/json")

@app.post("/api/custom/course-outline/init-chat")
async def init_course_outline_chat(request: Request):
    """Pre-create Chat Session & persona so subsequent preview calls are faster."""
    cookies = request.cookies
    persona_id = await get_contentbuilder_persona_id(cookies)
    chat_id = await create_onyx_chat_session(persona_id, cookies)
    return {"personaId": persona_id, "chatSessionId": chat_id}

# ======================= End Wizard Section ==============================

# === Wizard Outline helpers & cache ===
OUTLINE_PREVIEW_CACHE: Dict[str, str] = {}  # chat_session_id -> raw markdown outline
QUIZ_PREVIEW_CACHE: Dict[str, str] = {}  # chat_session_id -> raw quiz content

def _apply_title_edits_to_outline(original_md: str, edited_outline: Dict[str, Any]) -> str:
    """Return a markdown outline that reflects the *structure* provided in
    `edited_outline` (modules & lessons) while preserving the original header.

    Instead of trying to patch-in titles at the old positions, we rebuild each
    module's lesson list from scratch. This guarantees correctness even when
    lessons were inserted, removed or reordered in the UI.
    """

    # ---- 1. Normalise `edited_outline` ----
    sections: Optional[List[Any]] = None
    if isinstance(edited_outline, dict):
        sections = edited_outline.get("sections") or edited_outline.get("modules")
    elif isinstance(edited_outline, list):
        sections = edited_outline

    if not sections:
        return original_md  # nothing to merge -> return original untouched

    # ---- 2. Preserve the very first non-empty line (usually Universal Header) ----
    header_line = None
    for line in original_md.splitlines():
        if line.strip():
            header_line = line.rstrip()
            break

    out_lines: List[str] = []
    if header_line:
        out_lines.append(header_line)
        out_lines.append("")  # spacer line to match original formatting

    # ---- 3. Rebuild modules & lessons ----
    for idx, sec in enumerate(sections):
        # Module title
        title = sec.get("title") if isinstance(sec, dict) else str(sec)
        out_lines.append(f"## Module {idx + 1}: {title.strip()}")

        # Lessons
        lessons_list: List[Any] = []
        if isinstance(sec, dict):
            lessons_list = sec.get("lessons", []) or []
        elif isinstance(sec, list):
            lessons_list = sec

        for ls in lessons_list:
            ls_raw = ls.get("title") if isinstance(ls, dict) else str(ls)
            if not isinstance(ls_raw, str):
                ls_raw = str(ls_raw)

            segments = ls_raw.split("\n")
            main_line = segments[0].strip()
            out_lines.append(f"- **{main_line}**")

            for extra in segments[1:]:
                extra = extra.rstrip()
                if extra:
                    out_lines.append(f"  {extra}")

        out_lines.append("")  # blank line between modules for readability

    return "\n".join(out_lines).rstrip()  # drop trailing newline

# ------------------- Utility: extract project name from AI markdown header -------------------

_HEADER_RE = re.compile(r"^\*\*(?P<name>[^*]+)\*\*\s*:\s*\*\*.+")


def _extract_project_name_from_markdown(md: str) -> Optional[str]:
    """Return the first **Project Name** element found in the Universal Product Header.

    The header line looks like:
        **Project Name** : **Course Outline** : **Course Outline**
    We return "Project Name" (stripped).
    """
    if not md:
        return None
    first_line = md.splitlines()[0].strip()
    m = _HEADER_RE.match(first_line)
    if m:
        return m.group("name").strip()
    return None

# --- PDF Lesson helper and wizard endpoints ---

# Ensure a design template for PDF Lesson exists, return its ID
async def _ensure_pdf_lesson_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_PDF_LESSON)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "PDF Lesson", DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM, "PDF Lesson", COMPONENT_NAME_PDF_LESSON,
        )
        return row["id"]

# Ensure a design template for Slide Deck exists, return its ID
async def _ensure_slide_deck_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_SLIDE_DECK)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Slide Deck", DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM, "Slide Deck", COMPONENT_NAME_SLIDE_DECK,
        )
        return row["id"]


# Ensure a design template for Video Lesson Presentation exists, return its ID
async def _ensure_video_lesson_presentation_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Video Lesson Presentation", DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM, "Video Lesson Presentation", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION,
        )
        return row["id"]


# Ensure a design template for Text Presentation exists, return its ID
async def _ensure_text_presentation_template(pool: asyncpg.Pool) -> int:
    """Ensure text presentation template exists and return its ID"""
    try:
        # Check if text presentation template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Text Presentation' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create text presentation template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Text Presentation Template",
            "Create a comprehensive text presentation with clear structure, engaging content, and professional formatting.",
            "Text Presentation",
            COMPONENT_NAME_TEXT_PRESENTATION,
            "/text-presentation.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring text presentation template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure text presentation template")


# -------- Lesson Presentation (PDF Lesson) Wizard ---------

class LessonWizardPreview(BaseModel):
    outlineProjectId: Optional[int] = None  # Parent Training Plan project id
    lessonTitle: Optional[str] = None      # Specific lesson to generate, optional when prompt-based
    lengthRange: Optional[str] = None      # e.g. "400-500 words"
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text


class LessonWizardFinalize(BaseModel):
    outlineProjectId: Optional[int] = None
    lessonTitle: str
    lengthRange: Optional[str] = None
    aiResponse: str                        # User-edited markdown / plain text
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder


@app.post("/api/custom/lesson-presentation/preview")
async def wizard_lesson_preview(payload: LessonWizardPreview, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Ensure chat session
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        persona_id = await get_contentbuilder_persona_id(cookies)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Build wizard request for assistant persona
    is_video_lesson = payload.productType == "video_lesson_presentation"
    wizard_dict: Dict[str, Any] = {
        "product": "Video Lesson Slides Deck" if is_video_lesson else "Slides Deck",
        "action": "preview",
        "language": payload.language,
        "slidesCount": payload.slidesCount or 5,
        "generateVoiceover": is_video_lesson,  # Flag to indicate voiceover generation
        "theme": payload.theme or "dark-purple",  # Use selected theme or default
    }
    if payload.outlineProjectId is not None:
        wizard_dict["outlineProjectId"] = payload.outlineProjectId
        
        # Fetch outline name to include in wizard request
        try:
            # Get current user ID to fetch the outline
            onyx_user_id = await get_current_onyx_user_id(request)
            
            # Fetch outline name from database
            async with pool.acquire() as conn:
                outline_row = await conn.fetchrow(
                    "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                    payload.outlineProjectId, onyx_user_id
                )
                if outline_row:
                    wizard_dict["outlineName"] = outline_row["project_name"]
        except Exception as e:
            logger.warning(f"Failed to fetch outline name for project {payload.outlineProjectId}: {e}")
            # Continue without outline name - not critical for preview
            
    if payload.lessonTitle:
        wizard_dict["lessonTitle"] = payload.lessonTitle
    if payload.prompt:
        wizard_dict["prompt"] = payload.prompt

    wizard_dict["importantRules"] = "IMPORTANT: DO NOT CREATE CONCLUSION SLIDES. ONLY CREATE EDUCATIONAL SLIDES. DO NOT CREATE SLIDES WITH TITLES LIKE 'Conclusion', 'Summary', 'Wrap-Up', 'Thank You', 'Further Reading', 'Additional Resources', 'Questions', 'Open Floor for Questions', 'Feedback'. DO NOT MAKE SECOND SLIDE BE A TITLE SLIDE. DO NOT USE 'content-slide' SLIDES"
    wizard_dict["importantRules"] += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.
    """
    
    # Add file context if provided
    if payload.fromFiles:
        wizard_dict["fromFiles"] = True
        if payload.folderIds:
            wizard_dict["folderIds"] = payload.folderIds
        if payload.fileIds:
            wizard_dict["fileIds"] = payload.fileIds

    # Add text context if provided - use compression for large texts
    if payload.fromText and payload.userText:
        wizard_dict["fromText"] = True
        wizard_dict["textMode"] = payload.textMode
        
        if len(payload.userText) > TEXT_SIZE_THRESHOLD:
            # Compress large text to reduce payload size
            compressed_text = compress_text(payload.userText)
            wizard_dict["userText"] = compressed_text
            wizard_dict["textCompressed"] = True
            logger.info(f"Using compressed text for large lesson content ({len(payload.userText)} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for smaller content
            wizard_dict["userText"] = payload.userText
            wizard_dict["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Decompress text if it was compressed
    if wizard_dict.get("textCompressed") and wizard_dict.get("userText"):
        try:
            decompressed_text = decompress_text(wizard_dict["userText"])
            wizard_dict["userText"] = decompressed_text
            wizard_dict["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed lesson text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress lesson text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wizard_dict) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wizard_dict.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[LESSON_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                # Step 1: Extract file context from Onyx
                folder_ids_list = []
                file_ids_list = []
                
                if payload.fromFiles and payload.folderIds:
                    folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                
                if payload.fromFiles and payload.fileIds:
                    file_ids_list = parse_id_list(payload.fileIds, "file")
                    logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                
                # Add virtual file ID if created for large text
                if wizard_dict.get("virtualFileId"):
                    file_ids_list.append(wizard_dict["virtualFileId"])
                    logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wizard_dict['virtualFileId']} to file_ids_list")
                
                # Extract context from Onyx
                logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                chunks_received = 0
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Video Lesson Presentation" if is_video_lesson else "Lesson Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[LESSON_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                chunks_received = 0
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[LESSON_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[LESSON_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[LESSON_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[LESSON_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[LESSON_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")
        # Send completion packet with the parsed outline.
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}

        print("FULL RESPOSE:", assistant_reply)

        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="text/plain")


@app.post("/api/custom/lesson-presentation/finalize")
async def wizard_lesson_finalize(payload: LessonWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    logger.info(f"Finalizing lesson presentation: {payload.lessonTitle}")
    
    # Validate required fields early
    if not payload.lessonTitle or not payload.lessonTitle.strip():
        raise HTTPException(status_code=400, detail="Lesson title is required")
    
    if not payload.aiResponse or not payload.aiResponse.strip():
        raise HTTPException(status_code=400, detail="AI response content is required")

    # Parse AI response to determine slide count for credit calculation
    try:
        slides_data = json.loads(payload.aiResponse)
        credits_needed = calculate_product_credits("lesson_presentation", slides_data)
    except:
        # If parsing fails, use default credit cost
        credits_needed = calculate_product_credits("lesson_presentation")

    # Get user ID and deduct credits for lesson presentation
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Lesson presentation finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for lesson presentation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for lesson presentation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    try:
        # Determine if this is a video lesson presentation
        is_video_lesson = payload.productType == "video_lesson_presentation"
        
        # Get the appropriate template with retry mechanism
        max_retries = 3
        template_id = None
        for attempt in range(max_retries):
            try:
                if is_video_lesson:
                    template_id = await _ensure_video_lesson_presentation_template(pool)
                else:
                    template_id = await _ensure_slide_deck_template(pool)
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"Failed to get template after {max_retries} attempts: {e}")
                    raise HTTPException(status_code=500, detail="Unable to initialize template")
                await asyncio.sleep(0.5)  # Brief delay before retry

        if not template_id:
            raise HTTPException(status_code=500, detail="Template initialization failed")

        # Get user ID
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lessonTitle.strip()
        if payload.outlineProjectId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineProjectId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lessonTitle.strip()}"
            except Exception as e:
                logger.warning(f"Failed to fetch outline name for lesson naming: {e}")
                # Continue with plain lesson title if outline fetch fails

        # Create project data
        project_data = ProjectCreateRequest(
            projectName=project_name,
            design_template_id=template_id,
            microProductName=None,
            aiResponse=payload.aiResponse.strip(),
            chatSessionId=payload.chatSessionId,
            outlineId=payload.outlineProjectId,  # Pass outlineId for consistent naming
            folder_id=int(payload.folderId) if payload.folderId else None,  # Add folder assignment
            theme=payload.theme  # Pass selected theme
        )
        
        # Create project with proper error handling
        try:
            created_project = await add_project_to_custom_db(project_data, onyx_user_id, pool)
        except HTTPException as e:
            # Re-raise HTTP exceptions as-is
            raise e
        except Exception as e:
            logger.error(f"Failed to create project: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Failed to create lesson project")

        # Validate the created project
        if not created_project or not created_project.id:
            logger.error("Project creation returned invalid result")
            raise HTTPException(status_code=500, detail="Project creation failed - invalid response")

        logger.info(f"Successfully finalized lesson presentation with project ID: {created_project.id}")

        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if row:
                    logger.info(f"[LESSON_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved presentation JSON for project {created_project.id}: {log_e}")

        # Return simple JSON response (not streaming for now)
        return {
            "id": created_project.id,
            "projectName": created_project.project_name,
            "message": "Lesson presentation finalized successfully"
        }
    
    except HTTPException:
        # Re-raise HTTP exceptions without modification
        raise
    except Exception as e:
        logger.error(f"Unexpected error in lesson finalization: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail="An unexpected error occurred during finalization"
        )

# --- New endpoint: list trashed projects for user ---

@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Restore trashed projects ---

@app.post("/api/custom/projects/restore-multiple", status_code=status.HTTP_200_OK)
async def restore_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for restore."})

    ids_to_restore: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # Expand scope to related lessons when requested
            if delete_request.scope == 'all':
                for pid in delete_request.project_ids:
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                        pid, onyx_user_id
                    )
                    if not row:
                        continue
                    pname: str = row["project_name"]
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        continue
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_restore.add(lr["id"])

            if not ids_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            # First, fetch all the data we need to restore
            projects_to_restore = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM trashed_projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(ids_to_restore), onyx_user_id)

            if not projects_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_restore:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into projects with safe values
                    await conn.execute("""
                        INSERT INTO projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type,
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from trashed_projects table
                await conn.execute(
                    "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(ids_to_restore), onyx_user_id
                )

        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully restored {len(ids_to_restore)} project(s)."})

    except Exception as e:
        logger.error(f"Error restoring projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while restoring projects." if IS_PRODUCTION else f"DB error while restoring projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


# --- Permanently delete trashed projects ---

@app.post("/api/custom/projects/delete-permanently", status_code=status.HTTP_200_OK)
async def delete_permanently(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for permanent deletion."})

    ids_to_delete: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            for pid in delete_request.project_ids:
                row = await conn.fetchrow(
                    "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                    pid, onyx_user_id
                )
                if not row:
                    continue
                pname: str = row["project_name"]
                # If this is an outline, cascade to its lessons
                if row["microproduct_type"] in ("Training Plan", "Course Outline"):
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_delete.add(lr["id"])

            # Perform deletion of all collected ids
            result = await conn.execute(
                "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id=$2",
                list(ids_to_delete), onyx_user_id
            )

        deleted_count = int(result.split(" ")[1]) if result else 0
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully deleted {deleted_count} project(s) permanently."})
    except Exception as e:
        logger.error(f"Error permanently deleting projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during permanent deletion." if IS_PRODUCTION else f"DB error during permanent deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# Add the new model for training plan editing
class TrainingPlanEditRequest(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: Optional[str] = None
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during edit
    # File context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs

@app.post("/api/custom/training-plan/edit")
async def edit_training_plan_with_prompt(payload: TrainingPlanEditRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Edit an existing training plan using AI prompt"""
    logger.info(f"[edit_training_plan_with_prompt] projectId={payload.projectId} prompt='{payload.prompt[:50]}...'")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    # Get or create chat session
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        persona_id = await get_contentbuilder_persona_id(cookies)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Convert existing training plan to markdown format for AI processing
    existing_content = row["microproduct_content"]
    current_outline = ""
    
    if existing_content:
        # Convert existing training plan to markdown format with full details
        content_data = existing_content
        if isinstance(content_data, dict):
            main_title = content_data.get("mainTitle", "Training Plan")
            current_outline = f"# {main_title}\n\n"
            
            sections = content_data.get("sections", [])
            for section in sections:
                section_id = section.get("id", "")
                section_title = section.get("title", "")
                total_hours = section.get("totalHours", 0.0)
                # Get module quality tier information for preservation
                section_quality_tier = section.get("quality_tier", "")
                
                # Convert special characters to safe ASCII for AI processing
                # We'll convert back after AI response to preserve user-visible format
                if section_id and section_title:
                    # Replace № with # for AI processing (encoding-safe)
                    safe_section_id = section_id.replace("№", "#")
                    if section_id != safe_section_id:
                        logger.info(f"[SMART_EDIT_ENCODING] Converted '{section_id}' to '{safe_section_id}' for AI processing")
                    # Check if section_id already contains "Module" keyword
                    if "Module" in safe_section_id or "Модуль" in safe_section_id:
                        current_outline += f"## {safe_section_id}: {section_title}\n"
                    else:
                        # For other formats (#1, mod1, etc.), preserve them exactly as they are
                        current_outline += f"## {safe_section_id}: {section_title}\n"
                else:
                    # Fallback for empty IDs
                    current_outline += f"## {section_title}\n"
                current_outline += f"**Total Hours:** {total_hours}\n"
                if section_quality_tier:
                    current_outline += f"**Module Quality Tier:** {section_quality_tier}\n"
                current_outline += "\n"
                
                lessons = section.get("lessons", [])
                if lessons:
                    current_outline += "### Lessons:\n"
                    for idx, lesson in enumerate(lessons, 1):
                        lesson_title = lesson.get("title", "")
                        lesson_hours = lesson.get("hours", 1.0)
                        lesson_source = lesson.get("source", "Create from scratch")
                        
                        # Get check details
                        check = lesson.get("check", {})
                        check_type = check.get("type", "none")
                        check_text = check.get("text", "No")
                        
                        # Get content availability
                        content_available = lesson.get("contentAvailable", {})
                        content_type = content_available.get("type", "yes")
                        content_text = content_available.get("text", "100%")
                        
                        # Get quality tier information for preservation
                        lesson_quality_tier = lesson.get("quality_tier", "")
                        
                        current_outline += f"{idx}. **{lesson_title}**\n"
                        current_outline += f"   - Hours: {lesson_hours}\n"
                        current_outline += f"   - Source: {lesson_source}\n"
                        current_outline += f"   - Assessment: {check_type} ({check_text})\n"
                        current_outline += f"   - Content Available: {content_type} ({content_text})\n"
                        if lesson_quality_tier:
                            current_outline += f"   - Quality Tier: {lesson_quality_tier}\n"
                        current_outline += "\n"
                else:
                    current_outline += "*No lessons defined*\n\n"
                current_outline += "\n"

    # Prepare wizard payload
    wiz_payload = {
        "product": "Training Plan Edit",
        "prompt": payload.prompt,
        "language": payload.language,
        "originalOutline": current_outline,
        "editMode": True
    }

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

    # Stream the response
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
        
        # NEW: Check if we should use OpenAI directly instead of Onyx
        if should_use_openai_direct(payload):
            logger.info(f"[SMART_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[SMART_EDIT_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                chunks_received = 0
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[SMART_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[SMART_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[SMART_EDIT_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[SMART_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
            except Exception as e:
                logger.error(f"[SMART_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # EXISTING: Use Onyx when file context is present
        else:
            logger.info(f"[SMART_EDIT_STREAM] ❌ USING ONYX API (file context detected)")
            logger.info(f"[SMART_EDIT_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                async with httpx.AsyncClient(timeout=timeout_duration) as client:
                    # Parse folder and file IDs for Onyx
                    folder_ids_list = []
                    file_ids_list = []
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    send_payload = {
                        "chat_session_id": chat_id,
                        "message": wizard_message,
                        "parent_message_id": None,
                        "file_descriptors": [],
                        "user_file_ids": file_ids_list,
                        "user_folder_ids": folder_ids_list,
                        "prompt_id": None,
                        "search_doc_ids": None,
                        "retrieval_options": {"run_search": "never", "real_time": False},
                        "stream_response": True,
                    }
                    logger.info(f"[PREVIEW_ONYX] Sending request to Onyx /chat/send-message with payload: user_file_ids={file_ids_list}, user_folder_ids={folder_ids_list}")
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=send_payload, cookies=cookies) as resp:
                        logger.info(f"[PREVIEW_ONYX] Response status: {resp.status_code}")
                        async for raw_line in resp.aiter_lines():
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info("[PREVIEW_ONYX] Received [DONE] from Onyx stream")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    delta_text = pkt["answer_piece"].replace("\\n", "\n")
                                    assistant_reply += delta_text
                                    logger.debug(f"[PREVIEW_ONYX] Received chunk: {delta_text[:80]}")
                                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                            except Exception as e:
                                logger.error(f"[PREVIEW_ONYX] Error parsing chunk: {e} | Raw: {line[:100]}")
                                continue

                            # send keep-alive every 8s
                            now = asyncio.get_event_loop().time()
                            if now - last_send > 8:
                                yield b" "
                                last_send = now
            except Exception as e:
                logger.error(f"[PREVIEW_ONYX] Exception in streaming: {e}")
                raise

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")

        # Convert back from safe ASCII characters to original special characters
        # Replace # back to № to restore original format for user display
        assistant_reply_restored = assistant_reply.replace("## #", "## №")
        if assistant_reply_restored != assistant_reply:
            logger.info(f"[SMART_EDIT_ENCODING] Restored special characters in AI response")
        
        # Update the cached version and the one used for parsing
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply_restored
        
        # Use the restored version for all subsequent processing
        assistant_reply = assistant_reply_restored
        
        # NEW: Parse AI response into structured TrainingPlanDetails but DON'T save to database yet
        # This is for preview - user will confirm before saving
        updated_content_dict: Optional[Dict[str, Any]] = None
        try:
            # Use the proper LLM parser to convert AI response to TrainingPlanDetails
            # Use the SAME parsing instructions as normal generation to ensure consistent ID handling
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `quality_tier` (string, optional): Quality tier for this module. PRESERVE EXACTLY from source if mentioned as "Module Quality Tier: X". If not specified, omit this field entirely.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `quality_tier` (string, optional): Quality tier for this lesson. PRESERVE EXACTLY from source if mentioned as "Quality Tier: X". If not specified, omit this field entirely.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
            
            # Create a default TrainingPlanDetails instance for error handling
            # Preserve theme from existing content or use payload theme
            theme_to_use = "cherry"
            if existing_content and isinstance(existing_content, dict):
                theme_to_use = existing_content.get("theme", "cherry")
            else:
                theme_to_use = payload.theme or "cherry"
                
            default_training_plan = TrainingPlanDetails(
                mainTitle=row["project_name"],
                sections=[],
                detectedLanguage=detect_language(assistant_reply),
                theme=theme_to_use
            )
            
            # Example JSON structure for the LLM parser
            llm_json_example = json.dumps({
                "mainTitle": "Example Training Plan",
                "sections": [
                    {
                        "id": "№1",
                        "title": "Introduction to Topic",
                        "totalHours": 10,
                        "quality_tier": "premium",
                        "lessons": [
                            {
                                "title": "Lesson 1: Basics",
                                "hours": 2,
                                "source": "Create from scratch",
                                "completionTime": "5m",
                                "quality_tier": "interactive",
                                "check": {"type": "test", "text": "Test"},
                                "contentAvailable": {"type": "yes", "text": "100%"}
                            }
                        ],
                        "autoCalculateHours": True
                    }
                ],
                "detectedLanguage": "en",
                "theme": theme_to_use
            })
            
            logger.info(f"[SMART_EDIT_PARSER] Parsing AI response with length: {len(assistant_reply)}")
            logger.info(f"[SMART_EDIT_PARSER] AI response preview: {assistant_reply[:300]}{'...' if len(assistant_reply) > 300 else ''}")
            
            parsed_training_plan = await parse_ai_response_with_llm(
                ai_response=assistant_reply,
                project_name=row["project_name"],
                target_model=TrainingPlanDetails,
                default_error_model_instance=default_training_plan,
                dynamic_instructions=component_specific_instructions,
                target_json_example=llm_json_example
            )
            
            if parsed_training_plan:
                # Preserve the original language and theme
                if existing_content and isinstance(existing_content, dict):
                    # Preserve original language
                    original_language = existing_content.get("detectedLanguage", payload.language)
                    parsed_training_plan.detectedLanguage = original_language
                    logger.info(f"[SMART_EDIT_LANGUAGE] Preserved original language: {original_language}")
                    
                    # Preserve original theme
                    original_theme = existing_content.get("theme", "cherry")
                    parsed_training_plan.theme = original_theme
                    logger.info(f"[SMART_EDIT_THEME] Preserved original theme: {original_theme}")
                else:
                    # Use the language and theme from the request payload if available
                    parsed_training_plan.detectedLanguage = payload.language or "en"
                    parsed_training_plan.theme = payload.theme or "cherry"
                    logger.info(f"[SMART_EDIT_LANGUAGE] Using language from payload: {payload.language}")
                    logger.info(f"[SMART_EDIT_THEME] Using theme from payload: {payload.theme}")
                
                # Post-process module IDs to ensure № character is preserved
                for section in parsed_training_plan.sections:
                    if section.id:
                        # Fix module IDs that lost the № character
                        if section.id.isdigit():
                            # Plain number like "2" -> "№2"
                            section.id = f"№{section.id}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                        elif section.id.startswith("#"):
                            # Hash format like "#2" -> "№2"
                            number = section.id[1:]
                            section.id = f"№{number}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                        elif not section.id.startswith("№"):
                            # Other formats without № - try to extract number and format correctly
                            import re
                            number_match = re.search(r'\d+', section.id)
                            if number_match:
                                number = number_match.group()
                                section.id = f"№{number}"
                                logger.info(f"[SMART_EDIT_ID_FIX] Fixed ID format to '{section.id}'")
                
                updated_content_dict = parsed_training_plan.model_dump(mode='json', exclude_none=True)
                
                logger.info(f"[SMART_EDIT_PREVIEW] Generated preview for training plan projectId={payload.projectId}")
            
        except Exception as e:
            logger.error(f"[SMART_EDIT_ERROR] Error parsing training plan: {e}")
            # Fall back to the preview-only mode if parsing fails
            updated_content_dict = None

        # Send completion packet with updatedContent for frontend preview
        # Note: This is now a PREVIEW - user must confirm to save to database
        if updated_content_dict:
            done_packet = {"type": "done", "updatedContent": updated_content_dict, "isPreview": True}
        else:
            # Fallback to old format if parsing failed
            done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        
        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="application/json")

class SmartEditConfirmRequest(BaseModel):
    projectId: int
    updatedContent: dict
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during confirmation

@app.post("/api/custom/training-plan/confirm-edit")
async def confirm_training_plan_edit(payload: SmartEditConfirmRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Confirm and save smart-edit changes to the database"""
    logger.info(f"[confirm_training_plan_edit] projectId={payload.projectId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Verify the project exists and belongs to the user
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    try:
        # Log the content structure for debugging
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content structure: {type(payload.updatedContent)}")
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content keys: {list(payload.updatedContent.keys()) if isinstance(payload.updatedContent, dict) else 'Not a dict'}")
        
        # Save the confirmed changes to the database
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE projects 
                SET microproduct_content = $1
                WHERE id = $2 AND onyx_user_id = $3
            """, payload.updatedContent, payload.projectId, onyx_user_id)
        
        logger.info(f"[SMART_EDIT_CONFIRMED] Successfully saved changes for training plan projectId={payload.projectId}")
        
        return {"success": True, "message": "Changes confirmed and saved successfully"}
        
    except Exception as e:
        logger.error(f"[SMART_EDIT_CONFIRM_ERROR] Error saving confirmed changes: {e}")
        raise HTTPException(status_code=500, detail="Failed to save changes")

# Add the finalize model for training plan editing
class TrainingPlanEditFinalize(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: str
    editedOutline: Dict[str, Any]
    language: str = "en"

@app.post("/api/custom/training-plan/finalize")
async def finalize_training_plan_edit(payload: TrainingPlanEditFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize and apply the edited training plan"""
    logger.info(f"[finalize_training_plan_edit] projectId={payload.projectId} chatSessionId={payload.chatSessionId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get the cached preview
    cached_preview = OUTLINE_PREVIEW_CACHE.get(payload.chatSessionId)
    if not cached_preview:
        raise HTTPException(status_code=400, detail="No preview found for this session. Please regenerate the preview.")
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")
    
    # Parse the edited outline from the cached preview using LLM-based parsing
    try:
        # Create a default TrainingPlanDetails instance for error handling
        default_training_plan = TrainingPlanDetails(
            mainTitle=row["project_name"],
            sections=[],
            detectedLanguage=detect_language(cached_preview)
        )
        
        # Component-specific instructions for TrainingPlanDetails parsing
        component_specific_instructions = """
            Parse the training plan outline into a structured JSON format. Extract all modules and their lessons with complete details.

            **Main Object:**
            * `mainTitle` (string): The main title of the training plan.
            * `sections` (array): List of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
        
        # Example JSON structure for the LLM parser
        llm_json_example = json.dumps({
            "mainTitle": "Example Training Plan",
            "sections": [
                {
                    "id": "№1",
                    "title": "Introduction to Topic",
                    "totalHours": 10,
                    "lessons": [
                        {
                            "title": "Lesson 1: Basics",
                            "hours": 2,
                            "source": "Create from scratch",
                            "completionTime": "5m",
                            "check": {"type": "test", "text": "Test"},
                            "contentAvailable": {"type": "yes", "text": "100%"}
                        }
                    ],
                    "autoCalculateHours": True
                }
            ],
            "detectedLanguage": "en",
            "theme": "cherry"
        })
        
        # First, parse the outline to get auto-calculated totalHours
        parsed_orig = _parse_outline_markdown(cached_preview)
        logger.info(f"[FINALIZE] Parsed outline with {len(parsed_orig)} modules")
        
        # Create a mapping of module titles to auto-calculated totalHours
        auto_calculated_hours = {}
        for module in parsed_orig:
            title = module.get('title', '')
            total_hours = module.get('totalHours', 0.0)
            auto_calculated_hours[title] = total_hours
            logger.info(f"[FINALIZE] Auto-calculated hours for '{title}': {total_hours}")
        
        training_plan_details = await parse_ai_response_with_llm(
            ai_response=cached_preview,
            project_name=row["project_name"],
            target_model=TrainingPlanDetails,
            default_error_model_instance=default_training_plan,
            dynamic_instructions=component_specific_instructions,
            target_json_example=llm_json_example
        )
        
        if not training_plan_details:
            raise HTTPException(status_code=400, detail="Failed to parse the edited training plan")
        
        # Override LLM-calculated totalHours with auto-calculated values
        for section in training_plan_details.sections:
            section_title = section.title
            if section_title in auto_calculated_hours:
                original_hours = section.totalHours
                section.totalHours = auto_calculated_hours[section_title]
                logger.info(f"[FINALIZE] Overrode totalHours for '{section_title}': {original_hours} -> {section.totalHours}")
            else:
                logger.warning(f"[FINALIZE] No auto-calculated hours found for section: '{section_title}'")
        
        # Post-process module IDs to ensure № character is preserved
        for section in training_plan_details.sections:
            if section.id:
                # Fix module IDs that lost the № character
                if section.id.isdigit():
                    # Plain number like "2" -> "№2"
                    section.id = f"№{section.id}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                elif section.id.startswith("#"):
                    # Hash format like "#2" -> "№2"
                    number = section.id[1:]
                    section.id = f"№{number}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                elif not section.id.startswith("№"):
                    # Other formats without № - try to extract number and format correctly
                    import re
                    number_match = re.search(r'\d+', section.id)
                    if number_match:
                        number = number_match.group()
                        section.id = f"№{number}"
                        logger.info(f"[FINALIZE_ID_FIX] Fixed ID format to '{section.id}'")
        
        # Update the project with the new content
        await conn.execute("""
            UPDATE projects 
            SET microproduct_content = $1
            WHERE id = $2 AND onyx_user_id = $3
        """, json.dumps(training_plan_details.dict()), payload.projectId, onyx_user_id)
        
        logger.info(f"[FINALIZE_SUCCESS] Updated training plan projectId={payload.projectId}")
        
        # Clean up the cache
        OUTLINE_PREVIEW_CACHE.pop(payload.chatSessionId, None)
        
        return {"success": True, "message": "Training plan updated successfully"}
        
    except Exception as e:
        logger.error(f"[FINALIZE_ERROR] Error finalizing training plan: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to finalize training plan: {str(e)}")

# --- Folders API Models ---
class ProjectFolderCreateRequest(BaseModel):
    name: str
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate

class ProjectFolderResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate

class ProjectFolderListResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    order: int
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    project_count: int
    total_lessons: int
    total_hours: int
    total_completion_time: int
    model_config = {"from_attributes": True}

class ProjectFolderRenameRequest(BaseModel):
    name: str

class ProjectFolderMoveRequest(BaseModel):
    parent_id: Optional[int] = None

class ProjectFolderTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int

# --- Folders API Endpoints ---
@app.get("/api/custom/projects/folders", response_model=List[ProjectFolderListResponse])
async def list_folders(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT 
            pf.id, 
            pf.name, 
            pf.created_at, 
            pf."order", 
            pf.parent_id,
            COALESCE(pf.quality_tier, 'medium') as quality_tier,
            COALESCE(pf.custom_rate, 200) as custom_rate,
            COUNT(p.id) as project_count,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COUNT(*)::int 
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_lessons,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'hours' IS NOT NULL AND lesson->>'hours' != '' 
                                    THEN (lesson->>'hours')::float
                                    ELSE 0 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_hours,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                    THEN (
                                        -- Extract numeric part using regex, handling all language units (m, м, хв)
                                        CASE 
                                            WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                            THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                            ELSE 5
                                        END
                                    )
                                    ELSE 5 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_completion_time
        FROM project_folders pf
        LEFT JOIN projects p ON pf.id = p.folder_id
        WHERE pf.onyx_user_id = $1
        GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id
        ORDER BY pf."order" ASC, pf.created_at ASC;
    """
    async with pool.acquire() as conn:
        rows = await conn.fetch(query, onyx_user_id)
    return [ProjectFolderListResponse(**dict(row)) for row in rows]

@app.post("/api/custom/projects/folders", response_model=ProjectFolderResponse)
async def create_folder(req: ProjectFolderCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    async with pool.acquire() as conn:
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
        
        query = "INSERT INTO project_folders (onyx_user_id, name, parent_id, quality_tier, custom_rate) VALUES ($1, $2, $3, $4, $5) RETURNING id, name, created_at, parent_id, quality_tier, custom_rate;"
        row = await conn.fetchrow(query, onyx_user_id, req.name, req.parent_id, req.quality_tier, req.custom_rate)
    return ProjectFolderResponse(**dict(row))

@app.patch("/api/custom/projects/folders/{folder_id}", response_model=ProjectFolderResponse)
async def rename_folder(folder_id: int, req: ProjectFolderRenameRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "UPDATE project_folders SET name = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at;"
    async with pool.acquire() as conn:
        row = await conn.fetchrow(query, req.name, folder_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Folder not found")
    return ProjectFolderResponse(**dict(row))

@app.delete("/api/custom/projects/folders/{folder_id}", status_code=204)
async def delete_folder(folder_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # Set folder_id to NULL for all projects in this folder (preserve projects)
    async with pool.acquire() as conn:
        await conn.execute("UPDATE projects SET folder_id = NULL WHERE folder_id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
        result = await conn.execute("DELETE FROM project_folders WHERE id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
    if result == "DELETE 0":
        raise HTTPException(status_code=404, detail="Folder not found")
    return JSONResponse(status_code=204, content={})

@app.put("/api/custom/projects/folders/{folder_id}/move", response_model=ProjectFolderResponse)
async def move_folder(folder_id: int, req: ProjectFolderMoveRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Move a folder to a different parent folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
            
            # Prevent circular references - check if the target parent is a descendant of this folder
            if req.parent_id == folder_id:
                raise HTTPException(status_code=400, detail="Cannot move folder into itself")
            
            # Check for circular references by traversing up the tree
            current_parent_id = req.parent_id
            while current_parent_id is not None:
                if current_parent_id == folder_id:
                    raise HTTPException(status_code=400, detail="Cannot move folder into its own descendant")
                parent = await conn.fetchrow(
                    "SELECT parent_id FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                    current_parent_id, onyx_user_id
                )
                if not parent:
                    break
                current_parent_id = parent['parent_id']
        
        # Update the folder's parent_id
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET parent_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at, parent_id",
            req.parent_id, folder_id, onyx_user_id
        )
        
        return ProjectFolderResponse(**dict(updated_folder))

@app.patch("/api/custom/projects/folders/{folder_id}/tier", response_model=ProjectFolderResponse)
async def update_folder_tier(folder_id: int, req: ProjectFolderTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier of a folder and recalculate creation hours for all projects in the folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the folder's quality_tier and custom_rate
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET quality_tier = $1, custom_rate = $2 WHERE id = $3 AND onyx_user_id = $4 RETURNING id, name, created_at, parent_id, quality_tier, custom_rate",
            req.quality_tier, req.custom_rate, folder_id, onyx_user_id
        )
        
        # Get all projects in this folder (including subfolders recursively)
        projects_to_update = await conn.fetch("""
            WITH RECURSIVE folder_tree AS (
                -- Base case: the target folder
                SELECT id, parent_id FROM project_folders WHERE id = $1
                UNION ALL
                -- Recursive case: child folders
                SELECT pf.id, pf.parent_id 
                FROM project_folders pf
                INNER JOIN folder_tree ft ON pf.parent_id = ft.id
            )
            SELECT DISTINCT p.id, p.microproduct_content, p.folder_id
            FROM projects p
            INNER JOIN folder_tree ft ON p.folder_id = ft.id
            WHERE p.microproduct_content IS NOT NULL 
            AND p.microproduct_content->>'sections' IS NOT NULL
        """, folder_id)
        
        # Update creation hours for each project based on the new tier and custom rate
        for project in projects_to_update:
            try:
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    total_completion_time = 0
                    
                    # Calculate total completion time and update tier names and hours
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            # Clear any existing module-level tier settings to ensure folder-level tier takes precedence
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            
                            # Update the module's tier name to match the new folder tier
                            section['quality_tier'] = req.quality_tier
                                
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Clear any existing lesson-level tier settings to ensure folder-level tier takes precedence
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    
                                    # Update the tier name to match the new folder tier
                                    lesson['quality_tier'] = req.quality_tier
                                    
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to total completion time
                                    total_completion_time += completion_time_minutes
                                    
                                    # Recalculate hours with new folder rate using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project['id']} creation hours: {e}")
                continue
        
        return ProjectFolderResponse(**dict(updated_folder))

# --- Update project queries to support folder_id (backward compatible) ---
# In all project list endpoints, add folder_id to SELECT and response models, and allow filtering by folder_id (optional)
# ... existing code ...

class ProjectFolderUpdateRequest(BaseModel):
    folder_id: Optional[int] = None
    model_config = {"from_attributes": True}

@app.put("/api/custom/projects/update/{project_id}", response_model=ProjectDB)
async def update_project_in_db(project_id: int, project_update_data: ProjectUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        db_microproduct_name_to_store = project_update_data.microProductName
        current_component_name = None
        # Fetch current component_name, project_name and content to detect renames/diffs
        old_project_name: Optional[str] = None
        old_microproduct_content: Optional[dict] = None
        async with pool.acquire() as conn:
            project_row = await conn.fetchrow("SELECT p.project_name, p.microproduct_content, dt.component_name FROM projects p JOIN design_templates dt ON p.design_template_id = dt.id WHERE p.id = $1 AND p.onyx_user_id = $2", project_id, onyx_user_id)
            if not project_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or not owned by user.")
            current_component_name = project_row["component_name"]
            old_project_name = project_row["project_name"]
            try:
                old_microproduct_content = dict(project_row["microproduct_content"]) if project_row["microproduct_content"] else None
            except Exception:
                old_microproduct_content = project_row["microproduct_content"] if isinstance(project_row["microproduct_content"], dict) else None

        if (not db_microproduct_name_to_store or not db_microproduct_name_to_store.strip()) and project_update_data.design_template_id:
            async with pool.acquire() as conn: design_row = await conn.fetchrow("SELECT template_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
            if design_row: db_microproduct_name_to_store = design_row["template_name"]

        content_to_store_for_db = project_update_data.microProductContent.model_dump(mode='json', exclude_none=True) if project_update_data.microProductContent else None
        
        # 🔍 BACKEND SAVE LOGGING: What we're about to store in database
        if content_to_store_for_db:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Storing content to DB: {json.dumps(content_to_store_for_db, indent=2)}")
            if 'contentBlocks' in content_to_store_for_db:
                image_blocks = [block for block in content_to_store_for_db['contentBlocks'] if block.get('type') == 'image']
                logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Image blocks to store: {json.dumps(image_blocks, indent=2)}")
        else:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - No content to store (content_to_store_for_db is None)")

        derived_product_type = None; derived_microproduct_type = None
        if project_update_data.design_template_id is not None:
            async with pool.acquire() as conn: design_template = await conn.fetchrow("SELECT microproduct_type, template_name, component_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
            if design_template:
                derived_product_type = design_template["microproduct_type"]
                derived_microproduct_type = design_template["template_name"]
                current_component_name = design_template["component_name"]

        update_clauses = []; update_values = []; arg_idx = 1
        
        # Handle project name updates and sync with Training Plan mainTitle
        project_name_updated = False
        if project_update_data.projectName is not None: 
            update_clauses.append(f"project_name = ${arg_idx}")
            update_values.append(project_update_data.projectName)
            arg_idx += 1
            project_name_updated = True
        if db_microproduct_name_to_store is not None: update_clauses.append(f"microproduct_name = ${arg_idx}"); update_values.append(db_microproduct_name_to_store); arg_idx +=1
        if project_update_data.design_template_id is not None:
            update_clauses.append(f"design_template_id = ${arg_idx}"); update_values.append(project_update_data.design_template_id); arg_idx +=1
            if derived_product_type: update_clauses.append(f"product_type = ${arg_idx}"); update_values.append(derived_product_type); arg_idx += 1
            if derived_microproduct_type: update_clauses.append(f"microproduct_type = ${arg_idx}"); update_values.append(derived_microproduct_type); arg_idx += 1
        if project_update_data.microProductContent is not None: 
            update_clauses.append(f"microproduct_content = ${arg_idx}")
            update_values.append(content_to_store_for_db); arg_idx += 1
            
            # SYNC TITLES: For Training Plans, keep project_name and mainTitle synchronized
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN and content_to_store_for_db:
                try:
                    # Extract mainTitle from the content
                    main_title = content_to_store_for_db.get('mainTitle')
                    if main_title and isinstance(main_title, str) and main_title.strip():
                        # Update project_name to match mainTitle
                        update_clauses.append(f"project_name = ${arg_idx}")
                        update_values.append(main_title.strip())
                        arg_idx += 1
                        project_name_updated = True
                except Exception as e:
                    logger.warning(f"Could not sync mainTitle to project_name for project {project_id}: {e}")

        # SYNC TITLES: If only project_name was updated (not content), sync it to mainTitle for Training Plans
        if (project_name_updated and project_update_data.microProductContent is None and 
            current_component_name == COMPONENT_NAME_TRAINING_PLAN):
            try:
                # Get current content to update mainTitle
                async with pool.acquire() as conn:
                    current_row = await conn.fetchrow(
                        "SELECT microproduct_content FROM projects WHERE id = $1 AND onyx_user_id = $2", 
                        project_id, onyx_user_id
                    )
                    if current_row and current_row["microproduct_content"]:
                        current_content = dict(current_row["microproduct_content"])
                        current_content["mainTitle"] = project_update_data.projectName
                        update_clauses.append(f"microproduct_content = ${arg_idx}")
                        update_values.append(current_content)
                        arg_idx += 1
            except Exception as e:
                logger.warning(f"Could not sync project_name to mainTitle for project {project_id}: {e}")

        if not update_clauses:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No update data provided.")

        update_values.extend([project_id, onyx_user_id])
        update_query = f"UPDATE projects SET {', '.join(update_clauses)} WHERE id = ${arg_idx} AND onyx_user_id = ${arg_idx + 1} RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name, microproduct_content, design_template_id, created_at, custom_rate, quality_tier;"

        async with pool.acquire() as conn: row = await conn.fetchrow(update_query, *update_values)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or update failed.")

        # --- Propagate outline/lesson renames to connected products (best-effort) ---
        try:
            # Only for Training Plans
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                new_project_name = row["project_name"]
                # 1) If outline name changed, update prefix for all connected products
                if project_name_updated and old_project_name and new_project_name and old_project_name.strip() != new_project_name.strip():
                    old_prefix = f"{old_project_name.strip()}: "
                    new_prefix = f"{new_project_name.strip()}: "
                    async with pool.acquire() as conn:
                        children = await conn.fetch(
                            "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND is_standalone = FALSE AND project_name LIKE $2",
                            onyx_user_id, old_prefix + "%"
                        )
                        for child in children:
                            child_id = child["id"]
                            child_pn = child["project_name"] or ""
                            child_mpname = child["microproduct_name"]
                            # Compute new names
                            if ": " in child_pn:
                                suffix = child_pn.split(": ", 1)[1]
                                updated_project_name = new_prefix + suffix
                            else:
                                updated_project_name = child_pn  # unexpected, skip
                            # Update microproduct_name if it matches the old full or is None
                            updated_micro_name = child_mpname
                            if isinstance(child_mpname, str):
                                if child_mpname == child_pn:
                                    updated_micro_name = updated_project_name
                            elif child_mpname is None:
                                updated_micro_name = updated_project_name
                            await conn.execute(
                                "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name) WHERE id = $3 AND onyx_user_id = $4",
                                updated_project_name, updated_micro_name, child_id, onyx_user_id
                            )
                
                # 2) If lesson titles changed inside outline content, rename exact-matching children
                # Compute simple diff by position (module, lesson index)
                def extract_titles(content: Optional[dict]) -> list[list[str]]:
                    titles: list[list[str]] = []
                    if not content or not isinstance(content, dict):
                        return titles
                    sections = content.get("sections") or []
                    for sec in sections:
                        sec_titles: list[str] = []
                        lessons = sec.get("lessons") if isinstance(sec, dict) else []
                        for les in lessons:
                            if isinstance(les, dict):
                                title = str(les.get("title") or les.get("name") or "").strip()
                            else:
                                title = str(les).strip()
                            sec_titles.append(title)
                        titles.append(sec_titles)
                    return titles
                old_titles_by_section = extract_titles(old_microproduct_content)
                new_titles_by_section = extract_titles(content_to_store_for_db if project_update_data.microProductContent is not None else old_microproduct_content)
                rename_pairs: list[tuple[str, str]] = []
                if old_titles_by_section and new_titles_by_section and len(old_titles_by_section) == len(new_titles_by_section):
                    for sec_idx in range(len(old_titles_by_section)):
                        old_ls = old_titles_by_section[sec_idx]
                        new_ls = new_titles_by_section[sec_idx]
                        for li in range(min(len(old_ls), len(new_ls))):
                            old_t = (old_ls[li] or "").strip()
                            new_t = (new_ls[li] or "").strip()
                            if old_t and new_t and old_t != new_t:
                                rename_pairs.append((old_t, new_t))
                if rename_pairs:
                    async with pool.acquire() as conn:
                        for (old_title, new_title) in rename_pairs:
                            old_full = f"{(row['project_name'] or new_project_name).strip()}: {old_title}"
                            new_full = f"{(row['project_name'] or new_project_name).strip()}: {new_title}"
                            children = await conn.fetch(
                                "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2",
                                onyx_user_id, old_full
                            )
                            for child in children:
                                child_id = child["id"]
                                child_mpname = child["microproduct_name"]
                                child_content = child["microproduct_content"]
                                # Update microproduct_name smartly: replace exact matches or lesson-only
                                updated_micro_name = child_mpname
                                if isinstance(child_mpname, str):
                                    if child_mpname == old_full:
                                        updated_micro_name = new_full
                                    elif child_mpname == old_title:
                                        updated_micro_name = new_title
                                elif child_mpname is None:
                                    updated_micro_name = new_full
                                # Update content titles if present
                                updated_content = child_content
                                try:
                                    if isinstance(child_content, dict):
                                        # Quiz
                                        if 'quizTitle' in child_content and isinstance(child_content['quizTitle'], str):
                                            if child_content['quizTitle'].strip() in (old_title, old_full):
                                                child_content['quizTitle'] = new_title
                                        # Text Presentation
                                        if 'textTitle' in child_content and isinstance(child_content['textTitle'], str):
                                            if child_content['textTitle'].strip() in (old_title, old_full):
                                                child_content['textTitle'] = new_title
                                        updated_content = child_content
                                except Exception as e:
                                    logger.warning(f"[RENAME_PROPAGATION] Failed to update content titles for child {child_id}: {e}")
                                await conn.execute(
                                    "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name), microproduct_content = COALESCE($3, microproduct_content) WHERE id = $4 AND onyx_user_id = $5",
                                    new_full, updated_micro_name, updated_content, child_id, onyx_user_id
                                )
        except Exception as e:
            logger.error(f"[RENAME_PROPAGATION] Error during rename propagation for project {project_id}: {e}", exc_info=not IS_PRODUCTION)

        db_content = row["microproduct_content"]
        
        # 🔍 BACKEND RETRIEVE LOGGING: What we got back from database
        logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Retrieved content from DB: {json.dumps(db_content, indent=2) if db_content else 'None'}")
        if db_content and isinstance(db_content, dict) and 'contentBlocks' in db_content:
            image_blocks = [block for block in db_content['contentBlocks'] if block.get('type') == 'image']
            logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Image blocks retrieved: {json.dumps(image_blocks, indent=2)}")
        
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - About to validate with component: {current_component_name}")
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - Validating as TextPresentationDetails")
                    final_content_for_model = TextPresentationDetails(**db_content)
                    logger.info(f"✅ [BACKEND VALIDATION] Project {project_id} - TextPresentationDetails validation successful")
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content and db_content['slides']:
                        db_content['slides'] = normalize_slide_props(db_content['slides'], current_component_name)
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                
                # 🔍 BACKEND VALIDATION RESULT LOGGING
                if final_content_for_model and hasattr(final_content_for_model, 'contentBlocks'):
                    result_dict = final_content_for_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final validated content: {json.dumps(result_dict, indent=2)}")
                    if 'contentBlocks' in result_dict:
                        result_image_blocks = [block for block in result_dict['contentBlocks'] if block.get('type') == 'image']
                        logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final image blocks: {json.dumps(result_image_blocks, indent=2)}")
                
            except Exception as e_parse:
                logger.error(f"❌ [BACKEND VALIDATION ERROR] Project {project_id} - Error parsing updated content from DB: {e_parse}", exc_info=not IS_PRODUCTION)

        return ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_model,
            design_template_id=row["design_template_id"], created_at=row["created_at"],
            custom_rate=row["custom_rate"], quality_tier=row["quality_tier"]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating project." if IS_PRODUCTION else f"DB error on project update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/projects/{project_id}/folder", response_model=ProjectDB)
async def update_project_folder(project_id: int, update_data: ProjectFolderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update a project's folder assignment and recalculate creation hours based on the new folder's tier"""
    async with pool.acquire() as conn:
        # Verify project belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # If folder_id is provided, verify it exists and belongs to user
        if update_data.folder_id is not None:
            folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                update_data.folder_id, onyx_user_id
            )
            if not folder:
                raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the project's folder_id
        updated_project = await conn.fetchrow(
            "UPDATE projects SET folder_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING *",
            update_data.folder_id, project_id, onyx_user_id
        )
        
        # If the project has content and is being moved to a folder, recalculate creation hours
        if update_data.folder_id is not None and project['microproduct_content']:
            try:
                # Get the folder's custom rate
                folder_custom_rate = await get_folder_custom_rate(update_data.folder_id, pool)
                
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    
                    # Update the hours in each lesson and recalculate section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Calculate hours using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with tier-adjusted sum
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Round all hours in the content to ensure they are integers
                    content = round_hours_in_content(content)
                    
                    # Update the project in the database with new hours
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project_id
                    )
                    
                    # Update the returned project data
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        project_id, onyx_user_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours after folder move: {e}")
        
        # Parse the content properly based on component type
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        
        if db_content and isinstance(db_content, dict):
            try:
                # Get the component name to determine the content type
                component_row = await conn.fetchrow(
                    "SELECT dt.component_name FROM projects p JOIN design_templates dt ON p.design_template_id = dt.id WHERE p.id = $1",
                    project_id
                )
                current_component_name = component_row["component_name"] if component_row else COMPONENT_NAME_TRAINING_PLAN
                
                # Round hours to integers before parsing to prevent float validation errors
                if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = round_hours_in_content(db_content)
                
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"]
        )

@app.patch("/api/custom/projects/{project_id}/tier", response_model=ProjectDB)
async def update_project_tier(project_id: int, req: ProjectTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier and custom rate of a project and recalculate creation hours"""
    async with pool.acquire() as conn:
        # Verify the project exists and belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # Update the project's quality_tier and custom_rate
        updated_project = await conn.fetchrow(
            "UPDATE projects SET quality_tier = $1, custom_rate = $2 WHERE id = $3 AND onyx_user_id = $4 RETURNING *",
            req.quality_tier, req.custom_rate, project_id, onyx_user_id
        )
        
        # If the project has content, recalculate creation hours
        if project['microproduct_content']:
            try:
                content = dict(project['microproduct_content'])
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    
                    # Update tier names and sum existing hours for section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            # Clear any existing module-level tier settings to ensure project-level tier takes precedence
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            
                            # Update the module's tier name to match the new project tier
                            section['quality_tier'] = req.quality_tier
                                
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Clear any existing lesson-level tier settings to ensure project-level tier takes precedence
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    
                                    # Update the tier name to match the new project tier  
                                    lesson['quality_tier'] = req.quality_tier
                                    
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Recalculate hours with new project rate using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
                    # Re-fetch the updated project
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1",
                        project_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours: {e}")
        
        # Get current component name for proper content parsing
        current_component_name = None
        if updated_project["design_template_id"]:
            design_template = await conn.fetchrow(
                "SELECT component_name FROM design_templates WHERE id = $1", 
                updated_project["design_template_id"]
            )
            if design_template:
                current_component_name = design_template["component_name"]
        
        # Parse the content into the appropriate model
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"],
            custom_rate=updated_project["custom_rate"],
            quality_tier=updated_project["quality_tier"]
        )

class ProjectOrderUpdateRequest(BaseModel):
    orders: List[Dict[str, int]]  # List of {projectId: int, order: int}

@app.put("/api/custom/projects/update-order", status_code=status.HTTP_200_OK)
async def update_project_order(order_data: ProjectOrderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the order of multiple projects"""
    try:
        async with pool.acquire() as conn:
            # Update each project's order
            for order_item in order_data.orders:
                project_id = order_item.get('projectId')
                order = order_item.get('order')
                
                if project_id is not None and order is not None:
                    # Verify project belongs to user and update order
                    result = await conn.execute(
                        "UPDATE projects SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                        order, project_id, onyx_user_id
                    )
                    
                    if result == "UPDATE 0":
                        logger.warning(f"Project {project_id} not found or not owned by user {onyx_user_id}")
        
        return {"message": "Project order updated successfully"}
    except Exception as e:
        logger.error(f"Error updating project order: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update project order")

@app.put("/api/custom/projects/folders/update-order")
async def update_folder_order(
    orders: List[Dict[str, int]], 
    onyx_user_id: str = Depends(get_current_onyx_user_id), 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Update the order of folders"""
    async with pool.acquire() as conn:
        for order_data in orders:
            folder_id = order_data.get("folderId")
            order = order_data.get("order")
            if folder_id is not None and order is not None:
                await conn.execute(
                    "UPDATE project_folders SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                    order, folder_id, onyx_user_id
                )
    return {"message": "Folder order updated successfully"}

@app.get("/api/custom/projects/{project_id}/lesson-data")
async def get_project_lesson_data(project_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Get lesson data for a project with tier-adjusted creation hours"""
    try:
        async with pool.acquire() as conn:
            # Get project details including folder_id
            project = await conn.fetchrow(
                "SELECT p.microproduct_content, p.folder_id, dt.component_name FROM projects p JOIN design_templates dt ON p.design_template_id = dt.id WHERE p.id = $1 AND p.onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
            if not project:
                raise HTTPException(status_code=404, detail="Project not found")
            
            content = project["microproduct_content"]
            component_name = project["component_name"]
            folder_id = project["folder_id"]
            
            # Only Training Plans have lesson data
            if component_name != COMPONENT_NAME_TRAINING_PLAN or not content:
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            
            # Get the folder's custom rate (with inheritance from parent)
            folder_custom_rate = 200  # Default custom rate
            if folder_id:
                folder_custom_rate = await get_folder_custom_rate(folder_id, pool)
            
            # Parse the training plan content
            try:
                if isinstance(content, dict):
                    sections = content.get("sections", [])
                    total_lessons = 0
                    total_hours = 0
                    total_completion_time = 0
                    sections_data = []
                    
                    for section in sections:
                        if isinstance(section, dict):
                            lessons = section.get("lessons", [])
                            section_lessons = len(lessons)
                            section_hours = 0
                            section_completion_time = 0
                            
                            total_lessons += section_lessons
                            
                            # Sum up completion time and use existing lesson hours for this section
                            for lesson in lessons:
                                if isinstance(lesson, dict):
                                    # Parse completion time (handles all language units: m, м, хв) - treat missing as 5 minutes
                                    completion_time_str = lesson.get("completionTime", "")
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            # Extract numeric part using regex to handle all language units
                                            import re
                                            numbers = re.findall(r'\d+', time_str)
                                            if numbers:
                                                try:
                                                    # If it contains 'h' (hour indicator), convert to minutes
                                                    if 'h' in time_str.lower():
                                                        completion_time_minutes = int(numbers[0]) * 60
                                                    else:
                                                        # For minutes (m, м, хв), just use the number
                                                        completion_time_minutes = int(numbers[0])
                                                except (ValueError, IndexError):
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # No numbers found, use 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to totals
                                    section_completion_time += completion_time_minutes
                                    total_completion_time += completion_time_minutes
                                    
                                    # Use existing lesson hours if available, otherwise calculate with folder rate
                                    if lesson.get('hours'):
                                        try:
                                            lesson_creation_hours = float(lesson['hours'])
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                        except (ValueError, TypeError):
                                            # If hours parsing fails, calculate with completion time
                                            lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                    else:
                                        # No existing hours, calculate with completion time
                                        lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                        section_hours += lesson_creation_hours
                                        total_hours += lesson_creation_hours
                            
                            # Add section data with tier-adjusted totals
                            sections_data.append({
                                "id": section.get("id", ""),
                                "title": section.get("title", ""),
                                "totalHours": round(section_hours),
                                "totalCompletionTime": section_completion_time,
                                "lessonCount": section_lessons
                            })
                    
                    return {
                        "lessonCount": total_lessons, 
                        "totalHours": round(total_hours), 
                        "completionTime": total_completion_time,
                        "sections": sections_data
                    }
                else:
                    return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            except Exception as e:
                logger.warning(f"Error parsing lesson data for project {project_id}: {e}")
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting lesson data for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to get lesson data")

@app.get("/api/custom/pdf/projects-list", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_projects_list_pdf(
    folder_id: Optional[int] = Query(None),
    column_visibility: Optional[str] = Query(None),  # JSON string of column visibility settings
    client_name: Optional[str] = Query(None),  # Client name for PDF header customization
    selected_folders: Optional[str] = Query(None),  # JSON string of selected folder IDs
    selected_projects: Optional[str] = Query(None),  # JSON string of selected project IDs
    column_widths: Optional[str] = Query(None),  # JSON string of column width settings
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download projects list as PDF with all folders expanded, deduplicated like the products page."""
    try:
        # Parse column visibility settings
        column_visibility_settings = {
            'title': True,
            'created': False,
            'creator': False,
            'numberOfLessons': True,
            'estCreationTime': True,
            'estCompletionTime': True
        }
        if column_visibility:
            try:
                parsed_settings = json.loads(column_visibility)
                column_visibility_settings.update(parsed_settings)
            except json.JSONDecodeError:
                logger.warning("Invalid column_visibility JSON, using defaults")

        # Fetch projects and folders data
        async with pool.acquire() as conn:
            # Fetch projects
            projects_query = """
                SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
                       dt.template_name as design_template_name,
                       dt.microproduct_type as design_microproduct_type,
                       p.folder_id, p."order", p.microproduct_content
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.onyx_user_id = $1
                ORDER BY p."order" ASC, p.created_at DESC;
            """
            projects_params = [onyx_user_id]
            
            if folder_id is not None:
                projects_query = projects_query.replace("WHERE p.onyx_user_id = $1", "WHERE p.onyx_user_id = $1 AND p.folder_id = $2")
                projects_params.append(folder_id)
            
            projects_rows = await conn.fetch(projects_query, *projects_params)
            
            # Fetch folders with hierarchical structure (only if not viewing a specific folder)
            folders_data = []
            if folder_id is None:
                folders_query = """
                    SELECT 
                        pf.id, 
                        pf.name, 
                        pf.created_at, 
                        pf."order", 
                        pf.parent_id,
                        pf.quality_tier,
                        pf.custom_rate,
                        COUNT(p.id) as project_count,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COUNT(*)::int 
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_lessons,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM((lesson->>'hours')::float), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_hours,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM(
                                            CASE 
                                                WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                                THEN (
                                                    -- Extract numeric part using regex, handling all language units (m, м, хв)
                                                    CASE 
                                                        WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                                        THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                                        ELSE 5
                                                    END
                                                )
                                                ELSE 5 
                                            END
                                        ), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_completion_time
                    FROM project_folders pf
                    LEFT JOIN projects p ON pf.id = p.folder_id
                    WHERE pf.onyx_user_id = $1
                    GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id, pf.quality_tier, pf.custom_rate
                    ORDER BY pf."order" ASC, pf.created_at ASC;
                """
                folders_rows = await conn.fetch(folders_query, onyx_user_id)
                folders_data = [dict(row) for row in folders_rows]

        # Process projects data
        projects_data = []
        for row in projects_rows:
            row_dict = dict(row)
            
            # Calculate individual project times
            total_lessons = 0
            total_hours = 0.0
            total_completion_time = 0
            
            if row_dict.get('microproduct_content') and isinstance(row_dict['microproduct_content'], dict):
                content = row_dict['microproduct_content']
                if content.get('sections') and isinstance(content['sections'], list):
                    for section in content['sections']:
                        if section.get('lessons') and isinstance(section['lessons'], list):
                            for lesson in section['lessons']:
                                total_lessons += 1
                                if lesson.get('hours'):
                                    try:
                                        total_hours += float(lesson['hours'])
                                    except (ValueError, TypeError):
                                        pass
                                
                                # Calculate completion time - treat missing completion time as 5 minutes
                                completion_time_str = lesson.get('completionTime', '')
                                if completion_time_str:
                                    time_str = str(completion_time_str).strip()
                                    if time_str and time_str != '':
                                        if time_str.endswith('m'):
                                            try:
                                                minutes = int(time_str[:-1])
                                                total_completion_time += minutes
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.endswith('h'):
                                            try:
                                                hours = int(time_str[:-1])
                                                total_completion_time += (hours * 60)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.isdigit():
                                            try:
                                                total_completion_time += int(time_str)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        else:
                                            total_completion_time += 5  # Fallback to 5 minutes
                                    else:
                                        total_completion_time += 5  # Empty string, use 5 minutes
                                else:
                                    total_completion_time += 5  # No completion time, use 5 minutes
            
            projects_data.append({
                'id': row_dict['id'],
                'title': row_dict.get('project_name') or row_dict.get('microproduct_name') or 'Untitled',
                'created_at': row_dict['created_at'],
                'created_by': 'You',
                'design_microproduct_type': row_dict.get('design_microproduct_type'),
                'folder_id': row_dict.get('folder_id'),
                'order': row_dict.get('order', 0),
                'microproduct_content': row_dict.get('microproduct_content'),
                'total_lessons': total_lessons,
                'total_hours': round(total_hours),
                'total_completion_time': total_completion_time
            })

        # --- Deduplicate projects: only show top-level products and outlines, hide lessons/quizzes that belong to an outline ---
        def deduplicate_projects(projects_arr):
            outline_names = set()
            filtered_projects = []
            grouped = {}
            # First pass: collect all outline names and group by title
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    outline_names.add(proj['title'].strip())
                if proj['title'] not in grouped:
                    grouped[proj['title']] = {'outline': None, 'others': []}
                if is_outline:
                    if not grouped[proj['title']]['outline']:
                        grouped[proj['title']]['outline'] = proj
                else:
                    grouped[proj['title']]['others'].append(proj)
            # Second pass: filter projects
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    filtered_projects.append(proj)
                else:
                    project_title = proj['title'].strip()
                    belongs_to_outline = False
                    group_for_this_title = grouped[proj['title']]
                    if group_for_this_title and group_for_this_title['outline']:
                        belongs_to_outline = True
                    if not belongs_to_outline and ': ' in project_title:
                        outline_part = project_title.split(': ')[0].strip()
                        if outline_part in outline_names:
                            belongs_to_outline = True
                    if not belongs_to_outline:
                        filtered_projects.append(proj)
            return filtered_projects

        projects_data = deduplicate_projects(projects_data)

        # Build folder tree structure
        def build_folder_tree(folders):
            folder_map = {}
            root_folders = []
            
            # Create folder map
            for folder in folders:
                folder['children'] = []
                folder_map[folder['id']] = folder
            
            # Build tree structure
            for folder in folders:
                if folder['parent_id'] is None:
                    root_folders.append(folder)
                else:
                    parent = folder_map.get(folder['parent_id'])
                    if parent:
                        parent['children'].append(folder)
            
            return root_folders

        # Group projects by folder
        folder_projects = {}
        unassigned_projects = []
        
        for project in projects_data:
            if project['folder_id']:
                if project['folder_id'] not in folder_projects:
                    folder_projects[project['folder_id']] = []
                folder_projects[project['folder_id']].append(project)
            else:
                unassigned_projects.append(project)

        # Build hierarchical folder structure
        folder_tree = build_folder_tree(folders_data) if folders_data else []

        # Calculate recursive totals for folders (including subfolder projects)
        def calculate_recursive_totals(folder):
            # Start with direct project totals
            direct_projects = folder_projects.get(folder['id'], [])
            total_lessons = sum(p['total_lessons'] for p in direct_projects)
            total_hours = sum(p['total_hours'] for p in direct_projects)
            total_completion_time = sum(p['total_completion_time'] for p in direct_projects)
            total_items = len(direct_projects)
            
            # Add subfolder totals recursively
            if folder.get('children'):
                for child in folder['children']:
                    child_totals = calculate_recursive_totals(child)
                    total_lessons += child_totals['total_lessons']
                    total_hours += child_totals['total_hours']
                    total_completion_time += child_totals['total_completion_time']
                    total_items += child_totals['total_items']
            
            # Update folder with recursive totals
            folder['total_lessons'] = total_lessons
            folder['total_hours'] = total_hours
            folder['total_completion_time'] = total_completion_time
            folder['project_count'] = total_items
            
            return {
                'total_lessons': total_lessons,
                'total_hours': total_hours,
                'total_completion_time': total_completion_time,
                'total_items': total_items
            }

        # Calculate recursive totals for all root folders
        for folder in folder_tree:
            calculate_recursive_totals(folder)

        # Helper function to get tier color
        def get_tier_color(tier):
            tier_colors = {
                'basic': '#22c55e',        # green-500
                'interactive': '#f97316',  # orange-500
                'advanced': '#a855f7',     # purple-500
                'immersive': '#3b82f6',    # blue-500
                # Legacy tier support
                'starter': '#22c55e',      # green-500 (mapped to basic)
                'medium': '#f97316',       # orange-500 (mapped to interactive)
                'professional': '#3b82f6'  # blue-500 (mapped to immersive)
            }
            return tier_colors.get(tier, '#f97316')  # default to interactive

        # Helper function to check if folder has course outlines
        def has_course_outlines(folder_id):
            projects = folder_projects.get(folder_id, [])
            return any(p.get('design_microproduct_type', '').lower() == 'training plan' for p in projects)

        # Helper function to check if folder or any subfolder has course outlines
        def has_course_outlines_recursive(folder):
            # Check direct projects
            if has_course_outlines(folder['id']):
                return True
            
            # Check subfolders recursively
            if folder.get('children'):
                for child in folder['children']:
                    if has_course_outlines_recursive(child):
                        return True
            
            return False

        # Add tier information and check for course outlines
        def add_tier_info(folder):
            folder['tier_color'] = get_tier_color(folder.get('quality_tier', 'interactive'))
            folder['has_course_outlines'] = has_course_outlines_recursive(folder)
            
            # Recursively process children
            if folder.get('children'):
                for child in folder['children']:
                    add_tier_info(child)

        # Add tier information to all folders
        for folder in folder_tree:
            add_tier_info(folder)

        # Filter data based on selected folders and projects
        if selected_folders or selected_projects:
            try:
                selected_folder_ids = set()
                selected_project_ids = set()
                
                # Parse selected folders
                if selected_folders:
                    selected_folder_ids = set(json.loads(selected_folders))
                
                # Parse selected projects
                if selected_projects:
                    selected_project_ids = set(json.loads(selected_projects))
                
                # Filter folders - only include selected folders and their children
                def filter_folders_recursive(folders_list):
                    filtered_folders = []
                    for folder in folders_list:
                        # Include folder if it's selected or if any of its children are selected
                        if folder['id'] in selected_folder_ids:
                            filtered_folders.append(folder)
                        else:
                            # Check if any children are selected
                            if folder.get('children'):
                                filtered_children = filter_folders_recursive(folder['children'])
                                if filtered_children:
                                    folder_copy = folder.copy()
                                    folder_copy['children'] = filtered_children
                                    filtered_folders.append(folder_copy)
                    return filtered_folders
                
                filtered_folder_tree = filter_folders_recursive(folder_tree)
                
                # Filter folder projects - only include projects from selected folders
                filtered_folder_projects = {}
                for folder_id, projects in folder_projects.items():
                    if folder_id in selected_folder_ids:
                        # Filter projects within this folder
                        if selected_project_ids:
                            filtered_projects = [p for p in projects if p['id'] in selected_project_ids]
                            if filtered_projects:
                                filtered_folder_projects[folder_id] = filtered_projects
                        else:
                            filtered_folder_projects[folder_id] = projects
                
                # Filter unassigned projects
                filtered_unassigned_projects = unassigned_projects
                if selected_project_ids:
                    filtered_unassigned_projects = [p for p in unassigned_projects if p['id'] in selected_project_ids]
                
                # Use filtered data
                folder_tree = filtered_folder_tree
                folder_projects = filtered_folder_projects
                unassigned_projects = filtered_unassigned_projects
                
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing selected folders/projects: {e}. Using all data.")
                # If parsing fails, use all data (fallback)

        # Parse column widths if provided
        column_widths_settings = {}
        if column_widths:
            try:
                column_widths_settings = json.loads(column_widths)
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing column widths: {e}. Using default widths.")
                column_widths_settings = {}

        # Prepare data for template
        template_data = {
            'folders': folder_tree,  # Use hierarchical structure
            'folder_projects': folder_projects,
            'unassigned_projects': unassigned_projects,
            'column_visibility': column_visibility_settings,
            'column_widths': column_widths_settings,
            'folder_id': folder_id,
            'client_name': client_name,  # Client name for header customization
            'generated_at': datetime.now().isoformat()
        }

        # Generate PDF
        unique_output_filename = f"projects_list_{onyx_user_id}_{uuid.uuid4().hex[:12]}.pdf"
        pdf_path = await generate_pdf_from_html_template("projects_list_pdf_template.html", template_data, unique_output_filename)
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        user_friendly_filename = f"projects_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating projects list PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate PDF: {str(e)[:200]}")


# Quiz endpoints
class QuizWizardPreview(BaseModel):
    outlineId: Optional[int] = None  # Parent Training Plan project id
    lesson: Optional[str] = None      # Specific lesson to generate quiz for, optional when prompt-based
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"  # comma-separated question types
    questionCount: int = 10  # Number of questions to generate
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text

class QuizWizardFinalize(BaseModel):
    outlineId: Optional[int] = None
    lesson: str
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    aiResponse: str                        # User-edited quiz data
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"
    questionCount: int = 10  # Number of questions to generate
    language: str = "en"
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: user edits tracking (like in Course Outline)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False

class QuizEditRequest(BaseModel):
    currentContent: str
    editPrompt: str
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    questionTypes: Optional[str] = None
    language: str = "en"
    fromFiles: bool = False
    fromText: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    textMode: Optional[str] = None
    questionCount: int = 10
    chatSessionId: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False

async def _ensure_quiz_template(pool: asyncpg.Pool) -> int:
    """Ensure quiz design template exists, return template ID"""
    try:
        # Check if quiz template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Quiz' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create quiz template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Quiz Template",
            "Create an interactive quiz with various question types including multiple choice, multi-select, matching, sorting, and open answer questions.",
            "Quiz",
            COMPONENT_NAME_QUIZ,
            "/quiz.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring quiz template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure quiz template")

@app.post("/api/custom/quiz/generate")
async def quiz_generate(payload: QuizWizardPreview, request: Request):
    """Generate quiz content with streaming response"""
    logger.info(f"[QUIZ_PREVIEW_START] Quiz preview initiated")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] questionTypes={payload.questionTypes} lang={payload.language}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_PREVIEW_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz",
        "prompt": payload.prompt or "Create a quiz",
        "language": payload.language,
        "questionTypes": payload.questionTypes,
        "questionCount": payload.questionCount,
    }

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"Failed to create virtual file for large text: {e}")
                # Fallback to chunking if virtual file creation fails
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations - For quizzes: questions, answers, explanations ALL must be in {payload.language}"  

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[QUIZ_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[QUIZ_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[QUIZ_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                # Step 1: Extract file context from Onyx
                folder_ids_list = []
                file_ids_list = []
                
                if payload.fromFiles and payload.folderIds:
                    folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                
                if payload.fromFiles and payload.fileIds:
                    file_ids_list = parse_id_list(payload.fileIds, "file")
                    logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                
                # Add virtual file ID if created for large text
                if wiz_payload.get("virtualFileId"):
                    file_ids_list.append(wiz_payload["virtualFileId"])
                    logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                
                # Extract context from Onyx
                logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Quiz"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[QUIZ_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[QUIZ_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[QUIZ_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[QUIZ_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[QUIZ_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[QUIZ_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/quiz/edit")
async def quiz_edit(payload: QuizEditRequest, request: Request):
    """Edit quiz content with streaming response"""
    logger.info(f"[QUIZ_EDIT_START] Quiz edit initiated")
    logger.info(f"[QUIZ_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,
        "originalContent": payload.currentContent,
        "editMode": True,
        "isCleanContent": payload.isCleanContent
    }

    # Add context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName
    if payload.questionTypes:
        wiz_payload["questionTypes"] = payload.questionTypes
    if payload.questionCount:
        wiz_payload["questionCount"] = payload.questionCount

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided
    if payload.fromText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[QUIZ_EDIT_STREAM] Starting streamer")
        logger.info(f"[QUIZ_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for quiz editing
        logger.info(f"[QUIZ_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for quiz editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[QUIZ_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[QUIZ_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[QUIZ_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[QUIZ_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[QUIZ_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        
        # NEW: Cache the quiz content for later finalization
        if chat_id:
            QUIZ_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[QUIZ_PREVIEW_CACHE] Cached quiz content for chat_id={chat_id}, length={len(assistant_reply)}")
        
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/quiz/finalize")
async def quiz_finalize(payload: QuizWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize quiz creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get user ID and deduct credits for quiz creation
    try:
        credits_needed = calculate_product_credits("quiz")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Quiz creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for quiz creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for quiz creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this quiz finalization to prevent duplicates
    quiz_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this quiz is already being processed
    if quiz_key in ACTIVE_QUIZ_FINALIZE_KEYS:
        logger.warning(f"[QUIZ_FINALIZE_DUPLICATE] Quiz finalization already in progress for key: {quiz_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Quiz finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(quiz_key)
    QUIZ_FINALIZE_TIMESTAMPS[quiz_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Cleaned up stale quiz key: {stale_key}")
    
    try:
        # NEW: Check for user edits and decide strategy (like in Course Outline)
        use_direct_parser = False
        use_ai_parser = True
        
        if payload.hasUserEdits and payload.originalContent:
            # User has made edits - check if they're significant
            any_changes = _any_quiz_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No quiz changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Quiz changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # Ensure quiz template exists
        template_id = await _ensure_quiz_template(pool)
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lesson.strip()
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip()}"
                        logger.info(f"[QUIZ_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[QUIZ_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[QUIZ_FINALIZE_NAMING] Failed to fetch outline name for quiz naming: {e}")
                # Continue with plain lesson title if outline fetch fails
        else:
            logger.info(f"[QUIZ_FINALIZE_NAMING] No outline ID provided, using standalone naming: {project_name}")
        
        logger.info(f"[QUIZ_FINALIZE_START] Starting quiz finalization for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] quiz_key: {quiz_key}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] isCleanContent: {payload.isCleanContent}")
        
        # NEW: Choose parsing strategy based on user edits
        if use_direct_parser:
            # DIRECT PARSER PATH: Use cached content directly since no changes were made
            logger.info("Using direct parser path for quiz finalization")
            
            # Use the original content for parsing since no changes were made
            content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
            
            parsed_quiz = await parse_ai_response_with_llm(
                ai_response=content_to_parse,
                project_name=project_name,
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
            logger.info("Direct parser path completed successfully")
        else:
            # AI PARSER PATH: Use AI for parsing (original behavior)
            logger.info("Using AI parser path for quiz finalization")
            
            # NEW: Handle clean content (questions only) differently
            if payload.isCleanContent:
                logger.info("Processing clean content (questions only) - will generate options and answers")
                # For clean content, we need to generate complete quiz with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains ONLY quiz questions without options or answers. You need to generate a complete quiz with:
                1. Multiple choice options (A, B, C, D) for each question
                2. Correct answers
                3. Explanations for each answer

                REQUIREMENTS:
                1. Extract the quiz title from the content or use the lesson name
                2. For each question, generate:
                   - "question_type": "multiple-choice" (default)
                   - "question_text": The question text
                   - "options": Array with {{"id": "A", "text": "option text"}} for 4 options
                   - "correct_option_id": "A" (or appropriate letter)
                   - "explanation": Detailed explanation for the correct answer

                CRITICAL RULES:
                - Generate realistic and relevant options for each question
                - Make sure only one option is correct
                - Provide detailed explanations
                - Language: {payload.language}
                - Question types: {payload.questionTypes}
                """
            else:
                # Regular content with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """
            
                        # Parse the quiz data using LLM - only call once with consistent project name
            parsed_quiz = await parse_ai_response_with_llm(
                ai_response=payload.aiResponse,
                project_name=project_name,  # Use consistent project name
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=dynamic_instructions,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
        
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsed quiz title: {parsed_quiz.quizTitle}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Number of questions: {len(parsed_quiz.questions)}")
        
        # NEW: Hardcoded title extraction from first line of AI response
        try:
            extracted_title = project_name.split(":")[0].replace("Quiz - ", "").strip()
        except Exception as e:
            logger.error(f"[QUIZ_FINALIZE_TITLE_EXTRACTION] Error extracting title: {e}")
            extracted_title = None
        
        # Use extracted title if available, otherwise use parsed title or fallback
        if extracted_title:
            parsed_quiz.quizTitle = project_name.split(":")[-1].strip()
            logger.info(f"[QUIZ_FINALIZE_TITLE_EXTRACTION] Using hardcoded title: '{parsed_quiz.quizTitle}'")
        
        # Detect language if not provided
        if not parsed_quiz.detectedLanguage:
            parsed_quiz.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no questions, create a basic quiz structure
        if not parsed_quiz.questions:
            logger.warning(f"[QUIZ_FINALIZE_FALLBACK] LLM parsing failed for quiz, creating fallback structure")
            # Create a simple quiz with the AI response as content
            parsed_quiz.quizTitle = project_name
            parsed_quiz.questions = [
                {
                    "question_type": "open-answer",
                    "question_text": "Please review the quiz content and answer the questions.",
                    "acceptable_answers": ["See quiz content for answers"],
                    "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                }
            ]
        else:
            # Validate that all questions have the required question_type field
            valid_questions = []
            for i, question in enumerate(parsed_quiz.questions):
                if hasattr(question, 'question_type') and question.question_type:
                    valid_questions.append(question)
                else:
                    logger.warning(f"[QUIZ_FINALIZE_VALIDATION] Question {i} missing question_type, converting to open-answer")
                    # Convert to open-answer if question_type is missing
                    if hasattr(question, 'question_text'):
                        valid_questions.append({
                            "question_type": "open-answer",
                            "question_text": question.question_text,
                            "acceptable_answers": ["See original content for answer"],
                            "explanation": "This question was converted from the original format."
                        })
            
            if not valid_questions:
                logger.warning(f"[QUIZ_FINALIZE_VALIDATION] No valid questions found, creating fallback structure")
                parsed_quiz.questions = [
                    {
                        "question_type": "open-answer",
                        "question_text": "Please review the quiz content and answer the questions.",
                        "acceptable_answers": ["See quiz content for answers"],
                        "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                    }
                ]
            else:
                parsed_quiz.questions = valid_questions
        
        # Always use the consistent project name for database storage
        # The quiz title from parsed_quiz.quizTitle is used for display purposes only
        final_project_name = project_name
        
        logger.info(f"[QUIZ_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # Determine if this is a standalone quiz or part of an outline
        is_standalone_quiz = payload.outlineId is None
        
        # For quiz components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Quiz",  # product_type
                COMPONENT_NAME_QUIZ,  # microproduct_type - use the correct component name
                final_project_name,  # microproduct_name
                parsed_quiz.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_quiz,  # is_standalone
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create quiz project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        logger.info(f"[QUIZ_FINALIZE_SUCCESS] Quiz finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_quiz}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[QUIZ_FINALIZE_ERROR] Error in quiz finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(quiz_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(quiz_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Removed quiz_key from active set: {quiz_key}")

@app.delete("/api/custom/lessons/{lesson_id}", status_code=204)
async def delete_lesson(lesson_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Delete a lesson project permanently"""
    try:
        async with pool.acquire() as conn:
            # First, verify the lesson exists and belongs to the user
            lesson = await conn.fetchrow(
                "SELECT id, project_name, microproduct_type FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if not lesson:
                raise HTTPException(status_code=404, detail="Lesson not found or not owned by user")
            
            # Check if this is actually a lesson (not a training plan/course outline)
            if lesson['microproduct_type'] in ('Training Plan', 'Course Outline'):
                raise HTTPException(status_code=400, detail="Cannot delete training plans or course outlines. Please delete individual lessons instead.")
            
            # Delete the lesson
            result = await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(status_code=404, detail="Lesson not found")
            
            logger.info(f"User {onyx_user_id} deleted lesson {lesson_id} ({lesson['project_name']})")
            return JSONResponse(status_code=204, content={})
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting lesson {lesson_id} for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the lesson." if IS_PRODUCTION else f"Database error during lesson deletion: {str(e)}"
        raise HTTPException(status_code=500, detail=detail_msg)


# Default quiz JSON example for LLM parsing
DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM = """
{
  "quizTitle": "Example Quiz with All Question Types",
  "questions": [
    {
      "question_type": "multiple-choice",
      "question_text": "What is the capital of France?",
      "options": [
        {"id": "A", "text": "London"},
        {"id": "B", "text": "Paris"},
        {"id": "C", "text": "Berlin"},
        {"id": "D", "text": "Madrid"}
      ],
      "correct_option_id": "B",
      "explanation": "Paris is the capital and largest city of France."
    },
    {
      "question_type": "multi-select",
      "question_text": "Which of the following are programming languages?",
      "options": [
        {"id": "A", "text": "Python"},
        {"id": "B", "text": "HTML"},
        {"id": "C", "text": "JavaScript"},
        {"id": "D", "text": "CSS"}
      ],
      "correct_option_ids": ["A", "C"],
      "explanation": "Python and JavaScript are programming languages, while HTML and CSS are markup/styling languages."
    },
    {
      "question_type": "matching",
      "question_text": "Match the countries with their capitals:",
      "prompts": [
        {"id": "A", "text": "Germany"},
        {"id": "B", "text": "Italy"},
        {"id": "C", "text": "Spain"}
      ],
      "options": [
        {"id": "1", "text": "Berlin"},
        {"id": "2", "text": "Rome"},
        {"id": "3", "text": "Madrid"}
      ],
      "correct_matches": {"A": "1", "B": "2", "C": "3"},
      "explanation": "Germany-Berlin, Italy-Rome, Spain-Madrid are the correct country-capital pairs."
    },
    {
      "question_type": "sorting",
      "question_text": "Arrange the following steps in the correct order for a sales process:",
      "items_to_sort": [
        {"id": "step1", "text": "Identify customer needs"},
        {"id": "step2", "text": "Present solution"},
        {"id": "step3", "text": "Handle objections"},
        {"id": "step4", "text": "Close the sale"}
      ],
      "correct_order": ["step1", "step2", "step3", "step4"],
      "explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
    },
    {
      "question_type": "open-answer",
      "question_text": "What are the three key elements of an effective elevator pitch?",
      "acceptable_answers": [
        "Problem, Solution, Call to Action",
        "Problem statement, Your solution, What you want them to do next",
        "The issue, How you solve it, What action to take"
      ],
      "explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
    }
  ],
  "detectedLanguage": "en"
}

CRITICAL REQUIREMENTS:
- Output ONLY the JSON object, no other text or formatting
- Every question MUST have "question_type" field with exact values: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
- Use exact field names as shown above
- All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
- The "question_type" field is MANDATORY for every question
"""

# Default text presentation JSON example for LLM parsing
DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Example Text Presentation with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 2, "text": "Main Title of the Presentation" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

# Text Presentation Pydantic models
class TextPresentationWizardPreview(BaseModel):
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    prompt: Optional[str] = None
    language: str = "en"
    length: str = "medium"
    styles: Optional[str] = None
    fromFiles: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    fromText: bool = False
    textMode: Optional[str] = None
    userText: Optional[str] = None
    chatSessionId: Optional[str] = None

class TextPresentationWizardFinalize(BaseModel):
    aiResponse: str
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: User edits tracking (like in Quiz)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    isCleanContent: Optional[bool] = False

class TextPresentationEditRequest(BaseModel):
    content: str
    editPrompt: str
    language: Optional[str] = "en"  # Add language field with default fallback
    chatSessionId: Optional[str] = None
    isCleanContent: Optional[bool] = False

@app.post("/api/custom/text-presentation/generate")
async def text_presentation_generate(payload: TextPresentationWizardPreview, request: Request):
    """Generate text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_START] Text presentation preview initiated")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] lang={payload.language}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation",
        "prompt": payload.prompt or "Create a comprehensive text presentation",
        "language": payload.language,
        "length": payload.length,
    }

    # Add styles if provided
    if payload.styles:
        wiz_payload["styles"] = payload.styles

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        if text_length > LARGE_TEXT_THRESHOLD:
            # Use virtual file system for large texts to prevent AI memory issues
            logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system")
            try:
                virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                wiz_payload["virtualFileId"] = virtual_file_id
                wiz_payload["textCompressed"] = False
                logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
            except Exception as e:
                logger.error(f"Failed to create virtual file for large text: {e}")
                # Fallback to chunking if virtual file creation fails
                chunks = chunk_text(payload.userText)
                if len(chunks) == 1:
                    # Single chunk, use compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for large content ({text_length} -> {len(compressed_text)} chars)")
                else:
                    # Multiple chunks, use first chunk with compression
                    first_chunk = chunks[0]
                    compressed_chunk = compress_text(first_chunk)
                    wiz_payload["userText"] = compressed_chunk
                    wiz_payload["textCompressed"] = True
                    wiz_payload["textChunked"] = True
                    wiz_payload["totalChunks"] = len(chunks)
                    logger.info(f"Fallback to first chunk with compression ({text_length} -> {len(compressed_chunk)} chars, {len(chunks)} total chunks)")
        elif text_length > TEXT_SIZE_THRESHOLD:
            # Compress medium text to reduce payload size
            logger.info(f"Text exceeds compression threshold ({TEXT_SIZE_THRESHOLD}), using compression")
            compressed_text = compress_text(payload.userText)
            wiz_payload["userText"] = compressed_text
            wiz_payload["textCompressed"] = True
            logger.info(f"Using compressed text for medium content ({text_length} -> {len(compressed_text)} chars)")
        else:
            # Use direct text for small content
            logger.info(f"Using direct text for small content ({text_length} chars)")
            wiz_payload["userText"] = payload.userText
            wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[TEXT_PRESENTATION_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                # Step 1: Extract file context from Onyx
                folder_ids_list = []
                file_ids_list = []
                
                if payload.fromFiles and payload.folderIds:
                    folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                
                if payload.fromFiles and payload.fileIds:
                    file_ids_list = parse_id_list(payload.fileIds, "file")
                    logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                
                # Add virtual file ID if created for large text
                if wiz_payload.get("virtualFileId"):
                    file_ids_list.append(wiz_payload["virtualFileId"])
                    logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                
                # Extract context from Onyx
                logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Text Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[TEXT_PRESENTATION_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[TEXT_PRESENTATION_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[TEXT_PRESENTATION_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[TEXT_PRESENTATION_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/text-presentation/edit")
async def text_presentation_edit(payload: TextPresentationEditRequest, request: Request):
    """Edit text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_EDIT_START] Text presentation edit initiated")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,  # Use the language from the request
        "originalContent": payload.content,
        "editMode": True,
        "isCleanContent": getattr(payload, 'isCleanContent', False)
    }

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Starting streamer")
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for text presentation editing
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for text presentation editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[TEXT_PRESENTATION_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/text-presentation/finalize")
async def text_presentation_finalize(payload: TextPresentationWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize text presentation creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    styles_param = getattr(payload, 'styles', None)
    logger.info(f"[TEXT_PRESENTATION_FINALIZE] styles param: {styles_param}")
    
    # Get user ID and deduct credits for one-pager creation
    try:
        credits_needed = calculate_product_credits("one_pager")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "One-pager creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for one-pager creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for one-pager creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this text presentation finalization to prevent duplicates
    text_presentation_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this text presentation is already being processed
    if text_presentation_key in ACTIVE_QUIZ_FINALIZE_KEYS:  # Reuse the same set for simplicity
        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_DUPLICATE] Text presentation finalization already in progress for key: {text_presentation_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Text presentation finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(text_presentation_key)
    QUIZ_FINALIZE_TIMESTAMPS[text_presentation_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Cleaned up stale text presentation key: {stale_key}")
    
    try:
        # Ensure text presentation template exists
        template_id = await _ensure_text_presentation_template(pool)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_TEMPLATE] Template ID: {template_id}")
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lesson.strip() if payload.lesson else "Standalone Presentation"
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip() if payload.lesson else 'Standalone Presentation'}"
                        logger.info(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Failed to fetch outline name for text presentation naming: {e}")
                # Continue with plain lesson title if outline fetch fails
        else:
            logger.info(f"[TEXT_PRESENTATION_FINALIZE_NAMING] No outline ID provided, using standalone naming: {project_name}")
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_START] Starting text presentation finalization for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] text_presentation_key: {text_presentation_key}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] hasUserEdits: {getattr(payload, 'hasUserEdits', False)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
        
        # NEW: Check for user edits and decide strategy (like in Quiz)
        use_direct_parser = False
        use_ai_parser = True
        
        if getattr(payload, 'hasUserEdits', False) and getattr(payload, 'originalContent', None):
            # User has made edits - check if they're significant
            any_changes = _any_text_presentation_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No text presentation changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Text presentation changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # NEW: Choose parsing strategy based on user edits
        if use_direct_parser:
            # DIRECT PARSER PATH: Use cached content directly since no changes were made
            logger.info("Using direct parser path for text presentation finalization")
            
            # Use the original content for parsing since no changes were made
            content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
        else:
            # AI PARSER PATH: Use the provided content (which may be clean titles only)
            logger.info("Using AI parser path for text presentation finalization")
            
            # NEW: Check if we have clean content (only titles without descriptions)
            if getattr(payload, 'isCleanContent', False):
                logger.info("Detected clean content - titles only, will generate descriptions for empty sections")
                
                # Parse the clean content to identify sections that need content generation
                content_to_parse = await _generate_content_for_clean_titles(
                    clean_content=payload.aiResponse,
                    original_content=payload.originalContent,
                    language=payload.language
                )
            else:
                content_to_parse = payload.aiResponse
        
        # Parse the text presentation data using LLM - only call once with consistent project name
        parsed_text_presentation = await parse_ai_response_with_llm(
            ai_response=content_to_parse,
            project_name=project_name,  # Use consistent project name
            target_model=TextPresentationDetails,
            default_error_model_instance=TextPresentationDetails(
                textTitle=project_name,
                contentBlocks=[],
                detectedLanguage=payload.language
            ),
            dynamic_instructions=f"""
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

            1.  **`type: "headline"`**
                * `level` (integer):
                    * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                    * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                    * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                    * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
                * `text` (string): Headline text.
                * `iconName` (string, optional): Based on level and context as described above.
                * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be strings or other nested content blocks.
                * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`):
                    * Can be simple strings for basic numbered points.
                    * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                        * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                        * The `items` of this *inner* `bullet_list` should then be:
                            1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                            2. A `paragraph` block (for the main descriptive text).
                            3. Optionally, another `paragraph` block with `isRecommendation: true`.
                    * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **General Parsing Rules & Icon Names:**
            * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
            * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
            * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
            * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
            * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
            * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """,
            target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
        )
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsed text title: {parsed_text_presentation.textTitle}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Number of content blocks: {len(parsed_text_presentation.contentBlocks)}")

        logger.info(parsed_text_presentation.contentBlocks)
        
        # Detect language if not provided
        if not parsed_text_presentation.detectedLanguage:
            parsed_text_presentation.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no content blocks, create a basic structure
        if not parsed_text_presentation.contentBlocks:
            logger.warning(f"[TEXT_PRESENTATION_FINALIZE_FALLBACK] LLM parsing failed for text presentation, creating fallback structure")
            # Create a simple text presentation with the AI response as content
            parsed_text_presentation.textTitle = project_name
            parsed_text_presentation.contentBlocks = [
                {
                    "type": "paragraph",
                    "text": payload.aiResponse
                }
            ]
        else:
            # Validate that all content blocks have the required type field
            valid_content_blocks = []
            for i, block in enumerate(parsed_text_presentation.contentBlocks):
                if hasattr(block, 'type') and block.type:
                    valid_content_blocks.append(block)
                else:
                    logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] Content block {i} missing type, converting to paragraph")
                    # Convert to paragraph if type is missing
                    if hasattr(block, 'text'):
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": block.text
                        })
                    elif hasattr(block, 'items'):
                        valid_content_blocks.append({
                            "type": "bullet_list",
                            "items": block.items
                        })
                    else:
                        # Fallback to paragraph with string representation
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": str(block)
                        })
            
            if not valid_content_blocks:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] No valid content blocks found, creating fallback structure")
                parsed_text_presentation.contentBlocks = [
                    {
                        "type": "paragraph",
                        "text": payload.aiResponse
                    }
                ]
            else:
                parsed_text_presentation.contentBlocks = valid_content_blocks
        
        # Always use the consistent project name for database storage
        # The text title from parsed_text_presentation.textTitle is used for display purposes only
        final_project_name = project_name
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_text_presentation = payload.outlineId is None
        
        # For text presentation components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW(), $10)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Text Presentation",  # product_type
                COMPONENT_NAME_TEXT_PRESENTATION,  # microproduct_type - use the correct component name
                project_name,  # microproduct_name
                parsed_text_presentation.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_text_presentation,  # is_standalone - consistent with outline connection
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create text presentation project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                content_row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if content_row:
                    logger.info(f"[TEXT_PRESENTATION_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(content_row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved text presentation JSON for project {created_project.id}: {log_e}")
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_SUCCESS] Text presentation finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_text_presentation}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[TEXT_PRESENTATION_FINALIZE_ERROR] Error in text presentation finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(text_presentation_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(text_presentation_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Removed text_presentation_key from active set: {text_presentation_key}")

@app.get("/api/custom/projects/latest-by-chat")
async def get_latest_project_by_chat(chatId: str = Query(..., alias="chatId"), onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Return the most recently created project for the given source_chat_session_id
    for the current user. This is used by finalize fallbacks to navigate reliably
    even when the original finalize request times out.
    """
    try:
        chat_uuid = uuid.UUID(chatId)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid chatId format. Must be UUID")

    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id, project_name, design_template_id, product_type, microproduct_type
                FROM projects
                WHERE onyx_user_id = $1 AND source_chat_session_id = $2
                ORDER BY created_at DESC
                LIMIT 1
                """,
                onyx_user_id, chat_uuid
            )
        if not row:
            return JSONResponse(status_code=404, content={"detail": "No project found for chat session"})
        return {
            "id": row["id"],
            "projectName": row["project_name"],
            "productType": row.get("product_type"),
            "microproductType": row.get("microproduct_type"),
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching latest project by chat: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to fetch latest project by chat session")

# ============================
# CREDITS MANAGEMENT ENDPOINTS
# ============================

@app.get("/api/custom/credits/me", response_model=UserCredits)
async def get_my_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get current user's credit balance (auto-creates if new user)"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # This will auto-create the user if they don't exist yet
        credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        return credits
    except Exception as e:
        logger.error(f"Error getting user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve credits")

@app.get("/api/custom/admin/credits/users", response_model=List[UserCredits])
async def list_all_user_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to list all user credits"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM user_credits 
                ORDER BY updated_at DESC
            """)
            return [UserCredits(**dict(row)) for row in rows]
    except Exception as e:
        logger.error(f"Error listing user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: Usage analytics across all users
@app.get("/api/custom/admin/credits/usage-analytics", response_model=CreditUsageAnalyticsResponse)
async def get_usage_analytics(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT COALESCE(product_type, 'Unknown') as product_type,
                       COALESCE(SUM(credits), 0) AS credits_used
                FROM credit_transactions
                WHERE type = 'product_generation'
                GROUP BY COALESCE(product_type, 'Unknown')
                ORDER BY credits_used DESC
                """
            )
            usage_by_product = [ProductUsage(product_type=row["product_type"], credits_used=int(row["credits_used"] or 0)) for row in rows]
            total_credits = sum(u.credits_used for u in usage_by_product)
            return CreditUsageAnalyticsResponse(usage_by_product=usage_by_product, total_credits_used=total_credits)
    except Exception as e:
        logger.error(f"Error fetching usage analytics: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch usage analytics")

@app.post("/api/custom/admin/credits/migrate-users")
async def migrate_onyx_users_to_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to manually trigger migration of Onyx users to credits table"""
    await verify_admin_user(request)
    
    try:
        # Use the same migration function as startup
        migrated_count = await migrate_onyx_users_to_credits_table()
        
        return {
            "success": True,
            "message": f"Successfully migrated {migrated_count} new users with 100 credits each",
            "users_migrated": migrated_count
        }
    except Exception as e:
        logger.error(f"Error migrating users: {e}")
        raise HTTPException(status_code=500, detail="Failed to migrate users")

@app.post("/api/custom/admin/credits/modify", response_model=CreditTransactionResponse)
async def modify_user_credits(
    transaction: CreditTransactionRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to add or remove credits for a user by email"""
    await verify_admin_user(request)
    
    try:
        if transaction.amount <= 0:
            raise HTTPException(status_code=400, detail="Amount must be positive")
        
        updated_credits = await modify_user_credits_by_email(
            transaction.user_email,
            transaction.amount,
            transaction.action,
            pool,
            transaction.reason
        )
        
        action_msg = "added to" if transaction.action == "add" else "removed from"
        message = f"Successfully {action_msg} {transaction.user_email}: {transaction.amount} credits"
        
        return CreditTransactionResponse(
            success=True,
            message=message,
            new_balance=updated_credits.credits_balance,
            user_credits=updated_credits
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error modifying user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to modify credits")

@app.get("/api/custom/admin/credits/user/{user_email}", response_model=UserCredits)
async def get_user_credits_by_email(
    user_email: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to get specific user's credits by email"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM user_credits WHERE onyx_user_id = $1",
                user_email
            )
            
            if not row:
                # Create entry for user if doesn't exist
                row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                    VALUES ($1, $2, $3)
                    RETURNING *
                """, user_email, user_email.split('@')[0], 0)
            
            return UserCredits(**dict(row))
            
    except Exception as e:
        logger.error(f"Error getting user credits by email: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: User transaction history (purchases + product generations)
@app.get("/api/custom/admin/credits/user/{user_id}/transactions", response_model=UserTransactionHistoryResponse)
async def get_user_transactions(
    user_id: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    async with pool.acquire() as conn:
        # Resolve user by numeric id or email
        if user_id.isdigit():
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE id = $1", int(user_id))
        else:
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE onyx_user_id = $1", user_id)

        if not user_row:
            raise HTTPException(status_code=404, detail="User not found")

        tx_rows = await conn.fetch(
            """
            SELECT id, type, title, credits, created_at, product_type
            FROM credit_transactions
            WHERE onyx_user_id = $1
            ORDER BY created_at DESC
            LIMIT 200
            """,
            user_row["onyx_user_id"]
        )

        activities = [
            TimelineActivity(
                id=str(r["id"]),
                type=r["type"],
                title=r["title"] or (r["type"].replace('_',' ').title()),
                credits=int(r["credits"] or 0),
                timestamp=r["created_at"],
                product_type=r["product_type"]
            )
            for r in tx_rows
        ]

        return UserTransactionHistoryResponse(
            user_id=int(user_row["id"]),
            user_email=user_row["onyx_user_id"],
            user_name=user_row["name"],
            transactions=activities
        )
        

@app.post("/api/custom/projects/duplicate/{project_id}", response_model=ProjectDuplicationResponse)
async def duplicate_project(project_id: int, request: Request, user_id: str = Depends(get_current_onyx_user_id)):
    """
    Duplicate a project. If it's a Training Plan, also duplicate all connected products (lessons, quizzes, etc.).
    Enhanced with proper transaction management and complete field mapping.
    """
    async with DB_POOL.acquire() as conn:
        # Start transaction for atomic operations
        async with conn.transaction():
            try:
                # Fetch original project with all fields
                orig = await conn.fetchrow("SELECT * FROM projects WHERE id = $1", project_id)
                if not orig:
                    raise HTTPException(status_code=404, detail="Project not found")
                
                # Verify user ownership
                if orig['onyx_user_id'] != user_id:
                    raise HTTPException(status_code=403, detail="Access denied")
                
                new_name = f"Copy of {orig['project_name']}"
                now = datetime.now(timezone.utc)
                
                logger.info(f"Starting duplication of project {project_id} (type: {orig['microproduct_type']}) for user {user_id}")
                
                if orig['microproduct_type'] == "Training Plan":
                    # Training Plan duplication - handle connected products
                    new_session_id = str(uuid4())
                    
                    # Duplicate the main Training Plan with all fields
                    new_outline_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                        RETURNING id
                        """,
                        user_id,
                        new_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],  # JSONB will be handled automatically by asyncpg
                        orig['design_template_id'],
                        now,
                        new_session_id,
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Created new Training Plan with ID {new_outline_id}")
                    
                    # Find all connected products using the same naming patterns as frontend
                    # Get all user's projects to search through
                    all_projects = await conn.fetch(
                        "SELECT * FROM projects WHERE onyx_user_id = $1 ORDER BY created_at",
                        user_id
                    )
                    
                    # Find connected products using frontend naming patterns
                    connected = []
                    original_outline_name = orig['project_name'].strip()
                    
                    for project in all_projects:
                        if project['id'] == orig['id']:
                            continue  # Skip the original training plan.
                        
                        project_name = project['project_name'].strip()
                        micro_name = project['microproduct_name']
                        
                        # Skip other Training Plans
                        if project['microproduct_type'] == "Training Plan":
                            continue
                        
                        is_connected = False
                        
                        # Method 1: Legacy matching - project name matches outline and microProductName matches lesson
                        if project_name == original_outline_name and micro_name:
                            is_connected = True
                            logger.info(f"Found connected product via legacy matching: {project_name} (micro: {micro_name})")
                        
                        # Method 2: New naming convention - project name follows "Outline Name: Lesson Title" pattern
                        elif ': ' in project_name:
                            outline_part = project_name.split(': ')[0].strip()
                            if outline_part == original_outline_name:
                                is_connected = True
                                logger.info(f"Found connected product via new pattern: {project_name}")
                        
                        # Method 3: Legacy patterns for backward compatibility
                        # Legacy Quiz pattern - "Quiz - Outline Name: Lesson Title"
                        elif project_name.startswith('Quiz - ') and ': ' in project_name:
                            quiz_part = project_name.replace('Quiz - ', '', 1)
                            if ': ' in quiz_part:
                                outline_part = quiz_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy quiz pattern: {project_name}")
                        
                        # Legacy Text Presentation pattern - "Text Presentation - Outline Name: Lesson Title"
                        elif project_name.startswith('Text Presentation - ') and ': ' in project_name:
                            text_part = project_name.replace('Text Presentation - ', '', 1)
                            if ': ' in text_part:
                                outline_part = text_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy text presentation pattern: {project_name}")
                        
                        # Method 4: Alternative pattern - project name matches lesson title directly
                        # This is for cases where the lesson title became the project name
                        elif orig['microproduct_content']:
                            # Check if this project name matches any lesson title in the training plan
                            try:
                                content = orig['microproduct_content']
                                if isinstance(content, dict) and 'sections' in content:
                                    for section in content['sections']:
                                        if 'lessons' in section:
                                            for lesson in section['lessons']:
                                                lesson_title = lesson.get('title', '').strip()
                                                if lesson_title and lesson_title == project_name:
                                                    is_connected = True
                                                    logger.info(f"Found connected product via lesson title matching: {project_name}")
                                                    break
                                        if is_connected:
                                            break
                                    if is_connected:
                                        break
                            except Exception as e:
                                logger.warning(f"Error checking lesson title matching for {project_name}: {e}")
                        
                        if is_connected:
                            connected.append(project)
                    
                    logger.info(f"Found {len(connected)} connected products to duplicate")
                    
                    # Duplicate each connected product
                    duplicated_products = []
                    for i, prod in enumerate(connected):
                        try:
                            # Smart name replacement - handle various naming patterns
                            prod_name = prod['project_name']
                            if prod_name.startswith(orig['project_name']):
                                prod_name = prod_name.replace(orig['project_name'], new_name, 1)
                            else:
                                # If name doesn't start with parent name, just add "Copy of" prefix
                                prod_name = f"Copy of {prod_name}"
                            
                            # Update microproduct name if it references the parent
                            micro_name = prod['microproduct_name']
                            if micro_name and micro_name.startswith(orig['project_name']):
                                micro_name = micro_name.replace(orig['project_name'], new_name, 1)
                            
                            # Insert the duplicated product with all fields
                            new_prod_id = await conn.fetchval(
                                """
                                INSERT INTO projects (
                                    onyx_user_id, project_name, product_type, microproduct_type, 
                                    microproduct_name, microproduct_content, design_template_id, 
                                    created_at, source_chat_session_id, folder_id, "order", 
                                    is_standalone, completion_time, custom_rate, quality_tier
                                )
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                                RETURNING id
                                """,
                                user_id,
                                prod_name,
                                prod['product_type'],
                                prod['microproduct_type'],
                                micro_name,
                                prod['microproduct_content'],  # JSONB content preserved
                                prod['design_template_id'],
                                now,
                                new_session_id,  # Link to new Training Plan
                                prod['folder_id'],
                                prod['order'],
                                prod['is_standalone'],
                                prod['completion_time'],
                                prod['custom_rate'],
                                prod['quality_tier']
                            )
                            
                            duplicated_products.append({
                                'original_id': prod['id'],
                                'new_id': new_prod_id,
                                'type': prod['microproduct_type'],
                                'name': prod_name
                            })
                            
                            logger.info(f"Duplicated {prod['microproduct_type']} '{prod['project_name']}' -> '{prod_name}' (ID: {new_prod_id})")
                            
                        except Exception as e:
                            logger.error(f"Failed to duplicate connected product {prod['id']} ({prod['microproduct_type']}): {str(e)}")
                            # Re-raise to trigger transaction rollback
                            raise HTTPException(
                                status_code=500, 
                                detail=f"Failed to duplicate {prod['microproduct_type']} '{prod['project_name']}': {str(e)}"
                            )
                    
                    logger.info(f"Successfully duplicated Training Plan and {len(duplicated_products)} connected products")
                    
                    return {
                        "id": new_outline_id,
                        "name": new_name,
                        "type": "Training Plan",
                        "connected_products": duplicated_products,
                        "total_products_duplicated": len(duplicated_products) + 1
                    }
                    
                else:
                    # Regular product duplication (non-Training Plan)
                    new_prod_name = f"Copy of {orig['project_name']}"
                    
                    new_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                        RETURNING id
                        """,
                        user_id,
                        new_prod_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],
                        orig['design_template_id'],
                        now,
                        str(uuid4()),  # New session ID for standalone product
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Successfully duplicated {orig['microproduct_type']} '{orig['project_name']}' -> '{new_prod_name}' (ID: {new_id})")
                    
                    return {
                        "id": new_id,
                        "name": new_prod_name,
                        "type": orig['microproduct_type'],
                        "total_products_duplicated": 1
                    }
                    
            except HTTPException:
                # Re-raise HTTP exceptions (these are expected errors)
                raise
            except Exception as e:
                logger.error(f"Unexpected error during project duplication: {str(e)}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to duplicate project: {str(e)}"
                )

def _any_quiz_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited quiz content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Quiz content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No quiz changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during quiz change detection (assuming changes made): {e}")
        return True

def _any_text_presentation_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited text presentation content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Text presentation content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No text presentation changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during text presentation change detection (assuming changes made): {e}")
        return True

async def _generate_content_for_clean_titles(clean_content: str, original_content: str, language: str) -> str:
    """Generate content for clean titles (titles without descriptions)"""
    try:
        logger.info("Starting content generation for clean titles")
        
        # Parse the clean content to identify sections
        sections = []
        lines = clean_content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if this is a header (## Title)
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if header_match:
                # Save previous section if exists
                if current_section:
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    'title': header_match.group(2).strip(),
                    'content': '',
                    'needs_content': True
                }
            elif current_section:
                # This is content for the current section
                current_section['content'] += line + '\n'
                current_section['needs_content'] = False
        
        # Add the last section
        if current_section:
            sections.append(current_section)
        
        logger.info(f"Found {len(sections)} sections, {sum(1 for s in sections if s['needs_content'])} need content generation")
        
        # Generate content for sections that need it
        for section in sections:
            if section['needs_content']:
                logger.info(f"Generating content for section: {section['title']}")
                
                # Create prompt for content generation
                prompt = f"""Generate comprehensive content for the following section title in {language} language:

Title: {section['title']}

Please provide detailed, informative content that explains the topic thoroughly. The content should be:
- Educational and informative
- Well-structured with paragraphs
- Include relevant examples or explanations
- Match the tone and style of a professional presentation

Generate the content:"""
                
                try:
                    # Use OpenAI to generate content
                    response = await stream_openai_response_direct(prompt)
                    section['content'] = response
                    logger.info(f"Generated {len(response)} characters for section: {section['title']}")
                except Exception as e:
                    logger.error(f"Failed to generate content for section {section['title']}: {e}")
                    # Fallback to a simple description
                    section['content'] = f"This section covers {section['title']}. Please refer to the original content for detailed information."
        
        # Reconstruct the content with generated descriptions
        result_content = ""
        for section in sections:
            result_content += f"## {section['title']}\n\n{section['content']}\n\n"
        
        logger.info(f"Content generation completed. Total length: {len(result_content)} characters")
        return result_content.strip()
        
    except Exception as e:
        logger.error(f"Error in content generation for clean titles: {e}")
        # Fallback to original content
        return clean_content

async def stream_openai_response_direct(prompt: str, model: str = None) -> str:
    """
    Get a complete response directly from OpenAI API (non-streaming).
    Returns the full response as a string.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_DIRECT] Starting direct OpenAI request with model {model}")
        logger.info(f"[OPENAI_DIRECT] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_DIRECT] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the chat completion (non-streaming)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.2
        )
        
        if response.choices and len(response.choices) > 0:
            content = response.choices[0].message.content
            logger.info(f"[OPENAI_DIRECT] Response received: {len(content)} characters")
            return content
        else:
            logger.error(f"[OPENAI_DIRECT] No content in response")
            return ""
            
    except Exception as e:
        logger.error(f"[OPENAI_DIRECT] Error in OpenAI direct request: {e}", exc_info=True)
        return f"Error generating content: {str(e)}"