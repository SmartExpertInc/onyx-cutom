# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone, date
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
# NEW: Google Gemini imports for image generation
import google.generativeai as genai
from uuid import uuid4
from cryptography.fernet import Fernet

# Load environment variables from a local .env if present
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# NEW: PDF manipulation imports
try:
    from PyPDF2 import PdfMerger
except ImportError:
    PdfMerger = None

# Feature management models
from app.models.feature_models import (
    FeatureDefinition, UserFeature, UserFeatureWithDetails,
    BulkFeatureToggleRequest, FeatureToggleRequest, UserTypeAssignmentRequest, UserTypeAssignmentRequest
)
from app.models.feature_models import UserTypeAssignmentRequest

# Workspace management models and services
from app.models.workspace_models import (
    Workspace, WorkspaceCreate, WorkspaceUpdate,
    WorkspaceWithMembers, WorkspaceRole, WorkspaceRoleCreate, WorkspaceRoleUpdate,
    WorkspaceMember, WorkspaceMemberCreate, WorkspaceMemberUpdate,
    ProductAccess, ProductAccessCreate
)
from app.services.workspace_service import WorkspaceService
from app.services.role_service import RoleService
from app.services.product_access_service import ProductAccessService

# Product JSON indexing service (for products-as-context feature)
from app.services.product_json_indexer import upload_product_json_to_onyx

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_VIDEO_LESSON_PRESENTATION = "VideoLessonPresentationDisplay"  # New component for video lesson presentations
COMPONENT_NAME_VIDEO_PRODUCT = "VideoProductDisplay"  # New component for generated video products
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"
COMPONENT_NAME_LESSON_PLAN = "LessonPlanDisplay"  # New component for lesson plans

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

# NEW: Google Gemini API configuration
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# SerpAPI configuration (env-based; avoid hardcoding secrets)
SERPAPI_KEY = os.getenv("SERPAPI_KEY")

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# Stripe configuration (custom extensions only)
STRIPE_SECRET_KEY = os.getenv("STRIPE_SECRET_KEY")
STRIPE_BILLING_RETURN_URL = os.getenv("STRIPE_BILLING_RETURN_URL")

# Price ID mappings from env (optional but recommended)
PRICE_TO_TIER: dict[str, str] = {}
TIER_TO_CREDITS: dict[str, int] = {
    "pro_monthly": 600,
    "business_monthly": 2000,
    "pro_yearly": 7200,
    "business_yearly": 24000,
}

def _load_price_maps_once() -> tuple[dict[str, dict], dict[str, str]]:
    """Load price id → addon mapping and tier mapping from env once."""
    addon_map: dict[str, dict] = {}
    def _reg(price_id: Optional[str], addon_type: str, units: int):
        if price_id:
            addon_map[price_id] = {"type": addon_type, "units": units}

    # Connectors recurring (merge env with provided price IDs)
    _reg(os.getenv("STRIPE_PRICE_CONNECTORS_1"), "connectors", 1)
    _reg(os.getenv("STRIPE_PRICE_CONNECTORS_5"), "connectors", 5)
    _reg(os.getenv("STRIPE_PRICE_CONNECTORS_10"), "connectors", 10)
    # Storage recurring (GB)
    _reg(os.getenv("STRIPE_PRICE_STORAGE_1GB"), "storage", 1)
    _reg(os.getenv("STRIPE_PRICE_STORAGE_5GB"), "storage", 5)
    _reg(os.getenv("STRIPE_PRICE_STORAGE_10GB"), "storage", 10)
    # One-time credits packs (units = credits amount)
    _reg(os.getenv("STRIPE_PRICE_CREDITS_100"), "credits", 100)
    _reg(os.getenv("STRIPE_PRICE_CREDITS_300"), "credits", 300)
    # Provided mapping (explicit IDs)
    provided_map = {
        # credits
        "price_1SGHlMH2U2KQUmUhkXKhj4g3": ("credits", 100),
        "price_1SGHm0H2U2KQUmUhG5utzGFf": ("credits", 300),
        "price_1SGHmYH2U2KQUmUh89PNgGAx": ("credits", 1000),
        # storage (monthly)
        "price_1SGHjIH2U2KQUmUhpWRcRxxH": ("storage", 1),
        "price_1SGHk9H2U2KQUmUhLrwnk2tQ": ("storage", 5),
        "price_1SGHkgH2U2KQUmUh0hI2Mp07": ("storage", 10),
        # connectors (monthly)
        "price_1SGHegH2U2KQUmUh4guOuoV7": ("connectors", 1),
        "price_1SGHgFH2U2KQUmUhS0Blys9w": ("connectors", 5),
        "price_1SGHgZH2U2KQUmUhSuFJ6SOi": ("connectors", 10),
    }
    for pid, (ptype, units) in provided_map.items():
        addon_map[pid] = {"type": ptype, "units": units}

    # Hard-wired tier price IDs
    price_to_tier: dict[str, str] = {
        "price_1SEBM4H2U2KQUmUhkn6A7Hlm": "pro_monthly",      # Pro
        "price_1SEBTeH2U2KQUmUhi02e1uC9": "business_monthly", # Business
        "price_1SEBUCH2U2KQUmUhkym5Q9TS": "pro_yearly",       # Pro Yearly
        "price_1SEBUoH2U2KQUmUhMktbhCsm": "business_yearly",  # Business Yearly
    }
    # Also support env vars as fallback
    def _tier(price_id: Optional[str], tier: str):
        if price_id:
            price_to_tier[price_id] = tier
    _tier(os.getenv("STRIPE_PRICE_PRO_MONTHLY"), "pro_monthly")
    _tier(os.getenv("STRIPE_PRICE_BUSINESS_MONTHLY"), "business_monthly")
    _tier(os.getenv("STRIPE_PRICE_PRO_YEARLY"), "pro_yearly")
    _tier(os.getenv("STRIPE_PRICE_BUSINESS_YEARLY"), "business_yearly")
    return addon_map, price_to_tier

PRICE_TO_ADDON, PRICE_TO_TIER = _load_price_maps_once()

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

# Feature flag to prefer OpenAI Web Search over SerpAPI
USE_OPENAI_WEB_SEARCH = os.getenv("USE_OPENAI_WEB_SEARCH", "false").strip().lower() in {"1", "true", "yes", "on"}
logger.info(f"[RESEARCH] USE_OPENAI_WEB_SEARCH={USE_OPENAI_WEB_SEARCH}")

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

def parse_id_list(id_string: str, context_name: str) -> List[int]:
    """
    Parse a comma-separated string of IDs, handling negative integers (like -1 for special cases).
    
    Args:
        id_string: Comma-separated string of IDs (e.g., "1,2,3" or "-1" or "42")
        context_name: Context name for logging (e.g., "folder" or "file")
    
    Returns:
        List of parsed integer IDs
    """
    if not id_string:
        return []
    
    id_list = []
    try:
        for id_part in id_string.split(','):
            id_stripped = id_part.strip()
            if id_stripped.lstrip('-').isdigit():  # Allow negative numbers
                id_list.append(int(id_stripped))
            elif id_stripped:  # Log non-empty invalid parts
                logger.warning(f"[ID_PARSING] Skipping invalid {context_name} ID: '{id_stripped}'")
        
        logger.debug(f"[ID_PARSING] Parsed {context_name} IDs from '{id_string}': {id_list}")
        return id_list
    except Exception as e:
        logger.error(f"[ID_PARSING] Failed to parse {context_name} IDs from '{id_string}': {e}")
        return []

def should_use_hybrid_approach(payload) -> bool:
    """
    Determine if we should use the hybrid approach (Onyx for context extraction + OpenAI for generation).
    Returns True only when actual file context is present, not for text-only scenarios.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # For text-only scenarios, use direct approach (send text directly in wizard request)
    has_text_only = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText and
        not has_files  # Only text, no files
    )
    
    # Check if Knowledge Base search is requested
    has_knowledge_base = (
        hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
    )
    
    # Check if connector-based filtering is requested (including SmartDrive files)
    logger.info(f"🔍 [HYBRID_CHECK] Checking connector filtering:")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'fromConnectors'): {hasattr(payload, 'fromConnectors')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.fromConnectors: {getattr(payload, 'fromConnectors', None)}")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'connectorSources'): {hasattr(payload, 'connectorSources')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.connectorSources: {getattr(payload, 'connectorSources', None)}")
    logger.info(f"🔍 [HYBRID_CHECK] hasattr(payload, 'selectedFiles'): {hasattr(payload, 'selectedFiles')}")
    logger.info(f"🔍 [HYBRID_CHECK] payload.selectedFiles: {getattr(payload, 'selectedFiles', None)}")
    
    has_connector_filtering = (
        hasattr(payload, 'fromConnectors') and payload.fromConnectors and
        (
            (hasattr(payload, 'connectorSources') and payload.connectorSources) or
            (hasattr(payload, 'selectedFiles') and payload.selectedFiles)
        )
    )
    
    logger.info(f"🔍 [HYBRID_CHECK] Final has_connector_filtering: {has_connector_filtering}")

    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use hybrid approach when there's file context, text context, Knowledge Base search, or connector filtering
    use_hybrid = has_files or has_text_context or has_knowledge_base or has_connector_filtering
    
    logger.info(f"[HYBRID_SELECTION] has_files={has_files}, has_text_only={has_text_only}, has_knowledge_base={has_knowledge_base}, has_connector_filtering={has_connector_filtering}, use_hybrid={use_hybrid}")
    
    # EXTENSIVE DEBUG: Show why connector filtering failed
    if hasattr(payload, 'fromConnectors') and payload.fromConnectors:
        logger.info(f"🔍 [HYBRID_DEBUG] fromConnectors=True but connector_filtering={has_connector_filtering}")
        logger.info(f"🔍 [HYBRID_DEBUG] connectorSources check: hasattr={hasattr(payload, 'connectorSources')}, value={getattr(payload, 'connectorSources', None)}, truthy={bool(getattr(payload, 'connectorSources', None))}")
        logger.info(f"🔍 [HYBRID_DEBUG] selectedFiles check: hasattr={hasattr(payload, 'selectedFiles')}, value={getattr(payload, 'selectedFiles', None)}, truthy={bool(getattr(payload, 'selectedFiles', None))}")
    if has_text_only:
        logger.info(f"[HYBRID_SELECTION] ✅ Using DIRECT approach for text-only scenario (no file conversion)")
    return use_hybrid

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Prepare email placeholder
    user_email = None
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Determine timeout
        is_timeout = response.status_code in (408, 504)
        
        # For errors/timeouts, try to resolve user email
        if is_timeout or response.status_code >= 400:
            try:
                _, user_email_candidate = await get_user_identifiers_for_workspace(request)
                user_email = user_email_candidate
            except Exception:
                user_email = None
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, user_email, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, is_timeout, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                """, request_id, request.url.path, request.method, user_id,
                     user_email, response.status_code, response_time_ms, request_size,
                     response_size, None, is_timeout, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Determine timeout status and status_code
        try:
            import httpx, asyncio
        except Exception:
            pass
        
        is_timeout = False
        status_code = 500
        try:
            if ('httpx' in globals() and isinstance(e, httpx.TimeoutException)) or ('asyncio' in globals() and isinstance(e, asyncio.TimeoutError)):
                is_timeout = True
                status_code = 504
        except Exception:
            pass
        
        # Try to resolve user email for error cases
        try:
            _, user_email_candidate = await get_user_identifiers_for_workspace(request)
            user_email = user_email_candidate
        except Exception:
            user_email = None
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, user_email, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, is_timeout, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                """, request_id, request.url.path, request.method, user_id,
                     user_email, status_code, response_time_ms, request_size, None,
                     str(e), is_timeout, datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))

# Optionally include Nextcloud origins (when iframe points to direct Nextcloud domain rather than proxied /smartdrive)
try:
    from urllib.parse import urlparse
    def _to_origin(url_value: Optional[str]) -> Optional[str]:
        if not url_value:
            return None
        u = url_value.strip()
        if not u:
            return None
        try:
            p = urlparse(u)
            if p.scheme and p.netloc:
                return f"{p.scheme}://{p.netloc}"
            if u.startswith("http://") or u.startswith("https://"):
                return u.rstrip('/')
            return None
        except Exception:
            return None

    nc_candidates = [
        os.environ.get("NEXTCLOUD_BASE_URL"),
        os.environ.get("NEXTCLOUD_PUBLIC_SHARE_DOMAIN"),
        os.environ.get("NEXTCLOUD_ALLOWED_ORIGIN"),
    ]
    nc_origins = [o for o in map(_to_origin, nc_candidates) if o]
    if nc_origins:
        effective_origins = list(set(effective_origins + nc_origins))
except Exception:
    pass

if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    # Optional identity fields for richer admin display
    email: Optional[str] = None
    display_identity: Optional[str] = None
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

# Response model for admin credits list with enriched identity fields
class AdminUserCredits(UserCredits):
    email: Optional[str] = None
    display_identity: Optional[str] = None

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

class ImageBlock(BaseContentBlock):
    type: str = "image"
    src: str
    alt: Optional[str] = None
    caption: Optional[str] = None
    width: Optional[Union[int, str]] = None
    height: Optional[Union[int, str]] = None
    alignment: Optional[str] = "center"
    borderRadius: Optional[str] = "8px"
    maxWidth: Optional[str] = "100%"
    # Layout mode fields for positioning
    layoutMode: Optional[str] = None  # 'standalone', 'inline-left', 'inline-right'
    layoutPartnerIndex: Optional[int] = None  # Index of the content block to pair with for side-by-side layouts
    layoutProportion: Optional[str] = None  # '50-50', '60-40', '40-60', '70-30', '30-70'
    float: Optional[str] = None  # Legacy field for backward compatibility: 'left', 'right'

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# Lesson Plan Generation Models
class LessonPlanGenerationRequest(BaseModel):
    outlineProjectId: int
    lessonTitle: str
    moduleName: str
    lessonNumber: int
    recommendedProducts: List[str]

# Content Development Specifications Models for Lesson Plans
class ContentTextBlock(BaseModel):
    type: Literal["text"] = "text"
    block_title: str
    block_content: str  # Can contain plain text, bullet lists (with -), or numbered lists (with 1.)

class ContentProductBlock(BaseModel):
    type: Literal["product"] = "product"
    product_name: str  # e.g., "video-lesson", "presentation", "quiz", "one-pager"
    product_description: str

ContentBlock = Union[ContentTextBlock, ContentProductBlock]

class LessonPlanData(BaseModel):
    lessonTitle: str
    lessonObjectives: List[str]
    shortDescription: str
    contentDevelopmentSpecifications: List[ContentBlock]  # New flowing structure
    materials: List[str]
    suggestedPrompts: List[str]

class LessonPlanResponse(BaseModel):
    success: bool
    project_id: int
    lesson_plan_data: LessonPlanData
    message: str

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Advanced Data Science Mastery: From Theory to Production",
  "slides": [
    {
      "slideId": "slide_1_intro",
      "slideNumber": 1,
      "slideTitle": "Section: Advanced Data Science Mastery",
      "templateId": "title-slide",
      "previewKeyPoints": [
        "Comprehensive data science training program covering theory and practical applications",
        "Advanced techniques for real-world data challenges and production deployment",
        "Industry-standard tools and methodologies for professional data scientists",
        "Career development pathways and specialization opportunities in data science"
      ],
      "props": {
        "title": "Advanced Data Science Mastery",
        "subtitle": "From theoretical foundations to production-ready solutions and career advancement",
        "author": "Data Science Excellence Institute"
      }
    },
    {
      "slideId": "slide_2_statistical_foundations",
      "slideNumber": 2,
      "slideTitle": "Advanced Data Science Mastery — Statistical Foundations and Mathematical Prerequisites",
      "templateId": "two-column",
      "previewKeyPoints": [
        "Essential statistical concepts including probability distributions and hypothesis testing",
        "Linear algebra fundamentals for machine learning algorithms and dimensionality reduction",
        "Calculus applications in optimization and gradient-based learning methods",
        "Practical implementation using Python libraries like NumPy, SciPy, and statsmodels"
      ],
      "props": {
        "title": "Core Mathematical and Statistical Foundations",
        "leftTitle": "Statistical Concepts",
        "leftContent": "Probability distributions model uncertainty in data while hypothesis testing validates experimental assumptions.",
        "rightTitle": "Mathematical Prerequisites",
        "rightContent": "Linear algebra powers dimensionality reduction and neural networks while calculus enables gradient-based optimization."
      }
    },
    {
      "slideId": "slide_3_data_pipeline_architecture",
      "slideNumber": 3,
      "slideTitle": "Advanced Data Science Mastery — Enterprise Data Pipeline Architecture and ETL Processes",
      "templateId": "process-steps",
      "previewKeyPoints": [
        "End-to-end data pipeline design from ingestion to serving predictions at scale",
        "ETL and ELT processes with modern tools like Apache Airflow and dbt for workflow orchestration",
        "Data quality monitoring and validation frameworks to ensure reliable model inputs",
        "Scalable architecture patterns for handling big data processing and real-time streaming"
      ],
      "props": {
        "title": "Building Production-Ready Data Pipelines",
        "steps": [
          "Ingest data from databases, APIs, streams, and files with retries, validation, and error handling to ensure completeness and reliability at scale",
          "Transform and clean data using outlier handling, imputation, feature engineering, and normalization with Spark, Pandas, or dbt for reusable pipelines",
          "Monitor data quality via profiling, schema checks, statistical tests, and drift detection to catch issues early and maintain trustworthy inputs",
          "Serve models with Docker/Kubernetes and ML platforms (e.g., MLflow, Seldon) for reliable real-time and batch prediction endpoints",
          "Track latency, accuracy, throughput, and business KPIs with dashboards and alerts to keep pipelines healthy and actionable"
        ]
      }
    },
    {
      "slideId": "slide_4_machine_learning_algorithms",
      "slideNumber": 4,
      "slideTitle": "Advanced Data Science Mastery — Advanced Machine Learning Algorithms and Model Selection",
      "templateId": "bullet-points-right",
      "previewKeyPoints": [
        "Comprehensive overview of supervised learning algorithms from linear models to ensemble methods",
        "Unsupervised learning techniques for clustering, dimensionality reduction, and anomaly detection",
        "Deep learning architectures including CNNs, RNNs, and Transformers for various data types",
        "Model selection strategies, hyperparameter tuning, and cross-validation best practices"
      ],
      "props": {
        "title": "Comprehensive Machine Learning Algorithm Toolkit",
        "bullets": [
          "Supervised learning from linear/logistic regression to ensembles (Random Forest, XGBoost, LightGBM, SVM) with guidance on when to use which and key tradeoffs",
          "Unsupervised learning: clustering (K-means, DBSCAN, hierarchical), dimensionality reduction (PCA, t-SNE, UMAP), and anomaly detection for EDA and features",
          "Deep learning: CNNs for vision, RNNs/Transformers for NLP, plus transfer learning and attention to tackle complex patterns and limited labeled data",
          "Model selection and evaluation: proper cross-validation, hyperparameter tuning (grid/random/Bayesian), and metrics for classification, regression, ranking",
          "Ensembles and stacking to boost accuracy and robustness via voting, bagging, boosting, and layered learners that combine model strengths"
        ],
        "imagePrompt": "Realistic cinematic scene of data scientists collaborating in a modern machine learning lab with multiple monitors displaying algorithm visualizations, code, and model performance metrics. The scene features diverse professionals analyzing complex data patterns on large screens while discussing model architectures. Monitors and visualizations are [COLOR1], data scientists and workstations are [COLOR2], and lab environment is [COLOR3]. Cinematic photography with natural lighting, 50mm lens, three-quarter view, shallow depth of field.",
        "imageAlt": "Data scientists working on machine learning algorithms"
      }
    },
    {
      "slideId": "slide_5_feature_engineering",
      "slideNumber": 5,
      "slideTitle": "Advanced Data Science Mastery — Advanced Feature Engineering and Selection Techniques",
      "templateId": "four-box-grid",
      "previewKeyPoints": [
        "Systematic approaches to creating meaningful features from raw data across different domains",
        "Automated feature engineering tools and techniques for scaling feature creation processes",
        "Feature selection methods to identify most relevant variables and reduce dimensionality",
        "Domain-specific feature engineering for text, images, time series, and categorical data"
      ],
      "props": {
        "title": "Essential Feature Engineering Techniques",
        "boxes": [
          {
            "heading": "Domain Knowledge Integration",
            "text": "Leverage subject matter expertise to create meaningful features that capture underlying patterns and relationships specific to your problem domain."
          },
          {
            "heading": "Automated Feature Generation",
            "text": "Use tools like Featuretools and tsfresh to systematically generate hundreds of candidate features from temporal and relational data."
          },
          {
            "heading": "Feature Selection Methods",
            "text": "Apply statistical tests, recursive feature elimination, and model-based importance to identify the most predictive variables and reduce dimensionality."
          },
          {
            "heading": "Domain-Specific Transforms",
            "text": "Apply specialized encodings for text (TF-IDF, embeddings), images (CNN features), time series (lag features), and categorical data (target encoding)."
          }
        ]
      }
    },
    {
      "slideId": "slide_6_model_performance_metrics",
      "slideNumber": 6,
      "slideTitle": "Advanced Data Science Mastery — Comprehensive Model Evaluation and Performance Metrics",
      "templateId": "big-numbers",
      "previewKeyPoints": [
        "Essential classification metrics including precision, recall, F1-score, and AUC-ROC interpretation",
        "Regression evaluation methods with RMSE, MAE, and R-squared for different use cases",
        "Advanced metrics for imbalanced datasets and multi-class classification problems"
      ],
      "props": {
        "title": "Critical Performance Metrics for Model Evaluation",
        "subtitle": "Essential classification metrics including precision, recall, F1-score, and AUC-ROC interpretation, with regression evaluation methods and advanced metrics for imbalanced datasets and multi-class classification problems.",
        "steps": [
          {
            "value": "95%+",
            "label": "Model Accuracy Threshold",
            "description": "Balance precision and recall while accounting for class imbalance and business costs. Ensure consistent performance across segments and set alerts for degradation."
          },
          {
            "value": "0.85+",
            "label": "AUC-ROC Score Target",
            "description": "Indicates strong discriminative power for binary tasks like fraud or churn. Prefer scores >0.9 for high-stakes use cases."
          },
          {
            "value": "<5%",
            "label": "Acceptable Error Rate",
            "description": "Error budgets depend on domain. Critical systems need <1%, while recommendations can tolerate higher."
          }
        ]
      }
    },
    {
      "slideId": "slide_7_deployment_strategies",
      "slideNumber": 7,
      "slideTitle": "Model Deployment Strategies and MLOps Best Practices",
      "templateId": "four-box-grid",
      "previewKeyPoints": [
        "Containerization and orchestration strategies for scalable model deployment",
        "A/B testing frameworks for gradual model rollouts and performance monitoring",
        "Continuous integration and deployment pipelines for machine learning workflows",
        "Model versioning, monitoring, and automated retraining processes"
      ],
      "props": {
        "title": "Production Deployment and MLOps Excellence",
        "boxes": [
          {
            "heading": "Containerized Deployment",
            "text": "Docker + Kubernetes for scalable, reproducible serving with autoscaling, health checks, and zero-downtime rollouts."
          },
          {
            "heading": "A/B Testing Framework",
            "text": "Controlled rollouts with significance testing and KPI tracking to validate model impact before full deployment."
          },
          {
            "heading": "CI/CD Pipelines",
            "text": "Automated testing, validation, and deployments (e.g., GitHub Actions, Jenkins, MLflow) with safe rollback."
          },
          {
            "heading": "Monitoring & Alerting",
            "text": "Track performance, drift, latency, and health with dashboards and alerts for quick remediation."
          }
        ]
      }
    },
    {
      "slideId": "slide_8_industry_challenges",
      "slideNumber": 8,
      "slideTitle": "Common Industry Challenges and Proven Solutions",
      "templateId": "challenges-solutions",
      "previewKeyPoints": [
        "Data quality issues and systematic approaches to data validation and cleaning",
        "Scalability challenges when moving from prototype to production systems",
        "Model interpretability requirements for regulated industries and stakeholder buy-in",
        "Talent acquisition and team building strategies for successful data science organizations"
      ],
      "props": {
        "title": "Overcoming Real-World Data Science Obstacles",
        "challengesTitle": "Industry Challenges",
        "solutionsTitle": "Proven Solutions",
        "challenges": [
          "Data quality degrades model reliability",
          "Scaling prototypes to production systems",
          "Stakeholders require interpretable explainable models"
        ],
        "solutions": [
          "Implement validation and quality checks",
          "Design with cloud-native efficient patterns",
          "Use XAI tools with documentation"
        ]
      }
    },
    {
      "slideId": "slide_9_career_advancement",
      "slideNumber": 9,
      "slideTitle": "Data Science Career Paths and Specialization Areas",
      "templateId": "timeline",
      "previewKeyPoints": [
        "Career progression from junior data scientist to senior leadership roles",
        "Specialization opportunities in machine learning engineering, research, and business analytics",
        "Skills development roadmap for advancing in different data science career tracks",
        "Industry trends and emerging roles in artificial intelligence and data science"
      ],
      "props": {
        "title": "Professional Development Timeline and Career Specializations",
        "events": [
          {
            "date": "Year 1-2",
            "title": "Foundation Building",
            "description": "Build stats, Python/R, and ML basics; complete projects and assemble a strong portfolio."
          },
          {
            "date": "Year 2-4",
            "title": "Specialization Focus",
            "description": "Choose ML eng., data eng., or analytics; deepen algorithms, cloud, and domain expertise."
          },
          {
            "date": "Year 4-6",
            "title": "Senior Individual Contributor",
            "description": "Lead complex projects, mentor others, and drive MLOps and cross-functional outcomes."
          },
          {
            "date": "Year 6+",
            "title": "Leadership and Strategy",
            "description": "Move into management, research, or senior technical leadership; scale teams and impact."
          }
        ]
      }
    },
    {
      "slideId": "slide_10_emerging_technologies",
      "slideNumber": 10,
      "slideTitle": "Section: Emerging Technologies and Future Trends",
      "templateId": "process-steps",
      "previewKeyPoints": [
        "Latest developments in artificial intelligence including large language models and generative AI",
        "Quantum computing applications in machine learning and optimization problems",
        "Edge computing and federated learning for distributed AI systems",
        "Ethical AI considerations and responsible machine learning practices"
      ],
      "props": {
        "title": "Evolution of AI Technologies",
        "steps": [
          "Large language models like GPT-4 and Claude enable natural language understanding, generation, and reasoning at unprecedented scale with applications across content creation, coding assistance, and knowledge synthesis.",
          "Quantum computing promises exponential speedups for optimization problems, with quantum-inspired algorithms already improving classical machine learning performance in portfolio optimization and drug discovery.",
          "Edge computing and federated learning enable privacy-preserving AI by training models across distributed devices without centralizing sensitive data, crucial for healthcare and IoT applications.",
          "Responsible AI frameworks incorporate fairness metrics, explainability tools, and bias detection to ensure ethical deployment while maintaining transparency and accountability in automated decision-making systems."
        ]
      }
    },
    {
      "slideId": "slide_11_metrics_analytics",
      "slideNumber": 11,
      "slideTitle": "Emerging Technologies and Future Trends — Operational Analytics Dashboard Highlights",
      "templateId": "metrics-analytics",
      "previewKeyPoints": [
        "Key performance indicators tracked in day-to-day operations",
        "Link between analytics and business actions taken",
        "Alert thresholds and on-call procedures for anomalies",
        "Ownership and review cadence for metrics dashboards"
      ],
      "props": {
        "title": "Daily Metrics and Operational Insights",
        "metrics": [
          { "number": "12.3k", "text": "Daily active users across core products with 7-day rolling trend monitoring and threshold alerts for significant deviations from expected usage patterns." },
          { "number": "98.6%", "text": "Uptime for model-serving endpoints measured via synthetic probes, SLO mapping, and automatic incident creation when SLAs are breached." },
          { "number": "320ms", "text": "Median prediction latency for real-time inference with p95 and p99 tracked and auto-scaling triggers configured based on sustained load." },
          { "number": "0.7%", "text": "Error rate on requests including timeouts and failed responses; categorized by cause and mitigated via retry logic and circuit breakers." },
          { "number": "0.3", "text": "Data drift score computed nightly using PSI/KS metrics; alerts fire when exceeding thresholds prompting retraining investigations." },
          { "number": "42", "text": "Open data quality issues prioritized by severity, assigned owners, and target resolution dates to ensure pipeline reliability." }
        ]
      }
    },
    {
      "slideId": "slide_12_market_share",
      "slideNumber": 12,
      "slideTitle": "Emerging Technologies and Future Trends — Market Share by Segment and Year",
      "templateId": "market-share",
      "previewKeyPoints": [
        "Year-over-year changes in market penetration by segment",
        "Competitive positioning relative to primary rivals",
        "Regions and products driving overall growth"
      ],
      "props": {
        "title": "Market Share Overview",
        "subtitle": "Comparative view across segments and years",
        "chartData": [
          { "label": "Segment A", "description": "Enterprise customers in regulated industries", "percentage": 37, "color": "#3b82f6", "year": 2024 },
          { "label": "Segment B", "description": "Mid-market technology companies", "percentage": 28, "color": "#8b5cf6", "year": 2024 },
          { "label": "Segment C", "description": "SMB retail and services", "percentage": 22, "color": "#10b981", "year": 2024 },
          { "label": "Other", "description": "Long-tail customers", "percentage": 13, "color": "#f59e0b", "year": 2024 }
        ],
        "bottomText": "Expanding presence in enterprise while maintaining growth in mid-market."
      }
    },
    {
      "slideId": "slide_13_comparison",
      "slideNumber": 13,
      "slideTitle": "Solution Comparison Matrix",
      "templateId": "comparison-slide",
      "previewKeyPoints": [
        "Side-by-side evaluation of key features",
        "Pricing and support considerations",
        "Recommended options by use case"
      ],
      "props": {
        "title": "Feature Comparison",
        "subtitle": "Selecting the right approach by capability",
        "tableData": {
          "headers": ["Capability", "Option A", "Option B"],
          "rows": [
            ["Deployment Model", "Managed cloud service", "Self-hosted Kubernetes"],
            ["Latency (p95)", "< 400 ms", "< 250 ms"],
            ["Maintenance", "Low (SaaS managed)", "Medium (DevOps required)"],
            ["Cost Profile", "Usage-based pricing", "Fixed infra + ops"],
            ["Best For", "Fast time-to-value", "Full control & customization"]
          ]
        }
      }
    },
    {
      "slideId": "slide_14_table_dark",
      "slideNumber": 14,
      "slideTitle": "Feature Availability Matrix",
      "templateId": "table-dark",
      "previewKeyPoints": [
        "Feature comparison across different product tiers and versions",
        "Checkbox-based availability indicators for easy visualization",
        "Clear comparison matrix for stakeholder decision-making"
      ],
      "props": {
        "title": "Feature Availability by Product Version",
        "tableData": {
          "headers": ["Metric", "Free Tier", "Pro Tier", "Enterprise"],
          "rows": [
            ["Real-time Analytics", "", "✓", "✓"],
            ["API Access", "", "✓", "✓"],
            ["Custom Integrations", "", "", "✓"],
            ["24/7 Support", "", "", "✓"],
            ["Advanced Security", "", "✓", "✓"],
            ["White Label", "", "", "✓"]
          ]
        }
      }
    },
    {
      "slideId": "slide_15_table_light",
      "slideNumber": 15,
      "slideTitle": "Project Milestones (Light Theme)",
      "templateId": "table-light",
      "previewKeyPoints": [
        "Upcoming deliverables and responsible teams",
        "Dependencies and risk notes",
        "Tentative timelines"
      ],
      "props": {
        "title": "Milestone Plan",
        "tableData": {
          "headers": ["Milestone", "Owner", "Due"],
          "rows": [
            ["MVP Release", "Platform", "2024-11-15"],
            ["Security Review", "SecOps", "2024-12-01"],
            ["GA Launch", "Go-To-Market", "2025-01-10"]
          ]
        },
        "colors": {
          "headerBg": "#f3f4f6",
          "rowAltBg": "#ffffff"
        }
      }
    },
    {
      "slideId": "slide_16_event_list",
      "slideNumber": 16,
      "slideTitle": "Upcoming Events and Key Dates",
      "templateId": "timeline",
      "previewKeyPoints": [
        "Major internal and external events in the next quarter",
        "Deadlines that impact delivery timelines",
        "Engagement opportunities with stakeholders"
      ],
      "props": {
        "title": "Upcoming Events and Key Dates",
        "events": [
          { "date": "2024-11-05", "title": "Architecture Review", "description": "Validate scalability and security design decisions." },
          { "date": "2024-11-20", "title": "Customer Advisory Board", "description": "Gather feedback on beta features and onboarding." },
          { "date": "2024-12-03", "title": "Enablement Workshop", "description": "Train support and success teams on new tooling." },
          { "date": "2024-12-17", "title": "Public Webinar", "description": "Share best practices from early adopters." }
        ]
      }
    },
    {
      "slideId": "slide_17_pyramid",
      "slideNumber": 17,
      "slideTitle": "Capability Maturity Pyramid",
      "templateId": "pyramid",
      "previewKeyPoints": [
        "Progression from foundational to advanced capabilities",
        "Focus areas by maturity level",
        "Recommended next steps for improvement"
      ],
      "props": {
        "title": "Maturity Stages",
        "steps": [
          { "heading": "Strategic Optimization", "number": "01" },
          { "heading": "Production Excellence", "number": "02" },
          { "heading": "Operationalization", "number": "03" },
          { "heading": "Prototyping", "number": "04" },
          { "heading": "Foundations", "number": "05" }
        ]
      }
    },
    {
      "slideId": "slide_18_pie_chart",
      "slideNumber": 18,
      "slideTitle": "Resource Allocation Breakdown",
      "templateId": "pie-chart-infographics",
      "previewKeyPoints": [
        "Distribution of time and budget across activities",
        "Monthly movement and seasonal trends",
        "Areas for optimization and rebalancing"
      ],
      "props": {
        "title": "Team Allocation Overview",
        "chartData": {
          "segments": [
            { "label": "Data Engineering", "value": 35, "color": "#3b82f6" },
            { "label": "Modeling", "value": 30, "color": "#8b5cf6" },
            { "label": "MLOps", "value": 20, "color": "#10b981" },
            { "label": "Enablement", "value": 15, "color": "#f59e0b" }
          ]
        },
        "monthlyData": [62, 70, 65, 68, 72, 75, 73, 78, 80, 77, 74, 79],
        "chartSize": "large"
      }
    },
    {
      "slideId": "slide_19_comparison_table_dark",
      "slideNumber": 19,
      "slideTitle": "Feature Parity (Dark)",
      "templateId": "comparison-slide",
      "previewKeyPoints": [
        "Detailed parity view across vendors",
        "Critical features for shortlisting",
        "Notes for follow-up demos"
      ],
      "props": {
        "title": "Vendor Feature Parity",
        "tableData": {
          "headers": ["Feature", "Vendor X", "Vendor Y"],
          "rows": [
            ["RBAC", "Yes", "Partial"],
            ["Audit Logs", "Yes", "Yes"],
            ["SLA", "99.9%", "99.5%"],
            ["Hybrid Deploy", "No", "Yes"]
          ]
        }
      }
    },
    {
      "slideId": "slide_20_case_studies",
      "slideNumber": 20,
      "slideTitle": "Section: Real-World Data Science Applications",
      "templateId": "challenges-solutions",
      "previewKeyPoints": [
        "Common challenges faced in real-world data science projects",
        "Proven solutions and best practices from industry leaders",
        "Lessons learned from successful implementations"
      ],
      "props": {
        "title": "Overcoming Real-World Data Science Challenges",
        "challengesTitle": "Key Challenges",
        "solutionsTitle": "Proven Solutions",
        "challenges": [
          "Incomplete or messy datasets",
          "Model deployment at scale",
          "Stakeholder alignment and communication",
          "Balancing speed and accuracy"
        ],
        "solutions": [
          "Automated data quality pipelines",
          "MLOps and containerization strategies",
          "Executive dashboards and visualization",
          "Iterative development and A/B testing"
        ]
      }
    }
  ],
  "currentSlideId": "slide_1_intro",
  "detectedLanguage": "en"
} 
"""

DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example Video Lesson with Voiceover",
  "slides": [
    {
      "slideId": "slide_1_course_overview",
      "slideNumber": 1,
      "slideTitle": "Course Overview",
      "templateId": "course-overview-slide",
      "voiceoverText": "Welcome to this comprehensive course. Today we'll explore the fundamentals of our topic, breaking down complex concepts into easy-to-understand segments. This overview sets the stage for what you're about to learn.",
      "props": {
        "title": "Welcome to the Course",
        "subtitle": "This course introduces the main topic and learning objectives",
        "imagePath": "https://via.placeholder.com/600x400?text=Course+Image",
        "imageAlt": "Course overview illustration",
        "logoPath": "",
        "pageNumber": 1
      }
    },
    {
      "slideId": "slide_2_impact_statements",
      "slideNumber": 2,
      "slideTitle": "Impact Statistics",
      "templateId": "impact-statements-slide",
      "voiceoverText": "Let's look at some impressive statistics that demonstrate the real-world impact of what we're learning. These numbers show the tangible benefits and results you can expect to achieve.",
      "props": {
        "title": "Impact Statistics",
        "statements": [
          { "number": "95%", "description": "Success rate in implementation" },
          { "number": "3x", "description": "Increase in productivity" },
          { "number": "50%", "description": "Reduction in time spent" }
        ],
        "profileImagePath": "https://via.placeholder.com/200x200?text=Avatar",
        "pageNumber": 2,
        "logoNew": "https://via.placeholder.com/100x50?text=Logo"
      }
    },
    {
      "slideId": "slide_3_phishing_definition",
      "slideNumber": 3,
      "slideTitle": "Key Definitions",
      "templateId": "phishing-definition-slide",
      "voiceoverText": "Now let's define the key concepts we'll be working with. Understanding these definitions is crucial for building a solid foundation of knowledge.",
      "props": {
        "title": "Key Definitions",
        "definitions": [
          "First important definition with detailed explanation",
          "Second key concept with comprehensive description",
          "Third critical term with thorough explanation"
        ],
        "profileImagePath": "https://via.placeholder.com/200x200?text=Avatar",
        "rightImagePath": "https://via.placeholder.com/300x200?text=Definition+Image",
        "pageNumber": 3,
        "logoPath": ""
      }
    },
    {
      "slideId": "slide_4_soft_skills_assessment",
      "slideNumber": 4,
      "slideTitle": "Assessment Tips",
      "templateId": "soft-skills-assessment-slide",
      "voiceoverText": "Here are some essential tips for your assessment. These practical guidelines will help you prepare effectively and demonstrate your understanding of the material.",
      "props": {
        "title": "Assessment Tips",
        "tips": [
          { "text": "First important tip for success", "isHighlighted": true },
          { "text": "Second key recommendation for better results", "isHighlighted": false }
        ],
        "profileImagePath": "https://via.placeholder.com/200x200?text=Avatar",
        "pageNumber": 4,
        "logoPath": "",
        "logoText": "Assessment Guide"
      }
    },
    {
      "slideId": "slide_5_work_life_balance",
      "slideNumber": 5,
      "slideTitle": "Conclusion",
      "templateId": "work-life-balance-slide",
      "voiceoverText": "As we conclude this lesson, remember that applying what you've learned requires balance and practical implementation. These final thoughts will help you integrate the knowledge into your daily practice.",
      "props": {
        "title": "Conclusion and Next Steps",
        "content": "This comprehensive lesson has covered the essential concepts and practical applications. Moving forward, focus on implementing these strategies in your daily work while maintaining a healthy balance between learning and application.",
        "imagePath": "https://via.placeholder.com/400x300?text=Conclusion+Image",
        "pageNumber": 5,
        "logoPath": ""
      }
    }
  ],
  "currentSlideId": "slide_1_course_overview",
  "detectedLanguage": "en",
  "hasVoiceover": true
}
"""

async def normalize_slide_props(slides: List[Dict], component_name: str = None) -> List[Dict]:
    """
    Normalize slide props to match frontend template schemas.
    
    This function fixes common prop mismatches between AI-generated JSON
    and the expected frontend template schemas. Invalid slides are automatically
    removed to prevent rendering errors.
    
    Args:
        slides: List of slide dictionaries to normalize
        component_name: Component type (e.g., COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
                       Used to determine if voiceoverText should be preserved
    """
    if not slides:
        return slides
        
    normalized_slides = []
    
    for slide_index, slide in enumerate(slides):
        if not isinstance(slide, dict) or 'templateId' not in slide or 'props' not in slide:
            normalized_slides.append(slide)
            continue
            
        template_id = slide.get('templateId')
        props = slide.get('props', {})
        
        # Create a copy to avoid modifying the original
        normalized_slide = slide.copy()
        normalized_props = props.copy()
        
        try:
            # ENHANCED LOGGING: Log raw AI-parsed content for big-numbers and event-list templates
            if template_id in ['big-numbers', 'event-dates', 'event-list']:
                logger.info(f"=== RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                logger.info(f"Template ID: {template_id}")
                logger.info(f"Slide Title: {normalized_props.get('title', 'NO TITLE')}")
                logger.info(f"Raw Props Keys: {list(normalized_props.keys())}")
                logger.info(f"Raw Props Content: {normalized_props}")
                
                # Log specific content for big-numbers
                if template_id == 'big-numbers':
                    items = normalized_props.get('items', [])
                    numbers = normalized_props.get('numbers', [])
                    steps = normalized_props.get('steps', [])
                    logger.info(f"Big-Numbers Raw Items: {items}")
                    logger.info(f"Big-Numbers Raw Numbers: {numbers}")
                    logger.info(f"Big-Numbers Raw Steps: {steps}")
                
                # Log specific content for event-list/event-dates
                if template_id in ['event-dates', 'event-list']:
                    events = normalized_props.get('events', [])
                    items = normalized_props.get('items', [])
                    logger.info(f"Event-List Raw Events: {events}")
                    logger.info(f"Event-List Raw Items: {items}")
                
                logger.info(f"=== END RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
            
            # Fix template ID mappings first
            if template_id == 'event-dates':
                # Map event-dates (AI instruction) to event-list (frontend registry)
                template_id = 'event-list'
                normalized_slide['templateId'] = template_id
                
            # Ensure critical props are preserved for all templates
            # Fix missing imagePrompt and other content issues
            if 'imagePrompt' not in normalized_props and 'imageAlt' in normalized_props:
                # If we have imageAlt but no imagePrompt, use imageAlt as the prompt
                normalized_props['imagePrompt'] = normalized_props['imageAlt']
            
            # Generate missing imagePrompts for templates that require them
            if template_id in ['bullet-points', 'bullet-points-right'] and not normalized_props.get('imagePrompt'):
                title = normalized_props.get('title', 'concept')
                bullets = normalized_props.get('bullets', [])
                
                # Create a contextual prompt based on title and content keywords
                title_lower = title.lower()
                content_sample = ' '.join(bullets[:2]) if bullets else ''
                content_lower = content_sample.lower()
                
                if 'tool' in title_lower or 'software' in content_lower or 'application' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a developer using programming tools at a clean workstation. The scene features a young Asian woman sitting at a modern desk with a single laptop displaying simple code interface elements (no readable text) and one external monitor showing basic geometric development tool mockups. A wireless keyboard and mouse are positioned on the desk alongside a coffee cup. The laptop screen and coding interface are [COLOR1], the external monitor and keyboard are [COLOR2], and the desk and accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'trend' in title_lower or 'future' in title_lower or 'innovation' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of futuristic technology concepts in a clean tech environment. The scene features a Hispanic male scientist in a white lab coat standing next to a single large holographic display showing simple geometric patterns and flowing data visualizations (no readable text). A modern desk with a tablet displaying basic technology interface elements sits nearby. The holographic display and data flows are [COLOR1], the scientist's lab coat and tablet are [COLOR2], and the desk and lab environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'learn' in title_lower or 'education' in title_lower or 'skill' in content_lower or 'training' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of modern learning in a clean educational environment. The scene features a young Black female student sitting at a modern desk using a tablet displaying simple educational interface elements and geometric learning modules (no readable text). A single interactive whiteboard in the background shows basic diagrams with simple shapes and connecting lines. Educational materials like a notebook and digital stylus are positioned on the desk. The tablet interface and learning modules are [COLOR1], the interactive whiteboard and educational tools are [COLOR2], and the desk and educational environment are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                elif 'business' in title_lower or 'strategy' in content_lower or 'management' in content_lower:
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional business strategy and management in a modern corporate environment. The scene features a contemporary conference room with three business professionals engaged in strategic planning. A confident Latina businesswoman in a navy blazer stands at the left presenting to a large wall display showing simple business charts, process flows, and strategic diagrams with geometric shapes (no readable text). In the center, a Black male executive sits at a glass conference table reviewing documents and tablets displaying abstract business analytics as simple bar charts and pie segments. On the right, a Caucasian female manager takes notes while sitting in an ergonomic chair, with a laptop showing business interface mockups with geometric layouts. Business materials like documents, tablets, coffee cups, and strategic planning boards are arranged throughout the professional space. The presentation displays and business interfaces are [COLOR1], conference furniture and professional devices are [COLOR2], documents and planning materials are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                else:
                    # General professional/educational fallback
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of professional collaboration and knowledge sharing in a modern educational environment. The scene features three diverse professionals working together in a bright, contemporary workspace. A young Hispanic woman in business attire sits at a modern desk on the left, using a laptop displaying simple interface elements and geometric data visualizations (no readable text). In the center, a Black male professional stands presenting to a wall-mounted display showing abstract concepts as interconnected nodes, flowcharts, and simple diagrams with geometric shapes. On the right, a Caucasian female colleague sits in a comfortable chair reviewing materials on a tablet, with documents and notebooks arranged on a side table. Professional tools like laptops, tablets, notebooks, coffee cups, and presentation materials are positioned throughout the collaborative workspace. The digital displays and interface elements are [COLOR1], professional devices and presentation tools are [COLOR2], furniture and workspace accessories are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                normalized_props['imageAlt'] = f"Professional illustration for {title}"
            
            # Ensure subtitle/content exists for templates that need it
            if template_id in ['big-image-left', 'big-image-top']:
                if 'subtitle' not in normalized_props and 'content' in normalized_props:
                    normalized_props['subtitle'] = normalized_props['content']
                # Ensure subtitle is different from title
                if (normalized_props.get('subtitle') == normalized_props.get('title') and 
                    len(normalized_props.get('subtitle', '')) > 50):
                    # If subtitle equals title and is long, use it as subtitle and create shorter title
                    full_text = normalized_props['subtitle']
                    # Extract first sentence as title
                    sentences = full_text.split('. ')
                    if len(sentences) > 1:
                        normalized_props['title'] = sentences[0]
                        normalized_props['subtitle'] = '. '.join(sentences[1:])
                        
            # Removed fallback logic that converted metrics-analytics to bullet-points
                    
            # This big-numbers conversion logic is moved to after big-numbers normalization below
                
            # ENHANCED LOGGING: Log raw AI-parsed content for big-numbers and event-list templates
            if template_id in ['big-numbers', 'event-dates', 'event-list']:
                logger.info(f"=== RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                logger.info(f"Template ID: {template_id}")
                logger.info(f"Slide Title: {normalized_props.get('title', 'NO TITLE')}")
                logger.info(f"Raw Props Keys: {list(normalized_props.keys())}")
                logger.info(f"Raw Props Content: {normalized_props}")
                
                # Log specific content for big-numbers
                if template_id == 'big-numbers':
                    items = normalized_props.get('items', [])
                    numbers = normalized_props.get('numbers', [])
                    steps = normalized_props.get('steps', [])
                    logger.info(f"Big-Numbers Raw Items: {items}")
                    logger.info(f"Big-Numbers Raw Numbers: {numbers}")
                    logger.info(f"Big-Numbers Raw Steps: {steps}")
                
                # Log specific content for event-list/event-dates
                if template_id in ['event-dates', 'event-list']:
                    events = normalized_props.get('events', [])
                    items = normalized_props.get('items', [])
                    logger.info(f"Event-List Raw Events: {events}")
                    logger.info(f"Event-List Raw Items: {items}")
                
                logger.info(f"=== END RAW AI-PARSED CONTENT for slide {slide_index + 1} ===")
                
            # Fix big-numbers template props
            if template_id == 'big-numbers':
                # FIXED: Accept 'items', 'numbers', or 'steps' as the source array
                source_list = normalized_props.get('items')
                source_type = 'items'
                
                if not (isinstance(source_list, list) and source_list):
                    alt_list = normalized_props.get('numbers')
                    if isinstance(alt_list, list) and alt_list:
                        logger.info(f"Normalizing 'big-numbers' slide {slide_index + 1} from 'numbers' → 'items'")
                        source_list = alt_list
                        source_type = 'numbers'
                    else:
                        # FIXED: Also check for 'steps' which AI commonly generates
                        steps_list = normalized_props.get('steps')
                        if isinstance(steps_list, list) and steps_list:
                            logger.info(f"Normalizing 'big-numbers' slide {slide_index + 1} from 'steps' → 'items'")
                            source_list = steps_list
                            source_type = 'steps'
                        else:
                            source_list = []

                # Validate and coerce each item
                fixed_items = []
                for item in source_list:
                    if isinstance(item, dict):
                        fixed_item = {
                            'value': str(item.get('value') or item.get('number') or '').strip(),
                            'label': str(item.get('label') or item.get('title') or '').strip(),
                            'description': str(item.get('description') or item.get('desc') or item.get('text') or '').strip()
                        }
                        if fixed_item['value'] and fixed_item['label']:
                            fixed_items.append(fixed_item)

                # Pad/trim to exactly 3 items to preserve slide instead of skipping
                if len(fixed_items) != 3:
                    logger.warning(f"Coercing slide {slide_index + 1} with template 'big-numbers': Expected 3 items, got {len(fixed_items)}")
                    while len(fixed_items) < 3:
                        idx = len(fixed_items) + 1
                        fixed_items.append({'value': '0', 'label': f'Item {idx}', 'description': 'No description available'})
                    if len(fixed_items) > 3:
                        fixed_items = fixed_items[:3]

                # Frontend expects 'steps' not 'items' for big-numbers template
                normalized_props['steps'] = fixed_items
                # Drop legacy keys to unify shape
                if 'numbers' in normalized_props:
                    normalized_props.pop('numbers', None)
                if 'items' in normalized_props:
                    normalized_props.pop('items', None)
                
                # FIXED: Don't convert big-numbers to bullet-points to prevent mixed language content
                # Check if we generated placeholder content but preserve big-numbers template
                has_placeholder_content = all(
                    step.get('value', '').strip() in ['0', ''] and 
                    step.get('label', '').startswith('Item ') and
                    step.get('description', '') == 'No description available'
                    for step in fixed_items
                )
                
                # FIXED: Completely disable problematic conversion to bullet-points
                # This prevents mixed language content by preserving AI-generated big-numbers slides
                if has_placeholder_content:
                    logger.info(f"FIXED: Detected placeholder content in big-numbers slide {slide_index + 1}")
                    logger.info(f"Preserving big-numbers template instead of converting to bullet-points to prevent mixed language issues")
                
                # Always preserve big-numbers template and add image prompt if needed
                    if not normalized_props.get('imagePrompt'):
                        slide_title = normalized_props.get('title', 'concepts')
                    # Generate language-neutral image prompt for big-numbers
                    normalized_props['imagePrompt'] = f"Minimalist flat design illustration of a modern data analytics environment. The scene features a professional analyst working at a clean desk with multiple monitors displaying charts, graphs, and numerical data visualizations (no readable text or numbers). The workspace includes a laptop, tablet, and organized documents. Large windows provide natural light to the modern office space. The data visualizations and analytics displays are [COLOR1], the analyst's attire and equipment are [COLOR2], and the office environment and furniture are [COLOR3]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    normalized_props['imageAlt'] = f"Professional data visualization illustration for {slide_title}"
                
                # Log final processed big-numbers content
                logger.info(f"=== FINAL PROCESSED BIG-NUMBERS for slide {slide_index + 1} ===")
                logger.info(f"Final Steps: {normalized_props.get('steps', [])}")
                logger.info(f"Final Props: {normalized_props}")
                logger.info(f"=== END FINAL PROCESSED BIG-NUMBERS for slide {slide_index + 1} ===")
                    
            # Fix timeline template props
            elif template_id == 'timeline':
                # Convert steps[] to events[] if AI generated wrong format
                if 'steps' in normalized_props and 'events' not in normalized_props:
                    steps = normalized_props.pop('steps')
                    events = []
                    for step in steps:
                        if isinstance(step, dict):
                            # Convert step format to event format
                            event = {
                                'date': step.get('date') or step.get('heading') or step.get('title') or 'Event',
                                'title': step.get('title') or step.get('heading') or 'Event Title',
                                'description': step.get('description') or ''
                            }
                            events.append(event)
                    normalized_props['events'] = events
                    logger.info(f"Converted steps[] to events[] for timeline slide {slide_index + 1}")
                    
            # Fix process-steps template props
            elif template_id == 'process-steps':
                # Remove subtitle as process-steps template doesn't use it
                if 'subtitle' in normalized_props:
                    normalized_props.pop('subtitle')
                    logger.info(f"Removed subtitle from process-steps slide {slide_index + 1}")
                    
            # Fix four-box-grid template props
            elif template_id == 'four-box-grid':
                boxes = normalized_props.get('boxes', [])
                if boxes and isinstance(boxes, list):
                    fixed_boxes = []
                    for box in boxes:
                        if isinstance(box, dict):
                            # Convert title/content to heading/text
                            fixed_box = {
                                'heading': box.get('heading') or box.get('title', ''),
                                'text': box.get('text') or box.get('content', '')
                            }
                            if fixed_box['heading']:  # Only add if heading exists
                                fixed_boxes.append(fixed_box)
                    normalized_props['boxes'] = fixed_boxes
                    
            # Fix two-column template props
            elif template_id == 'two-column':
                # Handle missing right column content by splitting existing content if it's substantial
                if (not normalized_props.get('rightContent') or normalized_props.get('rightContent') == '') and normalized_props.get('leftContent'):
                    left_content = normalized_props.get('leftContent', '')
                    
                    # If left content is substantial and right is empty, try to split content intelligently
                    if len(left_content) > 100:  # Only split if there's substantial content
                        lines = left_content.split('\n')
                        if len(lines) >= 2:
                            # Split content roughly in half
                            mid_point = len(lines) // 2
                            left_part = '\n'.join(lines[:mid_point]).strip()
                            right_part = '\n'.join(lines[mid_point:]).strip()
                            
                            if left_part and right_part:
                                normalized_props['leftContent'] = left_part
                                normalized_props['rightContent'] = right_part
                                logger.info(f"Split two-column content for slide {slide_index + 1}")
                
                # Generate appropriate titles if missing
                if not normalized_props.get('rightTitle') or normalized_props.get('rightTitle') == '':
                    title = normalized_props.get('title', '')
                    left_title = normalized_props.get('leftTitle', '')
                    
                    # Generate contextual right title based on slide content
                    if 'advantages' in left_title.lower() or 'benefits' in left_title.lower():
                        normalized_props['rightTitle'] = 'Disadvantages' if 'advantages' in left_title.lower() else 'Challenges'
                    elif 'pros' in left_title.lower():
                        normalized_props['rightTitle'] = 'Cons'
                    elif 'before' in left_title.lower():
                        normalized_props['rightTitle'] = 'After'
                    elif 'strategies' in title.lower() or 'methods' in title.lower():
                        normalized_props['rightTitle'] = 'Implementation' if left_title else 'Best Practices'
                    else:
                        # Generic fallback
                        normalized_props['rightTitle'] = 'Additional Details'
                
                # Ensure leftTitle exists
                if not normalized_props.get('leftTitle'):
                    normalized_props['leftTitle'] = 'Key Points'
                
                # Generate missing image prompts for two-column templates (check each side independently)
                title = normalized_props.get('title', 'comparison')
                left_title = normalized_props.get('leftTitle', 'concept')
                right_title = normalized_props.get('rightTitle', 'concept')
                
                # Generate left image prompt if missing using detailed format
                if not normalized_props.get('leftImagePrompt') and normalized_props.get('leftContent'):
                    left_content_sample = normalized_props.get('leftContent', '')[:100].lower()
                    if 'network' in left_content_sample or 'event' in left_content_sample or 'meeting' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing people networking and collaborating. The scene features a group of diverse professionals in business attire engaging in conversations, exchanging ideas, and building connections in a modern corporate environment. People are positioned throughout the scene in small groups, with some standing and others sitting around tables. Professional networking activities are highlighted with [COLOR1], conversation indicators in [COLOR2], and background elements in [COLOR3]. NO text, labels, or readable content on any surfaces or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in left_content_sample or 'digital' in left_content_sample or 'system' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} featuring modern technology and digital systems. The scene shows interconnected technological components, digital interfaces, and system architectures with detailed visual elements. Main technology components are [COLOR1], connecting elements are [COLOR2], and supporting details are [COLOR3]. All screens and displays show abstract geometric patterns with NO readable text or labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'process' in left_content_sample or 'step' in left_content_sample or 'method' in left_content_sample:
                        normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of {left_title.lower()} showing a detailed process workflow. The scene features sequential steps connected by arrows, with clear visual indicators for each stage of the process. Process elements are rendered in [COLOR1], connecting arrows in [COLOR2], and step indicators in [COLOR3]. Use symbols and geometric shapes instead of text labels. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        left_content_lower = normalized_props.get('leftContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in left_content_lower or 'education' in left_content_lower or 'student' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of students learning in a modern classroom setting. The scene features diverse students sitting at desks with laptops, a teacher presenting at the front, and educational materials around the room. Students are engaged and taking notes, with some raising hands to ask questions. Student laptops and materials are [COLOR1], the teacher and presentation board are [COLOR2], and classroom furniture is [COLOR3]. No readable text on any surfaces. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in left_content_lower or 'meeting' in left_content_lower or 'team' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of professionals collaborating in a modern office meeting room. The scene features a diverse group of business people sitting around a conference table, engaged in discussion, with one person presenting ideas. Laptops and documents are on the table, and a presentation screen shows charts. Conference table and laptops are [COLOR1], people and presentation screen are [COLOR2], and office furniture is [COLOR3]. No readable text anywhere. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in left_content_lower or 'analysis' in left_content_lower or 'research' in left_content_lower:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of a data analyst working with information in a modern office. The scene features a focused professional at a desk with multiple monitors displaying charts and graphs, surrounded by organized workspace elements. The person is analyzing data patterns on screen while taking notes. Computer monitors and data visualizations are [COLOR1], the analyst and desk setup are [COLOR2], and office environment is [COLOR3]. All screens show abstract geometric patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['leftImagePrompt'] = f"Minimalist flat design illustration of people working together in a professional environment related to {left_title.lower()}. The scene features diverse professionals engaged in relevant activities, using modern tools and technology. The setting shows a clean, organized workspace with people collaborating effectively. Primary work elements are [COLOR1], people and main activities are [COLOR2], and environmental details are [COLOR3]. No readable text on any surfaces or screens. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Generate right image prompt if missing using detailed format
                if not normalized_props.get('rightImagePrompt') and normalized_props.get('rightContent'):
                    right_content_sample = normalized_props.get('rightContent', '')[:100].lower()
                    if 'association' in right_content_sample or 'professional' in right_content_sample or 'group' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing professional associations and industry connections. The scene features business professionals in formal attire attending conferences, participating in panel discussions, and engaging in professional development activities. A large conference hall setting with speakers, audience members, and networking areas. Association activities are highlighted in [COLOR1], professional interactions in [COLOR2], and venue elements in [COLOR3]. NO visible text on presentations, banners, or displays - use abstract symbols instead. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'technology' in right_content_sample or 'digital' in right_content_sample or 'system' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} featuring advanced technology and digital innovation. The scene shows cutting-edge technological solutions, modern interfaces, and innovative systems with comprehensive visual details. Technology components are [COLOR1], interface elements are [COLOR2], and innovation indicators are [COLOR3]. All digital displays show abstract geometric patterns with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    elif 'strategy' in right_content_sample or 'approach' in right_content_sample or 'solution' in right_content_sample:
                        normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of {right_title.lower()} showing strategic planning and solution implementation. The scene features strategic diagrams, planning documents, and implementation frameworks with detailed visual representations. Strategic elements are [COLOR1], planning components are [COLOR2], and implementation details are [COLOR3]. Documents and charts show abstract shapes and symbols with NO readable text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                    else:
                        # Create specific, contextual scene based on content
                        right_content_lower = normalized_props.get('rightContent', '').lower()
                        title_lower = title.lower()
                        
                        if 'learn' in right_content_lower or 'education' in right_content_lower or 'student' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of students using technology for learning. The scene features diverse students in a modern computer lab, working on individual projects with tablets and laptops. Some students are collaborating on assignments while others focus independently. A teacher walks among them providing guidance. Student devices and work materials are [COLOR1], students and teacher are [COLOR2], and lab environment is [COLOR3]. No readable text on any screens or materials. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'business' in right_content_lower or 'meeting' in right_content_lower or 'team' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of business professionals in a client presentation meeting. The scene features a confident presenter explaining concepts to seated clients, with visual aids displayed on a large screen. The atmosphere is professional and engaging, with participants actively listening and taking notes. Presentation equipment and materials are [COLOR1], presenter and clients are [COLOR2], and meeting room elements are [COLOR3]. No readable text on presentations or documents. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        elif 'data' in right_content_lower or 'analysis' in right_content_lower or 'research' in right_content_lower:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of researchers collaborating on data analysis in a modern lab setting. The scene features scientists and analysts working together around computer workstations, discussing findings and sharing insights. Multiple screens display data visualizations while team members point to specific patterns. Computer equipment and data displays are [COLOR1], researchers and team members are [COLOR2], and lab environment is [COLOR3]. All screens show abstract patterns without text. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                        else:
                            normalized_props['rightImagePrompt'] = f"Minimalist flat design illustration of professionals implementing solutions in a modern workplace related to {right_title.lower()}. The scene features a diverse team actively working on practical applications, using contemporary tools and methods. The environment shows organized, efficient operations with people engaged in meaningful work. Implementation tools and equipment are [COLOR1], team members and activities are [COLOR2], and workplace environment is [COLOR3]. No readable text on any surfaces or equipment. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
                
                # Handle case where AI used leftContent/rightContent but missing titles
                if normalized_props.get('leftContent') and not normalized_props.get('leftTitle'):
                    # Try to extract title from content
                    left_content = normalized_props.get('leftContent', '')
                    if '\n' in left_content:
                        lines = left_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['leftTitle'] = lines[0].strip()
                            normalized_props['leftContent'] = '\n'.join(lines[1:]).strip()
                
                if normalized_props.get('rightContent') and not normalized_props.get('rightTitle'):
                    # Try to extract title from content
                    right_content = normalized_props.get('rightContent', '')
                    if '\n' in right_content:
                        lines = right_content.split('\n')
                        if lines[0] and not lines[0].startswith('-') and not lines[0].startswith('•'):
                            normalized_props['rightTitle'] = lines[0].strip()
                            normalized_props['rightContent'] = '\n'.join(lines[1:]).strip()
                            
            # Fix challenges-solutions template props
            elif template_id == 'challenges-solutions':
                # Convert leftContent/rightContent to challenges/solutions arrays
                if 'leftContent' in normalized_props and 'challenges' not in normalized_props:
                    left_content = normalized_props.get('leftContent', '')
                    # Parse content into challenges array
                    challenges = []
                    for line in left_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('common challenges'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                challenges.append(clean_line)
                    
                    if challenges:
                        normalized_props['challenges'] = challenges
                        del normalized_props['leftContent']
                        
                if 'rightContent' in normalized_props and 'solutions' not in normalized_props:
                    right_content = normalized_props.get('rightContent', '')
                    # Parse content into solutions array
                    solutions = []
                    for line in right_content.split('\n'):
                        line = line.strip()
                        if line and not line.lower().startswith('recommended resources') and not line.lower().startswith('solutions'):
                            # Remove bullet points and dashes
                            clean_line = line.lstrip('•-* ').strip()
                            if clean_line:
                                solutions.append(clean_line)
                    
                    if solutions:
                        normalized_props['solutions'] = solutions
                        del normalized_props['rightContent']
                
                # Ensure required titles exist
                if 'challengesTitle' not in normalized_props:
                    normalized_props['challengesTitle'] = 'Challenges'
                if 'solutionsTitle' not in normalized_props:
                    normalized_props['solutionsTitle'] = 'Solutions'
            
            # Fix timeline template props
            elif template_id == 'timeline':
                # Convert "events" to "steps" and restructure data
                events = normalized_props.get('events', [])
                if events and isinstance(events, list):
                    steps = []
                    for event in events:
                        if isinstance(event, dict):
                            # Convert event structure to step structure
                            heading = (event.get('title') or event.get('heading') or event.get('date') or '').strip()
                            description = (event.get('description') or '').strip()
                            if heading or description:
                                steps.append({
                                    'heading': heading or 'Timeline Step',
                                    'description': description or 'No description available'
                                })
                    normalized_props['steps'] = steps
                    normalized_props.pop('events', None)  # Remove the old structure
                elif 'steps' not in normalized_props:
                    # Create default steps if no events or steps exist
                    normalized_props['steps'] = [
                        {'heading': 'Step 1', 'description': 'First milestone'},
                        {'heading': 'Step 2', 'description': 'Second milestone'},
                        {'heading': 'Step 3', 'description': 'Third milestone'},
                        {'heading': 'Step 4', 'description': 'Final milestone'}
                    ]
            
            # Fix pyramid template props
            elif template_id == 'pyramid':
                # Ensure 'steps' array exists by parsing from common inputs
                steps = normalized_props.get('steps', []) or normalized_props.get('items', [])
                levels = normalized_props.get('levels', [])
                
                # If we have levels data, use it to create proper steps
                if levels and isinstance(levels, list) and len(levels) >= 3:
                    parsed_items = []
                    for i, level in enumerate(levels[:3], start=1):
                        if isinstance(level, dict):
                            heading = level.get('text', f'Level {i}')
                            description = level.get('description', '')
                            parsed_items.append({'heading': heading, 'description': description})
                        else:
                            parsed_items.append({'heading': f'Level {i}', 'description': str(level) if level else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                
                # If no levels but we have steps, validate and fix them
                elif steps and isinstance(steps, list):
                    fixed_steps = []
                    for i, step in enumerate(steps[:3], start=1):
                        if isinstance(step, dict):
                            heading = step.get('heading', f'Level {i}')
                            description = step.get('description', '')
                            fixed_steps.append({'heading': heading, 'description': description})
                        else:
                            fixed_steps.append({'heading': f'Level {i}', 'description': str(step) if step else ''})
                    
                    # Ensure we have exactly 3 levels
                    while len(fixed_steps) < 3:
                        idx = len(fixed_steps) + 1
                        fixed_steps.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = fixed_steps[:3]
                
                # If no structured data, try to parse from content
                else:
                    import re
                    text = (normalized_props.get('content') or '').strip()
                    parsed_items = []
                    if text:
                        # Try to extract segments like **Heading**: description ... up to next **Heading**
                        pattern = re.compile(r"\*\*([^*]+)\*\*\s*:?[\s\-–—]*([^*]+?)(?=\s*\*\*[^*]+\*\*|$)", re.S)
                        for m in pattern.finditer(text):
                            heading = m.group(1).strip()
                            desc = m.group(2).strip().replace('\n', ' ')
                            if heading and desc:
                                parsed_items.append({'heading': heading, 'description': desc})
                    
                    if not parsed_items and text:
                        # Fallback: split into up to 3 sentence-like chunks
                        # First try double newlines, then periods.
                        chunks = [c.strip() for c in re.split(r"\n\n+", text) if c.strip()]
                        if not chunks:
                            chunks = [c.strip() for c in re.split(r"(?<=[.!?])\s+", text) if c.strip()]
                        for i, ch in enumerate(chunks[:3], start=1):
                            parsed_items.append({'heading': f'Level {i}', 'description': ch})
                    
                    # Ensure we have exactly 3 levels, but don't add "No description available"
                    while len(parsed_items) < 3:
                        idx = len(parsed_items) + 1
                        parsed_items.append({'heading': f'Level {idx}', 'description': ''})
                    
                    normalized_props['steps'] = parsed_items[:3]
                # Clean up: pyramid does not use a long 'content' blob when steps are present
                if normalized_props.get('steps') and 'content' in normalized_props:
                    pass  # keep content for now as optional; frontend ignores it
                    
            # Fix event-list template props
            elif template_id == 'event-list':
                # Ensure events array exists
                events = normalized_props.get('events', [])
                if not (isinstance(events, list) and events):
                    # Create default events if none exist
                    normalized_props['events'] = [
                        {'date': 'Event 1', 'description': 'Event description'},
                        {'date': 'Event 2', 'description': 'Event description'},
                        {'date': 'Event 3', 'description': 'Event description'}
                    ]
                else:
                    # Ensure each event has required fields
                    fixed_events = []
                    for event in events:
                        if isinstance(event, dict):
                            # FIXED: Use 'title' as description if 'description' is empty
                            description = event.get('description') or event.get('desc') or ''
                            if not description.strip():
                                # If description is empty, use title as description
                                description = event.get('title') or 'Event description'
                            
                            fixed_event = {
                                'date': str(event.get('date') or 'Event Date'),
                                'description': str(description)
                            }
                            fixed_events.append(fixed_event)
                    if fixed_events:
                        normalized_props['events'] = fixed_events
                
                # Log final processed event-list content
                logger.info(f"=== FINAL PROCESSED EVENT-LIST for slide {slide_index + 1} ===")
                logger.info(f"Final Events: {normalized_props.get('events', [])}")
                logger.info(f"Final Props: {normalized_props}")
                logger.info(f"=== END FINAL PROCESSED EVENT-LIST for slide {slide_index + 1} ===")
        
            # Fix bullet-points template props
            elif template_id in ['bullet-points', 'bullet-points-right']:
                bullets = normalized_props.get('bullets', [])
                if bullets and isinstance(bullets, list):
                    # Ensure bullets are strings and not empty
                    fixed_bullets = [str(bullet).strip() for bullet in bullets if str(bullet).strip()]
                    if fixed_bullets:
                        normalized_props['bullets'] = fixed_bullets
                    else:
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': No valid bullet points, adding placeholder")
                        normalized_props['bullets'] = ['No content available']
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid or missing bullets array, adding placeholder")
                    normalized_props['bullets'] = ['No content available']
            
            # Fix comparison-slide template props
            elif template_id == 'comparison-slide':
                table_data = normalized_props.get('tableData', {})
                if isinstance(table_data, dict):
                    # Ensure headers exist
                    if 'headers' not in table_data or not isinstance(table_data['headers'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid headers")
                        table_data['headers'] = ['Feature', 'Option A', 'Option B']
                    
                    # Ensure rows exist
                    if 'rows' not in table_data or not isinstance(table_data['rows'], list):
                        logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Missing or invalid rows")
                        table_data['rows'] = [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    
                    # Ensure all rows have the correct number of columns
                    expected_cols = len(table_data['headers'])
                    fixed_rows = []
                    for row_idx, row in enumerate(table_data['rows']):
                        if isinstance(row, list):
                            # Pad or trim row to match header count
                            if len(row) < expected_cols:
                                # Pad with empty strings
                                row = row + [''] * (expected_cols - len(row))
                            elif len(row) > expected_cols:
                                # Trim to match headers
                                row = row[:expected_cols]
                            fixed_rows.append([str(cell) for cell in row])
                        else:
                            logger.warning(f"Coercing slide {slide_index + 1}: Invalid row format at index {row_idx}")
                            # Create a placeholder row
                            fixed_rows.append([f'Row {row_idx + 1} Col {i + 1}' for i in range(expected_cols)])
                    
                    table_data['rows'] = fixed_rows
                    normalized_props['tableData'] = table_data
                    
                    logger.info(f"Fixed comparison table data for slide {slide_index + 1}: {len(table_data['headers'])} columns, {len(table_data['rows'])} rows")
                else:
                    logger.warning(f"Coercing slide {slide_index + 1} with template '{template_id}': Invalid tableData, adding default comparison")
                    normalized_props['tableData'] = {
                        'headers': ['Feature', 'Option A', 'Option B'],
                        'rows': [
                            ['Characteristic 1', 'Value A1', 'Value B1'],
                            ['Characteristic 2', 'Value A2', 'Value B2']
                        ]
                    }

                        # NEW TEMPLATES: Handle new 5-template system for video lesson presentations
            elif template_id == 'course-overview-slide':
                # Ensure required props exist
                if not normalized_props.get('title'):
                    normalized_props['title'] = 'Course Overview'
                if not normalized_props.get('subtitle'):
                    normalized_props['subtitle'] = 'Course introduction and objectives'
                if not normalized_props.get('imagePath'):
                    normalized_props['imagePath'] = 'https://via.placeholder.com/600x400?text=Course+Image'
                if not normalized_props.get('imageAlt'):
                    normalized_props['imageAlt'] = 'Course overview illustration'
                # Don't set logoPath - let it be empty so the "Your Logo" placeholder shows
                if not normalized_props.get('pageNumber'):
                    normalized_props['pageNumber'] = slide_index + 1
                    
            elif template_id == 'impact-statements-slide':
                # Ensure statements array exists and is properly formatted
                statements = normalized_props.get('statements', [])
                if not isinstance(statements, list) or len(statements) < 3:
                    normalized_props['statements'] = [
                        { "number": "95%", "description": "Success rate in implementation" },
                        { "number": "3x", "description": "Increase in productivity" },
                        { "number": "50%", "description": "Reduction in time spent" }
                    ]
                else:
                    # Ensure each statement has required fields
                    fixed_statements = []
                    for stmt in statements[:3]:  # Limit to 3 statements
                        if isinstance(stmt, dict):
                            fixed_stmt = {
                                'number': str(stmt.get('number', '0%')),
                                'description': str(stmt.get('description', 'No description'))
                            }
                            fixed_statements.append(fixed_stmt)
                        else:
                            fixed_statements.append({'number': '0%', 'description': str(stmt)})
                    normalized_props['statements'] = fixed_statements
                
                # Ensure other required props
                if not normalized_props.get('profileImagePath'):
                    normalized_props['profileImagePath'] = 'https://via.placeholder.com/200x200?text=Avatar'
                if not normalized_props.get('pageNumber'):
                    normalized_props['pageNumber'] = slide_index + 1
                # Don't set logoNew - let it be empty so the "Your Logo" placeholder shows
                    
            elif template_id == 'phishing-definition-slide':
                # Ensure definitions array exists and is properly formatted
                definitions = normalized_props.get('definitions', [])
                if not isinstance(definitions, list) or len(definitions) < 3:
                    normalized_props['definitions'] = [
                        "First important definition with detailed explanation",
                        "Second key concept with comprehensive description", 
                        "Third critical term with thorough explanation"
                    ]
                else:
                    # Ensure definitions are strings
                    fixed_definitions = []
                    for defn in definitions:
                        if isinstance(defn, str) and defn.strip():
                            fixed_definitions.append(defn.strip())
                        elif isinstance(defn, dict):
                            # Extract text from dict if needed
                            text = defn.get('text') or defn.get('description') or defn.get('content', '')
                            if text.strip():
                                fixed_definitions.append(str(text).strip())
                    # Pad to at least 3 definitions
                    while len(fixed_definitions) < 3:
                        fixed_definitions.append(f"Definition {len(fixed_definitions) + 1}")
                    normalized_props['definitions'] = fixed_definitions[:6]  # Max 6 definitions
                
                # Ensure other required props
                if not normalized_props.get('profileImagePath'):
                    normalized_props['profileImagePath'] = 'https://via.placeholder.com/200x200?text=Avatar'
                if not normalized_props.get('rightImagePath'):
                    normalized_props['rightImagePath'] = 'https://via.placeholder.com/300x200?text=Definition+Image'
                if not normalized_props.get('pageNumber'):
                    normalized_props['pageNumber'] = slide_index + 1
                # Don't set logoPath - let it be empty so the "Your Logo" placeholder shows
                    
            elif template_id == 'soft-skills-assessment-slide':
                # Ensure tips array exists and is properly formatted
                tips = normalized_props.get('tips', [])
                if not isinstance(tips, list) or len(tips) < 2:
                    normalized_props['tips'] = [
                        { "text": "First important tip for success", "isHighlighted": True },
                        { "text": "Second key recommendation for better results", "isHighlighted": False }
                    ]
                else:
                    # Ensure each tip has required fields
                    fixed_tips = []
                    for tip in tips[:2]:  # Limit to 2 tips
                        if isinstance(tip, dict):
                            fixed_tip = {
                                'text': str(tip.get('text', 'No tip text')),
                                'isHighlighted': bool(tip.get('isHighlighted', False))
                            }
                            fixed_tips.append(fixed_tip)
                        else:
                            fixed_tips.append({'text': str(tip), 'isHighlighted': False})
                    normalized_props['tips'] = fixed_tips
                
                # Ensure other required props
                if not normalized_props.get('profileImagePath'):
                    normalized_props['profileImagePath'] = 'https://via.placeholder.com/200x200?text=Avatar'
                if not normalized_props.get('pageNumber'):
                    normalized_props['pageNumber'] = slide_index + 1
                # Don't set logoPath - let it be empty so the "Your Logo" placeholder shows
                if not normalized_props.get('logoText'):
                    normalized_props['logoText'] = 'Assessment Guide'
                    
            elif template_id == 'work-life-balance-slide':
                # Ensure required props exist
                if not normalized_props.get('title'):
                    normalized_props['title'] = 'Conclusion and Next Steps'
                if not normalized_props.get('content'):
                    normalized_props['content'] = 'This comprehensive lesson has covered the essential concepts and practical applications. Moving forward, focus on implementing these strategies in your daily work while maintaining a healthy balance between learning and application.'
                if not normalized_props.get('imagePath'):
                    normalized_props['imagePath'] = 'https://via.placeholder.com/400x300?text=Conclusion+Image'
                if not normalized_props.get('pageNumber'):
                    normalized_props['pageNumber'] = slide_index + 1
                # Don't set logoPath - let it be empty so the "Your Logo" placeholder shows
        
            normalized_slide['props'] = normalized_props

            # Enforce realistic image prompt style (convert minimalist to realistic scene descriptors)
            def to_realistic(prompt: str) -> str:
                if not isinstance(prompt, str) or not prompt.strip():
                    return prompt
                p = prompt
                replacements = [
                    ('Minimalist flat design illustration', 'Realistic cinematic scene'),
                    ('modern corporate vector art', 'cinematic photography with natural lighting'),
                    ('flat colors', 'physically-based materials and textures'),
                    ('clean geometric shapes', 'real-world objects and surfaces'),
                    ('infographic', 'realistic scene'),
                    ('illustration', 'cinematic photography'),
                    ('vector', 'photographic'),
                    ('icon', 'object'),
                    ('isometric', 'cinematic'),
                    ('3D render', 'realistic photography'),
                    ('CGI', 'natural photography'),
                    ('cartoon', 'photographic'),
                    ('flat design', 'realistic scene'),
                    ('modern design', 'cinematic scene'),
                ]
                for a, b in replacements:
                    p = p.replace(a, b)
                if '35mm' not in p and '50mm' not in p and 'low-angle' not in p and 'three-quarter' not in p:
                    p += ' — cinematic 35mm lens, three-quarter view, soft rim light, shallow depth of field'
                return p

            if isinstance(normalized_props, dict):
                if 'imagePrompt' in normalized_props and isinstance(normalized_props['imagePrompt'], str):
                    normalized_props['imagePrompt'] = to_realistic(normalized_props['imagePrompt'])
                if 'leftImagePrompt' in normalized_props and isinstance(normalized_props['leftImagePrompt'], str):
                    normalized_props['leftImagePrompt'] = to_realistic(normalized_props['leftImagePrompt'])
                if 'rightImagePrompt' in normalized_props and isinstance(normalized_props['rightImagePrompt'], str):
                    normalized_props['rightImagePrompt'] = to_realistic(normalized_props['rightImagePrompt'])
            normalized_slide['props'] = normalized_props
            
            # Remove voiceoverText for non-video presentations
            if (component_name == COMPONENT_NAME_SLIDE_DECK and 
                'voiceoverText' in normalized_slide):
                logger.info(f"Removing voiceoverText from slide {slide_index + 1} for regular slide deck")
                normalized_slide.pop('voiceoverText', None)
            
            # Drop only obvious closing/thank you slides - be more specific to avoid dropping content
            title_lower = str(normalized_slide.get('slideTitle') or '').strip().lower()
            closing_keywords = [
                'thank you', 'thanks for', 'final thoughts', 'wrap up', 'wrap-up',
                "what's next", 'whats next', 'next steps for implementation'
            ]
            # Only drop if title starts with or exactly matches closing patterns
            should_drop = any(
                title_lower.startswith(k) or title_lower == k or 
                (k in ['thank you', 'thanks for'] and k in title_lower)
                for k in closing_keywords
            )
            if should_drop:
                logger.info(f"[NOTICE] Closing-type slide detected (not dropped): {slide_index + 1} titled '{normalized_slide.get('slideTitle')}'")
            
            normalized_slides.append(normalized_slide)
            
        except Exception as e:
            logger.error(f"Error normalizing slide {slide_index + 1} with template '{template_id}': {e}")
            logger.warning(f"Removing problematic slide {slide_index + 1}")

            # Log error to database if possible
            try:
                async with DB_POOL.acquire() as conn:
                    await save_slide_creation_error(conn, None, template_id, props, str(e))
            except Exception as err:
                logger.error(f"Failed to save slide_creation_error: {err}")

            continue  # Skip this slide
    
    logger.info(f"Slide normalization complete: {len(slides)} -> {len(normalized_slides)} slides (removed {len(slides) - len(normalized_slides)} invalid slides)")
    return normalized_slides

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

class QuizData(BaseModel):
    quizTitle: Optional[str] = None
    questions: List[AnyQuizQuestion] = Field(default_factory=list)
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True, "use_enum_values": True}

class TextPresentationDetails(BaseModel):
    textTitle: Optional[str] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- End: Add New Quiz Models ---

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, Dict[str, Any], None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")

BING_API_KEY = os.getenv("BING_API_KEY")
BING_API_URL = "https://api.bing.microsoft.com/v7.0/search"

# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}

# In-memory job status store (for demo; use Redis for production)
AI_AUDIT_PROGRESS = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""

async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AiAuditQuestionnaireRequest(BaseModel):
    companyWebsite: str
    language: str = "ru"  # Default to Russian

class AiAuditScrapedData(BaseModel):
    companyName: str
    companyDesc: str
    employees: str
    franchise: str
    onboardingProblems: str
    documents: list[str]
    documentsOther: str = ""
    priorities: list[str]
    priorityOther: str = ""

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock", "TableBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

class TableBlock(BaseContentBlock):
    type: str = "table"
    headers: List[str]
    rows: List[List[str]]
    caption: Optional[str] = None

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# --- End: Add New Quiz Models ---

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, Dict[str, Any], None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
        {
          "type": "numbered_list",
          "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
      "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
        {
          "type": "bullet_list",
          "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""


async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# --- End: Add New Quiz Models ---

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, Dict[str, Any], None]
# custom_extensions/backend/main.py
from fastapi import FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect
# NEW: OpenAI imports for direct usage
import openai
from openai import AsyncOpenAI
from uuid import uuid4

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

# NEW: OpenAI client for direct streaming
OPENAI_CLIENT = None

def get_openai_client():
    """Get or create the OpenAI client instance."""
    global OPENAI_CLIENT
    if OPENAI_CLIENT is None:
        api_key = LLM_API_KEY or LLM_API_KEY_FALLBACK
        if not api_key:
            raise ValueError("No OpenAI API key configured. Set OPENAI_API_KEY environment variable.")
        OPENAI_CLIENT = AsyncOpenAI(api_key=api_key)
    return OPENAI_CLIENT

async def stream_openai_response(prompt: str, model: str = None):
    """
    Stream response directly from OpenAI API.
    Yields dictionaries with 'type' and 'text' fields compatible with existing frontend.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_STREAM] Starting direct OpenAI streaming with model {model}")
        logger.info(f"[OPENAI_STREAM] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_STREAM] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the streaming chat completion
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=10000,  # Increased from 4000 to handle larger course outlines
            temperature=0.2
        )
        
        logger.info(f"[OPENAI_STREAM] Stream created successfully")
        
        # DEBUG: Collect full response for logging
        full_response = ""
        chunk_count = 0
        
        async for chunk in stream:
            chunk_count += 1
            logger.debug(f"[OPENAI_STREAM] Chunk {chunk_count}: {chunk}")
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                if choice.delta and choice.delta.content:
                    content = choice.delta.content
                    full_response += content  # DEBUG: Accumulate full response
                    yield {"type": "delta", "text": content}
                    
                # Check for finish reason
                if choice.finish_reason:
                    logger.info(f"[OPENAI_STREAM] Stream finished with reason: {choice.finish_reason}")
                    logger.info(f"[OPENAI_STREAM] Total chunks received: {chunk_count}")
                    logger.info(f"[OPENAI_STREAM] FULL RESPONSE:\n{full_response}")
                    break
                    
    except Exception as e:
        logger.error(f"[OPENAI_STREAM] Error in OpenAI streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"OpenAI streaming error: {str(e)}"}

def should_use_openai_direct(payload) -> bool:
    """
    Determine if we should use OpenAI directly instead of Onyx.
    Returns True when no file context is present.
    """
    # Check if files are explicitly provided
    has_files = (
        (hasattr(payload, 'fromFiles') and payload.fromFiles) or
        (hasattr(payload, 'folderIds') and payload.folderIds) or
        (hasattr(payload, 'fileIds') and payload.fileIds)
    )
    
    # Check if text context is provided (this still uses file system in some cases)
    has_text_context = (
        hasattr(payload, 'fromText') and payload.fromText and 
        hasattr(payload, 'userText') and payload.userText
    )
    
    # Use OpenAI directly only when there's no file context and no text context
    use_openai = not has_files and not has_text_context
    
    logger.info(f"[API_SELECTION] has_files={has_files}, has_text_context={has_text_context}, use_openai={use_openai}")
    return use_openai

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""



async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

# --- NEW: Slide-based Lesson Presentation Models ---
class ImagePlaceholder(BaseModel):
    size: str          # "LARGE", "MEDIUM", "SMALL", "BANNER", "BACKGROUND"
    position: str      # "LEFT", "RIGHT", "TOP_BANNER", "BACKGROUND", etc.
    description: str   # Description of the image content
    model_config = {"from_attributes": True}

class DeckSlide(BaseModel):
    slideId: str               
    slideNumber: int           
    slideTitle: str            
    templateId: str            # Зробити обов'язковим (без Optional)
    props: Dict[str, Any] = Field(default_factory=dict)  # Додати props
    voiceoverText: Optional[str] = None  # Optional voiceover text for video lessons
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)  # Опціонально для метаданих
    model_config = {"from_attributes": True}

class SlideDeckDetails(BaseModel):
    lessonTitle: str
    slides: List[DeckSlide] = Field(default_factory=list)
    currentSlideId: Optional[str] = None  # To store the active slide from frontend
    lessonNumber: Optional[int] = None    # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    hasVoiceover: Optional[bool] = None  # Flag indicating if any slide has voiceover
    theme: Optional[str] = None           # Selected theme for presentation
    model_config = {"from_attributes": True}

# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]

# custom_extensions/backend/main.py
from fastapi import Body, FastAPI, HTTPException, Depends, Request, status, File, UploadFile, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse

from typing import List, Optional, Dict, Any, Union, Type, ForwardRef, Set, Literal, Tuple
from pydantic import BaseModel, Field, RootModel
import re
import os
import asyncpg
from datetime import datetime, timezone
import httpx
from httpx import HTTPStatusError
import json
import uuid
import shutil
import logging
from locales.__init__ import LANG_CONFIG
import asyncio
import typing
import tempfile
import io
import gzip
import base64
import time
import uuid
import random
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import tiktoken
import inspect

# --- CONTROL VARIABLE FOR PRODUCTION LOGGING ---
# SET THIS TO True FOR PRODUCTION, False FOR DEVELOPMENT
IS_PRODUCTION = False  # Or True for production

# --- Logger ---
logger = logging.getLogger(__name__)
if IS_PRODUCTION:
    logging.basicConfig(level=logging.ERROR) # Production: Log only ERROR and CRITICAL
else:
    logging.basicConfig(level=logging.INFO)  # Development: Log INFO, WARNING, ERROR, CRITICAL


# --- Constants & DB Setup ---
CUSTOM_PROJECTS_DATABASE_URL = os.getenv("CUSTOM_PROJECTS_DATABASE_URL")
ONYX_DATABASE_URL = os.getenv("ONYX_DATABASE_URL")
ONYX_API_SERVER_URL = "http://api_server:8080" # Adjust if needed
ONYX_SESSION_COOKIE_NAME = os.getenv("ONYX_SESSION_COOKIE_NAME", "fastapiusersauth")

# Component name constants
COMPONENT_NAME_TRAINING_PLAN = "TrainingPlanTable"
COMPONENT_NAME_PDF_LESSON = "PdfLessonDisplay"
COMPONENT_NAME_SLIDE_DECK = "SlideDeckDisplay"
COMPONENT_NAME_VIDEO_LESSON = "VideoLessonDisplay"
COMPONENT_NAME_QUIZ = "QuizDisplay"
COMPONENT_NAME_TEXT_PRESENTATION = "TextPresentationDisplay"

# --- LLM Configuration for JSON Parsing ---
# === OpenAI ChatGPT configuration (replacing previous Cohere call) ===
LLM_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_API_KEY_FALLBACK = os.getenv("OPENAI_API_KEY_FALLBACK")
# Endpoint for Chat Completions
LLM_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/chat/completions")
# Default model to use – gpt-4o-mini provides strong JSON adherence
LLM_DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o-mini")

DB_POOL = None
# Track in-flight project creations to avoid duplicate processing (keyed by user+project)
ACTIVE_PROJECT_CREATE_KEYS: Set[str] = set()

# Track in-flight quiz finalizations to prevent duplicate processing
ACTIVE_QUIZ_FINALIZE_KEYS: Set[str] = set()

# Track quiz finalization timestamps for cleanup
QUIZ_FINALIZE_TIMESTAMPS: Dict[str, float] = {}


# --- Directory for Design Template Images ---
STATIC_DESIGN_IMAGES_DIR = "static_design_images"
os.makedirs(STATIC_DESIGN_IMAGES_DIR, exist_ok=True)

# --- Directory for Static Fonts ---
STATIC_FONTS_DIR = "static/fonts"
os.makedirs(STATIC_FONTS_DIR, exist_ok=True)

def inspect_list_items_recursively(data_structure: Any, path: str = ""):
    if isinstance(data_structure, dict):
        for key, value in data_structure.items():
            new_path = f"{path}.{key}" if path else key
            if key == "items": # Focus on 'items' keys
                logger.info(f"PDF Deep Inspect: Path: {new_path}, Type: {type(value)}, Is List: {isinstance(value, list)}, Value (first 100): {str(value)[:100]}")
                if not isinstance(value, list) and value is not None:
                    logger.error(f"PDF DEEP ERROR: Non-list 'items' at {new_path}. Type: {type(value)}, Value: {str(value)[:200]}")
            if isinstance(value, (dict, list)):
                inspect_list_items_recursively(value, new_path)
    elif isinstance(data_structure, list):
        for i, item in enumerate(data_structure):
            new_path = f"{path}[{i}]"
            if isinstance(item, (dict, list)):
                inspect_list_items_recursively(item, new_path)

DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM = """
{
  "mainTitle": "Example Training Program",
  "sections": [
    {
      "id": "№1",
      "title": "Introduction to Topic",
      "totalHours": 5.5,
      "lessons": [
        {
          "title": "Lesson 1.1: Basic Concepts",
          "check": {"type": "test", "text": "Knowledge Test"},
          "contentAvailable": {"type": "yes", "text": "100%"},
          "source": "Internal Documentation",
          "hours": 2.0,
          "completionTime": "6m"
        }
      ]
    }
  ],
  "detectedLanguage": "en"
}
"""

DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM = """
{
  "lessonTitle": "Example PDF Lesson with Nested Lists",
  "contentBlocks": [
    { "type": "headline", "level": 1, "text": "Main Title of the Lesson" },
    { "type": "paragraph", "text": "This is an introductory paragraph explaining the main concepts." },
    {
      "type": "bullet_list",
      "items": [
        "Top level item 1, demonstrating a simple string item.",
        {
          "type": "bullet_list",
          "iconName": "chevronRight",
          "items": [
            "Nested item A: This is a sub-item.",
            "Nested item B: Another sub-item to show structure.",
            {
              "type": "numbered_list",
              "items": [
                "Further nested numbered item 1.",
                "Further nested numbered item 2."
              ]
            }
          ]
        },
        "Top level item 2, followed by a nested numbered list.",
        {
          "type": "numbered_list",
          "items": [
            "Nested numbered 1: First point in nested ordered list.",
            "Nested numbered 2: Second point."
          ]
        },
        "Top level item 3."
      ]
    },
    { "type": "alert", "alertType": "info", "title": "Important Note", "text": "Alerts can provide contextual information or warnings." },
    {
      "type": "numbered_list",
      "items": [
        "Main numbered point 1.",
        {
          "type": "bullet_list",
          "items": [
            "Sub-bullet C under numbered list.",
            "Sub-bullet D, also useful for breaking down complex points."
          ]
        },
        "Main numbered point 2."
      ]
    },
    { "type": "section_break", "style": "dashed" }
  ],
  "detectedLanguage": "en"
}
"""


async def get_db_pool():
    if DB_POOL is None:
        detail_msg = "Database service not available." # Generic enough for production
        raise HTTPException(status_code=503, detail=detail_msg)
    return DB_POOL

app = FastAPI(title="Custom Extension Backend")

app.mount(f"/{STATIC_DESIGN_IMAGES_DIR}", StaticFiles(directory=STATIC_DESIGN_IMAGES_DIR), name="static_design_images")
app.mount("/fonts", StaticFiles(directory=STATIC_FONTS_DIR), name="static_fonts")

@app.middleware("http")
async def track_request_analytics(request: Request, call_next):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Get user ID if available
    user_id = None
    try:
        if hasattr(request.state, 'user_id'):
            user_id = request.state.user_id
    except:
        pass
    
    # Get request size
    request_size = None
    try:
        if request.method in ['POST', 'PUT', 'PATCH']:
            body = await request.body()
            request_size = len(body)
    except:
        pass
    
    try:
        response = await call_next(request)
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Get response size
        response_size = None
        try:
            if hasattr(response, 'body'):
                response_size = len(response.body)
        except:
            pass
        
        # Store analytics in database
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     response.status_code, response_time_ms, request_size,
                     response_size, None, datetime.now(timezone.utc))
        except Exception as e:
            logger.error(f"Failed to store request analytics: {e}")
        
        return response
        
    except Exception as e:
        end_time = time.time()
        response_time_ms = int((end_time - start_time) * 1000)
        
        # Store error analytics
        try:
            async with DB_POOL.acquire() as conn:
                await conn.execute("""
                    INSERT INTO request_analytics (
                        id, endpoint, method, user_id, status_code, 
                        response_time_ms, request_size_bytes, response_size_bytes,
                        error_message, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                """, request_id, request.url.path, request.method, user_id,
                     500, response_time_ms, request_size, None,
                     str(e), datetime.now(timezone.utc))
        except Exception as db_error:
            logger.error(f"Failed to store error analytics: {db_error}")
        
        raise

try:
    from app.services.pdf_generator import generate_pdf_from_html_template
    from app.core.config import settings
except ImportError:
    logger.warning("Could not import pdf_generator or settings from 'app' module. Using dummy implementations for PDF generation.")
    class DummySettings:
        CUSTOM_FRONTEND_URL = os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
    settings = DummySettings()
    async def generate_pdf_from_html_template(template_name: str, context_data: Dict[str, Any], output_filename: str) -> str:
        logger.info(f"PDF Generation Skipped (Dummy Service): Would generate for template {template_name} to {output_filename}")
        dummy_path = os.path.join("/app/tmp_pdf", output_filename)
        os.makedirs(os.path.dirname(dummy_path), exist_ok=True)
        with open(dummy_path, "w") as f: f.write(f"Dummy PDF for {output_filename} using context: {str(context_data)[:200]}")
        return dummy_path

@app.on_event("startup")
async def startup_event():
    global DB_POOL
    logger.info("Custom Backend starting...")
    if not CUSTOM_PROJECTS_DATABASE_URL:
        logger.critical("CRITICAL: CUSTOM_PROJECTS_DATABASE_URL env var not set.")
        return
    try:
        DB_POOL = await asyncpg.create_pool(dsn=CUSTOM_PROJECTS_DATABASE_URL, min_size=1, max_size=10,
                                            init=lambda conn: conn.set_type_codec(
                                                'jsonb',
                                                encoder=lambda value: json.dumps(value) if value is not None else None,
                                                decoder=lambda value: json.loads(value) if value is not None else None,
                                                schema='pg_catalog',
                                                format='text'
                                            ))
        async with DB_POOL.acquire() as connection:
            # --- Ensure user_billing table for Stripe linkage ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_billing (
                    onyx_user_id TEXT PRIMARY KEY,
                    stripe_customer_id TEXT,
                    subscription_status TEXT,
                    subscription_id TEXT,
                    current_price_id TEXT,
                    current_plan TEXT,
                    current_interval TEXT,
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_billing_customer ON user_billing(stripe_customer_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_billing_subscription ON user_billing(subscription_id);")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS initial_questionnaire (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL UNIQUE,
                    data JSONB NOT NULL
                );
            """)
            
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS slide_creation_errors (
                    id SERIAL PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    template_id TEXT NOT NULL,
                    props JSONB,
                    error_message TEXT,
                    created_at TIMESTAMPTZ DEFAULT now()
                );
            """)

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS design_templates (
                    id SERIAL PRIMARY KEY,
                    template_name TEXT NOT NULL UNIQUE,
                    template_structuring_prompt TEXT NOT NULL,
                    design_image_path TEXT,
                    microproduct_type TEXT,
                    component_name TEXT NOT NULL,
                    date_created TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    project_name TEXT NOT NULL,
                    product_type TEXT,
                    microproduct_type TEXT,
                    microproduct_name TEXT,
                    microproduct_content JSONB,
                    design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_name TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS product_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS microproduct_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS design_template_id INTEGER REFERENCES design_templates(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_onyx_user_id ON projects(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_design_template_id ON projects(design_template_id);")
            logger.info("'projects' table ensured and updated.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS microproduct_pipelines (
                    id SERIAL PRIMARY KEY,
                    pipeline_name TEXT NOT NULL,
                    pipeline_description TEXT,
                    is_prompts_data_collection BOOLEAN DEFAULT FALSE,
                    is_prompts_data_formating BOOLEAN DEFAULT FALSE,
                    prompts_data_collection JSONB,
                    prompts_data_formating JSONB,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_pipelines_name ON microproduct_pipelines(pipeline_name);")
            logger.info("'microproduct_pipelines' table ensured.")

            col_type_row = await connection.fetchrow(
                "SELECT data_type FROM information_schema.columns "
                "WHERE table_name = 'projects' AND column_name = 'microproduct_content';"
            )
            if col_type_row and col_type_row['data_type'] != 'jsonb':
                logger.info("Attempting to alter 'microproduct_content' column type to JSONB...")
                await connection.execute("ALTER TABLE projects ALTER COLUMN microproduct_content TYPE JSONB USING microproduct_content::text::jsonb;")
                logger.info("Successfully altered 'microproduct_content' to JSONB.")

            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            logger.info("'projects' table ensured and updated with 'source_chat_session_id'.")

            # --- Add source context tracking columns ---
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_context_type TEXT;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS source_context_data JSONB;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_source_context_type ON projects(source_context_type);")
            logger.info("'projects' table updated with source context tracking columns.")

            # --- Add lesson plan specific columns ---
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS lesson_plan_data JSONB;")
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS parent_outline_id INTEGER REFERENCES projects(id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_parent_outline_id ON projects(parent_outline_id);")
            logger.info("'projects' table updated with lesson plan columns.")

            # --- Add product-as-context column (Onyx document ID for product JSON) ---
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS product_json_onyx_id TEXT;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_product_json_onyx_id ON projects(product_json_onyx_id);")
            logger.info("'projects' table updated with product_json_onyx_id column for products-as-context feature.")

            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_name ON design_templates(template_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_design_templates_mptype ON design_templates(microproduct_type);")
            logger.info("'design_templates' table ensured.")

            # --- Initialize workspace database tables ---
            try:
                from app.core.database import init_database
                await init_database()
                logger.info("Workspace database tables initialized successfully")
            except Exception as db_init_error:
                logger.warning(f"Failed to initialize workspace database tables: {db_init_error}")

            # --- Ensure a soft-delete trash table for projects ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS trashed_projects (LIKE projects INCLUDING ALL);
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_trashed_projects_user ON trashed_projects(onyx_user_id);")
            logger.info("'trashed_projects' table ensured (soft-delete).")

            # --- Ensure user credits table ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_credits (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL UNIQUE,
                    name TEXT NOT NULL,
                    credits_balance INTEGER NOT NULL DEFAULT 0,
                    total_credits_used INTEGER NOT NULL DEFAULT 0,
                    credits_purchased INTEGER NOT NULL DEFAULT 0,
                    last_purchase_date TIMESTAMP WITH TIME ZONE,
                    subscription_tier TEXT DEFAULT 'basic',
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_credits_onyx_user_id ON user_credits(onyx_user_id);")
            logger.info("'user_credits' table ensured.")

            # --- Ensure entitlement overrides table ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_entitlement_overrides (
                    onyx_user_id TEXT PRIMARY KEY,
                    connectors_limit INTEGER,
                    storage_gb INTEGER,
                    slides_max INTEGER,
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_entitlements_user ON user_entitlement_overrides(onyx_user_id);")
            logger.info("'user_entitlement_overrides' table ensured.")

            # --- Ensure user_connectors table for quota tracking ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_connectors (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    onyx_connector_id INTEGER UNIQUE,
                    status TEXT DEFAULT 'active',
                    created_at TIMESTAMPTZ DEFAULT now(),
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_user ON user_connectors(onyx_user_id);")
            logger.info("'user_connectors' table ensured.")

            # --- Ensure base entitlements table (derived from Stripe plan/features) ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_entitlement_base (
                    onyx_user_id TEXT PRIMARY KEY,
                    connectors_limit INTEGER NOT NULL DEFAULT 0,
                    storage_gb INTEGER NOT NULL DEFAULT 1,
                    slides_max INTEGER NOT NULL DEFAULT 20,
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_entitlements_base_user ON user_entitlement_base(onyx_user_id);")
            logger.info("'user_entitlement_base' table ensured.")

            # --- Ensure user email cache (for admin listing) ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_email_cache (
                    onyx_user_id TEXT PRIMARY KEY,
                    email TEXT,
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_email_cache_user ON user_email_cache(onyx_user_id);")
            logger.info("'user_email_cache' table ensured.")

            # --- Ensure user storage usage table ---
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_storage_usage (
                    onyx_user_id TEXT PRIMARY KEY,
                    used_bytes BIGINT NOT NULL DEFAULT 0,
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_storage_usage_user ON user_storage_usage(onyx_user_id);")
            logger.info("'user_storage_usage' table ensured.")

            # NEW: Ensure credit transactions table for analytics/timeline
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS credit_transactions (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    user_credits_id INTEGER REFERENCES user_credits(id) ON DELETE CASCADE,
                    type TEXT NOT NULL CHECK (type IN ('purchase','product_generation','admin_removal')),
                    title TEXT,
                    product_type TEXT,
                    credits INTEGER NOT NULL CHECK (credits >= 0),
                    delta INTEGER NOT NULL,
                    reason TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_user ON credit_transactions(onyx_user_id, created_at DESC);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_type ON credit_transactions(type);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_tx_product ON credit_transactions(product_type);")
            
            # Migration: Update credit_transactions table to support admin_removal type
            try:
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    DROP CONSTRAINT IF EXISTS credit_transactions_type_check;
                """)
                await connection.execute("""
                    ALTER TABLE credit_transactions 
                    ADD CONSTRAINT credit_transactions_type_check 
                    CHECK (type IN ('purchase','product_generation','admin_removal'));
                """)
                logger.info("Updated credit_transactions table to support admin_removal type")
            except Exception as e:
                logger.warning(f"Could not update credit_transactions constraint: {e}")
            
            logger.info("'credit_transactions' table ensured.")

            # Ensure user_billing_addons table (recurring add-ons)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_billing_addons (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    stripe_customer_id TEXT,
                    stripe_subscription_id TEXT,
                    stripe_subscription_item_id TEXT,
                    stripe_price_id TEXT,
                    addon_type TEXT CHECK (addon_type IN ('connectors','storage')),
                    quantity INTEGER NOT NULL DEFAULT 1,
                    status TEXT,
                    current_period_end TIMESTAMPTZ,
                    created_at TIMESTAMPTZ DEFAULT now(),
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_addons_user ON user_billing_addons(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_addons_sub ON user_billing_addons(stripe_subscription_id);")
            logger.info("'user_billing_addons' table ensured.")

            # Ensure credit_grant_events table (audit)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS credit_grant_events (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    source TEXT CHECK (source IN ('tier_renewal','one_time_pack')),
                    amount INTEGER NOT NULL,
                    stripe_invoice_id TEXT,
                    created_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_grant_user ON credit_grant_events(onyx_user_id);")
            logger.info("'credit_grant_events' table ensured.")

            # Ensure processed_stripe_events for idempotency
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS processed_stripe_events (
                    event_id TEXT PRIMARY KEY,
                    created_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            logger.info("'processed_stripe_events' table ensured.")

            # Ensure user_billing_addons table (recurring add-ons)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS user_billing_addons (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    stripe_customer_id TEXT,
                    stripe_subscription_id TEXT,
                    stripe_subscription_item_id TEXT,
                    stripe_price_id TEXT,
                    addon_type TEXT CHECK (addon_type IN ('connectors','storage')),
                    quantity INTEGER NOT NULL DEFAULT 1,
                    status TEXT,
                    current_period_end TIMESTAMPTZ,
                    created_at TIMESTAMPTZ DEFAULT now(),
                    updated_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_addons_user ON user_billing_addons(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_addons_sub ON user_billing_addons(stripe_subscription_id);")
            logger.info("'user_billing_addons' table ensured.")

            # Ensure credit_grant_events table (audit)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS credit_grant_events (
                    id TEXT PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    source TEXT CHECK (source IN ('tier_renewal','one_time_pack')),
                    amount INTEGER NOT NULL,
                    stripe_invoice_id TEXT,
                    created_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_credit_grant_user ON credit_grant_events(onyx_user_id);")
            logger.info("'credit_grant_events' table ensured.")

            # Ensure processed_stripe_events for idempotency
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS processed_stripe_events (
                    event_id TEXT PRIMARY KEY,
                    created_at TIMESTAMPTZ DEFAULT now()
                );
                """
            )
            logger.info("'processed_stripe_events' table ensured.")

            # Note: User migration is now available only via admin interface
            # Automatic migration on startup has been disabled
            logger.info("User migration is available manually via /api/custom/admin/credits/migrate-users")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS project_folders (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    "order" INTEGER DEFAULT 0,
                    parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE
                );
            """)
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_onyx_user_id ON project_folders(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_folder_id ON projects(folder_id);")
            
            # Add parent_id column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS parent_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for parent_id column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_parent_id ON project_folders(parent_id);")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add order column to existing project_folders table if it doesn't exist
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e
            
            # Create index for order column
            try:
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_order ON project_folders(\"order\");")
            except Exception as e:
                # Index might already exist, which is fine
                if "already exists" not in str(e) and "duplicate key" not in str(e):
                    raise e
            
            # Add quality_tier column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS quality_tier TEXT DEFAULT 'medium';")
                logger.info("Added quality_tier column to project_folders table")
                
                # Update existing folders to have 'medium' tier if they don't have one
                await connection.execute("UPDATE project_folders SET quality_tier = 'medium' WHERE quality_tier IS NULL;")
                logger.info("Updated existing folders with default 'medium' tier")
                
                # Create index for quality_tier column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_quality_tier ON project_folders(quality_tier);")
                logger.info("Created index for quality_tier column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding quality_tier column: {e}")
                    raise e
            
            # Add custom_rate column to project_folders table
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS custom_rate INTEGER DEFAULT 200;")
                logger.info("Added custom_rate column to project_folders table")
                
                # Update existing folders to have default custom_rate if they don't have one
                await connection.execute("UPDATE project_folders SET custom_rate = 200 WHERE custom_rate IS NULL;")
                logger.info("Updated existing folders with default custom_rate")
                
                # Create index for custom_rate column
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_custom_rate ON project_folders(custom_rate);")
                logger.info("Created index for custom_rate column")
                
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding custom_rate column: {e}")
                    raise e
            
            # Add order column for project sorting
            await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_order ON projects(\"order\");")

            # Add completionTime column to projects table (for the new completion time feature)
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level custom rate and quality tier columns
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to projects table")
                
                # Create indexes for project-level custom rate and quality tier
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_custom_rate ON projects(custom_rate);")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_quality_tier ON projects(quality_tier);")
                logger.info("Created indexes for project-level custom_rate and quality_tier columns")
                
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level custom_rate/quality_tier columns: {e}")
                    raise e
            
            # Add is_advanced and advanced_rates to project_folders for advanced per-product rates
            try:
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS is_advanced BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS advanced_rates JSONB;")
                await connection.execute("ALTER TABLE project_folders ADD COLUMN IF NOT EXISTS completion_times JSONB;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_project_folders_is_advanced ON project_folders(is_advanced);")
                logger.info("Ensured is_advanced, advanced_rates, and completion_times on project_folders")
            except Exception as e:
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding is_advanced/advanced_rates to project_folders: {e}")
                    raise e

            # Add is_advanced and advanced_rates to projects for advanced per-product rates
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_advanced BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS advanced_rates JSONB;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS completion_times JSONB;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_advanced ON projects(is_advanced);")
                logger.info("Ensured is_advanced, advanced_rates, and completion_times on projects")
            except Exception as e:
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding is_advanced/advanced_rates to projects: {e}")
                    raise e
            
            # Add completionTime column to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS completion_time INTEGER;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add other missing columns to trashed_projects table to match projects table schema
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS source_chat_session_id UUID;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS folder_id INTEGER REFERENCES project_folders(id) ON DELETE SET NULL;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS \"order\" INTEGER DEFAULT 0;")
            except Exception as e:
                # Column might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    raise e

            # Add project-level columns to trashed_projects table
            try:
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS custom_rate INTEGER;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS quality_tier TEXT;")
                logger.info("Added custom_rate and quality_tier columns to trashed_projects table")
            except Exception as e:
                # Columns might already exist, which is fine
                if "already exists" not in str(e) and "duplicate column" not in str(e):
                    logger.error(f"Error adding project-level columns to trashed_projects: {e}")
                    raise e

            # CRITICAL FIX: Ensure order and completion_time columns are TEXT type to prevent casting errors
            try:
                logger.info("Applying critical fix: Ensuring order and completion_time columns are TEXT type")
                
                # Fix projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set projects.order and projects.completion_time to TEXT type")
                
                # Fix trashed_projects table - ensure TEXT type
                await connection.execute("""
                    ALTER TABLE trashed_projects 
                    ALTER COLUMN "order" TYPE TEXT,
                    ALTER COLUMN completion_time TYPE TEXT;
                """)
                logger.info("Successfully set trashed_projects.order and trashed_projects.completion_time to TEXT type")
                
                # Set default values for empty strings
                await connection.execute("""
                    UPDATE projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET "order" = '0' WHERE "order" IS NULL OR "order" = '';
                """)
                await connection.execute("""
                    UPDATE trashed_projects 
                    SET completion_time = '0' WHERE completion_time IS NULL OR completion_time = '';
                """)
                logger.info("Successfully set default values for empty order and completion_time fields")
                
            except Exception as e:
                logger.error(f"Error applying critical TEXT type fix: {e}")

            # Final verification - ensure all required columns exist with correct types
            try:
                # Verify projects table schema
                projects_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Projects table schema verification:")
                for row in projects_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
                # Verify trashed_projects table schema
                trashed_schema = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = 'trashed_projects' 
                    AND column_name IN ('order', 'completion_time', 'source_chat_session_id', 'folder_id')
                    ORDER BY column_name;
                """)
                
                logger.info("Trashed_projects table schema verification:")
                for row in trashed_schema:
                    logger.info(f"  {row['column_name']}: {row['data_type']} (nullable: {row['is_nullable']})")
                
            except Exception as e:
                logger.error(f"Error during schema verification: {e}")

            # Create request analytics table
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS request_analytics (
                    id TEXT PRIMARY KEY,
                    endpoint TEXT NOT NULL,
                    method TEXT NOT NULL,
                    user_id TEXT,
                    status_code INTEGER NOT NULL,
                    response_time_ms INTEGER NOT NULL,
                    request_size_bytes INTEGER,
                    response_size_bytes INTEGER,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_created_at ON request_analytics(created_at);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_endpoint ON request_analytics(endpoint);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_user_id ON request_analytics(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_status_code ON request_analytics(status_code);")
            logger.info("'request_analytics' table ensured.")

            # Add AI parser tracking columns to request_analytics table
            try:
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS is_ai_parser_request BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_tokens INTEGER;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_model TEXT;")
                await connection.execute("ALTER TABLE request_analytics ADD COLUMN IF NOT EXISTS ai_parser_project_name TEXT;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_request_analytics_ai_parser ON request_analytics(is_ai_parser_request);")
                logger.info("AI parser tracking columns added to request_analytics table.")
            except Exception as e:
                logger.warning(f"Error adding AI parser columns (may already exist): {e}")

            # Add is_standalone field to projects table to track standalone vs outline-based products
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_standalone ON projects(is_standalone);")
                logger.info("Added is_standalone column to projects table.")
                
                # Add same field to trashed_projects table to match schema
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS is_standalone BOOLEAN DEFAULT NULL;")
                logger.info("Added is_standalone column to trashed_projects table.")
                
                # For legacy support: Set is_standalone = NULL for all existing products
                # This allows the frontend filtering logic to handle legacy products gracefully
                # New products will have this field explicitly set during creation
                logger.info("Legacy support: is_standalone field defaults to NULL for existing products.")
                
            except Exception as e:
                logger.warning(f"Error adding is_standalone column (may already exist): {e}")

            # Add course_id field to projects table to track standalone vs outline-based products
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS course_id INTEGER DEFAULT NULL;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_course_id ON projects(course_id);")
                logger.info("Added course_id column to projects table.")

                # Add same field to trashed_projects table to match schema
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS course_id INTEGER DEFAULT NULL;")
                logger.info("Added course_id column to trashed_projects table.")

                # For legacy support: Set course_id = NULL for all existing products
                # This allows the frontend filtering logic to handle legacy products gracefully
                # New products will have this field explicitly set during creation
                logger.info("Legacy support: course_id field defaults to NULL for existing products.")

            except Exception as e:
                logger.warning(f"Error adding course_id column (may already exist): {e}")

            # ============================
            # SMART DRIVE DATABASE MIGRATIONS
            # ============================
            
            # SmartDrive Accounts: Per-user SmartDrive linkage with individual Nextcloud credentials
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS smartdrive_accounts (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL UNIQUE,
                    nextcloud_username VARCHAR(255),
                    nextcloud_password_encrypted TEXT,
                    nextcloud_base_url VARCHAR(512) DEFAULT 'http://nc1.contentbuilder.ai:8080',
                    sync_cursor JSONB DEFAULT '{}',
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    CONSTRAINT idx_smartdrive_accounts_onyx_user UNIQUE (onyx_user_id)
                );
            """)
            
            # Add new columns to existing tables (migration-safe)
            try:
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_username VARCHAR(255);")
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_password_encrypted TEXT;")
                await connection.execute("ALTER TABLE smartdrive_accounts ADD COLUMN IF NOT EXISTS nextcloud_base_url VARCHAR(512) DEFAULT 'http://nc1.contentbuilder.ai:8080';")
            except Exception as e:
                logger.info(f"Columns may already exist: {e}")
                pass
            # Add encryption helper functions - provide placeholder for old nextcloud_user_id column
            await connection.execute("INSERT INTO smartdrive_accounts (onyx_user_id, nextcloud_username, nextcloud_password_encrypted) VALUES ('system_encryption_key', '__encryption_key__', '__placeholder__') ON CONFLICT (onyx_user_id) DO NOTHING;")
            logger.info("'smartdrive_accounts' table ensured.")

            # SmartDrive Imports: Maps SmartDrive files to Onyx files with etags/checksums
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS smartdrive_imports (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    smartdrive_path VARCHAR(1000) NOT NULL,
                    onyx_file_id VARCHAR(255) NOT NULL,
                    etag VARCHAR(255),
                    checksum VARCHAR(255),
                    file_size BIGINT,
                    mime_type VARCHAR(255),
                    imported_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP WITH TIME ZONE,
                    CONSTRAINT idx_smartdrive_imports_user_path UNIQUE (onyx_user_id, smartdrive_path)
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_onyx_user_id ON smartdrive_imports(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_onyx_file_id ON smartdrive_imports(onyx_file_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_smartdrive_imports_imported_at ON smartdrive_imports(imported_at);")
            logger.info("'smartdrive_imports' table ensured.")

            # User Connectors: Per-user connector configs and encrypted tokens
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_connectors (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    source VARCHAR(100),
                    config JSONB DEFAULT '{}',
                    credentials_encrypted TEXT,
                    status VARCHAR(50) DEFAULT 'active',
                    last_sync_at TIMESTAMP WITH TIME ZONE,
                    last_error TEXT,
                    total_docs_indexed INTEGER DEFAULT 0,
                    onyx_connector_id INTEGER,
                    onyx_credential_id INTEGER,
                    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP
                );
            """)

            # Add new columns to existing tables (migration-safe)
            try:
                await connection.execute("ALTER TABLE user_connectors ADD COLUMN IF NOT EXISTS source VARCHAR(100);")
            except Exception as e:
                logger.info(f"Column 'source' may already exist in user_connectors: {e}")
                pass

            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_onyx_user_id ON user_connectors(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_source ON user_connectors(source);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_status ON user_connectors(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_connectors_created_at ON user_connectors(created_at);")
            logger.info("'user_connectors' table ensured.")

            # --- Ensure offers table ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS offers (
                    id SERIAL PRIMARY KEY,
                    onyx_user_id TEXT NOT NULL,
                    company_id INTEGER REFERENCES project_folders(id) ON DELETE CASCADE,
                    offer_name TEXT NOT NULL,
                    created_on TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    manager TEXT NOT NULL,
                    status TEXT NOT NULL CHECK (status IN (
                        'Draft',
                        'Internal Review', 
                        'Approved',
                        'Sent to Client',
                        'Viewed by Client',
                        'Negotiation',
                        'Accepted',
                        'Rejected',
                        'Archived'
                    )),
                    total_hours INTEGER DEFAULT 0,
                    link TEXT,
                    share_token TEXT UNIQUE,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            await connection.execute("ALTER TABLE offers ADD COLUMN IF NOT EXISTS share_token TEXT;")
            await connection.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_offers_share_token ON offers(share_token) WHERE share_token IS NOT NULL;")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_onyx_user_id ON offers(onyx_user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_company_id ON offers(company_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_status ON offers(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_offers_created_on ON offers(created_on);")
            logger.info("'offers' table ensured.")

            logger.info("Smart Drive database migrations completed successfully.")
            # --- Feature Management Tables ---
            await connection.execute("""
                CREATE TABLE IF NOT EXISTS feature_definitions (
                    id SERIAL PRIMARY KEY,
                    feature_name VARCHAR(100) UNIQUE NOT NULL,
                    display_name VARCHAR(200) NOT NULL,
                    description TEXT,
                    category VARCHAR(100),
                    is_active BOOLEAN DEFAULT true,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_feature_definitions_name ON feature_definitions(feature_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_feature_definitions_active ON feature_definitions(is_active);")
            logger.info("'feature_definitions' table ensured.")

            await connection.execute("""
                CREATE TABLE IF NOT EXISTS user_features (
                    id SERIAL PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    feature_name VARCHAR(100) NOT NULL,
                    is_enabled BOOLEAN DEFAULT false,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    UNIQUE(user_id, feature_name)
                );
            """)
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_user_id ON user_features(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_feature_name ON user_features(feature_name);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_user_features_enabled ON user_features(is_enabled);")
            
            # Migration: Ensure user_id column is TEXT type (not UUID)
            try:
                await connection.execute("""
                    ALTER TABLE user_features 
                    ALTER COLUMN user_id TYPE TEXT USING user_id::TEXT
                """)
                logger.info("Migrated user_features.user_id column to TEXT type.")
            except Exception as e:
                # This might fail if column is already TEXT or if there are no records
                logger.info(f"user_features.user_id column migration skipped (likely already TEXT): {e}")
            
            logger.info("'user_features' table ensured.")

            # Seed initial feature definitions
            try:
                initial_features = [
                    ('ai_audit_templates', 'AI Audit Templates', 'Access to AI-powered audit template generation', 'Templates'),
                    ('deloitte_banner', 'Deloitte Banner', 'Show Deloitte banner on Projects page', 'Branding'),
                    ('offers_tab', 'Offers Tab', 'Access to Offers tab in Projects', 'Navigation'),
                    ('workspace_tab', 'Workspace Tab', 'Access to Workspace tab in Projects', 'Navigation'),
                    ('export_to_lms', 'Export to LMS', 'Access to LMS export tab and functionality', 'Navigation'),
                    ('course_table', 'Course Table', 'Use classic course table (view) instead of new course view (view-new)', 'Navigation'),
                    ('video_lesson', 'Video Lesson', 'Allow creating Video Lessons in Generate page', 'Creation'),
                    ('lesson_draft', 'Lesson Draft', 'Allow creating and viewing Lesson Drafts', 'Creation'),
                    ('event_posters', 'Event Posters', 'Access to Event Poster creation functionality', 'Creation'),
                    ('chudo_market_themes', 'ChudoMarket Themes', 'Access to Chudo, Chudo 2, Forta, and Forta 2 presentation themes', 'Themes'),
                    ('col_assessment_type', 'Column: Assessment Type', 'Shows the Assessment Type column', 'Columns'),
                    ('col_content_volume', 'Column: Content Volume', 'Shows the Content Volume column', 'Columns'),
                    ('col_source', 'Column: Source', 'Shows the Source column', 'Columns'),
                    ('col_est_creation_time', 'Column: Est. Creation Time', 'Shows the Est. Creation Time column', 'Columns'),
                    ('col_est_completion_time', 'Column: Est. Completion Time', 'Shows the Est. Completion Time column', 'Columns'),
                    ('col_quality_tier', 'Column: Quality Tier', 'Shows the Quality Tier column', 'Columns'),
                    ('col_quiz', 'Column: Quiz', 'Shows the Quiz column', 'Columns'),
                    ('col_one_pager', 'Column: One-Pager', 'Shows the One-Pager column', 'Columns'),
                    ('col_video_presentation', 'Column: Video Lesson', 'Shows the Video Lesson column', 'Columns'),
                    ('col_lesson_presentation', 'Column: Presentation', 'Shows the Presentation column', 'Columns'),
                    ('export_scorm_2004', 'Export to SCORM 2004', 'Enable SCORM 2004 ZIP export button in course view', 'Exports'),
                    ('is_us_lms', 'LMS: Use US (.io)', 'If enabled (and DEV disabled), use https://app.smartexpert.io for LMS requests', 'LMS'),
                    ('is_dev_lms', 'LMS: Use DEV (.net dev)', 'If enabled, always use https://dev.smartexpert.net for LMS requests (overrides US/EU)', 'LMS'),
                    ('is_chudomaket', 'LMS: Use Chudomaket', 'Override and use https://lms.toliman.com.ua for all LMS requests', 'LMS'),
                    # SmartDrive Connector visibility flags (hidden by default unless enabled per user)
                    ('connector_s3', 'Amazon S3 (SmartDrive)', 'Show Amazon S3 connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_r2', 'Cloudflare R2 (SmartDrive)', 'Show Cloudflare R2 connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_google_cloud_storage', 'Google Cloud Storage (SmartDrive)', 'Show Google Cloud Storage connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_oci_storage', 'Oracle Cloud Storage (SmartDrive)', 'Show Oracle Cloud Storage connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_sharepoint', 'SharePoint (SmartDrive)', 'Show SharePoint connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_teams', 'Microsoft Teams (SmartDrive)', 'Show Microsoft Teams connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_discourse', 'Discourse (SmartDrive)', 'Show Discourse connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_gong', 'Gong (SmartDrive)', 'Show Gong connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_axero', 'Axero (SmartDrive)', 'Show Axero connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_mediawiki', 'MediaWiki (SmartDrive)', 'Show MediaWiki connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_bookstack', 'BookStack (SmartDrive)', 'Show BookStack connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_guru', 'Guru (SmartDrive)', 'Show Guru connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_slab', 'Slab (SmartDrive)', 'Show Slab connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_linear', 'Linear (SmartDrive)', 'Show Linear connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_highspot', 'Highspot (SmartDrive)', 'Show Highspot connector card on SmartDrive', 'SmartDrive Connectors'),
                    ('connector_loopio', 'Loopio (SmartDrive)', 'Show Loopio connector card on SmartDrive', 'SmartDrive Connectors'),
                ]

                for feature_name, display_name, description, category in initial_features:
                    await connection.execute("""
                        INSERT INTO feature_definitions (feature_name, display_name, description, category)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (feature_name) DO UPDATE SET
                            display_name = EXCLUDED.display_name,
                            description = EXCLUDED.description,
                            category = EXCLUDED.category,
                            is_active = true
                    """, feature_name, display_name, description, category)
                
                logger.info(f"Seeded {len(initial_features)} feature definitions.")
                
                # Deactivate unused features that are not wired
                unused_features = [
                    'advanced_analytics', 'bulk_operations', 'premium_support', 
                    'beta_features', 'api_access', 'custom_themes', 'advanced_export'
                ]
                
                for feature_name in unused_features:
                    await connection.execute("""
                        UPDATE feature_definitions 
                        SET is_active = false 
                        WHERE feature_name = $1
                    """, feature_name)
                
                logger.info(f"Deactivated {len(unused_features)} unused feature definitions.")
            except Exception as e:
                logger.warning(f"Error seeding feature definitions (may already exist): {e}")

            # Create feature entries for existing users (defaults: enable LMS flags)
            try:
                users = await connection.fetch("SELECT onyx_user_id FROM user_credits")
                
                if users:
                    # Get all active feature names
                    feature_names = await connection.fetch("SELECT feature_name FROM feature_definitions WHERE is_active = true")

                    # Feature names that should default to TRUE
                    default_true_features = { 'is_us_lms', 'is_dev_lms' }
                    
                    for user in users:
                        user_id = user['onyx_user_id']
                        for feature_row in feature_names:
                            feature_name = feature_row['feature_name']
                            is_enabled_default = feature_name in default_true_features
                            await connection.execute("""
                                INSERT INTO user_features (user_id, feature_name, is_enabled)
                                VALUES ($1, $2, $3)
                                ON CONFLICT (user_id, feature_name) DO NOTHING
                            """, user_id, feature_name, is_enabled_default)
                    
                    logger.info(f"Created feature entries for {len(users)} existing users with LMS defaults enabled.")
            except Exception as e:
                logger.warning(f"Error creating user feature entries (may already exist): {e}")

            # Add audit sharing fields to projects table
            try:
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS share_token UUID DEFAULT NULL;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS is_public BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS shared_at TIMESTAMP WITH TIME ZONE DEFAULT NULL;")
                await connection.execute("ALTER TABLE projects ADD COLUMN IF NOT EXISTS expires_at TIMESTAMP WITH TIME ZONE DEFAULT NULL;")
                await connection.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_projects_share_token ON projects(share_token) WHERE share_token IS NOT NULL;")
                await connection.execute("CREATE INDEX IF NOT EXISTS idx_projects_is_public ON projects(is_public);")
                logger.info("Added audit sharing columns to projects table.")
                
                # Add same fields to trashed_projects table to match schema
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS share_token UUID DEFAULT NULL;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS is_public BOOLEAN DEFAULT FALSE;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS shared_at TIMESTAMP WITH TIME ZONE DEFAULT NULL;")
                await connection.execute("ALTER TABLE trashed_projects ADD COLUMN IF NOT EXISTS expires_at TIMESTAMP WITH TIME ZONE DEFAULT NULL;")
                logger.info("Added audit sharing columns to trashed_projects table.")
                
            except Exception as e:
                logger.warning(f"Error adding audit sharing columns (may already exist): {e}")

            # Migrate existing audits to use dedicated microproduct_type
            try:
                async with DB_POOL.acquire() as conn:
                    # Update existing audits that have AI audit names but wrong microproduct_type
                    result = await conn.execute("""
                        UPDATE projects 
                        SET product_type = 'AI Audit', microproduct_type = 'AI Audit'
                        WHERE (project_name LIKE '%AI-Аудит%' OR project_name LIKE '%AI-Audit%')
                        AND microproduct_type != 'AI Audit'
                    """)
                    logger.info(f"Updated {result.split()[-1]} existing audits to use 'AI Audit' microproduct_type")
                    
                    # Also update trashed_projects for consistency
                    await conn.execute("""
                        UPDATE trashed_projects 
                        SET product_type = 'AI Audit', microproduct_type = 'AI Audit'
                        WHERE (project_name LIKE '%AI-Аудит%' OR project_name LIKE '%AI-Audit%')
                        AND microproduct_type != 'AI Audit'
                    """)
                    logger.info("Updated trashed audits to use 'AI Audit' microproduct_type")
                    
            except Exception as e:
                logger.warning(f"Error migrating existing audits (may already be updated): {e}")

            logger.info("Database schema migration completed successfully.")
    except Exception as e:
        logger.critical(f"Failed to initialize custom DB pool or ensure tables: {e}", exc_info=not IS_PRODUCTION)
        DB_POOL = None

@app.on_event("shutdown")
async def shutdown_event():
    if DB_POOL:
        await DB_POOL.close()
        logger.info("Custom projects DB pool closed.")

effective_origins = list(set(filter(None, [
    "http://localhost:3001",
    "http://143.198.59.56:3001",
    "http://143.198.59.56:8088",
    os.environ.get("WEB_DOMAIN", "http://localhost:3000"),
    settings.CUSTOM_FRONTEND_URL if 'settings' in globals() and hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get("CUSTOM_FRONTEND_URL", "http://custom_frontend:3001")
])))
if not effective_origins: effective_origins = ["http://localhost:3001"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=effective_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StatusInfo(BaseModel):
    type: str = "unknown"
    text: str = ""
    model_config = {"from_attributes": True}

# Credits Models
class UserCredits(BaseModel):
    id: int
    onyx_user_id: str
    name: str
    credits_balance: int
    total_credits_used: int
    credits_purchased: int
    last_purchase_date: Optional[datetime]
    subscription_tier: str
    created_at: datetime
    updated_at: datetime
    model_config = {"from_attributes": True}

class CreditTransactionRequest(BaseModel):
    user_email: str
    amount: int
    action: Literal["add", "remove"]
    reason: Optional[str] = "Admin adjustment"

class CreditTransactionResponse(BaseModel):
    success: bool
    message: str
    new_balance: int
    user_credits: UserCredits

# Offers Models
class OfferBase(BaseModel):
    company_id: int
    offer_name: str
    manager: str
    status: str
    total_hours: int = 0
    # link is auto-generated, not provided in create requests

class OfferCreate(OfferBase):
    pass

class OfferUpdate(BaseModel):
    company_id: Optional[int] = None
    offer_name: Optional[str] = None
    manager: Optional[str] = None
    status: Optional[str] = None
    total_hours: Optional[int] = None
    created_on: Optional[datetime] = None
    # link is auto-generated and not editable

class OfferResponse(OfferBase):
    id: int
    onyx_user_id: str
    created_on: datetime
    created_at: datetime
    updated_at: datetime
    company_name: str  # Joined from project_folders

class OfferListResponse(BaseModel):
    offers: List[OfferResponse]
    total_count: int

# NEW: Analytics/timeline models
class ProductUsage(BaseModel):
    product_type: str
    credits_used: int

class CreditUsageAnalyticsResponse(BaseModel):
    usage_by_product: List[ProductUsage]
    total_credits_used: int

class TemplateTypeUsage(BaseModel):
    template_id: str
    total_generated: int
    client_count: int
    error_count: int
    last_usage: str

class SlidesAnalyticsResponse(BaseModel):
    usage_by_template: List[TemplateTypeUsage]

class SlideGenerationError(BaseModel):
    id: int
    user_id: str
    template_id: str
    props: Dict[str, Any]
    error_message: str
    created_at: datetime

class SlidesErrorsAnalyticsResponse(BaseModel):
    errors: List[SlideGenerationError]

class QuestionnaireAnswer(BaseModel):
    question: str
    answer: str

class UserQuestionnaire(BaseModel):
    onyx_user_id: str
    answers: List[QuestionnaireAnswer]

class UserQuestionnaireInsertRequest(BaseModel):
    onyx_user_id: str
    answers: List[QuestionnaireAnswer]

class TimelineActivity(BaseModel):
    id: str
    type: Literal['purchase', 'product_generation', 'admin_removal']
    title: str
    credits: int
    timestamp: datetime
    product_type: Optional[str] = None

class UserTransactionHistoryResponse(BaseModel):
    user_id: int
    user_email: str
    user_name: str
    transactions: List[TimelineActivity]

class LessonDetail(BaseModel):
    title: str
    check: StatusInfo = Field(default_factory=StatusInfo)
    contentAvailable: StatusInfo = Field(default_factory=StatusInfo)
    source: str = ""
    hours: int = 0
    completionTime: str = ""  # Estimated completion time in minutes (e.g., "5m", "6m", "7m", "8m")
    custom_rate: Optional[int] = None  # Individual lesson-level custom rate override
    quality_tier: Optional[str] = None  # Individual lesson-level quality tier override
    recommended_content_types: Optional[Dict[str, Any]] = None
    model_config = {"from_attributes": True}

class SectionDetail(BaseModel):
    id: str
    title: str
    totalHours: int = 0
    totalCompletionTime: Optional[int] = None  # Total completion time in minutes for the section
    lessons: List[LessonDetail] = Field(default_factory=list)
    autoCalculateHours: bool = True
    custom_rate: Optional[int] = None  # Module-level custom rate override
    quality_tier: Optional[str] = None  # Module-level quality tier override
    model_config = {"from_attributes": True}

class TrainingPlanDetails(BaseModel):
    mainTitle: Optional[str] = None
    sections: List[SectionDetail] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    # Store user preferences on which optional columns to show in UI (frontend reads this)
    displayOptions: Optional[Dict[str, bool]] = None  # e.g., {"knowledgeCheck": true, "contentAvailability": false}
    # Store theme selection for styling
    theme: Optional[str] = "cherry"  # Default theme
    model_config = {"from_attributes": True}

AnyContentBlock = Union["HeadlineBlock", "ParagraphBlock", "BulletListBlock", "NumberedListBlock", "AlertBlock", "SectionBreakBlock"]
ListItem = Union[str, AnyContentBlock, List[AnyContentBlock]]

class BaseContentBlock(BaseModel):
    type: str
    model_config = {"from_attributes": True}

class HeadlineBlock(BaseContentBlock):
    type: str = "headline"
    level: int = Field(ge=1, le=4)
    text: str
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    textColor: Optional[str] = None
    isImportant: Optional[bool] = Field(default=False, description="Set to true if this headline (typically Level 4) and its immediately following single block (list or paragraph) form an important section to be visually boxed.")

class ParagraphBlock(BaseContentBlock):
    type: str = "paragraph"
    text: str
    isRecommendation: Optional[bool] = Field(default=False, description="Set to true if this paragraph is a 'recommendation' within a numbered list item, to be styled distinctly.")

class BulletListBlock(BaseContentBlock):
    type: Literal['bullet_list'] = 'bullet_list'
    items: List[ListItem] = []
    iconName: Optional[str] = None

class NumberedListBlock(BaseContentBlock):
    type: Literal['numbered_list'] = 'numbered_list'
    items: List[ListItem] = []

class AlertBlock(BaseContentBlock):
    type: str = "alert"
    title: Optional[str] = None
    text: str
    alertType: str = "info"
    iconName: Optional[str] = None
    backgroundColor: Optional[str] = None
    borderColor: Optional[str] = None
    textColor: Optional[str] = None
    iconColor: Optional[str] = None

class SectionBreakBlock(BaseContentBlock):
    type: str = "section_break"
    style: Optional[str] = "solid"

AnyContentBlockValue = Union[
    HeadlineBlock, ParagraphBlock, BulletListBlock, NumberedListBlock, AlertBlock, SectionBreakBlock, TableBlock, ImageBlock
]

class PdfLessonDetails(BaseModel):
    lessonTitle: str
    # Optional: sequential number of the lesson inside the parent Training Plan
    lessonNumber: Optional[int] = None
    contentBlocks: List[AnyContentBlockValue] = Field(default_factory=list)
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonSlideData(BaseModel):
    slideId: str
    slideNumber: int
    slideTitle: str
    displayedText: Optional[str] = None
    imagePath: Optional[str] = None
    videoPath: Optional[str] = None
    voiceoverText: Optional[str] = None
    displayedPictureDescription: Optional[str] = None
    displayedVideoDescription: Optional[str] = None
    model_config = {"from_attributes": True}

class VideoLessonData(BaseModel):
    mainPresentationTitle: str
    slides: List[VideoLessonSlideData] = Field(default_factory=list)
    currentSlideId: Optional[str] = None # To store the active slide from frontend
    lessonNumber: Optional[int] = None  # Sequential number in Training Plan
    detectedLanguage: Optional[str] = None
    model_config = {"from_attributes": True}


# --- Start: Add New Quiz Models ---

class QuizQuestionOption(BaseModel):
    id: str  # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingPrompt(BaseModel):
    id: str # e.g., "A", "B", "C"
    text: str
    model_config = {"from_attributes": True}

class MatchingOption(BaseModel):
    id: str # e.g., "1", "2", "3"
    text: str
    model_config = {"from_attributes": True}

class SortableItem(BaseModel):
    id: str # e.g., "step1", "step2"
    text: str
    model_config = {"from_attributes": True}

class BaseQuestion(BaseModel):
    question_text: str
    explanation: Optional[str] = None
    model_config = {"from_attributes": True}

class MultipleChoiceQuestion(BaseQuestion):
    question_type: Literal["multiple-choice"]
    options: List[QuizQuestionOption]
    correct_option_id: str
    model_config = {"from_attributes": True}

class MultiSelectQuestion(BaseQuestion):
    question_type: Literal["multi-select"]
    options: List[QuizQuestionOption]
    correct_option_ids: List[str]
    model_config = {"from_attributes": True}

class MatchingQuestion(BaseQuestion):
    question_type: Literal["matching"]
    prompts: List[MatchingPrompt]
    options: List[MatchingOption]
    correct_matches: Dict[str, str]  # Maps prompt.id to option.id, e.g. {"A": "3", "B": "1"}
    model_config = {"from_attributes": True}

class SortingQuestion(BaseQuestion):
    question_type: Literal["sorting"]
    items_to_sort: List[SortableItem]
    correct_order: List[str]  # List of SortableItem.id in the correct sequence
    model_config = {"from_attributes": True}

class OpenAnswerQuestion(BaseQuestion):
    question_type: Literal["open-answer"]
    acceptable_answers: List[str]
    model_config = {"from_attributes": True}

AnyQuizQuestion = Union[
    MultipleChoiceQuestion,
    MultiSelectQuestion,
    MatchingQuestion,
    SortingQuestion,
    OpenAnswerQuestion
]


# +++ NEW MODEL FOR AI AUDIT LANDING +++
class AIAuditLandingDetails(BaseModel):
    projectId: int
    projectName: str
    companyName: str
    companyDescription: str
    jobPositions: List[dict] = Field(default_factory=list)
    workforceCrisis: dict = Field(default_factory=dict)
    courseOutlineModules: List[dict] = Field(default_factory=list)
    courseTemplates: List[dict] = Field(default_factory=list)
    language: Optional[str] = None
    model_config = {"from_attributes": True}
# +++ END NEW MODEL +++

MicroProductContentType = Union[TrainingPlanDetails, PdfLessonDetails, VideoLessonData, SlideDeckDetails, QuizData, TextPresentationDetails, AIAuditLandingDetails, Dict[str, Any], None]

class DesignTemplateBase(BaseModel):
    template_name: str
    template_structuring_prompt: str
    microproduct_type: str
    component_name: str
    model_config = {"from_attributes": True}

class DesignTemplateCreate(DesignTemplateBase):
    design_image_path: Optional[str] = None

class DesignTemplateUpdate(BaseModel):
    template_name: Optional[str] = None
    template_structuring_prompt: Optional[str] = None
    microproduct_type: Optional[str] = None
    component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class DesignTemplateResponse(DesignTemplateBase):
    id: int
    design_image_path: Optional[str] = None
    date_created: datetime

class ProjectCreateRequest(BaseModel):
    projectName: str
    design_template_id: int
    microProductName: Optional[str] = None
    aiResponse: str
    chatSessionId: Optional[uuid.UUID] = None
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    folder_id: Optional[int] = None  # Add folder_id for automatic folder assignment
    theme: Optional[str] = None      # Selected theme for presentations
    # Source context tracking
    source_context_type: Optional[str] = None  # 'files', 'connectors', 'knowledge_base', 'text', 'prompt'
    source_context_data: Optional[dict] = None  # JSON data about the source
    model_config = {"from_attributes": True}

class ProjectDB(BaseModel):
    id: int
    onyx_user_id: str
    project_name: str
    product_type: Optional[str] = None
    microproduct_type: Optional[str] = None
    microproduct_name: Optional[str] = None
    microproduct_content: Optional[MicroProductContentType] = None
    design_template_id: Optional[int] = None
    created_at: datetime
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None
    model_config = {"from_attributes": True}

class MicroProductApiResponse(BaseModel):
    name: str
    slug: str
    project_id: int
    design_template_id: int
    component_name: str
    parentProjectName: Optional[str] = None
    webLinkPath: Optional[str] = None
    pdfLinkPath: Optional[str] = None
    details: Optional[MicroProductContentType] = None
    sourceChatSessionId: Optional[uuid.UUID] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    lesson_plan_data: Optional[Dict[str, Any]] = None  # Add lesson plan data field
    model_config = {"from_attributes": True}

class ProjectApiResponse(BaseModel):
    id: int
    projectName: str
    projectSlug: str
    microproduct_name: Optional[str] = None
    design_template_name: Optional[str] = None
    design_microproduct_type: Optional[str] = None
    created_at: datetime
    design_template_id: Optional[int] = None
    folder_id: Optional[int] = None
    order: Optional[int] = None
    source_chat_session_id: Optional[str] = None
    is_standalone: Optional[bool] = None  # Track whether this is standalone or part of an outline
    course_id: Optional[int] = None  # Track associated course ID for non-standalone products
    model_config = {"from_attributes": True}

class ProjectDetailForEditResponse(BaseModel):
    id: int
    projectName: str
    microProductName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductContent: Optional[MicroProductContentType] = None
    createdAt: Optional[datetime] = None
    design_template_name: Optional[str] = None
    design_component_name: Optional[str] = None
    design_image_path: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectUpdateRequest(BaseModel):
    projectName: Optional[str] = None
    design_template_id: Optional[int] = None
    microProductName: Optional[str] = None
    microProductContent: Optional[Dict[str, Any]] = None
    custom_rate: Optional[int] = None
    quality_tier: Optional[str] = None
    model_config = {"from_attributes": True}

class ProjectTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

class ErrorDetail(BaseModel):
    detail: str

class RequestAnalytics(BaseModel):
    id: str
    endpoint: str
    method: str
    user_id: Optional[str] = None
    status_code: int
    response_time_ms: int
    request_size_bytes: Optional[int] = None
    response_size_bytes: Optional[int] = None
    error_message: Optional[str] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class ProjectsDeleteRequest(BaseModel):
    project_ids: List[int]
    scope: Optional[str] = 'self'

class MicroproductPipelineBase(BaseModel):
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool = Field(False, alias="is_prompts_data_collection")
    is_structuring_prompts: bool = Field(False, alias="is_prompts_data_formating")
    discovery_prompts_list: Optional[List[str]] = Field(default_factory=list)
    structuring_prompts_list: Optional[List[str]] = Field(default_factory=list)
    model_config = {"from_attributes": True, "populate_by_name": True}

class MicroproductPipelineCreateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineUpdateRequest(MicroproductPipelineBase):
    pass

class MicroproductPipelineDBRaw(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_prompts_data_collection: bool
    is_prompts_data_formating: bool
    prompts_data_collection: Optional[Dict[str, str]] = None
    prompts_data_formating: Optional[Dict[str, str]] = None
    created_at: datetime
    model_config = {"from_attributes": True}

class MicroproductPipelineGetResponse(BaseModel):
    id: int
    pipeline_name: str
    pipeline_description: Optional[str] = None
    is_discovery_prompts: bool
    is_structuring_prompts: bool
    discovery_prompts_list: List[str] = Field(default_factory=list)
    structuring_prompts_list: List[str] = Field(default_factory=list)
    created_at: datetime
    model_config = {"from_attributes": True}

class DuplicatedProductInfo(BaseModel):
    original_id: int
    new_id: int
    type: str
    name: str

class ProjectDuplicationResponse(BaseModel):
    id: int
    name: str
    type: str
    connected_products: Optional[List[DuplicatedProductInfo]] = None
    total_products_duplicated: int
    model_config = {"from_attributes": True}

    @classmethod
    def from_db_model(cls, db_model: MicroproductPipelineDBRaw) -> "MicroproductPipelineGetResponse":
        discovery_list = [db_model.prompts_data_collection[key] for key in sorted(db_model.prompts_data_collection.keys(), key=int)] if db_model.prompts_data_collection else []
        structuring_list = [db_model.prompts_data_formating[key] for key in sorted(db_model.prompts_data_formating.keys(), key=int)] if db_model.prompts_data_formating else []
        return cls(
            id=db_model.id,
            pipeline_name=db_model.pipeline_name,
            pipeline_description=db_model.pipeline_description,
            is_discovery_prompts=db_model.is_prompts_data_collection,
            is_structuring_prompts=db_model.is_prompts_data_formating,
            discovery_prompts_list=discovery_list,
            structuring_prompts_list=structuring_list,
            created_at=db_model.created_at
        )

# --- Authentication and Utility Functions ---
# async def bing_company_research(company_name: str, company_desc: str) -> str:
#     if not BING_API_KEY:
#         return "(Bing API key not configured)"
#     query = f"{company_name} {company_desc} официальный сайт отзывы новости"
#     headers = {"Ocp-Apim-Subscription-Key": BING_API_KEY}
#     params = {"q": query, "mkt": "ru-RU"}
#     async with httpx.AsyncClient(timeout=15.0) as client:
#         resp = await client.get(BING_API_URL, headers=headers, params=params)
#         resp.raise_for_status()
#         data = resp.json()
#     # Extract summary: snippet, knowledge panel, news, etc.
#     snippets = []
#     if "webPages" in data:
#         for item in data["webPages"].get("value", [])[:3]:
#             snippets.append(item.get("snippet", ""))
#     if "entities" in data and data["entities"].get("value"):
#         for ent in data["entities"]["value"]:
#             if ent.get("description"):
#                 snippets.append(ent["description"])
#     if "news" in data and data["news"].get("value"):
#         for news in data["news"]["value"][:2]:
#             snippets.append(f"Новость: {news.get('name', '')} — {news.get('description', '')}")
#     return "\n".join(snippets)

async def serpapi_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    """
    Uses SerpAPI to gather:
    - General company info (snippets, knowledge panel, about, etc.)
    - Website-specific info (site: queries)
    - Open job listings (site:company_website jobs/careers, and generic queries)
    Returns a structured string with all findings.
    """
    url = "https://serpapi.com/search.json"
    async with httpx.AsyncClient(timeout=20.0) as client:
        # 1. General company info
        search_query = company_name
        if company_desc and company_desc.strip():
            search_query = f"{company_name} {company_desc}"
        
        params_general = {
            "q": search_query,
            "engine": "google",
            "api_key": SERPAPI_KEY,
            "hl": "ru"
        }
        try:
            resp = await client.get(url, params=params_general)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            logger.error(f"❌ [SERPAPI] Error in general search: {e}")
            # If general search fails, try with just the company name
            params_general["q"] = company_name
        resp = await client.get(url, params=params_general)
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if "organic_results" in data:
            for item in data["organic_results"][:3]:
                if "snippet" in item:
                    general_snippets.append(item["snippet"])
        if "knowledge_graph" in data:
            kg = data["knowledge_graph"]
            if "description" in kg:
                general_snippets.append(kg["description"])
            if "title" in kg:
                general_snippets.append(f"Название: {kg['title']}")
            if "type" in kg:
                general_snippets.append(f"Тип: {kg['type']}")
            if "website" in kg:
                general_snippets.append(f"Сайт: {kg['website']}")
            if "address" in kg:
                general_snippets.append(f"Адрес: {kg['address']}")
            if "phone" in kg:
                general_snippets.append(f"Телефон: {kg['phone']}")
        general_info = "\n".join(general_snippets) or "(Нет релевантных данных SerpAPI)"

        # 2. Website-specific info
        website_info = ""
        if company_website:
            params_site = {
                "q": f"site:{company_website} о компании информация контакты",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp2 = await client.get(url, params=params_site)
            resp2.raise_for_status()
            data2 = resp2.json()
            site_snippets = []
            if "organic_results" in data2:
                for item in data2["organic_results"][:3]:
                    if "snippet" in item:
                        site_snippets.append(item["snippet"])
            website_info = "\n".join(site_snippets) or "(Нет информации по сайту)"

        # 3. Open job listings (site:company_website + jobs/careers)
        jobs_info = ""
        jobs_snippets = []
        if company_website:
            # Try site:company_website jobs/careers
            params_jobs_site = {
                "q": f"site:{company_website} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp3 = await client.get(url, params=params_jobs_site)
            resp3.raise_for_status()
            data3 = resp3.json()
            if "organic_results" in data3:
                for item in data3["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        # If not enough, try generic company_name + jobs
        if len(jobs_snippets) < 2:
            params_jobs_generic = {
                "q": f"{company_name} вакансии OR careers OR jobs OR работа",
                "engine": "google",
                "api_key": SERPAPI_KEY,
                "hl": "ru"
            }
            resp4 = await client.get(url, params=params_jobs_generic)
            resp4.raise_for_status()
            data4 = resp4.json()
            if "organic_results" in data4:
                for item in data4["organic_results"][:5]:
                    title = item.get("title", "")
                    link = item.get("link", "")
                    snippet = item.get("snippet", "")
                    jobs_snippets.append(f"{title}\n{snippet}\n{link}")
        jobs_info = "\n\n".join(jobs_snippets) or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[SerpAPI General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def openai_company_research(company_name: str, company_desc: str, company_website: str, language: str = "en") -> str:
    """
    Uses OpenAI Responses API with the web search tool to gather:
    - General company info (with citations)
    - Website-specific info (site: queries)
    - Open job listings (careers/jobs)
    Returns a structured string matching existing format.
    """
    try:
        client = get_openai_client()
        model = LLM_DEFAULT_MODEL or "gpt-4o-mini"

        # Build a concise instruction with consistent output formatting
        instruction = (
            f"Research company '{company_name}'. Language: {language}.\n"
            f"Website: {company_website or 'N/A'}. Description: {company_desc or 'N/A'}.\n\n"
            "Tasks:\n"
            "1) General company info with 1-3 bullet highlights and citations.\n"
            "2) From the official site, summarize 'about/contact' info (use site: queries), include URLs.\n"
            "3) Find current open roles (careers/jobs) with titles + links (max 5).\n\n"
            "Output EXACTLY these sections and nothing else:\n"
            "[SerpAPI General Info]\n...\n\n[Website Info]\n...\n\n[Open Positions]\n...\n"
        )

        # Configure web search tool (preview) – use simplest form per official docs
        # Note: filters/allowed_domains not supported in current API
        tools = [
            {"type": "web_search_preview"}
        ]

        # Use gpt-4o or gpt-4o-mini (web_search_preview supported models)
        # Override model if it's not compatible
        if model not in ["gpt-4o", "gpt-4o-mini", "gpt-4.1"]:
            model = "gpt-4o-mini"
            logger.info(f"[OPENAI_WEB_SEARCH] Overriding model to {model} for web_search_preview compatibility")

        resp = await client.responses.create(
            model=model,
            tools=tools,
            input=instruction
        )

        # Prefer output_text convenience; fallback to manual extraction
        text = getattr(resp, "output_text", None)
        if not text:
            try:
                # SDK returns .output as a list of content parts
                parts = []
                for item in getattr(resp, "output", []) or []:
                    if isinstance(item, dict):
                        # text segments might be under item["content"][...]["text"]
                        for c in item.get("content", []) or []:
                            if isinstance(c, dict) and c.get("type") == "output_text":
                                parts.append(c.get("text", ""))
                    else:
                        parts.append(str(item))
                text = "\n".join(p for p in parts if p)
            except Exception:
                text = None

        if text:
            return text

        # Fallback minimal message if response parsing failed
        return "[SerpAPI General Info]\n(No data)\n\n[Website Info]\n(No data)\n\n[Open Positions]\n(No data)"
    except Exception as e:
        logger.error(f"❌ [OPENAI_WEB_SEARCH] Error: {e}")
        # Re-raise to allow caller to fallback
        raise

async def company_research(company_name: str, company_desc: str, company_website: str, language: str = "en") -> str:
    """
    Abstraction over research providers. Uses OpenAI web search when enabled,
    falls back to SerpAPI on error or when disabled.
    """
    if USE_OPENAI_WEB_SEARCH:
        try:
            logger.info("[RESEARCH] Using OpenAI web search path")
            return await openai_company_research(company_name, company_desc, company_website, language)
        except Exception as e:
            logger.warning(f"[RESEARCH] OpenAI web search failed, falling back to SerpAPI: {e}")

    logger.info("[RESEARCH] Using SerpAPI path")
    return await serpapi_company_research(company_name, company_desc, company_website)

async def duckduckgo_company_research(company_name: str, company_desc: str, company_website: str) -> str:
    # Step 1: General info
    general_query = f"{company_name} {company_desc} официальный сайт отзывы новости"
    url = "https://api.duckduckgo.com/"
    params = {
        "format": "json",
        "no_redirect": 1,
        "no_html": 1,
        "skip_disambig": 1,
    }
    async with httpx.AsyncClient(timeout=10.0) as client:
        # General info
        resp = await client.get(url, params={**params, "q": general_query})
        resp.raise_for_status()
        data = resp.json()
        general_snippets = []
        if data.get("AbstractText"):
            general_snippets.append(data["AbstractText"])
        if data.get("RelatedTopics"):
            for topic in data["RelatedTopics"]:
                if isinstance(topic, dict) and topic.get("Text"):
                    general_snippets.append(topic["Text"])
        if data.get("Answer"):
            general_snippets.append(data["Answer"])
        if data.get("Definition"):
            general_snippets.append(data["Definition"])
        general_info = "\n".join([s for s in general_snippets if s]).strip() or "(Нет релевантных данных DuckDuckGo)"

        # Step 2: Info about the company website
        website_info = ""
        if company_website:
            website_query = f"site:{company_website} о компании информация контакты"
            resp2 = await client.get(url, params={**params, "q": website_query})
            resp2.raise_for_status()
            data2 = resp2.json()
            website_snippets = []
            if data2.get("AbstractText"):
                website_snippets.append(data2["AbstractText"])
            if data2.get("RelatedTopics"):
                for topic in data2["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        website_snippets.append(topic["Text"])
            if data2.get("Answer"):
                website_snippets.append(data2["Answer"])
            if data2.get("Definition"):
                website_snippets.append(data2["Definition"])
            website_info = "\n".join([s for s in website_snippets if s]).strip() or "(Нет информации по сайту)"

        # Step 3: Open positions
        jobs_info = ""
        if company_website:
            jobs_query = f"site:{company_website} вакансии OR careers OR jobs"
            resp3 = await client.get(url, params={**params, "q": jobs_query})
            resp3.raise_for_status()
            data3 = resp3.json()
            jobs_snippets = []
            if data3.get("AbstractText"):
                jobs_snippets.append(data3["AbstractText"])
            if data3.get("RelatedTopics"):
                for topic in data3["RelatedTopics"]:
                    if isinstance(topic, dict) and topic.get("Text"):
                        jobs_snippets.append(topic["Text"])
            if data3.get("Answer"):
                jobs_snippets.append(data3["Answer"])
            if data3.get("Definition"):
                jobs_snippets.append(data3["Definition"])
            jobs_info = "\n".join([s for s in jobs_snippets if s]).strip() or "(Нет информации о вакансиях)"

    # Combine all
    combined = (
        f"[DuckDuckGo General Info]\n{general_info}\n\n"
        f"[Website Info]\n{website_info}\n\n"
        f"[Open Positions]\n{jobs_info}"
    )
    return combined

async def get_current_onyx_user_id(request: Request) -> str:
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: return dev_user_id
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id)
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_id: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def get_current_onyx_user_with_email(request: Request) -> tuple[str, str]:
    """Get both user ID and email from Onyx"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            return str(onyx_user_id), user_email
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in get_current_onyx_user_with_email: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def verify_admin_user(request: Request) -> tuple[str, str]:
    """Verify that the current user is an admin using Onyx's built-in role system"""
    session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
    if not session_cookie_value:
        dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
        if dev_user_id and not IS_PRODUCTION: 
            return dev_user_id, "dev@example.com"  # Dev fallback
        detail_msg = "Authentication required." if IS_PRODUCTION else f"Onyx session cookie '{ONYX_SESSION_COOKIE_NAME}' missing."
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=detail_msg)

    onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            
            onyx_user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email", "")
            user_role = user_data.get("role", "")
            
            if not onyx_user_id:
                logger.error("Could not extract user ID from Onyx user data.")
                detail_msg = "User ID extraction failed." if IS_PRODUCTION else "Could not extract user ID from Onyx."
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
            # Check if user has admin role in Onyx
            if user_role != "admin":
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN, 
                    detail="Access denied. Admin privileges required."
                )
            
            return str(onyx_user_id), user_email
            
    except httpx.HTTPStatusError as e:
        logger.error(f"Onyx API '{onyx_user_info_url}' call failed. Status: {e.response.status_code}, Response: {e.response.text[:500]}", exc_info=not IS_PRODUCTION)
        detail_msg = "Onyx user validation failed." if IS_PRODUCTION else f"Onyx user validation failed ({e.response.status_code})."
        raise HTTPException(status_code=e.response.status_code, detail=detail_msg)
    except httpx.RequestError as e:
        logger.error(f"Error requesting user info from Onyx '{onyx_user_info_url}': {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not connect to Onyx auth service." if IS_PRODUCTION else f"Could not connect to Onyx auth service: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=detail_msg)
    except Exception as e:
        logger.error(f"Unexpected error in verify_admin_user: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Internal user validation error." if IS_PRODUCTION else f"Internal user validation error: {str(e)[:100]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

def create_slug(text: Optional[str]) -> str:
    if not text: return "default-slug"
    text_processed = str(text).lower()
    text_processed = re.sub(r'\s+', '-', text_processed)
    text_processed = re.sub(r'[^\wа-яёa-z0-9\-]+', '', text_processed, flags=re.UNICODE | re.IGNORECASE)
    return text_processed or "generated-slug"

def get_tier_ratio(tier: str) -> int:
    """Get the creation hours ratio for a given tier (legacy support)"""
    ratios = {
        'starter': 120,
        'medium': 200,
        'advanced': 320,
        'professional': 450,
        'basic': 100,
        'interactive': 200,
        'immersive': 700
    }
    return ratios.get(tier, 200)  # Default to medium (200) if tier not found

def calculate_creation_hours(completion_time_minutes: int, custom_rate: int) -> int:
    """Calculate creation hours based on completion time and custom rate, rounded to nearest integer"""
    if completion_time_minutes <= 0:
        return 0
    
    # Convert completion time from minutes to hours, then multiply by custom rate
    completion_hours = completion_time_minutes / 60.0
    creation_hours = completion_hours * custom_rate
    return round(creation_hours)


def analyze_lesson_content_recommendations(lesson_title: str, quality_tier: Optional[str], existing_content: Optional[Dict[str, bool]] = None) -> Dict[str, Any]:
    """Smart, robust combo recommendations per tier.
    Returns a "primary" list of product types composing the chosen combo.
    Types: 'one-pager' | 'presentation' | 'quiz' | 'video-lesson'
    """
    import hashlib

    if existing_content is None:
        existing_content = {}

    title = (lesson_title or "").strip().lower()
    tier = (quality_tier or "interactive").strip().lower()

    # Keyword signals
    kw_one_pager = ["introduction", "overview", "basics", "summary", "quick", "reference", "primer", "cheatsheet"]
    kw_presentation = ["tutorial", "step-by-step", "process", "method", "workflow", "guide", "how to", "how-to", "walkthrough"]
    kw_video = ["demo", "walkthrough", "show", "demonstrate", "visual", "hands-on", "practical", "screencast", "recording"]
    kw_quiz = ["test", "check", "verify", "practice", "exercise", "assessment", "evaluation", "quiz"]

    def score_for(keys: list[str]) -> float:
        hits = sum(1 for k in keys if k in title)
        return min(1.0, hits / 3.0)  # saturate after 3 hits

    s_one = score_for(kw_one_pager)
    s_pres = score_for(kw_presentation)
    s_vid = score_for(kw_video)
    s_quiz = score_for(kw_quiz)

    # Deterministic variety seed per lesson
    seed_val = int(hashlib.sha1(f"{title}|{tier}".encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF

    # Define candidate combos per tier
    # combos are arrays of product types constituting the recommendation
    if tier == "basic":
        combos = [
            ["one-pager"],
            ["presentation"],
        ]
        # weights prefer brevity/overview to one-pager, procedural to presentation
        weights = [
            0.55 + 0.35 * s_one - 0.10 * s_pres,
            0.45 + 0.35 * s_pres - 0.10 * s_one,
        ]
    elif tier == "interactive":
        combos = [
            ["presentation", "quiz"],
            ["presentation"],
            ["one-pager", "quiz"],
        ]
        weights = [
            0.40 + 0.30 * s_pres + 0.30 * s_quiz,  # pres+quiz
            0.30 + 0.50 * s_pres - 0.10 * s_quiz,  # pres
            0.30 + 0.40 * s_one + 0.30 * s_quiz,   # one+quiz
        ]
    elif tier == "advanced":
        combos = [
            ["presentation", "quiz"],
            ["video-lesson", "quiz"],
        ]
        weights = [
            0.50 + 0.30 * s_pres + 0.20 * s_quiz,
            0.50 + 0.40 * s_vid + 0.20 * s_quiz,
        ]
    else:  # immersive
        combos = [
            ["video-lesson", "quiz"],
            ["video-lesson"],
        ]
        weights = [
            0.60 + 0.25 * s_vid + 0.15 * s_quiz,
            0.40 + 0.60 * s_vid - 0.10 * s_quiz,
        ]

    # Normalize weights, add small hash-based jitter for deterministic variety
    eps = 1e-6
    jitter = [(i + 1) * 0.0005 * seed_val for i in range(len(weights))]
    norm_weights = [max(eps, w + jitter[i]) for i, w in enumerate(weights)]

    # Sort combos by weight desc, break ties deterministically
    ranked = sorted(range(len(combos)), key=lambda i: (-norm_weights[i], i))

    # Choose the best combo that doesn’t fully collide with existing content
    chosen: Optional[List[str]] = None
    for idx in ranked:
        c = combos[idx]
        # If combo has two items and one exists, we still propose the remaining one; if all exist, skip.
        missing = [t for t in c if not existing_content.get(t, False)]
        if missing:
            chosen = missing
            break

    # Fallback to the top combo if everything existed (rare)
    if not chosen:
        chosen = combos[ranked[0]]

    return {
        "primary": chosen,
        "reasoning": (
            f"tier={tier}; signals(one={s_one:.2f}, pres={s_pres:.2f}, video={s_vid:.2f}, quiz={s_quiz:.2f}); "
            f"seed={seed_val:.3f}; combos={combos}"
        ),
        "last_updated": datetime.utcnow().isoformat(),
        "quality_tier_used": tier,
    }

# --- Completion time from recommendations ---
PRODUCT_COMPLETION_RANGES = {
    "one-pager": (2, 3),
    "presentation": (5, 10),
    "quiz": (5, 7),
    "video-lesson": (2, 5),
}

def compute_completion_time_from_recommendations(primary_types: list[str]) -> str:
    total = 0
    for p in primary_types:
        r = PRODUCT_COMPLETION_RANGES.get(p)
        if not r:
            continue
        total += random.randint(r[0], r[1])
    if total <= 0:
        total = 5
    return f"{total}m"

def sanitize_training_plan_for_parse(content: Dict[str, Any]) -> Dict[str, Any]:
    try:
        sections = content.get('sections') or []
        for section in sections:
            lessons = section.get('lessons') or []
            for lesson in lessons:
                if isinstance(lesson, dict):
                    # keep recommended_content_types for persistence
                    pass
    except Exception:
        pass
    return content


def round_hours_in_content(content: Any) -> Any:
    """Recursively round all hours fields to integers in content structure"""
    if isinstance(content, dict):
        result = {}
        for key, value in content.items():
            if key == 'hours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif key == 'totalHours' and isinstance(value, (int, float)):
                result[key] = round(value)
            elif isinstance(value, (dict, list)):
                result[key] = round_hours_in_content(value)
            else:
                result[key] = value
        return result
    elif isinstance(content, list):
        return [round_hours_in_content(item) for item in content]
    else:
        return content

async def get_folder_tier(folder_id: int, pool: asyncpg.Pool) -> str:
    """Get the tier of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its tier
        folder = await conn.fetchrow(
            "SELECT quality_tier, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 'interactive'  # Default tier
        
        # If folder has its own tier, use it
        if folder['quality_tier']:
            return folder['quality_tier']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_tier(folder['parent_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

async def get_folder_custom_rate(folder_id: int, pool: asyncpg.Pool) -> int:
    """Get the custom rate of a folder, inheriting from parent if not set"""
    async with pool.acquire() as conn:
        # Get the folder and its custom rate
        folder = await conn.fetchrow(
            "SELECT custom_rate, parent_id FROM project_folders WHERE id = $1",
            folder_id
        )
        
        if not folder:
            return 200  # Default custom rate
        
        # If folder has its own custom rate, use it
        if folder['custom_rate']:
            return folder['custom_rate']
        
        # Otherwise, inherit from parent folder
        if folder['parent_id']:
            return await get_folder_custom_rate(folder['parent_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_custom_rate(project_id: int, pool: asyncpg.Pool) -> int:
    """Get the effective custom rate for a project, falling back to folder rate if not set"""
    async with pool.acquire() as conn:
        # Get the project's custom rate and folder_id
        project = await conn.fetchrow(
            "SELECT custom_rate, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 200  # Default custom rate
        
        # If project has its own custom rate, use it
        if project['custom_rate']:
            return project['custom_rate']
        
        # Otherwise, get the folder's custom rate
        if project['folder_id']:
            return await get_folder_custom_rate(project['folder_id'], pool)
        
        # Default to 200 custom rate
        return 200

async def get_project_quality_tier(project_id: int, pool: asyncpg.Pool) -> str:
    """Get the effective quality tier for a project, falling back to folder tier if not set"""
    async with pool.acquire() as conn:
        # Get the project's quality tier and folder_id
        project = await conn.fetchrow(
            "SELECT quality_tier, folder_id FROM projects WHERE id = $1",
            project_id
        )
        
        if not project:
            return 'interactive'  # Default tier
        
        # If project has its own quality tier, use it
        if project['quality_tier']:
            return project['quality_tier']
        
        # Otherwise, get the folder's quality tier
        if project['folder_id']:
            return await get_folder_tier(project['folder_id'], pool)
        
        # Default to interactive tier
        return 'interactive'

def get_lesson_effective_custom_rate(lesson: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a lesson, falling back to project rate if not set"""
    if lesson.get('custom_rate'):
        return lesson['custom_rate']
    return project_custom_rate

def get_lesson_effective_quality_tier(lesson: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a lesson, falling back to project tier if not set"""
    if lesson.get('quality_tier'):
        return lesson['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours(lesson: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a specific lesson using its tier settings"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        effective_custom_rate = get_lesson_effective_custom_rate(lesson, project_custom_rate)
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

def get_module_effective_custom_rate(section: dict, project_custom_rate: int) -> int:
    """Get the effective custom rate for a module, falling back to project rate if not set"""
    if section.get('custom_rate'):
        return section['custom_rate']
    return project_custom_rate

def get_module_effective_quality_tier(section: dict, project_quality_tier: str) -> str:
    """Get the effective quality tier for a module, falling back to project tier if not set"""
    if section.get('quality_tier'):
        return section['quality_tier']
    return project_quality_tier

def calculate_lesson_creation_hours_with_module_fallback(lesson: dict, section: dict, project_custom_rate: int) -> int:
    """Calculate creation hours for a lesson with module-level fallback"""
    completion_time_str = lesson.get('completionTime', '')
    if not completion_time_str:
        return 0
    
    try:
        completion_time_minutes = int(completion_time_str.replace('m', ''))
        
        # Check lesson-level tier first, then module-level, then project-level
        if lesson.get('custom_rate'):
            effective_custom_rate = lesson['custom_rate']
        elif section.get('custom_rate'):
            effective_custom_rate = section['custom_rate']
        else:
            effective_custom_rate = project_custom_rate
            
        return calculate_creation_hours(completion_time_minutes, effective_custom_rate)
    except (ValueError, AttributeError):
        return 0

# Define user types and their associated features
USER_TYPES = {
    "normal_hr": {
        "display_name": "Normal (HR)",
        "features": [
            "col_one_pager",
            "col_lesson_presentation", 
            "col_quiz",
            "export_to_lms"
        ]
    },
    "enterprise": {
        "display_name": "Enterprise",
        "features": [
            "col_one_pager",
            "col_lesson_presentation",
            "col_quiz",
            "deloitte_banner",
            "col_est_completion_time",
            "col_est_creation_time",
            "col_content_volume",
            "col_quality_tier",
            "lesson_draft",
            "offers_tab",
            "course_table"
        ]
    },
    "beta": {
        "display_name": "Beta",
        "features": [
            "ai_audit_templates",
            "deloitte_banner",
            "offers_tab",
            "workspace_tab",
            "video_lesson",
            "lesson_draft",
            "col_assessment_type",
            "col_content_volume",
            "col_source",
            "col_est_creation_time",
            "col_est_completion_time",
            "col_quality_tier",
            "col_quiz",
            "col_one_pager",
            "col_video_presentation",
            "col_lesson_presentation",
            "quality_tier"
        ]
    }
}

async def assign_default_user_type(user_id: str, conn: asyncpg.Connection):
    """Assign default 'Normal (HR)' user type to a new user"""
    try:
        default_user_type = "normal_hr"
        if default_user_type not in USER_TYPES:
            logger.warning(f"Default user type {default_user_type} not found in USER_TYPES")
            return
        
        user_type_info = USER_TYPES[default_user_type]
        features_to_enable = user_type_info["features"]
        
        # Enable features for the default user type
        features_assigned = 0
        for feature_name in features_to_enable:
            # Check if feature exists before trying to assign it
            feature_exists = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_name
            )
            
            if feature_exists:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, created_at, updated_at)
                    VALUES ($1, $2, true, NOW(), NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = true,
                        updated_at = NOW()
                """, user_id, feature_name)
                features_assigned += 1
            else:
                logger.warning(f"Feature {feature_name} not found or inactive for new user {user_id}")
        
        logger.info(f"Assigned default user type '{user_type_info['display_name']}' to new user {user_id} ({features_assigned} features enabled)")
        
    except Exception as e:
        logger.error(f"Error assigning default user type to new user {user_id}: {e}")
        # Don't raise exception to avoid blocking user creation

async def auto_provision_nextcloud_user(onyx_user_id: str, pool: asyncpg.Pool) -> bool:
    """Auto-provision a Nextcloud user account for a new Onyx user"""
    try:
        import secrets
        import re as _re
        
        async with pool.acquire() as conn:
            # Check if already provisioned
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if account and account.get("nextcloud_username") and account.get("nextcloud_password_encrypted"):
                logger.info(f"Nextcloud account already provisioned for user: {onyx_user_id}")
                return True
            
            base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
            nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
            nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
            
            if not (nc_admin_user and nc_admin_pass):
                logger.warning(f"Nextcloud admin credentials not configured, skipping auto-provision for user: {onyx_user_id}")
                return False

            # Ensure SmartDrive account record exists
            if not account:
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) DO NOTHING
                    """,
                    onyx_user_id,
                    '{}',
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )

            # Generate Nextcloud user ID and password
            raw_id = str(onyx_user_id)
            sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
            userid = f"sd_{sanitized[:24]}"
            new_password = secrets.token_urlsafe(16)

            # Normalize base URL to https if needed
            from urllib.parse import urlparse
            parsed = urlparse(base_url)
            if parsed.scheme == "http":
                base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
            else:
                base_url = (base_url or "").rstrip("/")
            ocs_base = base_url

            # Create Nextcloud user
            headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                create_resp = await client.post(
                    create_url,
                    data={"userid": userid, "password": new_password},
                    headers=headers,
                    auth=(nc_admin_user, nc_admin_pass)
                )
                
                # Parse OCS response
                ocs_ok = False
                reset_needed = False
                try:
                    j = create_resp.json()
                    sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                    if sc == 100:
                        ocs_ok = True
                    elif sc == 102:
                        reset_needed = True
                except Exception:
                    pass
                    
                if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                    # User exists, reset password
                    update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                    update_resp = await client.put(
                        update_url,
                        data={"key": "password", "value": new_password},
                        headers=headers,
                        auth=(nc_admin_user, nc_admin_pass)
                    )
                    if update_resp.status_code not in (200, 201, 204):
                        logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                    logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                    return False

            # Save encrypted credentials
            encrypted = encrypt_password(new_password)
            await conn.execute(
                """
                UPDATE smartdrive_accounts
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
                """,
                onyx_user_id, userid, encrypted, base_url, datetime.now(timezone.utc)
            )

            # Clean default skeleton files using comprehensive cleanup function
            try:
                deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                logger.info(f"[SmartDrive] Initial cleanup: removed {deleted_count} default files for new user {userid}")
            except Exception as cleanup_error:
                logger.error(f"[SmartDrive] Failed to cleanup default files for user {userid}: {cleanup_error}")
                # Don't fail the whole process if cleanup fails

        logger.info(f"Successfully auto-provisioned Nextcloud account for new user: {onyx_user_id} -> {userid}")
        return True
        
    except Exception as e:
        logger.error(f"Error auto-provisioning Nextcloud user for {onyx_user_id}: {e}")
        return False

async def auto_provision_nextcloud_user(onyx_user_id: str, pool: asyncpg.Pool) -> bool:
    """Auto-provision a Nextcloud user account for a new Onyx user"""
    try:
        import secrets
        import re as _re
        
        async with pool.acquire() as conn:
            # Check if already provisioned
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if account and account.get("nextcloud_username") and account.get("nextcloud_password_encrypted"):
                logger.info(f"Nextcloud account already provisioned for user: {onyx_user_id}")
                return True
            
            base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
            nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
            nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
            
            if not (nc_admin_user and nc_admin_pass):
                logger.warning(f"Nextcloud admin credentials not configured, skipping auto-provision for user: {onyx_user_id}")
                return False

            # Ensure SmartDrive account record exists
            if not account:
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) DO NOTHING
                    """,
                    onyx_user_id,
                    '{}',
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )

            # Generate Nextcloud user ID and password
            raw_id = str(onyx_user_id)
            sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
            userid = f"sd_{sanitized[:24]}"
            new_password = secrets.token_urlsafe(16)

            # Normalize base URL to https if needed
            from urllib.parse import urlparse
            parsed = urlparse(base_url)
            if parsed.scheme == "http":
                base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
            else:
                base_url = (base_url or "").rstrip("/")
            ocs_base = base_url

            # Create Nextcloud user
            headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                create_resp = await client.post(
                    create_url,
                    data={"userid": userid, "password": new_password},
                    headers=headers,
                    auth=(nc_admin_user, nc_admin_pass)
                )
                
                # Parse OCS response
                ocs_ok = False
                reset_needed = False
                try:
                    j = create_resp.json()
                    sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                    if sc == 100:
                        ocs_ok = True
                    elif sc == 102:
                        reset_needed = True
                except Exception:
                    pass
                    
                if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                    # User exists, reset password
                    update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                    update_resp = await client.put(
                        update_url,
                        data={"key": "password", "value": new_password},
                        headers=headers,
                        auth=(nc_admin_user, nc_admin_pass)
                    )
                    if update_resp.status_code not in (200, 201, 204):
                        logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                    logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                    return False

            # Save encrypted credentials
            encrypted = encrypt_password(new_password)
            await conn.execute(
                """
                UPDATE smartdrive_accounts
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
                """,
                onyx_user_id, userid, encrypted, base_url, datetime.now(timezone.utc)
            )

            # Clean default skeleton files using comprehensive cleanup function
            try:
                deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                logger.info(f"[SmartDrive] Initial cleanup: removed {deleted_count} default files for new user {userid}")
            except Exception as cleanup_error:
                logger.error(f"[SmartDrive] Failed to cleanup default files for user {userid}: {cleanup_error}")
                # Don't fail the whole process if cleanup fails

        logger.info(f"Successfully auto-provisioned Nextcloud account for new user: {onyx_user_id} -> {userid}")
        return True
        
    except Exception as e:
        logger.error(f"Error auto-provisioning Nextcloud user for {onyx_user_id}: {e}")
        return False

async def auto_provision_nextcloud_user(onyx_user_id: str, pool: asyncpg.Pool) -> bool:
    """Auto-provision a Nextcloud user account for a new Onyx user"""
    try:
        import secrets
        import re as _re
        
        async with pool.acquire() as conn:
            # Check if already provisioned
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if account and account.get("nextcloud_username") and account.get("nextcloud_password_encrypted"):
                logger.info(f"Nextcloud account already provisioned for user: {onyx_user_id}")
                return True
            
            base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
            nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
            nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
            
            if not (nc_admin_user and nc_admin_pass):
                logger.warning(f"Nextcloud admin credentials not configured, skipping auto-provision for user: {onyx_user_id}")
                return False

            # Ensure SmartDrive account record exists
            if not account:
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) DO NOTHING
                    """,
                    onyx_user_id,
                    '{}',
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )

            # Generate Nextcloud user ID and password
            raw_id = str(onyx_user_id)
            sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
            userid = f"sd_{sanitized[:24]}"
            new_password = secrets.token_urlsafe(16)

            # Normalize base URL to https if needed
            from urllib.parse import urlparse
            parsed = urlparse(base_url)
            if parsed.scheme == "http":
                base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
            else:
                base_url = (base_url or "").rstrip("/")
            ocs_base = base_url

            # Create Nextcloud user
            headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                create_resp = await client.post(
                    create_url,
                    data={"userid": userid, "password": new_password},
                    headers=headers,
                    auth=(nc_admin_user, nc_admin_pass)
                )
                
                # Parse OCS response
                ocs_ok = False
                reset_needed = False
                try:
                    j = create_resp.json()
                    sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                    if sc == 100:
                        ocs_ok = True
                    elif sc == 102:
                        reset_needed = True
                except Exception:
                    pass
                    
                if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                    # User exists, reset password
                    update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                    update_resp = await client.put(
                        update_url,
                        data={"key": "password", "value": new_password},
                        headers=headers,
                        auth=(nc_admin_user, nc_admin_pass)
                    )
                    if update_resp.status_code not in (200, 201, 204):
                        logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                    logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                    return False

            # Save encrypted credentials
            encrypted = encrypt_password(new_password)
            await conn.execute(
                """
                UPDATE smartdrive_accounts
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
                """,
                onyx_user_id, userid, encrypted, base_url, datetime.now(timezone.utc)
            )

            # Clean default skeleton files using comprehensive cleanup function
            try:
                deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                logger.info(f"[SmartDrive] Initial cleanup: removed {deleted_count} default files for new user {userid}")
            except Exception as cleanup_error:
                logger.error(f"[SmartDrive] Failed to cleanup default files for user {userid}: {cleanup_error}")
                # Don't fail the whole process if cleanup fails

        logger.info(f"Successfully auto-provisioned Nextcloud account for new user: {onyx_user_id} -> {userid}")
        return True
        
    except Exception as e:
        logger.error(f"Error auto-provisioning Nextcloud user for {onyx_user_id}: {e}")
        return False


async def provision_smartdrive_for_new_user(onyx_user_id: str, user_name: str, pool: asyncpg.Pool):
    """Background task to provision SmartDrive account for a new user"""
    try:
        async with pool.acquire() as conn:
            # Create SmartDrive account placeholder for new user
            try:
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) DO NOTHING
                    """,
                    onyx_user_id,
                    '{}',  # Empty JSON cursor
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )
                logger.info(f"Created SmartDrive account placeholder for new user: {onyx_user_id}")
                
                # Auto-provision Nextcloud user account and clean up default files
                # This used to happen when users first visited SmartDrive tab, now happens during registration
                base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
                nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
                nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
                
                if nc_admin_user and nc_admin_pass:
                    import secrets
                    import re as _re
                    
                    # Generate Nextcloud user ID and password
                    raw_id = str(onyx_user_id)
                    sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
                    userid = f"sd_{sanitized[:24]}"
                    new_password = secrets.token_urlsafe(16)

                    # Normalize base URL to https if needed
                    from urllib.parse import urlparse
                    parsed = urlparse(base_url)
                    if parsed.scheme == "http":
                        base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
                    else:
                        base_url = (base_url or "").rstrip("/")
                    ocs_base = base_url

                    # Create Nextcloud user
                    headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
                    async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                        create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                        create_resp = await client.post(
                            create_url,
                            data={"userid": userid, "password": new_password},
                            headers=headers,
                            auth=(nc_admin_user, nc_admin_pass)
                        )
                        
                        # Parse OCS response
                        ocs_ok = False
                        reset_needed = False
                        try:
                            j = create_resp.json()
                            sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                            if sc == 100:
                                ocs_ok = True
                            elif sc == 102:
                                reset_needed = True
                        except Exception:
                            pass
                            
                        if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                            # User exists, reset password
                            update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                            update_resp = await client.put(
                                update_url,
                                data={"key": "password", "value": new_password},
                                headers=headers,
                                auth=(nc_admin_user, nc_admin_pass)
                            )
                            if update_resp.status_code not in (200, 201, 204):
                                logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code}")
                        elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                            logger.error(f"Failed to create Nextcloud user: {create_resp.status_code}")
                            raise Exception("Failed to create Nextcloud user")

                    # Save encrypted credentials
                    encrypted = encrypt_password(new_password)
                    await conn.execute(
                        """
                        UPDATE smartdrive_accounts
                        SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                        WHERE onyx_user_id = $1
                        """,
                        onyx_user_id, userid, encrypted, base_url, datetime.now(timezone.utc)
                    )

                    # Clean default skeleton files using comprehensive cleanup function
                    try:
                        deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                        logger.info(f"[SmartDrive] Initial cleanup: removed {deleted_count} default files for new user {userid}")
                    except Exception as cleanup_error:
                        logger.error(f"[SmartDrive] Failed to cleanup default files for user {userid}: {cleanup_error}")
                        # Don't fail the whole process if cleanup fails

                    logger.info(f"[SmartDrive] Auto-provisioned Nextcloud account for new user: {onyx_user_id} -> {userid}")
                else:
                    logger.warning(f"Nextcloud admin credentials not configured, SmartDrive account will need manual setup for user: {onyx_user_id}")
                    
            except Exception as e:
                logger.error(f"Error creating/provisioning SmartDrive account for new user {onyx_user_id}: {e}")
                # Don't raise exception to avoid blocking user creation
            
            logger.info(f"[SmartDrive Background] Completed SmartDrive provisioning for user {onyx_user_id} ({user_name})")
    except Exception as e:
        logger.error(f"[SmartDrive Background] Fatal error in SmartDrive provisioning for {onyx_user_id}: {e}")


async def get_or_create_user_credits(onyx_user_id: str, user_name: str, pool: asyncpg.Pool, background_tasks: BackgroundTasks = None) -> UserCredits:
    """Get user credits or create if doesn't exist"""
    async with pool.acquire() as conn:
        # Try to get existing credits
        credits_row = await conn.fetchrow(
            "SELECT * FROM user_credits WHERE onyx_user_id = $1",
            onyx_user_id
        )
        
        if credits_row:
            return UserCredits(**dict(credits_row))
        
        # Create new user credits entry with default values
        new_credits_row = await conn.fetchrow("""
            INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
            VALUES ($1, $2, $3, $3)
            RETURNING *
        """, onyx_user_id, user_name, 100)  # Default 100 credits for new users
        
        # Assign default "Normal (HR)" user type to new users
        await assign_default_user_type(onyx_user_id, conn)
        
        logger.info(f"Created credits and assigned user type for new user {onyx_user_id} ({user_name}) with 100 credits")
        
        # Schedule SmartDrive provisioning in background if BackgroundTasks available
        if background_tasks:
            background_tasks.add_task(provision_smartdrive_for_new_user, onyx_user_id, user_name, pool)
            logger.info(f"[SmartDrive] Scheduled background provisioning for new user: {onyx_user_id}")
        else:
            # Fallback: provision inline if no background tasks (e.g., during migration)
            await provision_smartdrive_for_new_user(onyx_user_id, user_name, pool)
        
        return UserCredits(**dict(new_credits_row))

def calculate_product_credits(product_type: str, content_data: dict = None) -> int:
    """Calculate credit cost for product creation"""
    if product_type == "course_outline":
        return 5  # Course outline finalization costs 5 credits
    elif product_type == "lesson_presentation":
        # Calculate based on slide count
        if content_data and isinstance(content_data, dict):
            slides = content_data.get("slides", [])
            slide_count = len(slides) if slides else 0
            
            if slide_count <= 5:
                return 3
            elif slide_count <= 10:
                return 5
            else:
                return 10
        return 5  # Default if we can't determine slide count
    elif product_type in ["quiz", "one_pager"]:
        return 5  # Quiz and one-pager both cost 5 credits
    else:
        return 0  # Unknown product type, no cost

# Helper: reason -> product type fallback
def infer_product_type_from_reason(reason: str) -> str:
    r = (reason or "").lower()
    if "course outline" in r:
        return "Course Outline"
    if "lesson presentation" in r or "text presentation" in r or "presentation" in r:
        return "Presentation"
    if "quiz" in r:
        return "Quiz"
    if "one-pager" in r or "one pager" in r:
        return "One-Pager"
    if "video" in r:
        return "Video Lesson"
    return "Unknown"

# Helper: write a credit transaction row
async def log_credit_transaction(
    conn,
    *,
    onyx_user_id: str,
    user_credits_id: Optional[int],
    type_: Literal['purchase','product_generation','admin_removal'],
    amount: int,
    delta: int,
    title: Optional[str] = None,
    product_type: Optional[str] = None,
    reason: Optional[str] = None,
) -> None:
    tx_id = str(uuid.uuid4())
    await conn.execute(
        """
        INSERT INTO credit_transactions
            (id, onyx_user_id, user_credits_id, type, title, product_type, credits, delta, reason, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
        """,
        tx_id, onyx_user_id, user_credits_id, type_, title, product_type, abs(amount), delta, reason
    )

async def deduct_credits(
    onyx_user_id: str,
    amount: int,
    pool: asyncpg.Pool,
    reason: str = "Product creation",
    product_type: Optional[str] = None,
    title: Optional[str] = None,
) -> UserCredits:
    """Deduct credits from user balance with transaction safety"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Check current balance
            current_balance = await conn.fetchval(
                "SELECT credits_balance FROM user_credits WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if current_balance is None:
                raise HTTPException(status_code=404, detail="User credits not found")
            
            if current_balance < amount:
                raise HTTPException(
                    status_code=402, 
                    detail=f"Insufficient credits. Current balance: {current_balance}, Required: {amount}"
                )
            
            # Deduct credits and update usage
            updated_row = await conn.fetchrow("""
                UPDATE user_credits 
                SET credits_balance = credits_balance - $1,
                    total_credits_used = total_credits_used + $1,
                    updated_at = NOW()
                WHERE onyx_user_id = $2
                RETURNING *
            """, amount, onyx_user_id)

            # Log product_generation transaction
            user_row = await conn.fetchrow("SELECT id FROM user_credits WHERE onyx_user_id = $1", onyx_user_id)
            await log_credit_transaction(
                conn,
                onyx_user_id=onyx_user_id,
                user_credits_id=int(user_row["id"]) if user_row else None,
                type_='product_generation',
                amount=amount,
                delta=-abs(amount),
                title=title or reason,
                product_type=product_type or infer_product_type_from_reason(reason),
                reason=reason,
            )
            
            return UserCredits(**dict(updated_row))

async def modify_user_credits_by_email(user_email: str, amount: int, action: str, pool: asyncpg.Pool, reason: str = "Admin adjustment") -> UserCredits:
    """Modify user credits by email (for admin use)"""
    async with pool.acquire() as conn:
        async with conn.transaction():
            if action == "add":
                # Add credits (can be new user or existing)
                # Use email as onyx_user_id for simplicity in admin interface
                credits_row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance, credits_purchased)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (onyx_user_id) 
                    DO UPDATE SET 
                        credits_balance = user_credits.credits_balance + $3,
                        credits_purchased = user_credits.credits_purchased + $3,
                        last_purchase_date = NOW(),
                        updated_at = NOW()
                    RETURNING *
                """, user_email, user_email.split('@')[0], amount, amount)

                # Log purchase
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='purchase',
                    amount=amount,
                    delta=abs(amount),
                    title="Credits purchase",
                    product_type=None,
                    reason=reason or "Credits purchase",
                )
                
            elif action == "remove":
                # Remove credits (must exist)
                credits_row = await conn.fetchrow("""
                    UPDATE user_credits 
                    SET credits_balance = GREATEST(0, credits_balance - $1),
                        updated_at = NOW()
                    WHERE onyx_user_id = $2
                    RETURNING *
                """, amount, user_email)
                
                if not credits_row:
                    raise HTTPException(status_code=404, detail="User not found")

                # Log admin removal as 'admin_removal'
                await log_credit_transaction(
                    conn,
                    onyx_user_id=credits_row["onyx_user_id"],
                    user_credits_id=int(credits_row["id"]),
                    type_='admin_removal',
                    amount=amount,
                    delta=-abs(amount),
                    title="Admin adjustment",
                    product_type=None,
                    reason=reason or "Admin adjustment",
                )
            
            else:
                raise HTTPException(status_code=400, detail="Invalid action. Must be 'add' or 'remove'")
            
            return UserCredits(**dict(credits_row))

async def migrate_onyx_users_to_credits_table() -> int:
    """Migrate users from Onyx database to credits table"""
    if not ONYX_DATABASE_URL:
        raise Exception("ONYX_DATABASE_URL not configured")
    
    logger.info(f"Attempting to connect to Onyx database: {ONYX_DATABASE_URL}")
    
    # Try multiple possible database names
    possible_db_urls = [
        ONYX_DATABASE_URL,
        ONYX_DATABASE_URL.replace('/onyx_db', '/postgres'),  # Try postgres DB name
        ONYX_DATABASE_URL.replace('/postgres', '/onyx_db')   # Try onyx_db name
    ]
    
    # Remove duplicates while preserving order
    db_urls_to_try = []
    for url in possible_db_urls:
        if url not in db_urls_to_try:
            db_urls_to_try.append(url)
    
    onyx_pool = None
    last_error = None
    
    for db_url in db_urls_to_try:
        try:
            logger.info(f"Trying to connect to: {db_url}")
            # Connect to Onyx database
            onyx_pool = await asyncpg.create_pool(dsn=db_url, min_size=1, max_size=5)
        
            async with onyx_pool.acquire() as onyx_conn:
                # Get users from Onyx database
                onyx_users = await onyx_conn.fetch("""
                    SELECT 
                        id::text as onyx_user_id,
                        COALESCE(email, 'Unknown User') as name
                    FROM "user" 
                    WHERE is_active = true
                    AND role != 'ext_perm_user'
                    AND role != 'slack_user'
                    AND email NOT LIKE '%@danswer-api-key.invalid'
                """)
            
            if not onyx_users:
                logger.info("No Onyx users found to migrate")
                return 0
            
            logger.info(f"Found {len(onyx_users)} Onyx users to migrate")
            
            # Insert into custom database
            async with DB_POOL.acquire() as custom_conn:
                migrated_count = 0
                for user in onyx_users:
                    try:
                        # Check if user already exists with credits
                        existing_credits = await custom_conn.fetchrow(
                            "SELECT * FROM user_credits WHERE onyx_user_id = $1",
                            user['onyx_user_id']
                        )
                        
                        if existing_credits:
                            logger.info(f"User {user['onyx_user_id']} already has credits, skipping credit creation")
                        else:
                            # Insert user credits (original migration logic)
                            await custom_conn.execute("""
                            INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                            VALUES ($1, $2, 100)
                            ON CONFLICT (onyx_user_id) DO NOTHING
                            """, user['onyx_user_id'], user['name'])
                            logger.info(f"Created credits for user {user['onyx_user_id']}")
                        
                        # Check if user already has full SmartDrive provisioning
                        existing_smartdrive = await custom_conn.fetchrow(
                            "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                            user['onyx_user_id']
                        )
                        
                        # Ensure default user type is assigned immediately after credits
                        try:
                            await assign_default_user_type(user['onyx_user_id'], custom_conn)
                        except Exception as user_type_error:
                            logger.warning(f"Failed to assign user type to {user['onyx_user_id']}: {user_type_error}")

                        if existing_smartdrive and existing_smartdrive.get('nextcloud_username') and existing_smartdrive.get('nextcloud_password_encrypted'):
                            logger.info(f"User {user['onyx_user_id']} already has full SmartDrive provisioning, skipping")
                        else:
                            # Create SmartDrive account placeholder if it doesn't exist
                            if not existing_smartdrive:
                                await custom_conn.execute("""
                                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                                    VALUES ($1, $2, $3, $4)
                                    ON CONFLICT (onyx_user_id) DO NOTHING
                                """, user['onyx_user_id'], '{}', datetime.now(timezone.utc), datetime.now(timezone.utc))
                                logger.info(f"Created SmartDrive placeholder for user {user['onyx_user_id']}")
                            
                            # Full Nextcloud provisioning for users without it
                            try:
                                base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
                                nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
                                nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
                                
                                if nc_admin_user and nc_admin_pass:
                                    import secrets
                                    import re as _re
                                    
                                    # Generate Nextcloud user ID and password
                                    raw_id = str(user['onyx_user_id'])
                                    sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
                                    userid = f"sd_{sanitized[:24]}"
                                    new_password = secrets.token_urlsafe(16)

                                    # Normalize base URL to https if needed
                                    from urllib.parse import urlparse
                                    parsed = urlparse(base_url)
                                    if parsed.scheme == "http":
                                        base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
                                    else:
                                        base_url = (base_url or "").rstrip("/")
                                    ocs_base = base_url

                                    # Create Nextcloud user
                                    headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
                                    async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                                        create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                                        create_resp = await client.post(
                                            create_url,
                                            data={"userid": userid, "password": new_password},
                                            headers=headers,
                                            auth=(nc_admin_user, nc_admin_pass)
                                        )
                                        
                                        # Parse OCS response
                                        ocs_ok = False
                                        reset_needed = False
                                        try:
                                            j = create_resp.json()
                                            sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                                            if sc == 100:
                                                ocs_ok = True
                                            elif sc == 102:
                                                reset_needed = True
                                        except Exception:
                                            pass
                                            
                                        if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                                            # User exists, reset password
                                            update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                                            update_resp = await client.put(
                                                update_url,
                                                data={"key": "password", "value": new_password},
                                                headers=headers,
                                                auth=(nc_admin_user, nc_admin_pass)
                                            )
                                            if update_resp.status_code not in (200, 201, 204):
                                                logger.warning(f"Failed to reset Nextcloud password for migrated user {userid}: {update_resp.status_code}")
                                        elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                                            logger.error(f"Failed to create Nextcloud user for migrated user: {create_resp.status_code}")
                                            # Continue with migration even if Nextcloud fails
                                            raise Exception("Nextcloud creation failed")

                                    # Save encrypted credentials
                                    encrypted = encrypt_password(new_password)
                                    await custom_conn.execute(
                                        """
                                        UPDATE smartdrive_accounts
                                        SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                                        WHERE onyx_user_id = $1
                                        """,
                                        user['onyx_user_id'], userid, encrypted, base_url, datetime.now(timezone.utc)
                                    )

                                    # Clean default skeleton files using comprehensive cleanup function
                                    try:
                                        deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                                        logger.info(f"[SmartDrive Migration] Cleaned up {deleted_count} default files for migrated user {userid}")
                                    except Exception as cleanup_error:
                                        logger.error(f"[SmartDrive Migration] Failed to cleanup default files for user {userid}: {cleanup_error}")
                                        # Don't fail the whole migration if cleanup fails

                                    logger.info(f"[SmartDrive Migration] Fully provisioned Nextcloud account for migrated user: {user['onyx_user_id']} -> {userid}")
                                else:
                                    logger.warning(f"[SmartDrive Migration] Nextcloud admin credentials not configured, user {user['onyx_user_id']} will need manual setup")
                                    
                            except Exception as smartdrive_error:
                                logger.error(f"[SmartDrive Migration] Failed to provision Nextcloud for user {user['onyx_user_id']}: {smartdrive_error}")
                                # Don't fail the whole migration if SmartDrive provisioning fails
                        
                        # (User type assignment moved earlier to occur before SmartDrive provisioning)
                        
                        migrated_count += 1
                        logger.info(f"Processed user {user['onyx_user_id']} ({user['name']}) - migration complete")
                        
                    except Exception as e:
                        logger.warning(f"Failed to process user {user['onyx_user_id']}: {e}")
                
                logger.info(f"Successfully processed {migrated_count} users (skipped users with existing full provisioning)")
                return migrated_count
                
        except Exception as e:
            last_error = e
            logger.warning(f"Failed to connect to {db_url}: {e}")
            if onyx_pool:
                await onyx_pool.close()
                onyx_pool = None
            continue  # Try next database URL
        
        # If we got here, the connection worked, so break out of the loop
        break
    
    # If we tried all URLs and none worked, raise the last error
    if onyx_pool is None:
        raise Exception(f"Could not connect to any Onyx database. Last error: {last_error}")
    
    # This should never be reached, but just in case
    return 0

VIDEO_SCRIPT_LANG_STRINGS = {
    'ru': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Видео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Отображаемый текст:',
        'DISPLAYED_IMAGE_LABEL': 'Отображаемая картинка:',
        'DISPLAYED_VIDEO_LABEL': 'Отображаемое видео:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучки:',
        'NO_SLIDES_TEXT': 'Нет слайдов для отображения.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Название теста',
            'question': 'Вопрос',
            'correctAnswer': 'Правильный ответ',
            'correctAnswers': 'Правильные ответы',
            'acceptableAnswers': 'Допустимые ответы',
            'prompts': 'Элементы',
            'options': 'Варианты',
            'correctMatches': 'Правильные соответствия',
            'itemsToSort': 'Элементы для сортировки',
            'explanation': 'Объяснение',
            'multipleChoice': 'Один правильный ответ',
            'multiSelect': 'Несколько правильных ответов',
            'matching': 'Соответствие',
            'sorting': 'Сортировка',
            'openAnswer': 'Свободный ответ',
            'answerKey': 'Ключ ответов',
            'correctOrder': 'Правильный порядок',
            'emptyContent': '...',
        }
    },
    'en': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Video Lesson Script',
        'SLIDE_NUMBER_PREFIX': 'SLIDE №',
        'DISPLAYED_TEXT_LABEL': 'Displayed Text:',
        'DISPLAYED_IMAGE_LABEL': 'Displayed Image:',
        'DISPLAYED_VIDEO_LABEL': 'Displayed Video:',
        'VOICEOVER_TEXT_LABEL': 'Voiceover Text:',
        'NO_SLIDES_TEXT': 'No slides to display.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'COURSE',
        'lessonLabel': 'LESSON',
        'quiz': {
            'quizTitle': 'Quiz Title',
            'question': 'Question',
            'correctAnswer': 'Correct Answer',
            'correctAnswers': 'Correct Answers',
            'acceptableAnswers': 'Acceptable Answers',
            'prompts': 'Items',
            'options': 'Options',
            'correctMatches': 'Correct Matches',
            'itemsToSort': 'Items to Sort',
            'explanation': 'Explanation',
            'multipleChoice': 'Multiple Choice',
            'multiSelect': 'Multi-Select',
            'matching': 'Matching',
            'sorting': 'Sorting',
            'openAnswer': 'Open Answer',
            'answerKey': 'Answer Key',
            'correctOrder': 'Correct Order',
            'emptyContent': '...',
        }
    },
    'uk': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Відео урок',
        'SLIDE_NUMBER_PREFIX': 'СЛАЙД №',
        'DISPLAYED_TEXT_LABEL': 'Текст, що відображається:',
        'DISPLAYED_IMAGE_LABEL': 'Зображення, що відображається:',
        'DISPLAYED_VIDEO_LABEL': 'Відео, що відображається:',
        'VOICEOVER_TEXT_LABEL': 'Текст озвучення:',
        'NO_SLIDES_TEXT': 'Немає слайдів для відображення.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'КУРС',
        'lessonLabel': 'УРОК',
        'quiz': {
            'quizTitle': 'Назва тесту',
            'question': 'Питання',
            'correctAnswer': 'Правильна відповідь',
            'correctAnswers': 'Правильні відповіді',
            'acceptableAnswers': 'Допустимі відповіді',
            'prompts': 'Елементи',
            'options': 'Варіанти',
            'correctMatches': 'Правильні відповідності',
            'itemsToSort': 'Елементи для сортування',
            'explanation': 'Пояснення',
            'multipleChoice': 'Одна правильна відповідь',
            'multiSelect': 'Декілька правильних відповідей',
            'matching': 'Відповідність',
            'sorting': 'Сортування',
            'openAnswer': 'Вільна відповідь',
            'answerKey': 'Ключ відповідей',
            'correctOrder': 'Правильний порядок',
            'emptyContent': '...',
        }
    },
    'es': {
        'VIDEO_LESSON_SCRIPT_DEFAULT_TITLE': 'Guión de la lección en video',
        'SLIDE_NUMBER_PREFIX': 'DIAPOSITIVA №',
        'DISPLAYED_TEXT_LABEL': 'Texto mostrado:',
        'DISPLAYED_IMAGE_LABEL': 'Imagen mostrada:',
        'DISPLAYED_VIDEO_LABEL': 'Video mostrado:',
        'VOICEOVER_TEXT_LABEL': 'Texto de voz en off:',
        'NO_SLIDES_TEXT': 'No hay diapositivas para mostrar.',
        'EMPTY_CONTENT_PLACEHOLDER': '...',
        'courseLabel': 'CURSO',
        'lessonLabel': 'LECCIÓN',
        'quiz': {
            'quizTitle': 'Título del cuestionario',
            'question': 'Pregunta',
            'correctAnswer': 'Respuesta correcta',
            'correctAnswers': 'Respuestas correctas',
            'acceptableAnswers': 'Respuestas aceptables',
            'prompts': 'Elementos',
            'options': 'Opciones',
            'correctMatches': 'Correspondencias correctas',
            'itemsToSort': 'Elementos para ordenar',
            'explanation': 'Explicación',
            'multipleChoice': 'Opción múltiple',
            'multiSelect': 'Selección múltiple',
            'matching': 'Correspondencia',
            'sorting': 'Ordenamiento',
            'openAnswer': 'Respuesta abierta',
            'answerKey': 'Clave de respuestas',
            'correctOrder': 'Orden correcto',
            'emptyContent': '...',
        }
    }
}

def detect_language(text: str, configs: Dict[str, Dict[str, str]] = LANG_CONFIG) -> str:
    en_score = 0; ru_score = 0; uk_score = 0
    en_config = configs.get('en', {})
    ru_config = configs.get('ru', {})
    uk_config = configs.get('uk', {})

    if en_config.get('MODULE_KEYWORD') and en_config.get('LESSONS_HEADER_KEYWORD') and en_config.get('TOTAL_TIME_KEYWORD'):
        if en_config['MODULE_KEYWORD'] in text and \
           en_config['LESSONS_HEADER_KEYWORD'] in text and \
           en_config['TOTAL_TIME_KEYWORD'] in text:
            en_score += 3
    if ru_config.get('MODULE_KEYWORD') and ru_config.get('LESSONS_HEADER_KEYWORD') and ru_config.get('TOTAL_TIME_KEYWORD'):
        if ru_config['MODULE_KEYWORD'] in text and \
           ru_config['LESSONS_HEADER_KEYWORD'] in text and \
           ru_config['TOTAL_TIME_KEYWORD'] in text:
            ru_score += 3
    if uk_config.get('MODULE_KEYWORD') and uk_config.get('LESSONS_HEADER_KEYWORD') and uk_config.get('TOTAL_TIME_KEYWORD'):
        if uk_config['MODULE_KEYWORD'] in text and \
           uk_config['LESSONS_HEADER_KEYWORD'] in text and \
           uk_config['TOTAL_TIME_KEYWORD'] in text:
            uk_score += 3
    if en_score == 0 and ru_score == 0 and uk_score == 0:
        if en_config.get('MODULE_KEYWORD') and en_config['MODULE_KEYWORD'] in text: en_score +=1
        if ru_config.get('MODULE_KEYWORD') and ru_config['MODULE_KEYWORD'] in text: ru_score +=1
        if uk_config.get('MODULE_KEYWORD') and uk_config['MODULE_KEYWORD'] in text: uk_score +=1
        if en_config.get('TIME_KEYWORD') and en_config['TIME_KEYWORD'] in text: en_score +=1
        if ru_config.get('TIME_KEYWORD') and ru_config['TIME_KEYWORD'] in text: ru_score +=1
        if uk_config.get('TIME_KEYWORD') and uk_config['TIME_KEYWORD'] in text: uk_score +=1
        if en_score == 0 and ru_score == 0 and uk_score == 0:
            en_chars = sum(1 for char_ in text if 'a' <= char_.lower() <= 'z')
            cyrillic_chars = sum(1 for char_ in text if 'а' <= char_.lower() <= 'я' or char_.lower() in ['і', 'ї', 'є', 'ґ'])
            if en_chars > cyrillic_chars and en_chars > 10 :
                 en_score += 0.1
            elif cyrillic_chars > en_chars and cyrillic_chars > 10:
                if uk_score == 0: uk_score += 0.05
                if ru_score == 0: ru_score += 0.05
                ukrainian_specific_chars = sum(1 for char_ in text if char_.lower() in ['і', 'ї', 'є', 'ґ'])
                if ukrainian_specific_chars > 0:
                    uk_score += 0.05 * ukrainian_specific_chars
    if en_score > ru_score and en_score > uk_score: return 'en'
    if ru_score > en_score and ru_score > uk_score: return 'ru'
    if uk_score > en_score and uk_score > ru_score: return 'uk'
    if uk_score > 0 and uk_score >= ru_score and uk_score >= en_score: return 'uk'
    if ru_score > 0 and ru_score >= en_score : return 'ru'
    if en_score > 0 : return 'en'
    logger.warning("detect_language could not reliably determine language. Defaulting to 'en'.")
    return 'en'

def parse_training_plan_from_string(original_content_str: str, main_table_title: str) -> Optional[TrainingPlanDetails]:
    logger.warning("Old 'parse_training_plan_from_string' called. Ensure this is intended for legacy data.")
    return TrainingPlanDetails(mainTitle=f"Content for {main_table_title} (Old Parser)", sections=[], detectedLanguage='ru')

async def parse_ai_response_with_llm(
    ai_response: str,
    project_name: str,
    target_model: Type[BaseModel],
    default_error_model_instance: BaseModel,
    dynamic_instructions: str,
    target_json_example: str
) -> BaseModel:
    # Start timing for analytics
    start_time = time.time()
    
    # DEBUG: Log that the function was called
    logger.info(f"=== AI PARSER FUNCTION CALLED ===")
    logger.info(f"Project: {project_name}")
    logger.info(f"Target model: {target_model.__name__}")
    logger.info(f"AI response length: {len(ai_response)}")
    logger.info(f"DB_POOL available: {DB_POOL is not None}")
    logger.info(f"Call stack: {len(inspect.stack())} frames")
    logger.info(f"=== END FUNCTION CALL DEBUG ===")
    
    # Create a list of API keys to try, filtering out any that are not set
    api_keys_to_try = [key for key in [LLM_API_KEY, LLM_API_KEY_FALLBACK] if key]

    if not api_keys_to_try:
        logger.error(f"LLM_API_KEY not configured for {project_name}. Cannot parse AI response with LLM.")
        return default_error_model_instance

    prompt_message = f"""
You are a highly accurate text-to-JSON parsing assistant. Your task is to convert the *entirety* of the following unstructured text into a single, structured JSON object.
Ensure *all* relevant information from the "Raw text to parse" is included in your JSON output.
Pay close attention to data types: strings should be quoted, numerical values should be numbers, and lists should be arrays. Null values are not permitted for string fields; use an empty string "" instead if text is absent but the field is required according to the example structure.
Maintain the original language of the input text for all textual content in the JSON.

🚨 SPECIAL INSTRUCTION FOR VIDEO LESSONS: If the target model is SlideDeckDetails and the JSON example contains "voiceoverText" fields, you MUST generate voiceover text for every slide object. Look at the example JSON structure and ensure your output matches it exactly, including all voiceoverText fields and hasVoiceover flag.

Specific Instructions for this Content Type ({target_model.__name__}):
---
{dynamic_instructions}
---

The desired JSON output format is exemplified below. This example is CRUCIAL and your output MUST strictly follow this JSON format and structure.
---
{target_json_example}
---

Raw text to parse:
---
{ai_response}
---

Return ONLY the JSON object corresponding to the parsed text. Do not include any other explanatory text or markdown formatting (like ```json ... ```) around the JSON.
The entire output must be a single, valid JSON object and must include all relevant data found in the input, with textual content in the original language.
    """
    # OpenAI Chat API expects a list of chat messages
    system_msg = {"role": "system", "content": "You are a JSON parsing expert. You must output ONLY valid JSON in the exact format specified. Do not include any explanations, markdown formatting, or additional text. Your response must be a single, complete JSON object. CRITICAL: If the example JSON contains voiceoverText fields, your output MUST include them for every slide. Match the example structure exactly."}
    user_msg = {"role": "user", "content": prompt_message}
    base_payload: Dict[str, Any] = {"model": LLM_DEFAULT_MODEL, "messages": [system_msg, user_msg], "temperature": 0.1}
    # Ask the model to output pure JSON
    base_payload_with_rf = {**base_payload, "response_format": {"type": "json_object"}}
    detected_lang_by_rules = detect_language(ai_response)
    last_exception = None

    for i, api_key in enumerate(api_keys_to_try):
        attempt_number = i + 1
        logger.info(f"Attempting LLM call for '{project_name}' using API key #{attempt_number}.")
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        try:
            # Try with response_format first, then without if Cohere rejects it
            for pf_idx, payload_variant in enumerate([base_payload_with_rf, base_payload]):
                try:
                    # Remove per-request timeout so long parses are not cut off (backend will rely on upstream timeouts)
                    async with httpx.AsyncClient(timeout=None) as client:
                        response = await client.post(LLM_API_URL, headers=headers, json=payload_variant)
                        response.raise_for_status()
                    break  # success
                except httpx.HTTPStatusError as he:
                    # OpenAI returns 400 when response_format isn't supported for a given model
                    if he.response.status_code in (400, 422) and pf_idx == 0:
                        logger.info("LLM rejected response_format – retrying without it.")
                        continue
                    raise

            llm_api_response_data = response.json()

            # --- Process the Response ---
            json_text_output = None
            if "text" in llm_api_response_data: json_text_output = llm_api_response_data["text"]
            elif "chatHistory" in llm_api_response_data and llm_api_response_data["chatHistory"]:
                last_message = next((msg for msg in reversed(llm_api_response_data["chatHistory"]) if msg.get("role") == "CHATBOT"), None)
                if last_message and "message" in last_message: json_text_output = last_message["message"]
            elif llm_api_response_data.get("generations") and isinstance(llm_api_response_data["generations"], list) and llm_api_response_data["generations"][0].get("text"):
                json_text_output = llm_api_response_data["generations"][0]["text"]
            elif "choices" in llm_api_response_data and isinstance(llm_api_response_data["choices"], list) and llm_api_response_data["choices"]:
                # Support for OpenAI Chat Completions format: choices[0].message.content (or .delta.content in streaming)
                first_choice = llm_api_response_data["choices"][0]
                if isinstance(first_choice, dict):
                    # Standard chat completion response
                    if "message" in first_choice and isinstance(first_choice["message"], dict):
                        json_text_output = first_choice["message"].get("content")
                    # If using the delta format (streaming-style response aggregated by OpenAI)
                    if not json_text_output and "delta" in first_choice and isinstance(first_choice["delta"], dict):
                        json_text_output = first_choice["delta"].get("content")
                    # Fallback to any direct text field
                    if not json_text_output:
                        json_text_output = first_choice.get("text")

            if json_text_output is None:
                # Log the raw keys of the response for easier debugging
                logger.warning(
                    "No 'content' field found in LLM response. Raw keys: %s" % list(llm_api_response_data.keys())
                )
                logger.debug("Full LLM response: %s" % json.dumps(llm_api_response_data)[:1000])
                raise ValueError("LLM response did not contain an expected text field.")

            json_text_output = re.sub(r"^```json\s*|\s*```$", "", json_text_output.strip(), flags=re.MULTILINE)

            cleaned_json_str = json_text_output.strip()
            cleaned_json_str = re.sub(r"^```(?:json)?\s*|\s*```$", "", cleaned_json_str, flags=re.IGNORECASE | re.MULTILINE).strip()
            first_brace = cleaned_json_str.find('{')
            last_brace = cleaned_json_str.rfind('}')
            if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
                cleaned_json_str = cleaned_json_str[first_brace:last_brace + 1]
            if not cleaned_json_str.startswith('{'):
                cleaned_json_str = json_text_output.strip()
            json_text_output = cleaned_json_str

            try:
                parsed_json_data = json.loads(json_text_output)
            except json.JSONDecodeError:
                fixed_str = _clean_loose_json(json_text_output)
                try:
                    parsed_json_data = json.loads(fixed_str)
                except json.JSONDecodeError:
                    import ast
                    try:
                        parsed_json_data = ast.literal_eval(fixed_str)
                    except (ValueError, SyntaxError):
                        # Last-ditch heuristic: convert single quotes to double and
                        # Python constants to JSON equivalents, then try json.loads again.
                        brute = fixed_str
                        brute = re.sub(r"'([^']*)'", lambda m: '"' + m.group(1).replace('"', '\\"') + '"', brute)
                        brute = brute.replace("True", "true").replace("False", "false").replace("None", "null")
                        parsed_json_data = json.loads(brute)

            logger.debug(f'Cohere response: {parsed_json_data}')

            if 'detectedLanguage' not in parsed_json_data or not parsed_json_data['detectedLanguage']:
                parsed_json_data['detectedLanguage'] = detected_lang_by_rules

            if target_model == TrainingPlanDetails and ('mainTitle' not in parsed_json_data or not parsed_json_data['mainTitle']):
                parsed_json_data['mainTitle'] = project_name
            elif target_model == PdfLessonDetails and ('lessonTitle' not in parsed_json_data or not parsed_json_data['lessonTitle']):
                parsed_json_data['lessonTitle'] = project_name
            elif target_model == TextPresentationDetails and ('textTitle' not in parsed_json_data or not parsed_json_data['textTitle']):
                parsed_json_data['textTitle'] = project_name
            elif target_model == QuizData and ('quizTitle' not in parsed_json_data or not parsed_json_data['quizTitle']):
                parsed_json_data['quizTitle'] = project_name

            # Round hours to integers before validation to prevent float validation errors
            if target_model == TrainingPlanDetails:
                parsed_json_data = round_hours_in_content(parsed_json_data)
            
            validated_model = target_model.model_validate(parsed_json_data)
            logger.info(f"LLM parsing for '{project_name}' succeeded on attempt #{attempt_number}.")

            # Log AI parser usage
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    response_tokens = len(encoding.encode(json.dumps(parsed_json_data)))
                    total_tokens = prompt_tokens + response_tokens
                    
                    logger.info(f"=== AI PARSER LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Response tokens: {response_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"=== END AI PARSER LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 200, int((time.time() - start_time) * 1000), len(ai_response), len(json.dumps(parsed_json_data)), None, True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Database insert result: {result}")
                            logger.info(f"Successfully logged AI parser usage for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Database insert failed: {db_error}")
                            logger.error(f"Insert parameters: endpoint='/ai/parse', method='POST', status_code=200, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as e:
                    logger.warning(f"Failed to log AI parser usage: {e}")
                    logger.error(f"AI Parser logging error details: {str(e)}")

            return validated_model

        except Exception as e:
            last_exception = e
            logger.warning(
                f"LLM parsing attempt #{attempt_number} for '{project_name}' failed with {type(e).__name__}. "
                f"Details: {str(e)[:250]}. Trying next key if available."
            )
            
            # Log failed AI parser attempt
            if DB_POOL:
                try:
                    # Count tokens using tiktoken
                    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
                    prompt_tokens = len(encoding.encode(ai_response))
                    total_tokens = prompt_tokens  # No response tokens for failed attempts
                    
                    logger.info(f"=== AI PARSER FAILED LOGGING DEBUG ===")
                    logger.info(f"Project: {project_name}")
                    logger.info(f"Prompt tokens: {prompt_tokens}")
                    logger.info(f"Total tokens: {total_tokens}")
                    logger.info(f"Response time: {int((time.time() - start_time) * 1000)}ms")
                    logger.info(f"Error: {str(e)[:200]}")
                    logger.info(f"=== END AI PARSER FAILED LOGGING DEBUG ===")
                    
                    async with DB_POOL.acquire() as conn:
                        logger.info(f"About to insert failed AI parser record for {project_name}")
                        try:
                            result = await conn.execute(
                                "INSERT INTO request_analytics (id, endpoint, method, user_id, status_code, response_time_ms, request_size_bytes, response_size_bytes, error_message, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)",
                                str(uuid.uuid4()), '/ai/parse', 'POST', None, 500, int((time.time() - start_time) * 1000), len(ai_response), 0, str(e)[:500], True, total_tokens, LLM_DEFAULT_MODEL, project_name, datetime.now(timezone.utc)
                            )
                            logger.info(f"Failed attempt database insert result: {result}")
                            logger.info(f"Successfully logged failed AI parser attempt for {project_name}")
                        except Exception as db_error:
                            logger.error(f"Failed attempt database insert failed: {db_error}")
                            logger.error(f"Failed attempt insert parameters: endpoint='/ai/parse', method='POST', status_code=500, tokens={total_tokens}, model={LLM_DEFAULT_MODEL}, project={project_name}")
                            raise
                except Exception as log_error:
                    logger.warning(f"Failed to log AI parser error: {log_error}")
                    logger.error(f"AI Parser failed logging error details: {str(log_error)}")
            
            continue

    # --- Handle Final Failure ---
    # This block is reached only if the loop completes without a successful return.
    logger.error(f"All LLM API call attempts failed for '{project_name}'. Last error: {last_exception}")
    if hasattr(default_error_model_instance, 'detectedLanguage'):
        default_error_model_instance.detectedLanguage = detected_lang_by_rules
    return default_error_model_instance

def _clean_loose_json(text: str) -> str:
    """Attempt to fix common minor JSON issues produced by LLMs: trailing commas, smart quotes, etc."""
    # remove common markdown code fences again (defensive)
    text = re.sub(r"^```(?:json)?\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE | re.MULTILINE)
    # replace smart quotes with standard quotes
    for sq in ('\u201c', '\u201d', '\u2018', '\u2019'):
        text = text.replace(sq, '"')
    # strip trailing commas before object/array close
    text = re.sub(r",\s*(\}|\])", r"\1", text)
    # remove escaped newlines that are unnecessary
    text = text.replace('\\n', '\n')
    return text

# --- API Endpoints ---
@app.post("/api/custom/pipelines/add", response_model=MicroproductPipelineDBRaw, status_code=status.HTTP_201_CREATED)
async def add_pipeline(pipeline_data: MicroproductPipelineCreateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        INSERT INTO microproduct_pipelines (pipeline_name, pipeline_description, {db_is_discovery}, {db_is_structuring}, prompts_data_collection, prompts_data_formating, created_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create pipeline.")
        return MicroproductPipelineDBRaw(**dict(row))
    except Exception as e:
        logger.error(f"Error inserting pipeline: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding the pipeline." if IS_PRODUCTION else f"DB error on pipeline insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines", response_model=List[MicroproductPipelineGetResponse])
async def get_pipelines(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines ORDER BY created_at DESC;"
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query)
        pipelines_list = [MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row))) for row in rows]
        return pipelines_list
    except Exception as e:
        logger.error(f"Error fetching pipelines: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching pipelines." if IS_PRODUCTION else f"DB error fetching pipelines: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/pipelines/{pipeline_id}", response_model=MicroproductPipelineGetResponse)
async def get_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at FROM microproduct_pipelines WHERE id = $1;"
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return MicroproductPipelineGetResponse.from_db_model(MicroproductPipelineDBRaw(**dict(row)))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching the pipeline." if IS_PRODUCTION else f"DB error fetching pipeline: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/pipelines/update/{pipeline_id}", response_model=MicroproductPipelineDBRaw)
async def update_pipeline(pipeline_id: int, pipeline_data: MicroproductPipelineUpdateRequest, pool: asyncpg.Pool = Depends(get_db_pool)):
    discovery_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.discovery_prompts_list) if prompt.strip()} if pipeline_data.discovery_prompts_list else None
    structuring_prompts_json_for_db = {str(i+1): prompt for i, prompt in enumerate(pipeline_data.structuring_prompts_list) if prompt.strip()} if pipeline_data.structuring_prompts_list else None
    db_is_discovery = pipeline_data.model_fields['is_discovery_prompts'].alias if pipeline_data.model_fields['is_discovery_prompts'].alias else 'is_discovery_prompts'
    db_is_structuring = pipeline_data.model_fields['is_structuring_prompts'].alias if pipeline_data.model_fields['is_structuring_prompts'].alias else 'is_structuring_prompts'
    query = f"""
        UPDATE microproduct_pipelines SET pipeline_name = $1, pipeline_description = $2, {db_is_discovery} = $3, {db_is_structuring} = $4, prompts_data_collection = $5, prompts_data_formating = $6
        WHERE id = $7 RETURNING id, pipeline_name, pipeline_description, is_prompts_data_collection, is_prompts_data_formating, prompts_data_collection, prompts_data_formating, created_at;
    """
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, pipeline_data.pipeline_name, pipeline_data.pipeline_description,
                                      pipeline_data.is_discovery_prompts, pipeline_data.is_structuring_prompts,
                                      discovery_prompts_json_for_db, structuring_prompts_json_for_db, pipeline_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found or update failed.")
        return MicroproductPipelineDBRaw(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating the pipeline." if IS_PRODUCTION else f"DB error on pipeline update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/pipelines/delete/{pipeline_id}", status_code=status.HTTP_200_OK)
async def delete_pipeline(pipeline_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "DELETE FROM microproduct_pipelines WHERE id = $1 RETURNING id;"
    try:
        async with pool.acquire() as conn:
            deleted_id = await conn.fetchval(query, pipeline_id)
        if deleted_id is None:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Pipeline not found.")
        return {"detail": f"Successfully deleted pipeline with ID {pipeline_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting pipeline {pipeline_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the pipeline." if IS_PRODUCTION else f"DB error on pipeline deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_design_template_image(file: UploadFile = File(...)):
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 5 * 1024 * 1024
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving design image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/onepager/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_onepager_image(file: UploadFile = File(...)):
    """Upload an image for use in one-pagers"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for one-pager images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"onepager_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving one-pager image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

@app.post("/api/custom/presentation/upload_image", responses={200: {"description": "Image uploaded successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/your_image_name.png"}}}},400: {"description": "Invalid file type or other error", "model": ErrorDetail},413: {"description": "File too large", "model": ErrorDetail}})
async def upload_presentation_image(file: UploadFile = File(...)):
    """Upload an image for use in presentations"""
    allowed_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp"}; max_file_size = 10 * 1024 * 1024  # 10MB for presentation images
    file_content = await file.read()
    if len(file_content) > max_file_size:
        detail_msg = "File too large." if IS_PRODUCTION else f"File too large. Max size {max_file_size // (1024*1024)}MB."
        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=detail_msg)
    await file.seek(0)
    file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ".png"
    if file_extension not in allowed_extensions:
        detail_msg = "Invalid file type." if IS_PRODUCTION else f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail_msg)
    safe_filename_base = str(uuid.uuid4()); unique_filename = f"presentation_{safe_filename_base}{file_extension}"; file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
    try:
        with open(file_path_on_disk, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        logger.error(f"Error saving presentation image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "Could not save image." if IS_PRODUCTION else f"Could not save image: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        await file.close()
    web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
    return {"file_path": web_accessible_path}

# NEW: AI Image Generation Endpoint
class AIImageGenerationRequest(BaseModel):
    prompt: str = Field(..., description="Text prompt for image generation")
    width: int = Field(..., description="Image width in pixels", ge=256, le=1792)
    height: int = Field(..., description="Image height in pixels", ge=256, le=1792)
    quality: str = Field(default="standard", description="Image quality: standard or hd")
    style: str = Field(default="vivid", description="Image style: vivid or natural")
    model: str = Field(default="gemini-2.5-flash-image-preview", description="Image generation model to use")

@app.post("/api/custom/presentation/generate_image", responses={
    200: {"description": "Image generated successfully", "content": {"application/json": {"example": {"file_path": f"/{STATIC_DESIGN_IMAGES_DIR}/ai_generated_image.png"}}}},
    400: {"description": "Invalid request parameters", "model": ErrorDetail},
    500: {"description": "AI generation failed", "model": ErrorDetail}
})
async def generate_ai_image(request: AIImageGenerationRequest):
    """Generate an image using Google Gemini AI"""
    try:
        logger.info(f"[AI_IMAGE_GENERATION] Starting generation with prompt: '{request.prompt[:50]}...'")
        logger.info(f"[AI_IMAGE_GENERATION] Dimensions: {request.width}x{request.height}, Quality: {request.quality}, Style: {request.style}")
        
        # Validate dimensions (Gemini supports flexible dimensions, but we'll keep the same validation for consistency)
        valid_sizes = [(1024, 1024), (1792, 1024), (1024, 1792)]
        current_size = (request.width, request.height)
        
        if current_size not in valid_sizes:
            # Find the closest valid size based on aspect ratio
            aspect_ratio = request.width / request.height
            
            if aspect_ratio > 1.5:  # Landscape
                request.width, request.height = 1792, 1024
            elif aspect_ratio < 0.7:  # Portrait
                request.width, request.height = 1024, 1792
            else:  # Square-ish
                request.width, request.height = 1024, 1024
                
            logger.info(f"[AI_IMAGE_GENERATION] Adjusted dimensions from {current_size} to {request.width}x{request.height}")
        
        # Get Gemini client using the new API
        if not GEMINI_API_KEY:
            raise ValueError("No Gemini API key configured. Set GEMINI_API_KEY environment variable.")
        
        # Configure the existing genai module with API key
        genai.configure(api_key=GEMINI_API_KEY)
        
        # Generate image using Gemini
        model = genai.GenerativeModel('gemini-2.5-flash-image-preview')
        
        # 🔍 ENHANCED LOGGING: Log the request being sent to Gemini
        logger.info(f"🔍 [GEMINI API REQUEST] Sending request to Gemini API")
        logger.info(f"🔍 [GEMINI API REQUEST] Model: gemini-2.5-flash-image-preview")
        logger.info(f"🔍 [GEMINI API REQUEST] Prompt length: {len(request.prompt)} characters")
        logger.info(f"🔍 [GEMINI API REQUEST] Full prompt: '{request.prompt}'")
        
        response = model.generate_content(request.prompt)
        
        # 🔍 ENHANCED LOGGING: Log raw response from Gemini
        logger.info(f"🔍 [GEMINI API RESPONSE] Raw response received from Gemini")
        logger.info(f"🔍 [GEMINI API RESPONSE] Response type: {type(response)}")
        logger.info(f"🔍 [GEMINI API RESPONSE] Response has candidates: {hasattr(response, 'candidates')}")
        
        if hasattr(response, 'candidates'):
            logger.info(f"🔍 [GEMINI API RESPONSE] Number of candidates: {len(response.candidates) if response.candidates else 0}")
            if response.candidates:
                logger.info(f"🔍 [GEMINI API RESPONSE] First candidate type: {type(response.candidates[0])}")
                if hasattr(response.candidates[0], 'content'):
                    logger.info(f"🔍 [GEMINI API RESPONSE] Content type: {type(response.candidates[0].content)}")
                    if hasattr(response.candidates[0].content, 'parts'):
                        logger.info(f"🔍 [GEMINI API RESPONSE] Number of parts: {len(response.candidates[0].content.parts)}")
        
        if not response.candidates or len(response.candidates) == 0:
            logger.error(f"❌ [GEMINI API ERROR] No candidates in response")
            raise Exception("No image data received from Gemini")
        
        # Get the generated image data using the new API structure
        image_data_raw = None
        logger.info(f"🔍 [BASE64 EXTRACTION] Searching for image data in response parts...")
        
        for i, part in enumerate(response.candidates[0].content.parts):
            logger.info(f"🔍 [BASE64 EXTRACTION] Part {i}: type={type(part)}")
            logger.info(f"🔍 [BASE64 EXTRACTION] Part {i} has inline_data: {hasattr(part, 'inline_data')}")
            
            if hasattr(part, 'inline_data') and part.inline_data:
                logger.info(f"🔍 [BASE64 EXTRACTION] Found inline_data in part {i}")
                logger.info(f"🔍 [BASE64 EXTRACTION] inline_data type: {type(part.inline_data)}")
                logger.info(f"🔍 [BASE64 EXTRACTION] inline_data has data: {hasattr(part.inline_data, 'data')}")
                
                # 🔧 ROBUST DATA EXTRACTION: Try multiple ways to get the data
                extracted_data = None
                
                if hasattr(part.inline_data, 'data') and part.inline_data.data:
                    extracted_data = part.inline_data.data
                    logger.info(f"🔧 [EXTRACTION METHOD 1] Got data via .data attribute")
                elif hasattr(part.inline_data, 'data') and part.inline_data.data is not None:
                    extracted_data = part.inline_data.data
                    logger.info(f"🔧 [EXTRACTION METHOD 2] Got data via .data attribute (None check)")
                else:
                    # Try to access data through other possible attributes
                    logger.info(f"🔧 [EXTRACTION DEBUG] inline_data attributes: {dir(part.inline_data)}")
                    
                    # Check if there are other attributes that might contain the data
                    for attr_name in dir(part.inline_data):
                        if not attr_name.startswith('_'):
                            attr_value = getattr(part.inline_data, attr_name)
                            logger.info(f"🔧 [EXTRACTION DEBUG] {attr_name}: {type(attr_value)} - {len(attr_value) if hasattr(attr_value, '__len__') else 'No length'}")
                            
                            # If we find binary data that looks like an image
                            if isinstance(attr_value, bytes) and len(attr_value) > 1000 and (attr_value.startswith(b'\x89PNG') or attr_value.startswith(b'\xff\xd8\xff')):
                                extracted_data = attr_value
                                logger.info(f"🔧 [EXTRACTION METHOD 3] Found image data in {attr_name}")
                                break
                
                if extracted_data:
                    image_data_raw = extracted_data
                    logger.info(f"🔍 [DATA EXTRACTION] Extracted image data from Gemini")
                    logger.info(f"🔍 [DATA EXTRACTION] Data type: {type(image_data_raw)}")
                    logger.info(f"🔍 [DATA EXTRACTION] Data length: {len(image_data_raw) if image_data_raw else 0}")
                    if image_data_raw:
                        logger.info(f"🔍 [DATA EXTRACTION] First 100 chars: {image_data_raw[:100]}")
                        logger.info(f"🔍 [DATA EXTRACTION] Last 100 chars: {image_data_raw[-100:]}")
                    break
                else:
                    logger.warning(f"⚠️ [DATA EXTRACTION WARNING] Part {i} has inline_data but no extractable data")
        
        if not image_data_raw:
            logger.error(f"❌ [DATA EXTRACTION ERROR] No image data found in response parts")
            logger.error(f"❌ [DATA EXTRACTION ERROR] Response had {len(response.candidates[0].content.parts)} parts")
            for i, part in enumerate(response.candidates[0].content.parts):
                logger.error(f"❌ [DATA EXTRACTION ERROR] Part {i} details:")
                logger.error(f"❌ [DATA EXTRACTION ERROR] - Has inline_data: {hasattr(part, 'inline_data')}")
                if hasattr(part, 'inline_data') and part.inline_data:
                    logger.error(f"❌ [DATA EXTRACTION ERROR] - inline_data type: {type(part.inline_data)}")
                    logger.error(f"❌ [DATA EXTRACTION ERROR] - Has data attr: {hasattr(part.inline_data, 'data')}")
                    if hasattr(part.inline_data, 'data'):
                        logger.error(f"❌ [DATA EXTRACTION ERROR] - Data is None: {part.inline_data.data is None}")
                        logger.error(f"❌ [DATA EXTRACTION ERROR] - Data length: {len(part.inline_data.data) if part.inline_data.data else 'N/A'}")
            raise Exception("No image data received from Gemini")
        
        logger.info(f"✅ [DATA EXTRACTION] Successfully extracted image data")
        
        # 🔧 CRITICAL FIX: Detect if data is already binary or base64 string
        if isinstance(image_data_raw, bytes):
            # Data is already binary (PNG/JPEG), use directly
            logger.info(f"🔧 [DATA TYPE FIX] Data is already binary format - using directly")
            image_data = image_data_raw
        elif isinstance(image_data_raw, str):
            # Data is base64 string, decode it
            logger.info(f"🔧 [DATA TYPE FIX] Data is base64 string - decoding")
            try:
                import base64
                image_data = base64.b64decode(image_data_raw, validate=True)
                logger.info(f"✅ [BASE64 DECODING] Successfully decoded base64 string")
            except Exception as e:
                logger.error(f"❌ [BASE64 DECODING ERROR] Failed to decode base64 string: {e}")
                raise Exception(f"Invalid base64 data received from Gemini: {e}")
        else:
            logger.error(f"❌ [DATA TYPE ERROR] Unexpected data type: {type(image_data_raw)}")
            raise Exception(f"Unexpected data type from Gemini: {type(image_data_raw)}")
        
        logger.info(f"🔍 [FINAL DATA] Final image data size: {len(image_data)} bytes")
        logger.info(f"🔍 [FINAL DATA] Data type: {type(image_data)}")
        
        # Validate that it's actually image data
        if len(image_data) < 100:
            logger.error(f"❌ [IMAGE VALIDATION ERROR] Decoded data too small: {len(image_data)} bytes")
            raise Exception("Decoded image data is too small to be a valid image")
        
        # Check for PNG/JPEG headers
        if image_data.startswith(b'\x89PNG'):
            logger.info(f"✅ [IMAGE VALIDATION] Detected PNG format")
        elif image_data.startswith(b'\xff\xd8\xff'):
            logger.info(f"✅ [IMAGE VALIDATION] Detected JPEG format")
        else:
            logger.warning(f"⚠️ [IMAGE VALIDATION WARNING] Unknown image format, first 20 bytes: {image_data[:20]}")
        
        logger.info(f"🔍 [IMAGE VALIDATION] Image data first 20 bytes: {image_data[:20]}")
        logger.info(f"🔍 [IMAGE VALIDATION] Image data last 20 bytes: {image_data[-20:]}")
        
        # Save the image to disk
        safe_filename_base = str(uuid.uuid4())
        unique_filename = f"ai_generated_{safe_filename_base}.png"
        file_path_on_disk = os.path.join(STATIC_DESIGN_IMAGES_DIR, unique_filename)
        
        logger.info(f"🔍 [FILE WRITING] Starting file write operation")
        logger.info(f"🔍 [FILE WRITING] Static images directory: {STATIC_DESIGN_IMAGES_DIR}")
        logger.info(f"🔍 [FILE WRITING] Unique filename: {unique_filename}")
        logger.info(f"🔍 [FILE WRITING] Full file path: {file_path_on_disk}")
        logger.info(f"🔍 [FILE WRITING] Data size to write: {len(image_data)} bytes")
        
        # Check if directory exists
        if not os.path.exists(STATIC_DESIGN_IMAGES_DIR):
            logger.error(f"❌ [FILE WRITING ERROR] Directory does not exist: {STATIC_DESIGN_IMAGES_DIR}")
            raise Exception(f"Static images directory does not exist: {STATIC_DESIGN_IMAGES_DIR}")
        
        logger.info(f"✅ [FILE WRITING] Directory exists: {STATIC_DESIGN_IMAGES_DIR}")
        
        try:
            logger.info(f"🔍 [FILE WRITING] Opening file for writing: {file_path_on_disk}")
            with open(file_path_on_disk, "wb") as buffer:
                logger.info(f"🔍 [FILE WRITING] File opened successfully, writing data...")
                bytes_written = buffer.write(image_data)
                logger.info(f"✅ [FILE WRITING] Successfully wrote {bytes_written} bytes to file")
            
            # Verify file was written correctly
            if os.path.exists(file_path_on_disk):
                file_size = os.path.getsize(file_path_on_disk)
                logger.info(f"✅ [FILE VERIFICATION] File exists on disk")
                logger.info(f"🔍 [FILE VERIFICATION] File size on disk: {file_size} bytes")
                logger.info(f"🔍 [FILE VERIFICATION] Expected size: {len(image_data)} bytes")
                
                if file_size == len(image_data):
                    logger.info(f"✅ [FILE VERIFICATION] File size matches expected size")
                else:
                    logger.error(f"❌ [FILE VERIFICATION ERROR] File size mismatch! Expected: {len(image_data)}, Actual: {file_size}")
                
                # Try to read the file back to verify integrity
                try:
                    with open(file_path_on_disk, "rb") as verify_buffer:
                        verify_data = verify_buffer.read()
                        if verify_data == image_data:
                            logger.info(f"✅ [FILE VERIFICATION] File content matches original data")
                        else:
                            logger.error(f"❌ [FILE VERIFICATION ERROR] File content does not match original data")
                            logger.error(f"❌ [FILE VERIFICATION ERROR] Original first 20 bytes: {image_data[:20]}")
                            logger.error(f"❌ [FILE VERIFICATION ERROR] File first 20 bytes: {verify_data[:20]}")
                except Exception as verify_error:
                    logger.error(f"❌ [FILE VERIFICATION ERROR] Could not read file for verification: {verify_error}")
            else:
                logger.error(f"❌ [FILE VERIFICATION ERROR] File does not exist after writing: {file_path_on_disk}")
            
            web_accessible_path = f"/{STATIC_DESIGN_IMAGES_DIR}/{unique_filename}"
            logger.info(f"✅ [FILE WRITING] Image saved successfully: {web_accessible_path}")
            logger.info(f"🔍 [FILE WRITING] Web accessible path: {web_accessible_path}")
            logger.info(f"🔍 [FILE WRITING] Full URL would be: https://dev4.contentbuilder.ai{web_accessible_path}")
            
            return {
                "file_path": web_accessible_path,
                "prompt": request.prompt,
                "dimensions": {"width": request.width, "height": request.height},
                "quality": request.quality,
                "style": request.style
            }
            
        except Exception as e:
            logger.error(f"❌ [FILE WRITING ERROR] Error saving image to disk: {e}", exc_info=not IS_PRODUCTION)
            logger.error(f"❌ [FILE WRITING ERROR] File path attempted: {file_path_on_disk}")
            logger.error(f"❌ [FILE WRITING ERROR] Data size: {len(image_data)} bytes")
            detail_msg = "Could not save generated image." if IS_PRODUCTION else f"Could not save generated image: {str(e)}"
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[AI_IMAGE_GENERATION] Error generating image: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "AI image generation failed." if IS_PRODUCTION else f"AI image generation failed: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/design_templates/add", response_model=DesignTemplateResponse, status_code=status.HTTP_201_CREATED)
async def add_design_template(template_data: DesignTemplateCreate, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "INSERT INTO design_templates (template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created) VALUES ($1, $2, $3, $4, $5, $6) RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"
    try:
        async with pool.acquire() as conn:
            current_time = datetime.now(timezone.utc)
            row = await conn.fetchrow(query, template_data.template_name, template_data.template_structuring_prompt, template_data.design_image_path, template_data.microproduct_type, template_data.component_name, current_time)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Design template with this name already exists." if IS_PRODUCTION else f"Design template with name '{template_data.template_name}' already exists."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except Exception as e:
        logger.error(f"Error inserting design template: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding design template." if IS_PRODUCTION else f"DB error on design template insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates", response_model=List[DesignTemplateResponse])
async def get_design_templates_list(pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates ORDER BY date_created DESC;"
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query)
        return [DesignTemplateResponse(**dict(row)) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching design templates: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design templates." if IS_PRODUCTION else f"DB error fetching design templates: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/design_templates/{template_id}", response_model=DesignTemplateResponse)
async def get_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "SELECT id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created FROM design_templates WHERE id = $1;"
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, template_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")
        return DesignTemplateResponse(**dict(row))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching design template." if IS_PRODUCTION else f"DB error fetching design template: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/design_templates/update/{template_id}", response_model=DesignTemplateResponse)
async def update_design_template(template_id: int, template_data: DesignTemplateUpdate, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            existing_template_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", template_id)
            if not existing_template_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found")

        update_fields = template_data.model_dump(exclude_unset=True)
        if not update_fields:
            return DesignTemplateResponse(**dict(existing_template_row))

        set_clauses = []; update_values = []; i = 1
        for key, value in update_fields.items(): set_clauses.append(f"{key} = ${i}"); update_values.append(value); i += 1
        update_values.append(template_id)
        query = f"UPDATE design_templates SET {', '.join(set_clauses)} WHERE id = ${i} RETURNING id, template_name, template_structuring_prompt, design_image_path, microproduct_type, component_name, date_created;"

        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, *update_values)
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update design template.")
        return DesignTemplateResponse(**dict(row))
    except asyncpg.exceptions.UniqueViolationError:
        detail_msg = "Update would violate a unique constraint (e.g., template name)."
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=detail_msg)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while updating design template." if IS_PRODUCTION else f"DB error on design template update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.delete("/api/custom/design_templates/delete/{template_id}", status_code=status.HTTP_200_OK)
async def delete_design_template(template_id: int, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        async with pool.acquire() as conn:
            template_to_delete = await conn.fetchrow("SELECT design_image_path FROM design_templates WHERE id = $1", template_id)
            if not template_to_delete:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found.")

            if template_to_delete["design_image_path"]:
                filename_only = os.path.basename(template_to_delete["design_image_path"])
                full_image_path = os.path.join(STATIC_DESIGN_IMAGES_DIR, filename_only)
                if os.path.exists(full_image_path):
                    try:
                        os.remove(full_image_path)
                        logger.info(f"Successfully deleted image file: {full_image_path}")
                    except OSError as e_img:
                        logger.warning(f"Error deleting image file {full_image_path}: {e_img}. Continuing with DB record deletion.", exc_info=not IS_PRODUCTION)
                else:
                    logger.warning(f"Image file not found for deletion: {full_image_path}")
            deleted_count_status = await conn.execute("DELETE FROM design_templates WHERE id = $1", template_id)
        if deleted_count_status == "DELETE 0":
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Design template not found during delete, or already deleted.")
        return {"detail": f"Successfully initiated deletion for design template with ID {template_id}."}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting design template {template_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during design template deletion." if IS_PRODUCTION else f"DB error on design template deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS = [
    "Training Plan", "PDF Lesson", "Slide Deck", "Text Presentation"
]

# Constants for text size thresholds - Aggressive thresholds to prevent AI memory issues
TEXT_SIZE_THRESHOLD = 1500  # Characters - switch to compression for texts larger than this
LARGE_TEXT_THRESHOLD = 3000  # Characters - use virtual file system to prevent AI memory issues
VIRTUAL_TEXT_FILE_PREFIX = "paste_text_"

# Cache for virtual text files to prevent duplicate uploads
VIRTUAL_TEXT_FILE_CACHE: Dict[str, int] = {}

def compress_text(text_content: str) -> str:
    """
    Compress large text content using gzip and encode as base64.
    This reduces the payload size for large texts.
    """
    try:
        # Compress the text
        text_bytes = text_content.encode('utf-8')
        compressed = gzip.compress(text_bytes)
        # Encode as base64 for JSON transmission
        compressed_b64 = base64.b64encode(compressed).decode('utf-8')
        logger.info(f"Compressed text from {len(text_content)} chars to {len(compressed_b64)} chars (reduction: {(1 - len(compressed_b64)/len(text_content))*100:.1f}%)")
        return compressed_b64
    except Exception as e:
        logger.error(f"Error compressing text: {e}")
        # Return original text if compression fails
        return text_content

def decompress_text(compressed_b64: str) -> str:
    """
    Decompress base64-encoded gzipped text content.
    """
    try:
        # Decode from base64
        compressed = base64.b64decode(compressed_b64.encode('utf-8'))
        # Decompress
        text_bytes = gzip.decompress(compressed)
        return text_bytes.decode('utf-8')
    except Exception as e:
        logger.error(f"Error decompressing text: {e}")
        # Return original if decompression fails (assume it wasn't compressed)
        return compressed_b64

def chunk_text(text_content: str, max_chunk_size: int = 2000) -> List[str]:
    """
    Split large text into manageable chunks while preserving sentence boundaries.
    """
    if len(text_content) <= max_chunk_size:
        return [text_content]
    
    chunks = []
    current_chunk = ""
    
    # Split by sentences first, then by words if needed
    sentences = text_content.replace('\n', ' ').split('. ')
    
    for sentence in sentences:
        # Add period back if it's not the last sentence
        if not sentence.endswith('.') and sentence != sentences[-1]:
            sentence += '.'
        
        # If adding this sentence would exceed chunk size
        if len(current_chunk) + len(sentence) + 1 > max_chunk_size:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                # Single sentence is too long, split by words
                words = sentence.split()
                for word in words:
                    if len(current_chunk) + len(word) + 1 > max_chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            current_chunk = word
                        else:
                            # Single word is too long, truncate
                            chunks.append(word[:max_chunk_size])
                            current_chunk = ""
                    else:
                        current_chunk += " " + word if current_chunk else word
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    logger.info(f"Split text into {len(chunks)} chunks (original: {len(text_content)} chars)")
    return chunks



async def create_virtual_text_file(text_content: str, cookies: Dict[str, str]) -> int:
    """
    Create a virtual text file for large text content and return the file ID.
    Uses caching to prevent duplicate uploads of the same text.
    """
    try:
        # Create a hash of the text content for caching
        import hashlib
        text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
        
        # Check if we already have this text cached
        if text_hash in VIRTUAL_TEXT_FILE_CACHE:
            cached_file_id = VIRTUAL_TEXT_FILE_CACHE[text_hash]
            logger.info(f"Using cached virtual file for text hash {text_hash[:8]}... -> file ID: {cached_file_id}")
            return cached_file_id
        
        # Create a temporary file-like object with the text content
        text_bytes = text_content.encode('utf-8')
        text_file = io.BytesIO(text_bytes)
        
        # Create a filename with timestamp for uniqueness
        timestamp = int(asyncio.get_event_loop().time())
        filename = f"{VIRTUAL_TEXT_FILE_PREFIX}{timestamp}.txt"
        
        # Create FormData for file upload
        files = {
            'files': (filename, text_file, 'text/plain')
        }
        
        # Upload file to Onyx file system
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files
            logger.info(f"Uploading virtual text file: {filename} ({len(text_content)} chars)")
            
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/user/file/upload",
                files=files,
                cookies=cookies
            )
            response.raise_for_status()
            
            # Parse response to get file ID
            upload_result = response.json()
            if not upload_result or len(upload_result) == 0:
                raise HTTPException(status_code=500, detail="No file ID returned from upload response")
            
            file_id = upload_result[0].get('id')
            if not file_id:
                raise HTTPException(status_code=500, detail="Invalid file ID in upload response")
            
            logger.info(f"File uploaded successfully with ID: {file_id}")
            
            # Cache the file ID for this text content
            VIRTUAL_TEXT_FILE_CACHE[text_hash] = file_id
            
            # For text files, we don't need to wait for processing - they're immediately available
            # The 405 error suggests the status endpoint doesn't exist for simple text files
            logger.info(f"Virtual text file ready for use: {file_id}")
            return file_id
                    
    except Exception as e:
        logger.error(f"Error creating virtual text file: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail=f"Failed to create virtual text file: {str(e)}")

# --- Enhanced Hybrid Approach Functions ---

# Cache for file contexts to avoid repeated extraction
FILE_CONTEXT_CACHE: Dict[str, Dict[str, Any]] = {}
FILE_CONTEXT_CACHE_TTL = 3600  # 1 hour cache

async def extract_file_context_from_onyx(file_ids: List[int], folder_ids: List[int], cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract relevant context from files and folders using Onyx's capabilities.
    Returns structured context that can be used with OpenAI.
    """
    try:
        # Create cache key
        cache_key = f"{hash(tuple(sorted(file_ids)))}_{hash(tuple(sorted(folder_ids)))}"
        
        # Check cache first
        if cache_key in FILE_CONTEXT_CACHE:
            cached_data = FILE_CONTEXT_CACHE[cache_key]
            if time.time() - cached_data["timestamp"] < FILE_CONTEXT_CACHE_TTL:
                logger.info(f"[FILE_CONTEXT] Using cached context for key: {cache_key[:16]}...")
                return cached_data["context"]
        
        logger.info(f"[FILE_CONTEXT] Extracting context from {len(file_ids)} files and {len(folder_ids)} folders")
        
        extracted_context = {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {
                "total_files": len(file_ids),
                "total_folders": len(folder_ids),
                "extraction_time": time.time()
            }
        }
        
        # Extract file contexts with enhanced retry mechanism
        successful_extractions = 0
        for file_id in file_ids:
            file_context = None
            for retry_attempt in range(3):  # Up to 3 attempts per file
                try:
                    file_context = await extract_single_file_context(file_id, cookies)
                    if file_context and (file_context.get("summary") or file_context.get("content")):
                        # Check if this was a successful extraction (not a generic response or error)
                        content = file_context.get("content", "")
                        if any(phrase in content.lower() for phrase in ["file access issue", "not indexed", "could not access", "file_access_error"]):
                            logger.warning(f"[FILE_CONTEXT] File {file_id} has access issues (attempt {retry_attempt + 1})")
                            if retry_attempt < 2:  # Don't sleep on the last attempt
                                await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                                continue
                        
                        # Success - add to context
                        extracted_context["file_summaries"].append(file_context["summary"])
                        extracted_context["file_contents"].append(file_context["content"])
                        extracted_context["key_topics"].extend(file_context.get("topics", []))
                        successful_extractions += 1
                        logger.info(f"[FILE_CONTEXT] Successfully extracted context from file {file_id} (attempt {retry_attempt + 1})")
                        break  # Success, no need for more retries
                    else:
                        logger.warning(f"[FILE_CONTEXT] No valid context extracted from file {file_id} (attempt {retry_attempt + 1})")
                        if retry_attempt < 2:  # Don't sleep on the last attempt
                            await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
                except Exception as e:
                    logger.warning(f"[FILE_CONTEXT] Failed to extract context from file {file_id} (attempt {retry_attempt + 1}): {e}")
                    if retry_attempt < 2:  # Don't sleep on the last attempt
                        await asyncio.sleep(2 * (retry_attempt + 1))  # Exponential backoff
            
            if not file_context or not (file_context.get("summary") or file_context.get("content")):
                logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
        
        # Extract folder contexts
        for folder_id in folder_ids:
            try:
                folder_context = await extract_folder_context(folder_id, cookies)
                if folder_context and folder_context.get("summary"):
                    extracted_context["folder_contexts"].append(folder_context)
                    extracted_context["key_topics"].extend(folder_context.get("topics", []))
                    successful_extractions += 1
                    logger.info(f"[FILE_CONTEXT] Successfully extracted context from folder {folder_id}")
                else:
                    logger.warning(f"[FILE_CONTEXT] No valid context extracted from folder {folder_id}")
            except Exception as e:
                logger.warning(f"[FILE_CONTEXT] Failed to extract context from folder {folder_id}: {e}")
        
        # If no context was extracted successfully, provide a fallback
        if successful_extractions == 0:
            logger.warning(f"[FILE_CONTEXT] No context extracted successfully, providing fallback context")
            extracted_context["file_summaries"] = [f"File(s) provided for content creation (IDs: {file_ids + folder_ids})"]
            extracted_context["key_topics"] = ["content creation", "educational materials"]
            extracted_context["metadata"]["fallback_used"] = True
        
        # Remove duplicate topics
        extracted_context["key_topics"] = list(set(extracted_context["key_topics"]))
        
        # Cache the result
        FILE_CONTEXT_CACHE[cache_key] = {
            "context": extracted_context,
            "timestamp": time.time()
        }
        
        logger.info(f"[FILE_CONTEXT] Successfully extracted context: {len(extracted_context['file_summaries'])} file summaries, {len(extracted_context['key_topics'])} key topics")
        
        return extracted_context
        
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting file context: {e}", exc_info=True)
        return {
            "file_summaries": [],
            "file_contents": [],
            "folder_contexts": [],
            "key_topics": [],
            "metadata": {"error": str(e)}
        }

async def extract_connector_context_from_onyx(connector_sources: str, prompt: str, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from specific connectors using the Search persona with connector filtering.
    This function performs a comprehensive search within selected connectors only.
    Uses the same approach as Knowledge Base search but with connector source filtering.
    """
    try:
        logger.info(f"[CONNECTOR_CONTEXT] Starting connector search for sources: {connector_sources}")
        
        # Parse connector sources
        connector_list = [source.strip() for source in connector_sources.split(',') if source.strip()]
        logger.info(f"[CONNECTOR_CONTEXT] Parsed connector sources: {connector_list}")
        
        # Create a temporary chat session with the Search persona (ID 0)
        search_persona_id = 0
        temp_chat_id = await create_onyx_chat_session(search_persona_id, cookies)
        logger.info(f"[CONNECTOR_CONTEXT] Created search chat session: {temp_chat_id}")
        
        # Create a comprehensive search prompt (similar to Knowledge Base approach)
        search_prompt = f"""
        Please search within the following connector sources for information relevant to this topic: "{prompt}"
        
        Search only within these specific sources: {', '.join(connector_list)}
        
        I need you to:
        1. Search within the specified connector sources only
        2. Find the most relevant information related to this topic
        3. Provide a comprehensive summary of what you find
        4. Extract key topics, concepts, and important details
        5. Identify any specific examples, case studies, or practical applications
        
        Please format your response as:
        SUMMARY: [comprehensive summary of relevant information found]
        KEY_TOPICS: [comma-separated list of key topics and concepts]
        IMPORTANT_DETAILS: [specific details, examples, or practical information]
        RELEVANT_SOURCES: [mention of any specific documents or sources that were particularly relevant]
        
        Focus only on content from these connector sources: {', '.join(connector_list)}
        Be thorough and comprehensive in your search and analysis.
        """
        
        # Use the Search persona to perform the connector-filtered search
        logger.info(f"[CONNECTOR_CONTEXT] Sending search request to Search persona with connector filters")
        search_result = await enhanced_stream_chat_message_with_filters(temp_chat_id, search_prompt, cookies, connector_list)
        logger.info(f"[CONNECTOR_CONTEXT] Received search result ({len(search_result)} chars)")
        
        # Log the full response for debugging
        logger.info(f"[CONNECTOR_CONTEXT] Full search response: {search_result}")
        
        if len(search_result) == 0:
            logger.warning(f"[CONNECTOR_CONTEXT] Search result is empty! This might indicate no documents in connectors or search failed")
        
        # Parse the search result - handle Onyx response format (same as Knowledge Base)
        summary = ""
        key_topics = []
        important_details = ""
        relevant_sources = ""
        
        # Extract content flexibly using string searching
        logger.info(f"[CONNECTOR_CONTEXT] Starting content extraction from search result")
        
        if "SUMMARY:" in search_result:
            summary_start = search_result.find("SUMMARY:") + 8
            summary_end = search_result.find("KEY_TOPICS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("IMPORTANT_DETAILS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("RELEVANT_SOURCES:", summary_start)
            if summary_end == -1:
                summary_end = len(search_result)
            summary = search_result[summary_start:summary_end].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted summary: {len(summary)} chars")
        
        if "KEY_TOPICS:" in search_result:
            topics_start = search_result.find("KEY_TOPICS:") + 11
            topics_end = search_result.find("IMPORTANT_DETAILS:", topics_start)
            if topics_end == -1:
                topics_end = search_result.find("RELEVANT_SOURCES:", topics_start)
            if topics_end == -1:
                # Look for next section marker or end of text
                next_section = search_result.find("\n\n", topics_start)
                topics_end = next_section if next_section != -1 else len(search_result)
            topics_text = search_result[topics_start:topics_end].strip()
            key_topics = [t.strip() for t in topics_text.split(',') if t.strip()]
            logger.info(f"[CONNECTOR_CONTEXT] Extracted {len(key_topics)} key topics")
        
        if "IMPORTANT_DETAILS:" in search_result:
            details_start = search_result.find("IMPORTANT_DETAILS:") + 18
            details_end = search_result.find("RELEVANT_SOURCES:", details_start)
            if details_end == -1:
                details_end = len(search_result)
            important_details = search_result[details_start:details_end].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted important details: {len(important_details)} chars")
        
        if "RELEVANT_SOURCES:" in search_result:
            sources_start = search_result.find("RELEVANT_SOURCES:") + 17
            relevant_sources = search_result[sources_start:].strip()
            logger.info(f"[CONNECTOR_CONTEXT] Extracted relevant sources: {len(relevant_sources)} chars")
        
        # Final fallback if still no content
        if not summary and not key_topics:
            summary = search_result[:1000] + "..." if len(search_result) > 1000 else search_result
            key_topics = ["connector search"]
            logger.info(f"[CONNECTOR_CONTEXT] Using fallback summary from raw response")
        
        # Log the extracted information
        logger.info(f"[CONNECTOR_CONTEXT] Extracted summary: {summary[:200]}...")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted key topics: {key_topics}")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted important details: {important_details[:200]}...")
        logger.info(f"[CONNECTOR_CONTEXT] Extracted relevant sources: {relevant_sources[:200]}...")
        
        # Return context in the same format as knowledge base context
        return {
            "connector_search": True,
            "topic": prompt,
            "connector_sources": connector_list,
            "summary": summary,
            "key_topics": key_topics,
            "important_details": important_details,
            "relevant_sources": relevant_sources,
            "full_search_result": search_result,
            "file_summaries": [{
                "file_id": "connector_search",
                "name": f"Connector Search: {', '.join(connector_list)}",
                "summary": summary,
                "topics": key_topics,
                "key_info": important_details
            }]
        }
        
    except Exception as e:
        logger.error(f"[CONNECTOR_CONTEXT] Error extracting connector context: {e}", exc_info=True)
        # Return fallback context
        return {
            "connector_search": True,
            "topic": prompt,
            "connector_sources": connector_sources.split(','),
            "summary": f"Connector search failed for sources: {connector_sources}",
            "key_topics": ["search error"],
            "important_details": "Unable to search connectors",
            "relevant_sources": "",
            "full_search_result": "",
            "file_summaries": [{
                "file_id": "connector_search_error",
                "name": f"Connector Search Error: {connector_sources}",
                "summary": "Search failed",
                "topics": ["error"],
                "key_info": str(e)
            }]
        }

def _save_section_content(section_name: str, content_lines: list, local_vars: dict):
    """Helper function to save accumulated section content"""
    content = " ".join(content_lines).strip()
    if section_name == "summary":
        local_vars["summary"] = content
    elif section_name == "important_details":
        local_vars["important_details"] = content
    elif section_name == "relevant_sources":
        local_vars["relevant_sources"] = content

async def enhanced_stream_chat_message(chat_session_id: str, message: str, cookies: Dict[str, str]) -> str:
    """Enhanced version of stream_chat_message specifically for Knowledge Base searches with better streaming handling."""
    logger.info(f"[enhanced_stream_chat_message] Starting Knowledge Base search - chat_id={chat_session_id} len(message)={len(message)}")

    async with httpx.AsyncClient(timeout=600.0) as client:  # Longer timeout for KB searches
        retrieval_options = {
            "run_search": "always",  # Always search for Knowledge Base
            "real_time": False,
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": True,  # Force streaming for better control
        }
        
        logger.info(f"[enhanced_stream_chat_message] Sending request to {ONYX_API_SERVER_URL}/chat/send-message")
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/send-message",
            json=payload,
            cookies=cookies,
        )
        
        logger.info(f"[enhanced_stream_chat_message] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        
        # Handle the response
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[enhanced_stream_chat_message] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            last_log_length = 0
            import time
            start_time = time.time()
            last_activity_time = start_time
            max_idle_time = 120.0  # Wait up to 2 minutes without new content
            max_total_time = 600.0  # Maximum 10 minutes total
            
            logger.info(f"[enhanced_stream_chat_message] Starting to read lines from stream...")
            async for line in resp.aiter_lines():
                line_count += 1
                current_time = time.time()
                elapsed_time = current_time - start_time
                idle_time = current_time - last_activity_time
                
                # Log progress every 25 lines to track what's happening
                if line_count % 25 == 0:
                    logger.info(f"[enhanced_stream_chat_message] Progress: Line {line_count}, Elapsed: {elapsed_time:.1f}s, Idle: {idle_time:.1f}s, Chars: {len(full_answer)}")
                
                # Check for timeouts - but be more patient
                if elapsed_time > max_total_time:
                    logger.warning(f"[enhanced_stream_chat_message] Maximum total time ({max_total_time}s) exceeded after {line_count} lines, {len(full_answer)} chars")
                    break
                    
                # Only timeout on idle if we have NO content after significant time
                if idle_time > max_idle_time and len(full_answer) == 0 and elapsed_time > 60.0:
                    logger.warning(f"[enhanced_stream_chat_message] Maximum idle time ({max_idle_time}s) exceeded since last content, still no answer content after {line_count} lines, elapsed: {elapsed_time:.1f}s")
                    break
                
                if not line:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count}: Empty line")
                    continue
                    
                # Onyx doesn't use "data: " prefix - each line is a direct JSON object  
                # Skip empty lines but process all non-empty lines as JSON
                payload_text = line.strip()
                if not payload_text:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count}: Empty line")
                    continue
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[enhanced_stream_chat_message] Failed to parse JSON line {line_count}: {str(e)} | Line: {payload_text[:100]}")
                    continue
                
                # For the first 10 packets, log full content to understand structure
                if line_count <= 10:
                    packet_str = str(packet)[:500] if packet else "empty"
                    logger.info(f"[enhanced_stream_chat_message] Packet {line_count} content: {packet_str}")
                
                # Log packet structure for debugging (every 50 lines to avoid spam)
                if line_count % 50 == 0:
                    packet_keys = list(packet.keys()) if isinstance(packet, dict) else "not-dict"
                    logger.info(f"[enhanced_stream_chat_message] Line {line_count} packet keys: {packet_keys}")
                
                # Handle different Onyx packet types
                answer_content = None
                
                # Check for OnyxAnswerPiece
                if "answer_piece" in packet:
                    answer_piece = packet["answer_piece"]
                    if answer_piece is None:
                        # OnyxAnswerPiece with None signals end of answer
                        logger.info(f"[enhanced_stream_chat_message] Received answer termination signal (answer_piece=None) after {line_count} lines")
                        done_received = True
                        break
                    elif answer_piece:
                        answer_content = answer_piece
                        
                # Check for AgentAnswerPiece (agent search responses)
                elif packet.get("answer_type") and packet.get("answer_piece"):
                    answer_content = packet["answer_piece"]
                    logger.info(f"[enhanced_stream_chat_message] Received agent answer piece: {packet.get('answer_type')}")
                    
                # Check for QADocsResponse (search results)
                elif packet.get("top_documents") or packet.get("rephrased_query"):
                    logger.info(f"[enhanced_stream_chat_message] Received search results packet")
                    last_activity_time = current_time  # Reset timer for search activity
                    
                # Check for StreamStopInfo
                elif packet.get("stop_reason"):
                    if packet["stop_reason"] == "finished":
                        logger.info(f"[enhanced_stream_chat_message] Received stream stop signal: finished")
                        done_received = True
                        break
                    
                if answer_content:
                    full_answer += answer_content
                    last_activity_time = current_time  # Reset activity timer on content
                    
                    # Log progress every 200 chars to track streaming
                    if len(full_answer) - last_log_length >= 200:
                        logger.info(f"[enhanced_stream_chat_message] Accumulated {len(full_answer)} chars so far...")
                        last_log_length = len(full_answer)
                else:
                    # Log what we're getting for non-answer packets
                    if line_count <= 10 or line_count % 100 == 0:  # Log first 10 and every 100th
                        packet_preview = str(packet)[:200] if packet else "empty"
                        logger.debug(f"[enhanced_stream_chat_message] Line {line_count} - non-answer packet: {packet_preview}")
            
            # Stream ended - determine why
            logger.info(f"[enhanced_stream_chat_message] Stream reading loop ended naturally")
            final_elapsed = time.time() - start_time
            logger.info(f"[enhanced_stream_chat_message] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}, Elapsed: {final_elapsed:.1f}s")
            
            # If we got no content and stream ended quickly, something went wrong
            if len(full_answer) == 0 and final_elapsed < 60.0 and not done_received:
                logger.error(f"[enhanced_stream_chat_message] Stream ended prematurely! Only {final_elapsed:.1f}s elapsed, {line_count} lines processed, no content received")
                logger.error(f"[enhanced_stream_chat_message] This suggests an issue with the Onyx search or streaming connection")
                
            if not done_received and len(full_answer) == 0:
                logger.warning(f"[enhanced_stream_chat_message] Stream ended without [DONE] signal and no content - may be incomplete")
            elif not done_received:
                logger.warning(f"[enhanced_stream_chat_message] Stream ended without [DONE] signal but got {len(full_answer)} chars")
                
            # Ensure we have some minimum content or waited minimum time
            if len(full_answer) == 0 and final_elapsed < 60.0:
                logger.warning(f"[enhanced_stream_chat_message] No content received and insufficient wait time ({final_elapsed:.1f}s < 60s)")
                # Wait a bit more to see if content comes
                logger.info(f"[enhanced_stream_chat_message] Attempting extended wait for delayed response...")
                import asyncio
                await asyncio.sleep(5.0)  # Wait 5 more seconds
                
            return full_answer
        else:
            # Non-streaming response
            logger.info(f"[enhanced_stream_chat_message] Processing non-streaming response")
            try:
                data = resp.json()
                result = data.get("answer") or data.get("answer_citationless") or ""
                logger.info(f"[enhanced_stream_chat_message] Non-streaming result: {len(result)} chars")
                return result
            except Exception as e:
                logger.error(f"[enhanced_stream_chat_message] Failed to parse non-streaming response: {e}")
                return resp.text.strip()

async def enhanced_stream_chat_message_with_filters(chat_session_id: str, message: str, cookies: Dict[str, str], connector_sources: list) -> str:
    """Enhanced version of stream_chat_message for connector searches with source filtering."""
    logger.info(f"[enhanced_stream_chat_message_with_filters] Starting connector search - chat_id={chat_session_id} sources={connector_sources} len(message)={len(message)}")

    async with httpx.AsyncClient(timeout=600.0) as client:  # Longer timeout for searches
        retrieval_options = {
            "run_search": "always",  # Always search for connectors
            "real_time": False,
            "filters": {
                "connectorSources": connector_sources  # Filter by specific connector sources
            }
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": True,  # Force streaming for better control
        }
        
        logger.info(f"[enhanced_stream_chat_message_with_filters] Sending request to {ONYX_API_SERVER_URL}/chat/send-message with connector filters: {connector_sources}")
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/send-message",
            json=payload,
            cookies=cookies,
        )
        
        logger.info(f"[enhanced_stream_chat_message_with_filters] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        
        # Handle the response (EXACT same logic as enhanced_stream_chat_message for Knowledge Base)
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[enhanced_stream_chat_message_with_filters] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            last_log_length = 0
            import time
            start_time = time.time()
            last_activity_time = start_time
            max_idle_time = 120.0  # Wait up to 2 minutes without new content
            max_total_time = 600.0  # Maximum 10 minutes total
            
            logger.info(f"[enhanced_stream_chat_message_with_filters] Starting to read lines from stream...")
            async for line in resp.aiter_lines():
                line_count += 1
                current_time = time.time()
                elapsed_time = current_time - start_time
                idle_time = current_time - last_activity_time
                
                # Log progress every 25 lines to track what's happening
                if line_count % 25 == 0:
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Progress: Line {line_count}, Elapsed: {elapsed_time:.1f}s, Idle: {idle_time:.1f}s, Chars: {len(full_answer)}")
                
                # Check for timeouts - but be more patient
                if elapsed_time > max_total_time:
                    logger.warning(f"[enhanced_stream_chat_message_with_filters] Maximum total time ({max_total_time}s) exceeded after {line_count} lines, {len(full_answer)} chars")
                    break
                    
                # Only timeout on idle if we have NO content after significant time
                if idle_time > max_idle_time and len(full_answer) == 0 and elapsed_time > 60.0:
                    logger.warning(f"[enhanced_stream_chat_message_with_filters] Maximum idle time ({max_idle_time}s) exceeded since last content, still no answer content after {line_count} lines, elapsed: {elapsed_time:.1f}s")
                    break

                if not line:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count}: Empty line")
                    continue
                    
                # Onyx doesn't use "data: " prefix - each line is a direct JSON object  
                # Skip empty lines but process all non-empty lines as JSON
                payload_text = line.strip()
                if not payload_text:
                    if line_count <= 5:  # Log first few empty lines
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count}: Empty line")
                    continue
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[enhanced_stream_chat_message_with_filters] Failed to parse JSON line {line_count}: {str(e)} | Line: {payload_text[:100]}")
                    continue

                # For the first 10 packets, log full content to understand structure
                if line_count <= 10:
                    packet_str = str(packet)[:500] if packet else "empty"
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Packet {line_count} content: {packet_str}")

                # Log packet structure for debugging (every 50 lines to avoid spam)
                if line_count % 50 == 0:
                    packet_keys = list(packet.keys()) if isinstance(packet, dict) else "not-dict"
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Line {line_count} packet keys: {packet_keys}")

                # Handle different Onyx packet types (EXACT same as Knowledge Base)
                answer_content = None
                
                # Check for OnyxAnswerPiece
                if "answer_piece" in packet:
                    answer_piece = packet["answer_piece"]
                    if answer_piece is None:
                        # OnyxAnswerPiece with None signals end of answer
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Received answer termination signal (answer_piece=None) after {line_count} lines")
                        done_received = True
                        break
                    elif answer_piece:
                        answer_content = answer_piece
                        
                # Check for AgentAnswerPiece (agent search responses)
                elif packet.get("answer_type") and packet.get("answer_piece"):
                    answer_content = packet["answer_piece"]
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Received agent answer piece: {packet.get('answer_type')}")
                    
                # Check for QADocsResponse (search results)
                elif packet.get("top_documents") or packet.get("rephrased_query"):
                    logger.info(f"[enhanced_stream_chat_message_with_filters] Received search results packet")
                    last_activity_time = current_time  # Reset timer for search activity
                    
                # Check for StreamStopInfo
                elif packet.get("stop_reason"):
                    if packet["stop_reason"] == "finished":
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Received stream stop signal: finished")
                        done_received = True
                        break
                    
                if answer_content:
                    full_answer += answer_content
                    last_activity_time = current_time  # Reset activity timer on content
                    
                    # Log progress every 200 chars to track streaming
                    if len(full_answer) - last_log_length >= 200:
                        logger.info(f"[enhanced_stream_chat_message_with_filters] Accumulated {len(full_answer)} chars so far...")
                        last_log_length = len(full_answer)
                else:
                    # Log what we're getting for non-answer packets
                    if line_count <= 10 or line_count % 100 == 0:  # Log first 10 and every 100th
                        packet_preview = str(packet)[:200] if packet else "empty"
                        logger.debug(f"[enhanced_stream_chat_message_with_filters] Line {line_count} - non-answer packet: {packet_preview}")
            
            # Stream ended - determine why
            logger.info(f"[enhanced_stream_chat_message_with_filters] Stream reading loop ended naturally")
            final_elapsed = time.time() - start_time
            logger.info(f"[enhanced_stream_chat_message_with_filters] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}, Elapsed: {final_elapsed:.1f}s")
            
            # Log full raw response for debugging
            logger.info(f"[enhanced_stream_chat_message_with_filters] Full raw response: {full_answer}")
            
            # If we got no content and stream ended quickly, something went wrong
            if len(full_answer) == 0 and final_elapsed < 60.0 and not done_received:
                logger.error(f"[enhanced_stream_chat_message_with_filters] Stream ended prematurely! Only {final_elapsed:.1f}s elapsed, {line_count} lines processed, no content received")
                logger.error(f"[enhanced_stream_chat_message_with_filters] This suggests an issue with the Onyx search or streaming connection")
            
            return full_answer.strip()
            
        else:
            # Non-streaming response
            logger.info(f"[enhanced_stream_chat_message_with_filters] Processing non-streaming response")
            try:
                data = resp.json()
                result = data.get("answer") or data.get("answer_citationless") or ""
                logger.info(f"[enhanced_stream_chat_message_with_filters] Non-streaming result: {len(result)} chars")
                return result
            except Exception as e:
                logger.error(f"[enhanced_stream_chat_message_with_filters] Failed to parse non-streaming response: {e}")
                return resp.text.strip()

async def extract_knowledge_base_context(topic: str, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from the entire Knowledge Base using the Search persona.
    This function performs a comprehensive search across all documents in the Knowledge Base.
    """
    try:
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Starting Knowledge Base search for topic: {topic}")
        
        # Create a temporary chat session with the Search persona (ID 0)
        search_persona_id = 0
        temp_chat_id = await create_onyx_chat_session(search_persona_id, cookies)
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Created search chat session: {temp_chat_id}")
        
        # Create a comprehensive search prompt
        search_prompt = f"""
        Please search your entire Knowledge Base for information relevant to this topic: "{topic}"
        
        I need you to:
        1. Search across all available documents and knowledge sources
        2. Find the most relevant information related to this topic
        3. Provide a comprehensive summary of what you find
        4. Extract key topics, concepts, and important details
        5. Identify any specific examples, case studies, or practical applications
        
        Please format your response as:
        SUMMARY: [comprehensive summary of relevant information found]
        KEY_TOPICS: [comma-separated list of key topics and concepts]
        IMPORTANT_DETAILS: [specific details, examples, or practical information]
        RELEVANT_SOURCES: [mention of any specific documents or sources that were particularly relevant]
        
        Be thorough and comprehensive in your search and analysis.
        """
        
        # Use the Search persona to perform the Knowledge Base search
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Sending search request to Search persona")
        search_result = await enhanced_stream_chat_message(temp_chat_id, search_prompt, cookies)
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Received search result ({len(search_result)} chars)")
        
        # Log the full response for debugging
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Full search response: {search_result}")
        
        if len(search_result) == 0:
            logger.warning(f"[KNOWLEDGE_BASE_CONTEXT] Search result is empty! This might indicate no documents in Knowledge Base or search failed")
        
        # Parse the search result - handle Onyx response format  
        summary = ""
        key_topics = []
        important_details = ""
        relevant_sources = ""
        
        # Extract content flexibly using string searching
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Starting content extraction from search result")
        
        if "SUMMARY:" in search_result:
            summary_start = search_result.find("SUMMARY:") + 8
            summary_end = search_result.find("KEY_TOPICS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("IMPORTANT_DETAILS:", summary_start)
            if summary_end == -1:
                summary_end = search_result.find("RELEVANT_SOURCES:", summary_start)
            if summary_end == -1:
                summary_end = len(search_result)
            summary = search_result[summary_start:summary_end].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted summary: {len(summary)} chars")
        
        if "KEY_TOPICS:" in search_result:
            topics_start = search_result.find("KEY_TOPICS:") + 11
            topics_end = search_result.find("IMPORTANT_DETAILS:", topics_start)
            if topics_end == -1:
                topics_end = search_result.find("RELEVANT_SOURCES:", topics_start)
            if topics_end == -1:
                # Look for next section marker or end of text
                next_section = search_result.find("\n\n", topics_start)
                topics_end = next_section if next_section != -1 else len(search_result)
            topics_text = search_result[topics_start:topics_end].strip()
            key_topics = [t.strip() for t in topics_text.split(',') if t.strip()]
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted {len(key_topics)} key topics")
        
        if "IMPORTANT_DETAILS:" in search_result:
            details_start = search_result.find("IMPORTANT_DETAILS:") + 18
            details_end = search_result.find("RELEVANT_SOURCES:", details_start)
            if details_end == -1:
                details_end = len(search_result)
            important_details = search_result[details_start:details_end].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted important details: {len(important_details)} chars")
        
        if "RELEVANT_SOURCES:" in search_result:
            sources_start = search_result.find("RELEVANT_SOURCES:") + 17
            relevant_sources = search_result[sources_start:].strip()
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted relevant sources: {len(relevant_sources)} chars")
        
        # Final fallback if still no content
        if not summary and not key_topics:
            summary = search_result[:1000] + "..." if len(search_result) > 1000 else search_result
            key_topics = ["knowledge base search"]
            logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Using fallback summary from raw response")
        
        # Log the extracted information
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted summary: {summary[:200]}...")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted key topics: {key_topics}")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted important details: {important_details[:200]}...")
        logger.info(f"[KNOWLEDGE_BASE_CONTEXT] Extracted relevant sources: {relevant_sources[:200]}...")
        
        # Return context in the same format as file context
        return {
            "knowledge_base_search": True,
            "topic": topic,
            "summary": summary,
            "key_topics": key_topics,
            "important_details": important_details,
            "relevant_sources": relevant_sources,
            "full_search_result": search_result,
            "file_summaries": [{
                "file_id": "knowledge_base",
                "name": f"Knowledge Base Search: {topic}",
                "summary": summary,
                "topics": key_topics,
                "key_info": important_details
            }]
        }
        
    except Exception as e:
        logger.error(f"[KNOWLEDGE_BASE_CONTEXT] Error extracting Knowledge Base context: {e}", exc_info=True)
        # Return fallback context
        return {
            "knowledge_base_search": True,
            "topic": topic,
            "summary": f"Knowledge Base search failed for topic: {topic}",
            "key_topics": ["search error"],
            "important_details": "Unable to search Knowledge Base",
            "relevant_sources": "",
            "full_search_result": f"Error: {str(e)}",
            "file_summaries": []
        }

async def extract_single_file_context(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a single file using Onyx's chat API with 100% file attachment guarantee.
    """
    try:
        # Step 1: Verify file exists and is accessible
        file_info = await verify_file_accessibility(file_id, cookies)
        if not file_info:
            return {
                "file_id": file_id,
                "summary": f"File {file_id} is not accessible or does not exist",
                "topics": ["file access error"],
                "key_info": "File may need to be re-uploaded",
                "content": f"File {file_id} access verification failed"
            }
        
        # Step 2: Create a temporary chat session with forced file attachment
        persona_id = await get_contentbuilder_persona_id(cookies)
        temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
        
        # Step 3: Flexible analysis prompt that works with both text files and images
        analysis_prompt = f"""        
        Please describe:
        1. What is this file? (image, document, etc.)
        2. What does it contain or show? (min 500 words)
        3. What are the main topics, concepts, or subjects?
        4. What information would be most relevant for lesson planning or content creation?
        
        Format your response as:
        SUMMARY: [what this file contains/shows]
        TOPICS: [main topics or subjects, comma-separated]  
        KEY_INFO: [most educational/relevant information]
        """
        
        # Step 4: Multiple retry attempts with different strategies
        for attempt in range(3):
            try:
                result = await attempt_file_analysis_with_retry(
                    temp_chat_id, file_id, analysis_prompt, cookies, attempt
                )
                if result and not is_generic_response(result):
                    return parse_analysis_result(file_id, result)
                elif attempt < 2:
                    logger.warning(f"[FILE_CONTEXT] Attempt {attempt + 1} failed for file {file_id}, retrying...")
                    await asyncio.sleep(1)  # Brief delay before retry
                else:
                    logger.error(f"[FILE_CONTEXT] All attempts failed for file {file_id}")
                    break
            except Exception as e:
                logger.error(f"[FILE_CONTEXT] Attempt {attempt + 1} error for file {file_id}: {e}")
                if attempt < 2:
                    await asyncio.sleep(1)
                else:
                    raise
        
        # Step 5: Fallback response if all attempts fail
        return {
            "file_id": file_id,
            "summary": f"File analysis failed after multiple attempts (ID: {file_id})",
            "topics": ["analysis error", "file processing"],
            "key_info": "File may need manual review or re-upload",
            "content": f"Analysis failed for file {file_id} ({file_info.get('name', 'Unknown')})"
        }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting single file context for file {file_id}: {e}")
        return {
            "file_id": file_id,
            "summary": f"Error processing file {file_id}: {str(e)}",
            "topics": ["processing error"],
            "key_info": "File processing encountered an error",
            "content": f"Error: {str(e)}"
        }

async def verify_file_accessibility(file_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Verify that a file exists and is accessible before attempting analysis.
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Check file indexing status using the correct endpoint
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/file/indexing-status?file_ids={file_id}", 
                cookies=cookies
            )
            response.raise_for_status()
            status_data = response.json()
            
            is_indexed = status_data.get(str(file_id), False)
            file_status = "INDEXED" if is_indexed else "NOT_INDEXED"
            
            logger.info(f"[FILE_CONTEXT] File {file_id} indexing status: {file_status}")
            
            # For now, assume the file is accessible if we can check its status
            # The actual file content will be verified during the analysis phase
            return {
                "id": file_id,
                "name": f"file_{file_id}",  # We'll get the real name during analysis
                "status": file_status,
                "accessible": True  # Assume accessible, let the analysis phase handle actual access
            }
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] File accessibility check failed for {file_id}: {e}")
        # Return a basic info structure even if check fails
        return {
            "id": file_id,
            "name": f"file_{file_id}",
            "status": "UNKNOWN",
            "accessible": True  # Let the analysis phase determine actual accessibility
        }

async def attempt_file_analysis_with_retry(
    chat_id: str, 
    file_id: int, 
    prompt: str, 
    cookies: Dict[str, str], 
    attempt: int
) -> str:
    """
    Attempt file analysis with different strategies based on attempt number.
    """
    # Different strategies for each attempt
    strategies = [
        # Attempt 1: Standard approach with user_file_ids
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "never", "real_time": False},
            "force_direct_attachment": True
        },
        # Attempt 2: Force search tool with file-specific query
        {
            "user_file_ids": [file_id],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Analyze the content of file ID {file_id}"
        },
        # Attempt 3: Use file_descriptors as fallback with search
        {
            "file_descriptors": [{"id": str(file_id), "type": "USER_KNOWLEDGE", "name": f"file_{file_id}"}],
            "retrieval_options": {"run_search": "always", "real_time": False},
            "query_override": f"Find and analyze the content of file {file_id}"
        }
    ]
    
    strategy = strategies[attempt]
    
    async with httpx.AsyncClient(timeout=180.0) as client:
        payload = {
            "chat_session_id": chat_id,
            "message": prompt,
            "parent_message_id": None,
            "file_descriptors": strategy.get("file_descriptors", []),
            "user_file_ids": strategy.get("user_file_ids", []),
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": strategy["retrieval_options"],
            "stream_response": True,
            "query_override": strategy.get("query_override")
        }
        
        logger.info(f"[FILE_CONTEXT] Attempt {attempt + 1} for file {file_id} with strategy: {list(strategy.keys())}")
        
        try:
            # Try simple API first
            response = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                json=payload,
                cookies=cookies
            )
            response.raise_for_status()
            result = response.json()
            return result.get("answer", "")
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                # Fallback to streaming endpoint
                return await stream_file_analysis(client, payload, cookies, file_id)
            else:
                raise

async def stream_file_analysis(
    client: httpx.AsyncClient, 
    payload: Dict[str, Any], 
    cookies: Dict[str, str], 
    file_id: int
) -> str:
    """
    Stream file analysis response with enhanced error handling.
    """
    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
        resp.raise_for_status()
        analysis_text = ""
        line_count = 0
        file_mentioned = False
        
        async for raw_line in resp.aiter_lines():
            line_count += 1
            if not raw_line:
                continue
                
            line = raw_line.strip()
            if line.startswith("data:"):
                line = line.split("data:", 1)[1].strip()
                
            if line == "[DONE]":
                logger.info(f"[FILE_CONTEXT] Stream completed for file {file_id} after {line_count} lines")
                break
                
            try:
                pkt = json.loads(line)
                if "answer_piece" in pkt:
                    piece = pkt["answer_piece"].replace("\\n", "\n")
                    analysis_text += piece
                    
                    # Check if file is mentioned in the response
                    if str(file_id) in piece or "file" in piece.lower():
                        file_mentioned = True
                        
            except json.JSONDecodeError:
                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                continue
        
        logger.info(f"[FILE_CONTEXT] Stream processing completed for file {file_id}, "
                   f"total text length: {len(analysis_text)}, file mentioned: {file_mentioned}")
        
        return analysis_text

def is_generic_response(text: str) -> bool:
    """
    Check if the AI response is generic (indicating file access issues).
    """
    # Updated to be less strict for image descriptions and more specific
    generic_phrases = [
        "could you please share the file",
        "please share the file", 
        "paste its content",
        "upload the file",
        "provide the file",
        "share the document", 
        "i don't see any file attached",
        "no file was provided to me",
        "file_access_error",
        "please provide the content",
        "try a different file",
        "proceed based on a general topic",
        "using my knowledge"
    ]
    
    # Additional check: if response is very short and contains access issue, it's likely generic
    # But allow longer responses that might contain some useful info even if they mention access issues
    text_lower = text.lower()
    if len(text) < 150 and any(phrase in text_lower for phrase in [
        "cannot access", "unable to access", "don't have access", 
        "wasn't able to access", "access the file"
    ]):
        return True
    
    text_lower = text.lower()
    return any(phrase in text_lower for phrase in generic_phrases)

def parse_analysis_result(file_id: int, analysis_text: str) -> Dict[str, Any]:
    """
    Parse the analysis result and extract structured information.
    """
    summary = ""
    topics = []
    key_info = ""
    
    # Log the raw response for debugging
    logger.info(f"[FILE_CONTEXT] Raw analysis response for file {file_id} (length: {len(analysis_text)}): "
               f"{analysis_text[:500]}{'...' if len(analysis_text) > 500 else ''}")
    
    lines = analysis_text.split('\n')
    for line in lines:
        if line.startswith("SUMMARY:"):
            summary = line.replace("SUMMARY:", "").strip()
        elif line.startswith("TOPICS:"):
            topics_text = line.replace("TOPICS:", "").strip()
            topics = [t.strip() for t in topics_text.split(',') if t.strip()]
        elif line.startswith("KEY_INFO:"):
            key_info = line.replace("KEY_INFO:", "").strip()
    
    # If no structured response, try to extract meaningful content
    if not summary and analysis_text.strip():
        # Take first 200 characters as summary if no structured response
        summary = analysis_text.strip()[:200]
        if len(analysis_text) > 200:
            summary += "..."
        logger.info(f"[FILE_CONTEXT] No structured SUMMARY found, using first 200 chars as summary for file {file_id}")
    
    # If still no summary, use a fallback
    if not summary:
        summary = f"File content analyzed successfully (ID: {file_id})"
        logger.warning(f"[FILE_CONTEXT] No summary could be extracted for file {file_id}, using fallback")
    
    return {
        "file_id": file_id,
        "summary": summary,
        "topics": topics,
        "key_info": key_info,
        "content": analysis_text
    }

async def extract_folder_context(folder_id: int, cookies: Dict[str, str]) -> Dict[str, Any]:
    """
    Extract context from a folder by analyzing its files.
    """
    try:
        # Get folder files
        async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minutes timeout for large files like 200-page PDFs
            response = await client.get(
                f"{ONYX_API_SERVER_URL}/user/folder/{folder_id}",
                cookies=cookies
            )
            response.raise_for_status()
            
            folder_data = response.json()
            files = folder_data.get("files", [])
            
            if not files:
                return {"folder_id": folder_id, "summary": "Empty folder", "topics": []}
            
            # Create a temporary chat session to analyze folder content
            persona_id = await get_contentbuilder_persona_id(cookies)
            temp_chat_id = await create_onyx_chat_session(persona_id, cookies)
            
            # Analyze folder content
            analysis_prompt = f"""
            This folder contains {len(files)} files. Please analyze the overall theme and provide:
            1. A summary of what this folder is about (max 150 words)
            2. Key topics that are covered across all files
            3. The main purpose or theme of this collection
            
            Format your response as:
            SUMMARY: [summary here]
            TOPICS: [comma-separated topics]
            THEME: [main theme or purpose]
            """
            
            file_ids = [f["id"] for f in files if f.get("status") == "INDEXED"]
            
            if not file_ids:
                return {"folder_id": folder_id, "summary": "No indexed files in folder", "topics": []}
            
            payload = {
                "chat_session_id": temp_chat_id,
                "message": analysis_prompt,
                "parent_message_id": None,
                "file_descriptors": [],
                "user_file_ids": file_ids,
                "user_folder_ids": [],
                "prompt_id": None,
                "search_doc_ids": None,
                "retrieval_options": {"run_search": "never", "real_time": False},
                "stream_response": True,
            }
            
            # Try the simple API first, fallback to regular streaming endpoint
            try:
                response = await client.post(
                    f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api",
                    json=payload,
                    cookies=cookies
                )
                response.raise_for_status()
                result = response.json()
                analysis_text = result.get("answer", "")
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404:
                    logger.info(f"[FILE_CONTEXT] Simple API not available, using streaming endpoint for folder {folder_id}")
                    # Fallback to streaming endpoint
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=payload, cookies=cookies) as resp:
                        resp.raise_for_status()
                        analysis_text = ""
                        line_count = 0
                        async for raw_line in resp.aiter_lines():
                            line_count += 1
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info(f"[FILE_CONTEXT] Stream completed for folder {folder_id} after {line_count} lines")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    analysis_text += pkt["answer_piece"].replace("\\n", "\n")
                            except json.JSONDecodeError:
                                logger.debug(f"[FILE_CONTEXT] JSON decode error on line {line_count}: {line[:100]}")
                                continue
                        logger.info(f"[FILE_CONTEXT] Stream processing completed for folder {folder_id}, total text length: {len(analysis_text)}")
                else:
                    raise
            
            # Parse the analysis
            summary = ""
            topics = []
            theme = ""
            
            lines = analysis_text.split('\n')
            for line in lines:
                if line.startswith("SUMMARY:"):
                    summary = line.replace("SUMMARY:", "").strip()
                elif line.startswith("TOPICS:"):
                    topics_text = line.replace("TOPICS:", "").strip()
                    topics = [t.strip() for t in topics_text.split(',') if t.strip()]
                elif line.startswith("THEME:"):
                    theme = line.replace("THEME:", "").strip()
            
            return {
                "folder_id": folder_id,
                "folder_name": folder_data.get("name", ""),
                "summary": summary,
                "topics": topics,
                "theme": theme,
                "file_count": len(files)
            }
            
    except Exception as e:
        logger.error(f"[FILE_CONTEXT] Error extracting folder context for folder {folder_id}: {e}")
        return None

def build_enhanced_prompt_with_context(original_prompt: str, file_context: Union[Dict[str, Any], str], product_type: str) -> str:
    """
    Build an enhanced prompt that includes the extracted file context for OpenAI.
    Handles both dict (structured context) and str (fallback context) cases.
    """
    enhanced_prompt = f"""
{original_prompt}

--- CONTEXT FROM UPLOADED FILES ---

"""
    
    # Handle string file_context (fallback case)
    if isinstance(file_context, str):
        enhanced_prompt += file_context
        enhanced_prompt += "\n\nPlease create the content based on the information above.\n"
        return enhanced_prompt
    
    # Handle dict file_context (normal case)
    # Handle string file_context (fallback case)
    if isinstance(file_context, str):
        enhanced_prompt += file_context
        enhanced_prompt += "\n\nPlease create the content based on the information above.\n"
        return enhanced_prompt
    
    # Check if fallback was used (dict case)
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += "NOTE: File context extraction was limited, but files were provided for content creation.\n\n"
    
    # Add file summaries
    if file_context.get("file_summaries"):
        enhanced_prompt += "FILE SUMMARIES:\n"
        for i, summary in enumerate(file_context["file_summaries"], 1):
            enhanced_prompt += f"{i}. {summary}\n"
        enhanced_prompt += "\n"
    
    # Add folder contexts
    if file_context.get("folder_contexts"):
        enhanced_prompt += "FOLDER CONTEXTS:\n"
        for folder_ctx in file_context["folder_contexts"]:
            enhanced_prompt += f"• {folder_ctx.get('folder_name', 'Unknown')}: {folder_ctx.get('summary', '')}\n"
        enhanced_prompt += "\n"
    
    # Add key topics
    if file_context.get("key_topics"):
        enhanced_prompt += f"KEY TOPICS COVERED: {', '.join(file_context['key_topics'])}\n\n"
    
    # Add specific instructions for the product type with enhanced formatting guidance
    if product_type == "Course Outline":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR COURSE OUTLINE:
1. Use exactly this structure: ## Module [Number]: [Module Title]
2. Each module must be a separate H2 header starting with ##
3. Lessons must be numbered list items (1. 2. 3.) under each module

ENSURE: Create the requested number of modules, not a single module with all lessons.
"""
    elif product_type == "Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Lesson Presentation** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly.
"""
    elif product_type == "Video Lesson Presentation":
        enhanced_prompt += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]**
3. Use "---" separators between slides (with blank lines before and after each separator)
4. Example structure:
   **Slide 1: Introduction to Topic**
   [Content here]
   
   ---
   
   **Slide 2: Key Concepts**
   [Content here]
   
   ---
   
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.

ENSURE: Every slide follows the **Slide N: Title** format exactly for proper video lesson processing.
"""
    
    # Add specific instructions for the product type
    if file_context.get("metadata", {}).get("fallback_used"):
        enhanced_prompt += f"""
IMPORTANT: Files were provided for content creation. Create a {product_type} that is relevant to the uploaded materials.
If specific content details are not available, focus on creating high-quality educational content that would be appropriate for the file types provided.
"""
    else:
        enhanced_prompt += f"""
IMPORTANT: Use the context from the uploaded files to create a {product_type} that is relevant and accurate to the source materials. 
Ensure that the content aligns with the topics and information provided in the file summaries and folder contexts.
"""
    
    return enhanced_prompt

async def stream_hybrid_response(prompt: str, file_context: Union[Dict[str, Any], str], product_type: str, model: str = None):
    """
    Stream response using OpenAI with enhanced context from Onyx file extraction.
    """
    try:
        # Build enhanced prompt with file context
        enhanced_prompt = build_enhanced_prompt_with_context(prompt, file_context, product_type)
        
        logger.info(f"[HYBRID_STREAM] Starting hybrid streaming with enhanced context")
        logger.info(f"[HYBRID_STREAM] Original prompt length: {len(prompt)} chars")
        logger.info(f"[HYBRID_STREAM] Enhanced prompt length: {len(enhanced_prompt)} chars")
        logger.info(f"[HYBRID_STREAM] File context: {len(file_context.get('file_summaries', []))} summaries, {len(file_context.get('key_topics', []))} topics")
        
        # Use OpenAI with enhanced prompt
        async for chunk_data in stream_openai_response(enhanced_prompt, model):
            yield chunk_data
            
    except Exception as e:
        logger.error(f"[HYBRID_STREAM] Error in hybrid streaming: {e}", exc_info=True)
        yield {"type": "error", "text": f"Hybrid streaming error: {str(e)}"}

@app.get("/api/custom/microproduct_types", response_model=List[str])
async def get_allowed_microproduct_types_list_for_design_templates():
    return ALLOWED_MICROPRODUCT_TYPES_FOR_DESIGNS

# --- Project and MicroProduct Endpoints ---
def build_source_context(payload) -> tuple[Optional[str], Optional[dict]]:
    """
    Build source context type and data from a finalize payload.
    Returns (context_type, context_data) tuple.
    """
    context_type = None
    context_data = {}
    
    # Check for connector context
    if hasattr(payload, 'fromConnectors') and payload.fromConnectors:
        context_type = 'connectors'
        context_data = {
            'connector_ids': payload.connectorIds.split(',') if payload.connectorIds else [],
            'connector_sources': payload.connectorSources.split(',') if payload.connectorSources else []
        }
    # Check for Knowledge Base context
    elif hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase:
        context_type = 'knowledge_base'
        context_data = {'search_query': payload.prompt if hasattr(payload, 'prompt') else None}
    # Check for file context
    elif hasattr(payload, 'fromFiles') and payload.fromFiles:
        context_type = 'files'
        context_data = {
            'folder_ids': payload.folderIds.split(',') if payload.folderIds else [],
            'file_ids': payload.fileIds.split(',') if payload.fileIds else []
        }
    # Check for text context
    elif hasattr(payload, 'fromText') and payload.fromText:
        context_type = 'text'
        context_data = {
            'text_mode': payload.textMode if hasattr(payload, 'textMode') else None,
            'user_text': payload.userText if hasattr(payload, 'userText') and payload.userText else None,
            'user_text_length': len(payload.userText) if hasattr(payload, 'userText') and payload.userText else 0
        }
    # Default to prompt-based
    else:
        context_type = 'prompt'
        context_data = {
            'prompt': payload.prompt if hasattr(payload, 'prompt') else None,
            'prompt_length': len(payload.prompt) if hasattr(payload, 'prompt') and payload.prompt else 0
        }
    
    return context_type, context_data

async def save_slide_creation_error(
    conn,
    user_id: str,
    template_id: str,
    props: dict,
    error_message: str
):
    await conn.execute(
        """
        INSERT INTO slide_creation_errors (user_id, template_id, props, error_message)
        VALUES ($1, $2, $3, $4)
        """,
        user_id,
        template_id,
        json.dumps(props, ensure_ascii=False),
        error_message
    )
@app.post("/api/custom/projects/add", response_model=ProjectDB, status_code=status.HTTP_201_CREATED)
async def add_project_to_custom_db(project_data: ProjectCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # ---- Guard against duplicate concurrent submissions (same user+project name) ----
    lock_key = f"{onyx_user_id}:{project_data.projectName.strip().lower()}"
    if lock_key in ACTIVE_PROJECT_CREATE_KEYS:
        raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="Project creation already in progress.")
    
    ACTIVE_PROJECT_CREATE_KEYS.add(lock_key)
    
    # Auto-cleanup lock after maximum processing time to prevent deadlocks
    async def cleanup_lock_after_timeout():
        await asyncio.sleep(300)  # 5 minutes max processing time
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)
        logger.warning(f"Auto-cleaned stuck project creation lock: {lock_key}")
    
    # Start cleanup task in background
    asyncio.create_task(cleanup_lock_after_timeout())
    try:
        selected_design_template: Optional[DesignTemplateResponse] = None
        async with pool.acquire() as conn:
            design_row = await conn.fetchrow("SELECT * FROM design_templates WHERE id = $1", project_data.design_template_id)
            if not design_row:
                detail_msg = "Design template not found." if IS_PRODUCTION else f"Design Template with ID {project_data.design_template_id} not found."
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=detail_msg)
            selected_design_template = DesignTemplateResponse(**dict(design_row))

        db_microproduct_name_to_store = project_data.microProductName if project_data.microProductName and project_data.microProductName.strip() else selected_design_template.template_name

        target_content_model: Type[BaseModel]
        default_error_instance: BaseModel
        llm_json_example: str
        component_specific_instructions: str

        # Using the long specific instructions from the original user prompt
        if selected_design_template.component_name == COMPONENT_NAME_PDF_LESSON:
            target_content_model = PdfLessonDetails
            default_error_instance = PdfLessonDetails(lessonTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'PDF Lesson' content.
    Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

    **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Capture all information and hierarchical relationships. Maintain original language.

    **Global Fields:**
    1.  `lessonTitle` (string): Main lesson title for the document.
       - Look for patterns like "**Course Name** : **Lesson** : **Lesson Title**" or "**Lesson** : **Lesson Title**"
       - Extract ONLY the lesson title part (the last part after the last "**")
       - For example: "**Code Optimization Course** : **Lesson** : **Introduction to Optimization**" → extract "Introduction to Optimization"
       - For example: "**Lesson** : **JavaScript Basics**" → extract "JavaScript Basics"
       - Do NOT include the course name or "Lesson" label in the title
       - If no clear pattern is found, use the first meaningful title or heading
    2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
    3.  `detectedLanguage` (string): e.g., "en", "ru".

    **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

    1.  **`type: "headline"`**
        * `level` (integer):
            * `1`: Reserved for the main title of a document, usually handled by `lessonTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
            * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
            * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
            * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
        * `text` (string): Headline text.
        * `iconName` (string, optional): Based on level and context as described above.
        * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.


    2.  **`type: "paragraph"`**
        * `text` (string): Full paragraph text.
        * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thoght in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендация', 'Рекомендація' — or their localized equivalents, and isn't a part of the buller list.

    3.  **`type: "bullet_list"`**
        * `items` (array of `ListItem`): Can be strings or other nested content blocks.
        * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

    4.  **`type: "numbered_list"`**
        * `items` (array of `ListItem`):
            * Can be simple strings for basic numbered points.
            * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                * The `items` of this *inner* `bullet_list` should then be:
                    1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                    2. A `paragraph` block (for the main descriptive text).
                    3. Optionally, another `paragraph` block with `isRecommendation: true`.
            * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

    **General Parsing Rules & Icon Names:**
    * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
    * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
    * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
    * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
    * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
    * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

    Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
      • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
      • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
      • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

    Return ONLY the JSON object. 
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            target_content_model = TextPresentationDetails
            default_error_instance = TextPresentationDetails(textTitle=f"LLM Parsing Error for {project_data.projectName}", contentBlocks=[])
            llm_json_example = DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM # Can reuse this example structure
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string, optional): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):**

            1.  **`type: "headline"`**
                * `level` (integer): `2`, `3`, or `4`.
                * `text` (string): Headline text.
                * `iconName` (string, optional): If the raw text includes an icon name like `{iconName}`, extract it. Permissible Icon Names: `info`, `goal`, `star`, `apple`, `award`, `boxes`, `calendar`, `chart`, `clock`, `globe`.
                * `isImportant` (boolean, optional): If the raw text includes `{isImportant}`, set this to `true`. If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): Set to `true` if this paragraph should be styled as a recommendation (e.g., with a side border).

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be simple strings. Nested lists are supported; you can place a `bullet_list` or `numbered_list` inside another list's `items` array.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`): Can be simple strings or other blocks, including a `bullet_list` for nested content.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.

            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **Key Parsing Rules:**
            *   Parse `{isImportant}` on headlines to the `isImportant` boolean field.
            *   Parse `{iconName}` on headlines to the `iconName` string field.
            *   After extracting `iconName` and `isImportant` values, you MUST remove their corresponding `{...}` tags from the final headline `text` field. The user should not see these tags in the output text.
            *   If a paragraph starts with `**Recommendation:**` (or a localized translation like `**Рекомендация:**`, `**Рекомендація:**`), you MUST set the `isRecommendation` field on that paragraph block to `true` and remove the keyword itself from the final `text` field.
            *   Do NOT remove the `**` from the text for any other purpose; treat it as part of the text. It is critical that you preserve the double-asterisk (`**`) markdown for bold text within all `text` fields.
            *   You are encouraged to use a diverse range of the available `iconName` values to make the presentation visually engaging.
            *   If the raw text starts with `# Title`, this becomes the `textTitle`. The `contentBlocks` should not include this Level 1 headline. All other headlines (`##`, `###`, `####`) are content blocks.
            *   **If the source text contains a Markdown table or tabular data, and the 'tables' style is selected, you MUST output a `table` block as described above. Do NOT output Markdown tables or represent tables as lists or paragraphs.**

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_TRAINING_PLAN:
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Parsing Error for {project_data.projectName}", sections=[])
            llm_json_example = selected_design_template.template_structuring_prompt or DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): Sequential number formatted as "№1", "№2", … Always use this exact format; never "Module 1".
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
+                - IMPORTANT: When the raw text explicitly names the assessment (for example just "Test"), copy that word *exactly*—do not expand it to phrases such as "Knowledge Test", "Proficiency Test", or similar, and do not spell-correct it.
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}. DO NOT use "Content missing" or "Content Coverage:" or similar phrases in the text.

            **Parsing Rules & Constraints:**
+            • Except where explicit transformations are required by these instructions, reproduce every extracted text fragment verbatim — preserving spelling, punctuation, capitalisation, and line breaks. Absolutely do NOT paraphrase, translate, or autocorrect the source text.
            • Detect modules and lessons from headings, tables, or enumerations in the source text. Preserve their original order.
            • Always use dot as decimal separator for `hours` (e.g., 2.5).
            • If `hours` is written as "2 h 30 min", convert to 2.5.
            • Do not create empty arrays; if a module has no lessons, set `lessons: []`.
            • Never output null values for required string fields; use an empty string instead.
            • Ensure that every lesson belongs to a module; do not leave stray lessons.
            • Preserve bold (`**`) or italic (`*`) markdown that exists inside titles or texts.
            • Auxiliary keywords like "Goal", "Outcome", "Assessment" must be translated to the language of the content using the same localization rules described earlier.

            **Validation Checklist BEFORE returning JSON:**
            □ Each module id follows the "№X" pattern.
            □ No lesson titles start with "Lesson X.Y" or similar numbering patterns.
            □ Sum of `hours` in lessons equals `totalHours` if `autoCalculateHours` is false.
            □ Every `check.type` other than "none" has non-empty `text`.
            □ `detectedLanguage` is filled with a 2-letter code.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM  # Force use of new template format
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has 2-3 numerical metrics/statistics with clear values → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - If slide has event dates → use "event-list"
            - If slide has 6 numbered ideas → use "six-ideas-list"
            - If slide has challenges vs solutions → use "challenges-solutions"
            - If slide has analytics metrics in bullet points → use "metrics-analytics"
            - For standard content → use "content-slide"
            
            **CRITICAL TEMPLATE SELECTION RULES:**
            - NEVER use "big-numbers" unless content has exactly 2-3 clear numerical metrics with values, labels, and descriptions
            - NEVER use "metrics-analytics" unless content specifically mentions analytics/performance metrics  
            - If content has bullet points about concepts (not metrics), use "bullet-points" NOT "metrics-analytics"
            - If content mentions "evaluation", "analysis", or has bullet points about tracking/measuring, consider "bullet-points" first
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "steps" array with value/label/description
            - For timeline: parse chronological content into "events" array with date, title, and description fields
            - For pyramid: parse hierarchical content into "steps" array
            
            **CRITICAL IMAGE PROMPT EXTRACTION - PRESENTATION ILLUSTRATIONS:**
            - ALWAYS extract image prompts from [IMAGE_PLACEHOLDER] sections
            - Format: [IMAGE_PLACEHOLDER: SIZE | POSITION | DESCRIPTION]
            - Map DESCRIPTION to "imagePrompt" and "imageAlt" fields
            - **CRITICAL: Generate extremely detailed, descriptive prompts with specific visual elements**
            - **DETAILED PROMPT FORMAT REQUIREMENTS:**
              - Start with "Minimalist flat design illustration of [detailed subject/scene description]"
              - Include SPECIFIC visual elements: exact objects, people, layouts, arrangements
              - Describe COMPOSITION: positioning, spatial relationships, perspective
              - Detail CHARACTER descriptions: gender, age, clothing, poses, actions
              - Specify OBJECT details: shapes, sizes, orientations, interactions
              - Include ENVIRONMENTAL elements: setting, context, atmosphere
              - Use color placeholders: [COLOR1], [COLOR2], [COLOR3], [BACKGROUND]
              - End with style and background specifications
              - NO separate color descriptions or presentation context
            - **VISUAL ELEMENT REQUIREMENTS:**
              - **People**: Describe gender, ethnicity, age range, specific clothing, poses, facial expressions, interactions
              - **Objects**: Detail size, shape, orientation, material appearance, positioning relative to other elements
              - **Technology**: Specify device types, screen content, interface elements, connection indicators
              - **Architecture**: Describe building styles, structural elements, spatial relationships, interior/exterior details
              - **Data/Charts**: Detail chart types, data representation methods, axis labels, trend indicators
              - **Nature/Abstract**: Specify shapes, patterns, flow directions, organic vs geometric elements
            - **COMPOSITION REQUIREMENTS:**
              - Describe exact positioning: "person sitting on the left side", "laptop positioned at center-right"
              - Include spatial relationships: "behind", "in front of", "surrounding", "connected by"
              - Specify viewing angles: "front view", "three-quarter perspective", "top-down view"
              - Detail background/foreground layering: "foreground elements", "middle ground", "background context"
            - **COLOR PLACEHOLDER USAGE:**
              - [COLOR1] or [PRIMARY]: Main accent color for primary focal elements
              - [COLOR2] or [SECONDARY]: Secondary color for supporting elements, borders, text
              - [COLOR3] or [TERTIARY]: Accent color for details, highlights, subtle elements
              - [BACKGROUND]: Background color for the entire illustration
            - **ENHANCED PROMPT STRUCTURE:**
              - "Minimalist flat design illustration of [comprehensive scene description with 3-5 specific visual elements]. The scene features [detailed character/object descriptions with exact positioning]. [Additional environmental and compositional details]. [Specific color assignments for each visual element using placeholders]. The style is modern corporate vector art with clean geometric shapes and flat colors. The background is [BACKGROUND], completely clean and isolated."
            - **MANDATORY SCENE STRUCTURING:**
              - ALWAYS describe WHO is in the scene (specific people with demographics, clothing, poses)
              - ALWAYS describe WHERE they are positioned (left, center, right, foreground, background)
              - ALWAYS describe WHAT they are doing (specific actions, interactions, activities)
              - ALWAYS describe the SETTING details (furniture, equipment, environment specifics)
              - ALWAYS describe the LAYOUT (how elements are arranged spatially)
              - NEVER use vague terms like "featuring visual representations" or "playful design"
              - REPLACE abstract descriptions with concrete, observable elements
            - **SIMPLICITY REQUIREMENTS:**
              - LIMIT to 1-2 people maximum per illustration (never 3+ people)
              - SHOW 1-3 main visual elements only (avoid complex multi-panel setups)
              - FOCUS on clean, uncluttered compositions with plenty of white space
              - AVOID crowded scenes with multiple monitors, workstations, or complex layouts
              - PREFER single focal points rather than busy multi-element arrangements
            - **VISUAL ILLUSTRATION REQUIREMENTS:**
              - CREATE scenic illustrations NOT infographics or charts
              - AVOID "featuring icons representing" or "with clear labels" language
              - GENERATE actual scenes with objects, environments, and atmospheres
              - PREFER realistic scenarios over abstract concept representations
              - FOCUS on visual storytelling rather than information display
              - REPLACE "infographic" prompts with "illustration of [actual scene/environment]"
            - **DETAILED SCENE EXAMPLES (SIMPLE COMPOSITIONS):**
              - TEAM COLLABORATION: "two professionals at a clean desk: a Black woman in a blue blazer presenting to an Asian man in glasses who is taking notes on a single laptop, with one simple whiteboard showing basic geometric shapes in the background"
              - TECHNOLOGY SETUP: "a single modern workstation with one large monitor displaying simple geometric charts, a wireless keyboard, and minimal desk accessories"
              - DATA FLOW: "a simple network diagram with three circular nodes connected by clean arrow lines, showing data flow between connected points"
              - EDUCATIONAL SCENE: "one Hispanic female teacher standing next to a single wall chart with simple pictographic elements, facing two students sitting at clean desks"
              - LANGUAGE LEARNING: "one student at a modern desk using a tablet for language learning, with simple educational materials nearby"
            - **TEXT AND LABELING RESTRICTIONS:**
              - MINIMIZE text elements in illustrations - use symbols, icons, and visual indicators instead
              - AVOID readable text, labels, signs, or written content on screens, documents, or displays
              - USE abstract geometric shapes, simple icons, and visual patterns instead of text
              - REPLACE charts with text labels with simple bar charts, pie segments, or geometric data representations
              - AVOID books, documents, or papers with visible text - use blank documents or simple geometric patterns
              - USE color coding and visual hierarchy instead of text labels for differentiation
            - **MANDATORY REQUIREMENTS:**
              - NEVER include "presentation slide" or "for presentations" in the prompt
              - NEVER add separate color descriptions after the main scene description
              - ALWAYS describe at least 3-5 specific visual elements in detail
              - ALWAYS specify exact positioning and spatial relationships
              - ALWAYS include character demographics and specific object details
              - ALWAYS assign colors to specific elements using placeholders
              - MINIMIZE or eliminate text elements - focus on visual symbols and icons
              - NEVER leave imagePrompt fields empty - generate comprehensive, detailed prompts
            - **FORBIDDEN VAGUE LANGUAGE:**
              - NEVER use "featuring visual representations" - describe specific objects instead
              - NEVER use "playful design" - describe specific arrangement and visual elements
              - NEVER use "colorful illustration" - specify who, what, where, and how they're positioned
              - NEVER use "depicting [concept]" - describe the actual scene with people and objects
              - REPLACE abstract concepts with concrete scenes showing people engaged in specific activities
              - REPLACE "showing [topic]" with "scene features [specific people] doing [specific actions] in [specific setting]"
            - **FORBIDDEN INFOGRAPHIC LANGUAGE:**
              - NEVER use "infographic illustrating" - create actual scenic illustrations instead
              - NEVER use "featuring icons representing" - describe real objects and environments
              - NEVER use "with clear labels" or "arranged in a layout" - focus on natural scenes
              - NEVER use "icons for [concepts]" - create realistic environments where concepts occur
              - REPLACE "infographic of [topic]" with "illustration of [people doing topic-related activities in specific environment]"
              - REPLACE "featuring icons" with "scene showing [specific objects, people, and activities]"
            
            **TEMPLATE-SPECIFIC PROPS REQUIREMENTS:**
            
            For "big-image-left" and "big-image-top":
            - "title": Main slide heading
            - "subtitle": Descriptive content (NOT same as title)  
            - "imagePrompt": Detailed description for AI image generation
            - "imageAlt": Alt text for the image
            
            For "bullet-points" and "bullet-points-right":
            - "title": Main heading
            - "bullets": Array of bullet point strings
            - "imagePrompt": Description for supporting image (REQUIRED) - MUST be scenic illustration showing people in real environments, NOT infographics or icons. NEVER leave this field empty or the slide will use generic fallback prompts.
            - "imageAlt": Alt text for image
            
            For "two-column":
            - "title": Main slide title
            - "leftTitle": Left column heading  
            - "rightTitle": Right column heading (NEVER leave empty - always provide meaningful title)
            - "leftContent": Left column text content
            - "rightContent": Right column text content (NEVER leave empty - split slide content between columns)
            - "leftImagePrompt": Image prompt for left column (if applicable)
            - "rightImagePrompt": Image prompt for right column (if applicable)
            - **CRITICAL**: Two-column slides MUST have content in BOTH columns. Split slide content intelligently between left and right sections.
            
            For "big-numbers":
            - "title": Main heading
            - "steps": Array with exactly 3 items, each having:
              - "value": Numerical value or short metric (e.g., "25%", "3x", "$42")
              - "label": Short descriptive label (e.g., "Performance Improvement") 
              - "description": Detailed explanation of the metric
            - **CRITICAL**: ALWAYS provide exactly 3 steps. If slide content has less, expand into 3 logical points. If more than 3, group into 3 main categories.
            
            For "metrics-analytics":
            - "title": Main heading
            - "metrics": Array of metric descriptions (strings)

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title
            - NEVER create duplicate content for title and subtitle - extract different content
            - ALWAYS generate imagePrompt for templates that support images - NEVER leave imagePrompt fields empty
            - CRITICAL: bullet-points and bullet-points-right templates MUST include detailed imagePrompt fields

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
            target_content_model = SlideDeckDetails
            default_error_instance = SlideDeckDetails(
                lessonTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM  # Use video lesson template with voiceover
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Slide Deck' content with Component-Based template support.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON. Parse all slides provided without filtering or removing any content. Maintain original language and slide count.
            
            **VIDEO LESSON MODE: You are creating a Video Lesson Presentation with voiceover.**
            - This is NOT a regular slide deck - it's a Video Lesson that requires voiceover for every slide
            - You MUST generate voiceover text for each slide regardless of the input content
            - The voiceover is essential for the video lesson functionality
            - FAILURE TO INCLUDE VOICEOVER WILL RESULT IN AN INVALID OUTPUT
            
            🚨 CRITICAL REQUIREMENT: Every slide object MUST have a "voiceoverText" field with 2-4 sentences of conversational explanation. The root object MUST have "hasVoiceover": true. This is NON-NEGOTIABLE for Video Lesson Presentations.

            **CRITICAL: Parse Component-Based Slides with templateId and props**
            You must convert all slides to the component-based format using templateId and props. Parse every slide section provided in the input text.
            
            **VIDEO LESSON VOICEOVER REQUIREMENTS:**
            When parsing a Video Lesson Presentation, you MUST include voiceover text for each slide. The voiceover should:
            - Be conversational and engaging, as if speaking directly to the learner
            - Explain the slide content in detail, expanding on what's visually presented
            - Use natural transitions between concepts
            - Be approximately 30-60 seconds of speaking time per slide
            - Include clear explanations of complex concepts
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide
            
            **CRITICAL: You MUST generate voiceover text for EVERY slide in Video Lesson Presentations.**
            - Each slide object MUST include a "voiceoverText" field
            - The voiceover should be 2-4 sentences explaining the slide content
            - Set "hasVoiceover": true in the root object
            - If you don't see voiceover text in the input, GENERATE it based on the slide content
            
            **MANDATORY VOICEOVER GENERATION:**
            - For Video Lesson Presentations, you MUST create voiceover text for EVERY slide
            - Do NOT skip voiceover generation under any circumstances
            - Generate conversational, engaging voiceover that explains the slide content
            - Each voiceover should be 2-4 sentences (approximately 30-60 seconds of speaking time)
            - Use inclusive language ("we", "you", "let's") to create connection with the learner
            - Provide context and background information not visible on the slide
            - End with smooth transitions to the next slide

            **Global Fields:**
            1.  `lessonTitle` (string): Main title of the lesson/presentation.
                - Look for patterns like "**Course Name** : **Lesson Presentation** : **Title**" or similar
                - Extract ONLY the title part (the last part after the last "**")
                - If no clear pattern is found, use the first meaningful title or heading
            2.  `slides` (array): Ordered list of ALL slide objects in COMPONENT-BASED format.
            3.  `currentSlideId` (string, optional): ID of the currently active slide (can be null).
            4.  `lessonNumber` (integer, optional): Sequential number if part of a training plan.
            5.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk".
            6.  `hasVoiceover` (boolean, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST set this to true since every slide will have voiceover text.

            **SLIDE PARSING RULES - PARSE ALL SLIDES:**
            - Parse every slide section marked by "---" or slide separators in the input text
            - If input contains 15 slides, output exactly 15 slides in JSON
            - Do NOT filter or skip slides based on their titles or content
            - Do NOT remove slides with titles like "Questions", "Thank You", "Further Reading", etc.
            - Your job is to PARSE, not to validate or filter content

            **Component-Based Slide Object (`slides` array items):**
            * `slideId` (string): Generate unique identifier like "slide_1_intro", "slide_2_concepts" based on slide number and title.
            * `slideNumber` (integer): Sequential slide number from input (1, 2, 3, ...).
            * `slideTitle` (string): Extract descriptive title exactly as provided in the input.
            * `templateId` (string): Assign appropriate template based on content structure (see template guidelines below).
            * `props` (object): Template-specific properties containing the actual content from the slide.
            * `voiceoverText` (string, MANDATORY for Video Lessons): For Video Lesson Presentations, you MUST include conversational voiceover text that explains the slide content in detail. This field is REQUIRED for every slide in video lessons.

            **Template Assignment Guidelines:**
            Assign templateId based on the content structure of each slide:
            - If slide has large title + subtitle format → use "hero-title-slide" or "title-slide"
            - If slide has bullet points or lists → use "bullet-points" or "bullet-points-right"
            - If slide has two distinct sections → use "two-column" or "two-column-diversity"
            - If slide has numbered steps → use "process-steps"
            - If slide has 4 distinct points → use "four-box-grid"
            - If slide has metrics/statistics → use "big-numbers"
            - If slide has hierarchical content → use "pyramid"
            - If slide has timeline content → use "timeline"
            - For standard content → use "content-slide"
            
            **CRITICAL TABLE RULE:**
            - If ANY of these words appear in the prompt or slide content → MANDATORY USE `table-dark` or `table-light`:
              "table", "data table", "comparison table", "metrics table", "performance table", "results table", "statistics table", "summary table", "analysis table", "comparison data", "tabular data", "data comparison", "side by side", "versus", "vs", "compare", "comparison", "таблица", "сравнение", "сравнительная таблица", "данные", "метрики", "результаты", "статистика", "анализ", "сопоставление", "против", "по сравнению", "сравнительный анализ", "табличные данные", "структурированные данные"
            - Tables MUST use JSON props format with `tableData.headers` and `tableData.rows` arrays
            - NEVER use markdown tables or other formats for table content

            **Available Template IDs and their Props (must match exactly):**

            1. **`hero-title-slide`** - Hero opening slides:
            ```json
            "props": {
              "title": "Main slide title",
              "subtitle": "Detailed subtitle explaining the overview",
              "showAccent": true,
              "accentPosition": "left",
              "textAlign": "center",
              "titleSize": "xlarge",
              "subtitleSize": "medium"
            }
            ```

            2. **`title-slide`** - Simple title slides:
            ```json
            "props": {
              "title": "Presentation Title",
              "subtitle": "Compelling subtitle that captures attention",
              "author": "Author name",
              "date": "Date"
            }
            ```

            3. **`content-slide`** - Standard content slides:
            ```json
            "props": {
              "title": "Slide title",
              "content": "Main content with bullet points:\\n\\n• Point 1\\n• Point 2\\n• Point 3",
              "alignment": "left"
            }
            ```

            4. **`bullet-points`** - Formatted bullet point lists:
            ```json
            "props": {
              "title": "Key Points",
              "bullets": [
                "First important point with detailed explanation",
                "Second key insight with comprehensive analysis",
                "Third critical element with thorough examination",
                "Fourth essential consideration with strategic importance",
                "Fifth valuable perspective with actionable recommendations",
                "Sixth valuable perspective with actionable recommendations",
                "Seventh valuable perspective with actionable recommendations"
              ],
              "maxColumns": 2,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            5. **`two-column`** - Split layout:
            ```json
            "props": {
                "title": "Two Column Layout",
                "leftTitle": "Left Column",
                "leftContent": "Content for the left side with detailed explanations",
                "leftImageUrl": "https://via.placeholder.com/320x200?text=Left+Image",
                "leftImageAlt": "Description of left image",
                "leftImagePrompt": "Prompt for left image",
                "rightTitle": "Right Column",
                "rightContent": "Content for the right side with detailed information",
                "rightImageUrl": "https://via.placeholder.com/320x200?text=Right+Image",
                "rightImageAlt": "Description of right image",
                "rightImagePrompt": "Prompt for right image",
                "columnRatio": "50-50"
            }
            ```

            6. **`process-steps`** - Numbered process steps:
            ```json
            "props": {
              "title": "Process Steps",
              "steps": [
                "Step 1 with detailed description explaining what to do",
                "Step 2 with comprehensive explanation covering the process",
                "Step 3 with thorough description of the methodology",
                "Step 4 with in-depth explanation covering the final phase"
              ],
              "layout": "horizontal"
            }
            ```

            

            7. **`challenges-solutions`** - Problems vs solutions:
            ```json
            "props": {
              "title": "Challenges and Solutions",
              "challengesTitle": "Challenges",
              "solutionsTitle": "Solutions",
              "challenges": [
                "Brief five to six word challenge",
                "Another concise problem statement here"
              ],
              "solutions": [
                "Brief five to six word solution",
                "Another concise implementation approach here"
              ]
            }
            ```

            8. **`big-image-left`** - Large image on left:
            ```json
            "props": {
              "title": "Slide Title",
              "subtitle": "Subtitle or detailed description for the slide",
              "imageUrl": "https://via.placeholder.com/600x400?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration that visually represents the slide title",
              "imageSize": "large"
            }
            ```

            9. **`bullet-points-right`** - Title, subtitle, bullet points with image:
            ```json
            "props": {
              "title": "Key Points",
              "subtitle": "Short intro or context before the list",
              "bullets": [
                "First important point",
                "Second key insight",
                "Third critical element",
                "Fourth essential consideration",
                "Fifth valuable perspective"
              ],
              "maxColumns": 1,
              "bulletStyle": "dot",
              "imagePrompt": "A relevant illustration for the bullet points",
              "imageAlt": "Illustration for bullet points"
            }
            ```

            10. **`big-image-top`** - Large image on top:
            ```json
            "props": {
              "title": "Main Title",
              "subtitle": "Subtitle or content goes here",
              "imageUrl": "https://via.placeholder.com/700x350?text=Your+Image",
              "imageAlt": "Descriptive alt text",
              "imagePrompt": "A high-quality illustration for the topic",
              "imageSize": "large"
            }
            ```

            11. **`four-box-grid`** - Title and 4 boxes in 2x2 grid:
            ```json
            "props": {
              "title": "Main Title",
              "boxes": [
                { "heading": "Box 1", "text": "Detailed description with comprehensive explanations" },
                { "heading": "Box 2", "text": "Comprehensive explanation covering detailed insights" },
                { "heading": "Box 3", "text": "Thorough description spanning multiple sentences" },
                { "heading": "Box 4", "text": "In-depth explanation with actionable insights" }
              ]
            }
            ```

            12. **`timeline`** - Horizontal timeline with 4 events:
            ```json
            "props": {
              "title": "History and Evolution",
              "events": [
                { "date": "2020", "title": "Phase 1", "description": "Detailed description of the first phase" },
                { "date": "2021", "title": "Phase 2", "description": "Comprehensive explanation of the second phase" },
                { "date": "2022", "title": "Phase 3", "description": "Thorough description of the third phase" },
                { "date": "2023", "title": "Phase 4", "description": "In-depth explanation of the final phase" }
              ]
            }
            ```

            13. **`big-numbers`** - Three-column layout for metrics:
            ```json
            "props": {
              "title": "Key Metrics",
              "steps": [
                { "value": "25%", "label": "Performance Improvement", "description": "System performance improved by 25% after optimization" },
                { "value": "3x", "label": "Speed Increase", "description": "Processing speed increased 3 times faster than before" },
                { "value": "50%", "label": "Cost Reduction", "description": "Operating costs reduced by 50% through efficient design" }
              ]
            }
            ```

            14. **`pyramid`** - Pyramid diagram with 3 levels:
            ```json
            "props": {
              "title": "Hierarchical Structure",
              "subtitle": "Explanation of the hierarchical relationship between elements",
              "steps": [
                { "heading": "Top Level", "description": "Description of the highest level" },
                { "heading": "Middle Level", "description": "Description of the intermediate level" },
                { "heading": "Base Level", "description": "Description of the foundational level" }
              ]
            }
            ```

            **Content Parsing Instructions:**
            - Extract slide titles from headings or "**Slide N: Title**" format
            - Parse slide content and map to appropriate template props
            - For bullet-points: extract list items into "bullets" array
            - For two-column: split content into left and right sections
            - For process-steps: extract numbered or sequential items into "steps" array
            - For four-box-grid: parse "Box N:" format into "boxes" array
            - For big-numbers: parse table format into "items" array with value/label/description
            - For timeline: parse chronological content into "events" array with date, title, and description fields
            - For pyramid: parse hierarchical content into "steps" array

            **Critical Parsing Rules:**
            - Parse ALL slides provided in the input text - do not skip any
            - Maintain the exact number of slides from input to output
            - Assign appropriate templateId based on content structure, not validation rules
            - Preserve all content exactly as provided in the input
            - Generate sequential slideNumber values (1, 2, 3, ...)
            - Create descriptive slideId values based on number and title

            Important Localization Rule: All auxiliary headings or keywords must be in the same language as the surrounding content.

            Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_LESSON:
            target_content_model = VideoLessonData
            default_error_instance = VideoLessonData(
                mainPresentationTitle=f"LLM Parsing Error for {project_data.projectName}",
                slides=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
            {
  "mainPresentationTitle": "Курс: Обучение для рекрутера",
  "slides": [
    {
      "slideId": "slide_1_znakomstvo",
      "slideNumber": 1,
      "slideTitle": "Знакомство",
      "displayedText": "Знакомимся с основами рекрутинга и ключевыми обязанностями.",
      "displayedPictureDescription": "Улыбающиеся профессионалы в современном офисе.",
      "displayedVideoDescription": "Анимация воронки рекрутинга: поиск, отбор, интервью, оффер.",
      "voiceoverText": "Приветствую вас на курсе 'Обучение для рекрутера'! Начнем с основ. Этот модуль посвящен ключевым аспектам профессии."
    },
    {
      "slideId": "slide_2_instrumenty",
      "slideNumber": 2,
      "slideTitle": "Инструменты Рекрутера",
      "displayedText": "Рассматриваем основные инструменты для современного рекрутера.",
      "displayedPictureDescription": "Коллаж логотипов: LinkedIn, ATS, GitHub, поиск, календарь.",
      "displayedVideoDescription": "Анимация кликов по иконкам инструментов с краткими пояснениями их функций.",
      "voiceoverText": "Для успеха рекрутеру нужен арсенал инструментов. Рассмотрим основные категории и их назначение. Эффективное использование повысит вашу производительность."
    }
  ],
  "detectedLanguage": "ru"
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant. Your task is to convert the provided presentation slide content, which is in a specific structured text format, into a perfectly structured JSON object.

Your output MUST be a single, valid JSON object, and it must strictly adhere to the exact structure provided in the example JSON you have been given separately. Do not include any additional text, explanations, or conversational fillers outside the JSON object.

Input Text Structure and Extraction Rules:
The input text will describe a presentation or video lesson. The content within the fields (like slide titles, descriptions) can be in various languages (e.g., Ukrainian, Russian, English). You must extract the content exactly as it appears, preserving its original language, including any original formatting like line breaks within the content where present (e.g., in "Відображуваний Текст").

Overall Presentation Title:

This will be identified by a header like "Загальний Заголовок Курсу:" (or its equivalent in other languages like "ОБЩИЙ ЗАГОЛОВОК КУРСА:" or "Overall Course Title:").
Extract the text that immediately follows this bolded header as the value for the mainPresentationTitle field.
Individual Slides:

Each slide's information is clearly marked by consistently bolded headers.
slideNumber (integer): Look for "Номер Слайда:" (or equivalent, e.g., "Номер Слайда:", "Slide Number:"). Extract the numerical value that immediately follows this bolded header.
slideTitle (string): Look for "Заголовок Слайда:" (or equivalent, e.g., "Заголовок Слайда:", "Slide Title:"). Extract the text that immediately follows this bolded header.
displayedText (string): Look for "Відображуваний Текст:" (or equivalent, e.g., "Відображуваний Текст:", "Displayed Text:"). Extract all text that immediately follows this bolded header, up until the next bolded header. Preserve any internal line breaks or numbering.
displayedPictureDescription (string): Look for "Опис Зображення:" (or equivalent, e.g., "Опис Зображення:", "Image Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
displayedVideoDescription (string): Look for "Опис Відео:" (or equivalent, e.g., "Опис Відео:", "Video Description:"). Extract the text that immediately follows this bolded header, up until the next bolded header.
voiceoverText (string): Look for "Текст Озвучення:" (or equivalent, e.g., "Текст Озвучення:", "Voiceover Text:"). Extract the text that immediately follows this bolded header, up until the next bolded header or the end of the slide's content block.
slideId Generation:

For each slide, you must generate a unique slideId.
This ID should be a concatenation of the literal string "slide_", the slideNumber, and a simplified, lowercase version of the slideTitle.
To simplify the slideTitle for the ID, convert it to lowercase and replace all spaces with underscores (_). Remove any punctuation or special characters from the simplified title part of the ID. If the title is very long, consider using only the first few words to keep the ID concise, but ensure uniqueness. For example:
slideNumber: 1, slideTitle: "Вступ" -> slideId: "slide_1_вступ"
slideNumber: 2, slideTitle: "Питання 1" -> slideId: "slide_2_питання_1"
slideNumber: 3, slideTitle: "Варіанти відповіді" -> slideId: "slide_3_варіанти_відповіді"
slideNumber: 4, slideTitle: "Пояснення до Питання 1" -> slideId: "slide_4_пояснення_до_питання_1"
detectedLanguage (string):

This will be identified by a header like "Language of Content:" (or its equivalent, e.g., "Язык Контента:", "Мова Контенту:").
Extract the two-letter ISO 639-1 language code (e.g., "uk", "ru", "en") that immediately follows this label.
If this "Language of Content:" label is missing from the input, infer the primary language from the majority of the content (specifically the mainPresentationTitle and slideTitle fields) and use the appropriate two-letter ISO 639-1 code.
Key Parsing Rules & Constraints for 100% Reliability:

Header Recognition: Always identify fields by their bolded headers (e.g., "Номер Слайда:", "Заголовок Слайда:"). These bolded headers consistently precede the data you need to extract.
Exact Text Extraction: All extracted text content must be preserved exactly as it appears in the input, including its original capitalization, punctuation, and line breaks within the content block for a given field.
Field Presence: If a field's bolded header is present in the input but the text following it is empty before the next header, the corresponding JSON field should be an empty string (""). Do not use null or omit fields that are defined as strings in the target schema if their labels are present in the input.
Sequential Parsing: Process the text sequentially, extracting content associated with each bolded header until the next bolded header is encountered.
Return ONLY the JSON object.
            """
        elif selected_design_template.component_name == COMPONENT_NAME_QUIZ:
            target_content_model = QuizData
            default_error_instance = QuizData(
                quizTitle=f"LLM Parsing Error for {project_data.projectName}",
                questions=[]
            )
            llm_json_example = selected_design_template.template_structuring_prompt or """
{
"quizTitle": "Advanced Sales Techniques Quiz",
"detectedLanguage": "en",
"questions": [
{
"question_type": "multiple-choice",
"question_text": "Which technique involves assuming the sale is made?",
"options": [
{"id": "A", "text": "The 'Question Close'"},
{"id": "B", "text": "The 'Presumptive Close'"}
],
"correct_option_id": "B",
"explanation": "A presumptive close assumes the sale is made."
},
{
"question_type": "multi-select",
"question_text": "Which of the following are primary colors? (Select all that apply)",
"options": [
{"id": "A", "text": "Red"},
{"id": "B", "text": "Green"},
{"id": "C", "text": "Orange"},
{"id": "D", "text": "Blue"}
],
"correct_option_ids": ["A", "D"],
"explanation": "In the traditional subtractive model, the primary colors are Red, Yellow, and Blue."
},
{
"question_type": "matching",
"question_text": "Match each sales technique with its description:",
"prompts": [
{"id": "A", "text": "The 'Alternative Close'"},
{"id": "B", "text": "The 'Summary Close"}
],
"options": [
{"id": "1", "text": "Presenting two options to the customer"},
{"id": "2", "text": "Recapping key benefits before asking for the sale"}
],
"correct_matches": {"A": "1", "B": "2"},
"explanation": "The Alternative Close gives customers a choice between options, while the Summary Close reinforces value before closing."
},
{
"question_type": "sorting",
"question_text": "Arrange these steps in the correct order for a successful sales call:",
"items_to_sort": [
{"id": "step1", "text": "Identify customer needs"},
{"id": "step2", "text": "Present solution"},
{"id": "step3", "text": "Handle objections"},
{"id": "step4", "text": "Close the sale"}
],
"correct_order": ["step1", "step2", "step3", "step4"],
"explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
},
{
"question_type": "open-answer",
"question_text": "What are the three key elements of an effective elevator pitch?",
"acceptable_answers": [
"Problem, Solution, Call to Action",
"Problem statement, Your solution, What you want them to do next",
"The issue, How you solve it, What action to take"
],
"explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
}
]
}
            """
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Quiz' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the provided quiz content into a structured JSON object that captures all questions, their types, options, correct answers, and explanations.

            **Global Fields:**
            1. `quizTitle` (string): The main title of the quiz.
            2. `questions` (array): An array of question objects.
            3. `detectedLanguage` (string): e.g., "en", "ru".

            **Question Types and Their Structures:**

            1. **Multiple Choice (`question_type: "multiple-choice"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_id` (string): The ID of the correct option.
               * `explanation` (string, optional): Explanation of the correct answer.

            2. **Multi-Select (`question_type: "multi-select"`)**
               * `question_text` (string): The question text.
               * `options` (array): List of `QuizQuestionOption` objects with `id` and `text`.
               * `correct_option_ids` (array): Array of IDs of all correct options.
               * `explanation` (string, optional): Explanation of the correct answers.

            3. **Matching (`question_type: "matching"`)**
               * `question_text` (string): The question text.
               * `prompts` (array): List of `MatchingPrompt` objects with `id` and `text`.
               * `options` (array): List of `MatchingOption` objects with `id` and `text`.
               * `correct_matches` (object): Maps prompt IDs to option IDs.
               * `explanation` (string, optional): Explanation of the correct matches.

            4. **Sorting (`question_type: "sorting"`)**
               * `question_text` (string): The question text.
               * `items_to_sort` (array): List of `SortableItem` objects with `id` and `text`.
               * `correct_order` (array): Array of item IDs in the correct sequence.
               * `explanation` (string, optional): Explanation of the correct order.

            5. **Open Answer (`question_type: "open-answer"`)**
               * `question_text` (string): The question text.
               * `acceptable_answers` (array): List of acceptable answer strings.
               * `explanation` (string, optional): Explanation or additional context.

            **Key Parsing Rules:**
            1. Each question must have a unique type and appropriate fields for that type.
            2. Option IDs should be consistent (e.g., "A", "B", "C" for multiple choice).
            3. Maintain original language and formatting in all text fields.
            4. Include explanations where available to help users understand correct answers.
            5. Ensure all required fields are present for each question type.
            6. Validate that correct answers reference valid option IDs.

            Return ONLY the JSON object.
            """

        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # For lesson plans, preserve the original structure without parsing
            logger.info(f"Lesson plan detected for project {project_data.projectName}. Preserving original structure.")
            # Store the raw lesson plan data without parsing
            content_to_store_for_db = json.loads(project_data.aiResponse) if isinstance(project_data.aiResponse, str) else project_data.aiResponse
            derived_product_type = "lesson-plan"
            derived_microproduct_type = "Lesson Plan"
            
            # Skip the LLM parsing for lesson plans but continue with normal flow
            logger.info("Skipping LLM parsing for lesson plan - using raw data directly")
            # Set these variables to be used in the normal flow below
            target_content_model = None  # Not used for lesson plans
            default_error_instance = None  # Not used for lesson plans
            llm_json_example = ""  # Not used for lesson plans
            component_specific_instructions = ""  # Not used for lesson plans
            
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_PRODUCT:
            # For video products, we don't need LLM parsing since the content is already structured
            # The aiResponse contains the video metadata as JSON
            try:
                video_metadata = json.loads(project_data.aiResponse)
                # Store the video metadata directly without LLM parsing
                parsed_content_model_instance = video_metadata
                logger.info(f"Video product created with metadata: {video_metadata.get('videoJobId', 'unknown')}")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse video metadata JSON: {e}")
                # Create a fallback structure
                parsed_content_model_instance = {
                    "videoJobId": "unknown",
                    "videoUrl": "",
                    "thumbnailUrl": "",
                    "generatedAt": datetime.now().isoformat(),
                    "sourceSlides": [],
                    "component_name": "VideoProductDisplay"
                }
            
            # Skip LLM parsing for video products
            target_content_model = None
            default_error_instance = None
            llm_json_example = ""
            component_specific_instructions = ""
        else:
            logger.warning(f"Unknown component_name '{selected_design_template.component_name}' for DT ID {selected_design_template.id}. Defaulting to TrainingPlanDetails for parsing.")
            target_content_model = TrainingPlanDetails
            default_error_instance = TrainingPlanDetails(mainTitle=f"LLM Config Error for {project_data.projectName}", sections=[])
            llm_json_example = DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM
            component_specific_instructions = "Parse the content according to the JSON example provided."


        # Skip LLM parsing for video products since they already have structured content
        if selected_design_template.component_name == COMPONENT_NAME_VIDEO_PRODUCT:
            # parsed_content_model_instance is already set in the video product case above (line 12625)
            logger.info(f"Video product created, skipping LLM parsing - using direct JSON metadata")
            # Skip all LLM parsing - parsed_content_model_instance is already set
        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # Skip LLM parsing for lesson plans
            logger.info("Lesson plan detected - skipping LLM parsing entirely")
            parsed_content_model_instance = None  # Will not be used
        else:
            # Set detected language if the error instance supports it
            if hasattr(default_error_instance, 'detectedLanguage'):
                default_error_instance.detectedLanguage = detect_language(project_data.aiResponse)

        # LLM parsing flow (skipped for video products and lesson plans)
        if selected_design_template.component_name == COMPONENT_NAME_VIDEO_PRODUCT:
            # Already handled above - skip LLM parsing entirely
            pass
        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # Already handled above - skip LLM parsing entirely
            pass
        elif selected_design_template.component_name == COMPONENT_NAME_TRAINING_PLAN:
            # Fast path: Check if aiResponse is already valid JSON with sections (from preview)
            try:
                logger.info(f"[FAST_PATH_DEBUG] Checking aiResponse for Training Plan: {project_data.aiResponse[:200]}...")
                cached_json = json.loads(project_data.aiResponse.strip())
                logger.info(f"[FAST_PATH_DEBUG] JSON parsed successfully, type: {type(cached_json)}")
                if isinstance(cached_json, dict) and "sections" in cached_json:
                    logger.info(f"[FAST_PATH_DEBUG] JSON has sections field with {len(cached_json.get('sections', []))} sections")
                    logger.info(f"[FAST_PATH] Training Plan JSON detected, bypassing LLM parsing for {project_data.projectName}")
                    
                    # Clean lesson titles to remove "Lesson X.Y:" prefixes before creating TrainingPlanDetails
                    for section in cached_json.get("sections", []):
                        for lesson in section.get("lessons", []):
                            if isinstance(lesson, dict) and "title" in lesson:
                                original_title = lesson["title"]
                                # Remove "Lesson X.Y:" prefix using regex
                                import re
                                cleaned_title = re.sub(r'^Lesson\s+\d+\.\d+:\s*', '', original_title)
                                if cleaned_title != original_title:
                                    lesson["title"] = cleaned_title
                                    logger.info(f"[FAST_PATH_TITLE_CLEAN] '{original_title}' -> '{cleaned_title}'")
                    
                    # Round hours to integers before creating TrainingPlanDetails to prevent validation errors
                    cached_json = round_hours_in_content(cached_json)
                    logger.info(f"[FAST_PATH] Rounded hours to integers to prevent validation errors")
                    
                    parsed_content_model_instance = TrainingPlanDetails(**cached_json)
                    logger.info(f"[FAST_PATH_DEBUG] TrainingPlanDetails created successfully with {len(parsed_content_model_instance.sections)} sections")
                else:
                    logger.info(f"[FAST_PATH_DEBUG] JSON doesn't have sections, falling back to LLM parsing")
                    parsed_content_model_instance = await parse_ai_response_with_llm(
                        ai_response=project_data.aiResponse,
                        project_name=project_data.projectName,
                        target_model=target_content_model,
                        default_error_model_instance=default_error_instance,
                        dynamic_instructions=component_specific_instructions,
                        target_json_example=llm_json_example
                    )
            except (json.JSONDecodeError, KeyError, Exception) as e:
                logger.info(f"[FAST_PATH] JSON validation failed ({e}), falling back to LLM parsing")
                parsed_content_model_instance = await parse_ai_response_with_llm(
                    ai_response=project_data.aiResponse,
                    project_name=project_data.projectName,
                    target_model=target_content_model,
                    default_error_model_instance=default_error_instance,
                    dynamic_instructions=component_specific_instructions,
                    target_json_example=llm_json_example
                )
        # Add fast path for presentations (Slide Deck and Video Lesson Presentation) 
        elif selected_design_template.component_name in [COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION]:
            try:
                # 🔍 CRITICAL DEBUG: Log the raw AI response before parser processing
                logger.info(f"🔍 [PRESENTATION_AI_RESPONSE] Raw AI response for presentation:")
                logger.info(f"🔍 [PRESENTATION_AI_RESPONSE] Response length: {len(project_data.aiResponse)} characters")
                logger.info(f"🔍 [PRESENTATION_AI_RESPONSE] First 500 chars: {project_data.aiResponse[:500]}")
                logger.info(f"🔍 [PRESENTATION_AI_RESPONSE] Last 500 chars: {project_data.aiResponse[-500:]}")
                
                logger.info(f"[FAST_PATH_DEBUG] Checking aiResponse for Presentation: {project_data.aiResponse[:200]}...")
                cached_json = json.loads(project_data.aiResponse.strip())
                logger.info(f"[FAST_PATH_DEBUG] JSON parsed successfully, type: {type(cached_json)}")
                if isinstance(cached_json, dict) and "slides" in cached_json:
                    slides = cached_json.get('slides', [])
                    logger.info(f"[FAST_PATH_DEBUG] JSON has slides field with {len(slides)} slides")
                    
                    # 🔍 CRITICAL DEBUG: Log each slide before parser processing
                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slides before parser processing:")
                    for i, slide in enumerate(slides):
                        if isinstance(slide, dict):
                            template_id = slide.get('templateId', 'NO_TEMPLATE_ID')
                            slide_title = slide.get('slideTitle', 'NO_TITLE')
                            slide_number = slide.get('slideNumber', 'NO_NUMBER')
                            logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1}: templateId='{template_id}', slideTitle='{slide_title}', slideNumber={slide_number}")
                            
                            # Log props structure
                            props = slide.get('props', {})
                            if props:
                                logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} props keys: {list(props.keys())}")
                                # Log specific props for new templates
                                if template_id == 'course-overview-slide':
                                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} course-overview-slide props: title='{props.get('title', 'NO_TITLE')}', subtitle='{props.get('subtitle', 'NO_SUBTITLE')}'")
                                elif template_id == 'impact-statements-slide':
                                    statements = props.get('statements', [])
                                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} impact-statements-slide: {len(statements)} statements")
                                elif template_id == 'phishing-definition-slide':
                                    definitions = props.get('definitions', [])
                                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} phishing-definition-slide: {len(definitions)} definitions")
                                elif template_id == 'soft-skills-assessment-slide':
                                    tips = props.get('tips', [])
                                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} soft-skills-assessment-slide: {len(tips)} tips")
                                elif template_id == 'work-life-balance-slide':
                                    content = props.get('content', '')
                                    logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} work-life-balance-slide content: {content[:100]}{'...' if len(content) > 100 else ''}")
                            
                            # Log voiceover if present
                            if 'voiceoverText' in slide:
                                voiceover = slide['voiceoverText']
                                logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1} voiceover: {voiceover[:100]}{'...' if len(voiceover) > 100 else ''}")
                        else:
                            logger.info(f"🔍 [PRESENTATION_SLIDES_BEFORE_PARSER] Slide {i+1}: Not a dict, type={type(slide)}")
                    
                    logger.info(f"[FAST_PATH] Presentation JSON detected, bypassing LLM parsing for {project_data.projectName}")
                    
                    # Strip preview-only fields before model construction
                    try:
                        slides_list = cached_json.get('slides') or []
                        for s in slides_list:
                            if isinstance(s, dict) and 'previewKeyPoints' in s:
                                s.pop('previewKeyPoints', None)
                    except Exception as _cleanup_err:
                        logger.debug(f"[FAST_PATH_DEBUG] Failed to strip preview fields: {_cleanup_err}")
                    
                    parsed_content_model_instance = SlideDeckDetails(**cached_json)
                    logger.info(f"[FAST_PATH_DEBUG] SlideDeckDetails created successfully with {len(parsed_content_model_instance.slides)} slides")
                else:
                    logger.info(f"[FAST_PATH_DEBUG] JSON doesn't have slides, falling back to LLM parsing")
                    parsed_content_model_instance = await parse_ai_response_with_llm(
                        ai_response=project_data.aiResponse,
                        project_name=project_data.projectName,
                        target_model=target_content_model,
                        default_error_model_instance=default_error_instance,
                        dynamic_instructions=component_specific_instructions,
                        target_json_example=llm_json_example
                    )
            except (json.JSONDecodeError, KeyError, Exception) as e:
                logger.info(f"[FAST_PATH] Presentation JSON validation failed ({e}), falling back to LLM parsing")
                parsed_content_model_instance = await parse_ai_response_with_llm(
                    ai_response=project_data.aiResponse,
                    project_name=project_data.projectName,
                    target_model=target_content_model,
                    default_error_model_instance=default_error_instance,
                    dynamic_instructions=component_specific_instructions,
                    target_json_example=llm_json_example
                )
        else:
            parsed_content_model_instance = await parse_ai_response_with_llm(
                ai_response=project_data.aiResponse,
                project_name=project_data.projectName,
                target_model=target_content_model,
                default_error_model_instance=default_error_instance,
                dynamic_instructions=component_specific_instructions,
                target_json_example=llm_json_example
            )

        if selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            logger.info("Lesson plan detected - using raw data without parsing")
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_PRODUCT:
            logger.info("Video product detected - using direct JSON metadata without LLM parsing")
            logger.info(f"Video Product Content Type: {type(parsed_content_model_instance).__name__}")
            logger.info(f"Video Product Metadata: {json.dumps(parsed_content_model_instance)[:200]}")
        else:    
            logger.info(f"LLM Parsing Result Type: {type(parsed_content_model_instance).__name__}")
            logger.info(f"LLM Parsed Content (first 200 chars): {str(parsed_content_model_instance.model_dump_json())[:200]}") # Use model_dump_json()

        # Inject theme for slide decks from the finalize request
        if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
            parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'theme') and 
            hasattr(project_data, 'theme') and 
            project_data.theme):
            parsed_content_model_instance.theme = project_data.theme
            logger.info(f"Injected theme '{project_data.theme}' into slide deck")

        # Post-process module IDs for training plans to ensure № character is preserved
        if (parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'sections') and parsed_content_model_instance.sections):
            for section in parsed_content_model_instance.sections:
                if hasattr(section, 'id') and section.id:
                    # Fix module IDs that lost the № character
                    if section.id.isdigit():
                        # Plain number like "2" -> "№2"
                        section.id = f"№{section.id}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                    elif section.id.startswith("#"):
                        # Hash format like "#2" -> "№2"
                        number = section.id[1:]
                        section.id = f"№{number}"
                        logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                    elif not section.id.startswith("№"):
                        # Other formats without № - try to extract number and format correctly
                        import re
                        number_match = re.search(r'\d+', section.id)
                        if number_match:
                            number = number_match.group()
                            section.id = f"№{number}"
                            logger.info(f"[PROJECT_CREATE_ID_FIX] Fixed ID format to '{section.id}'")

        # Apply slide prop normalization for slide decks and video lesson presentations
        if (selected_design_template.component_name in [COMPONENT_NAME_SLIDE_DECK, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION] and 
            parsed_content_model_instance and
            hasattr(parsed_content_model_instance, 'slides') and 
            parsed_content_model_instance.slides):
            
            # Normalize slide props to fix schema mismatches
            slides_dict = [slide.model_dump() if hasattr(slide, 'model_dump') else dict(slide) for slide in parsed_content_model_instance.slides]
            
            # 🔍 CRITICAL DEBUG: Log slides before normalization
            logger.info(f"🔍 [SLIDES_BEFORE_NORMALIZATION] Slides before normalize_slide_props:")
            for i, slide in enumerate(slides_dict):
                if isinstance(slide, dict):
                    template_id = slide.get('templateId', 'NO_TEMPLATE_ID')
                    slide_title = slide.get('slideTitle', 'NO_TITLE')
                    logger.info(f"🔍 [SLIDES_BEFORE_NORMALIZATION] Slide {i+1}: templateId='{template_id}', slideTitle='{slide_title}'")
                    props = slide.get('props', {})
                    if props:
                        logger.info(f"🔍 [SLIDES_BEFORE_NORMALIZATION] Slide {i+1} props keys: {list(props.keys())}")
            
            normalized_slides = await normalize_slide_props(slides_dict, selected_design_template.component_name)
            
            # 🔍 CRITICAL DEBUG: Log slides after normalization
            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slides after normalize_slide_props:")
            for i, slide in enumerate(normalized_slides):
                if isinstance(slide, dict):
                    template_id = slide.get('templateId', 'NO_TEMPLATE_ID')
                    slide_title = slide.get('slideTitle', 'NO_TITLE')
                    logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1}: templateId='{template_id}', slideTitle='{slide_title}'")
                    props = slide.get('props', {})
                    if props:
                        logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} props keys: {list(props.keys())}")
                        # Log specific props for new templates after normalization
                        if template_id == 'course-overview-slide':
                            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} course-overview-slide props: title='{props.get('title', 'NO_TITLE')}', subtitle='{props.get('subtitle', 'NO_SUBTITLE')}', imagePath='{props.get('imagePath', 'NO_IMAGE')}'")
                        elif template_id == 'impact-statements-slide':
                            statements = props.get('statements', [])
                            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} impact-statements-slide: {len(statements)} statements")
                        elif template_id == 'phishing-definition-slide':
                            definitions = props.get('definitions', [])
                            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} phishing-definition-slide: {len(definitions)} definitions")
                        elif template_id == 'soft-skills-assessment-slide':
                            tips = props.get('tips', [])
                            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} soft-skills-assessment-slide: {len(tips)} tips")
                        elif template_id == 'work-life-balance-slide':
                            content = props.get('content', '')
                            logger.info(f"🔍 [SLIDES_AFTER_NORMALIZATION] Slide {i+1} work-life-balance-slide content: {content[:100]}{'...' if len(content) > 100 else ''}")
            
            # Update the content with normalized slides
            content_dict = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            content_dict['slides'] = normalized_slides
            
            # ✅ NEW: Set templateVersion='v2' for all newly created presentations
            content_dict['templateVersion'] = 'v2'
            logger.info("Set templateVersion='v2' for newly created presentation")
            
            # Remove hasVoiceover flag for regular slide decks
            if (selected_design_template.component_name == COMPONENT_NAME_SLIDE_DECK and 
                'hasVoiceover' in content_dict):
                logger.info("Removing hasVoiceover flag for regular slide deck")
                content_dict.pop('hasVoiceover', None)
            
            content_to_store_for_db = content_dict
            
            logger.info(f"Applied slide prop normalization for {len(normalized_slides)} slides")
        elif selected_design_template.component_name == COMPONENT_NAME_LESSON_PLAN:
            # For lesson plans, content_to_store_for_db was already set earlier - don't overwrite it
            logger.info("Lesson plan - using pre-set content_to_store_for_db")
        elif selected_design_template.component_name == COMPONENT_NAME_VIDEO_PRODUCT:
            # For video products, the content is already a dictionary
            content_to_store_for_db = parsed_content_model_instance
            logger.info(f"Video product content prepared for DB storage")
        else:
            content_to_store_for_db = parsed_content_model_instance.model_dump(mode='json', exclude_none=True)
            
        derived_product_type = selected_design_template.microproduct_type
        derived_microproduct_type = selected_design_template.template_name

        logger.info(f"Content prepared for DB storage (first 200 chars of JSON): {str(content_to_store_for_db)[:200]}")

        # Determine if this is a standalone product (default to True for general project creation)
        # For specific products like quizzes, this will be overridden in their dedicated endpoints
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_product = project_data.outlineId is None
        
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id,
            source_context_type, source_context_data
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW(), $11, $12, $13)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id,
                  source_context_type, source_context_data;
    """

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                project_data.projectName,
                derived_product_type,
                derived_microproduct_type,
                db_microproduct_name_to_store,
                content_to_store_for_db,
                project_data.design_template_id,
                project_data.chatSessionId,
                is_standalone_product,
                project_data.outlineId,
                project_data.folder_id,
                project_data.source_context_type,
                project_data.source_context_data
            )
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create project entry.")

        db_content_dict = row["microproduct_content"]
        final_content_for_response: Optional[MicroProductContentType] = None
        if db_content_dict and isinstance(db_content_dict, dict):
            component_name_from_db = selected_design_template.component_name
            try:
                if component_name_from_db == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_response = PdfLessonDetails(**db_content_dict)
                    logger.info("Re-parsed as PdfLessonDetails.")
                elif component_name_from_db == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_response = TextPresentationDetails(**db_content_dict)
                    logger.info("Re-parsed as TextPresentationDetails.")
                elif component_name_from_db == COMPONENT_NAME_TRAINING_PLAN:
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    final_content_for_response = TrainingPlanDetails(**db_content_dict)
                    logger.info("Re-parsed as TrainingPlanDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_response = VideoLessonData(**db_content_dict)
                    logger.info("Re-parsed as VideoLessonData.")
                elif component_name_from_db == COMPONENT_NAME_QUIZ:
                    final_content_for_response = QuizData(**db_content_dict)
                    logger.info("Re-parsed as QuizData.")
                elif component_name_from_db == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = await normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails.")
                elif component_name_from_db == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_dict and db_content_dict['slides']:
                        db_content_dict['slides'] = await normalize_slide_props(db_content_dict['slides'], component_name_from_db)
                    final_content_for_response = SlideDeckDetails(**db_content_dict)
                    logger.info("Re-parsed as SlideDeckDetails (Video Lesson Presentation).")
                elif component_name_from_db == COMPONENT_NAME_LESSON_PLAN:
                    # For lesson plans, preserve the original structure without parsing
                    logger.info("Re-parsing lesson plan - preserving original structure.")
                    final_content_for_response = db_content_dict
                elif component_name_from_db == COMPONENT_NAME_VIDEO_PRODUCT:
                    # For video products, return the raw dictionary data
                    final_content_for_response = db_content_dict
                    logger.info("Re-parsed as VideoProductDisplay (raw dictionary).")
                else:
                    logger.warning(f"Unknown component_name '{component_name_from_db}' when re-parsing content from DB on add. Attempting generic TrainingPlanDetails fallback.")
                    # Round hours to integers before parsing to prevent float validation errors
                    db_content_dict = round_hours_in_content(db_content_dict)
                    # Preserve custom fields (e.g., recommended_content_types) for edit view
                    final_content_for_response = db_content_dict
            except Exception as e_parse:
                logger.error(f"Error parsing content from DB on add (proj ID {row['id']}): {e_parse}", exc_info=not IS_PRODUCTION)

        return ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_response,
            design_template_id=row["design_template_id"], created_at=row["created_at"]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error inserting project: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while adding project." if IS_PRODUCTION else f"DB error on project insert: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    finally:
        # Always release the in-flight lock
        ACTIVE_PROJECT_CREATE_KEYS.discard(lock_key)


@app.get("/api/custom/projects/names", response_model=List[str], summary="Get unique project names for the user")
async def get_distinct_project_names_for_user(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT DISTINCT project_name
        FROM projects
        WHERE onyx_user_id = $1
        ORDER BY project_name ASC;
        """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        project_names: List[str] = [str(row["project_name"]) for row in rows if row["project_name"] is not None]
        return project_names
    except Exception as e:
        logger.error(f"Error fetching distinct project names for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project names." if IS_PRODUCTION else f"Database error while fetching project names: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.get("/api/custom/projects/{project_id}/edit", response_model=ProjectDetailForEditResponse)
async def get_project_details_for_edit(project_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT
            p.id, p.project_name, p.microproduct_name, p.microproduct_content, p.created_at,
            p.design_template_id, dt.template_name as design_template_name,
            dt.component_name as design_component_name,
            dt.design_image_path as design_image_path,
            p.product_type, p.microproduct_type
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND p.onyx_user_id = $2;
    """
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, project_id, onyx_user_id)
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found.")

        row_dict = dict(row)
        db_content_json = row_dict.get("microproduct_content")
        parsed_content_for_response: Optional[MicroProductContentType] = None
        component_name = row_dict.get("design_component_name")

        # Round hours to integers in the content before parsing
        if db_content_json and isinstance(db_content_json, dict):
            db_content_json = round_hours_in_content(db_content_json)
            
        if db_content_json and isinstance(db_content_json, dict):
            try:
                if component_name == COMPONENT_NAME_PDF_LESSON:
                    parsed_content_for_response = PdfLessonDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    parsed_content_for_response = TextPresentationDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content_json = sanitize_training_plan_for_parse(db_content_json)
                    parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON:
                    parsed_content_for_response = VideoLessonData(**db_content_json)
                elif component_name == COMPONENT_NAME_QUIZ:
                    parsed_content_for_response = QuizData(**db_content_json)
                elif component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = await normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                elif component_name == COMPONENT_NAME_VIDEO_LESSON_PRESENTATION:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content_json and db_content_json['slides']:
                        db_content_json['slides'] = await normalize_slide_props(db_content_json['slides'], component_name)
                    parsed_content_for_response = SlideDeckDetails(**db_content_json)
                else:
                    logger.warning(f"Unknown component_name '{component_name}' for project {project_id}. Trying fallbacks.", exc_info=not IS_PRODUCTION)
                    try: parsed_content_for_response = TrainingPlanDetails(**db_content_json)
                    except:
                        try: parsed_content_for_response = PdfLessonDetails(**db_content_json)
                        except Exception as e_parse_fallback: logger.error(f"Fallback parsing failed for project {project_id}: {e_parse_fallback}", exc_info=not IS_PRODUCTION)
            except Exception as e_main_parse:
                logger.error(f"Pydantic validation error for DB JSON (project {project_id}, component {component_name}): {e_main_parse}", exc_info=not IS_PRODUCTION)
        elif isinstance(db_content_json, str) and component_name == COMPONENT_NAME_TRAINING_PLAN:
                parsed_content_for_response = parse_training_plan_from_string(db_content_json, row_dict["project_name"])

        return ProjectDetailForEditResponse(
            id=row_dict["id"], projectName=row_dict["project_name"], microProductName=row_dict.get("microproduct_name"),
            design_template_id=row_dict.get("design_template_id"), microProductContent=parsed_content_for_response,
            createdAt=row_dict.get("created_at"), design_template_name=row_dict.get("design_template_name"),
            design_component_name=component_name, design_image_path=row_dict.get("design_image_path")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching project {project_id} for edit: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching project details." if IS_PRODUCTION else f"DB error fetching project details for edit: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

async def get_user_identifiers_for_workspace(request: Request) -> tuple[str, str]:
    """Get both user UUID and email for workspace access"""
    try:
        session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        if not session_cookie_value:
            dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
            if dev_user_id: 
                # For dev users, assume email format and return both
                return dev_user_id, dev_user_id
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authentication required")

        onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
        cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
            response.raise_for_status()
            user_data = response.json()
            
            user_id = user_data.get("userId") or user_data.get("id")
            user_email = user_data.get("email")
            
            if not user_id:
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="User ID extraction failed")
            
            return str(user_id), user_email or str(user_id)
    except Exception as e:
        logger.error(f"Error getting user identifiers: {e}")
        raise

@app.get("/api/custom/projects", response_model=List[ProjectApiResponse])
async def get_user_projects_list_from_db(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool),
    folder_id: Optional[int] = None
):
    # Get both UUID and email for the user
    user_uuid, user_email = await get_user_identifiers_for_workspace(request)
    
    # For backward compatibility with existing code
    onyx_user_id = user_uuid
    
    # For backward compatibility with existing code
    onyx_user_id = user_uuid
    
    # First, get projects owned by the user
    owned_projects_query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type,
               p.folder_id, p."order", p.microproduct_content, p.source_chat_session_id, p.is_standalone, p.course_id
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 {folder_filter}
    """
    
    # Then, get projects the user has access to through workspace permissions
    shared_projects_query = """
        SELECT DISTINCT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type,
               p.folder_id, p."order", p.microproduct_content, p.source_chat_session_id, p.is_standalone, p.course_id
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        INNER JOIN product_access pa ON p.id = pa.product_id
        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
        WHERE (wm.user_id = $1 OR wm.user_id = $2)
          AND wm.status = 'active'
          AND pa.access_type IN ('workspace', 'role', 'individual')
          AND (
              pa.access_type = 'workspace' 
              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
              OR (pa.access_type = 'individual' AND (pa.target_id = $1 OR pa.target_id = $2))
          )
          {folder_filter}
    """
    
    # Get both UUID and email for workspace access
    try:
        session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        if not session_cookie_value:
            dev_user_id = request.headers.get("X-Dev-Onyx-User-ID")
            if dev_user_id:
                user_uuid = dev_user_id
                user_email = dev_user_id  # For dev, assume email format
            else:
                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authentication required")
        else:
            onyx_user_info_url = f"{ONYX_API_SERVER_URL}/me"
            cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(onyx_user_info_url, cookies=cookies_to_forward)
                response.raise_for_status()
                user_data = response.json()
                
                user_uuid = str(user_data.get("userId") or user_data.get("id"))
                user_email = user_data.get("email") or user_uuid
    except Exception as e:
        logger.error(f"Error getting user identifiers: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="User identification failed")
    
    folder_filter = ""
    owned_params = [user_uuid]
    shared_params = [user_uuid, user_email]
    
    if folder_id is not None:
        folder_filter = "AND p.folder_id = $2"
        owned_params.append(folder_id)
        folder_filter_shared = "AND p.folder_id = $3"
        shared_params.append(folder_id)
    else:
        folder_filter_shared = ""
    
    owned_query = owned_projects_query.format(folder_filter=folder_filter)
    shared_query = shared_projects_query.format(folder_filter=folder_filter_shared)
    
    async with pool.acquire() as conn:
        # Get owned projects (use UUID)
        owned_rows = await conn.fetch(owned_query, *owned_params)
        
        # Get shared projects (use email)
        shared_rows = await conn.fetch(shared_query, *shared_params)
        
        # 🔍 DEBUG: Log workspace access results
        logger.info(f"🔍 [WORKSPACE ACCESS] User {user_uuid} (email: {user_email}) projects query results:")
        logger.info(f"   - Owned projects: {len(owned_rows)}")
        logger.info(f"   - Shared projects: {len(shared_rows)}")
        logger.info(f"   - Folder filter: {folder_id}")
        
        if owned_rows:
            logger.info(f"   - Owned project IDs: {[row['id'] for row in owned_rows]}")
        if shared_rows:
            logger.info(f"   - Shared project IDs: {[row['id'] for row in shared_rows]}")
        else:
            # Debug why no shared projects found
            logger.info(f"🔍 [WORKSPACE DEBUG] No shared projects found for user {onyx_user_id}. Investigating...")
            
            # Check workspace memberships using email (not UUID)
            membership_check = await conn.fetch("""
                SELECT wm.*, w.name as workspace_name, wr.name as role_name
                FROM workspace_members wm
                JOIN workspaces w ON wm.workspace_id = w.id
                JOIN workspace_roles wr ON wm.role_id = wr.id
                WHERE wm.user_id = $1
            """, user_email)
            
            logger.info(f"   - User workspace memberships: {len(membership_check)}")
            for membership in membership_check:
                logger.info(f"     * Workspace: {membership['workspace_name']} (ID: {membership['workspace_id']})")
                logger.info(f"       Role: {membership['role_name']} (ID: {membership['role_id']})")
                logger.info(f"       Status: {membership['status']}")
            
            if not membership_check:
                logger.info(f"   ❌ User {user_uuid} is not a member of any workspace!")
                logger.info(f"   💡 Add user to a workspace to enable shared project access")
                logger.info(f"   - User workspace memberships: 0")
                logger.info(f"   - No workspace memberships found - user needs to be added to a workspace")
            
            # Check product access records
            if membership_check:
                workspace_ids = [m['workspace_id'] for m in membership_check]
                access_check = await conn.fetch("""
                    SELECT pa.*, p.project_name, w.name as workspace_name
                    FROM product_access pa
                    JOIN projects p ON pa.product_id = p.id
                    JOIN workspaces w ON pa.workspace_id = w.id
                    WHERE pa.workspace_id = ANY($1::int[])
                """, workspace_ids)
                
                logger.info(f"   - Product access records in user's workspaces: {len(access_check)}")
                for access in access_check:
                    logger.info(f"     * Project: {access['project_name']} (ID: {access['product_id']})")
                    logger.info(f"       Workspace: {access['workspace_name']} (ID: {access['workspace_id']})")
                    logger.info(f"       Access Type: {access['access_type']}")
                    logger.info(f"       Target ID: {access['target_id']}")
            else:
                logger.info(f"   - No workspace memberships found - user needs to be added to a workspace")
        
        # Combine and deduplicate projects
        all_projects = {}
        
        # Process owned projects first
        for row_data in owned_rows:
            row_dict = dict(row_data)
            project_slug = create_slug(row_dict.get('project_name'))
            source_chat_session_id = row_dict.get("source_chat_session_id")
            if source_chat_session_id:
                source_chat_session_id = str(source_chat_session_id)
            
            all_projects[row_dict["id"]] = ProjectApiResponse(
                id=row_dict["id"], projectName=row_dict["project_name"], projectSlug=project_slug,
                microproduct_name=row_dict.get("microproduct_name"),
                design_template_name=row_dict.get("design_template_name"),
                design_microproduct_type=row_dict.get("design_microproduct_type"),
                created_at=row_dict["created_at"], design_template_id=row_dict.get("design_template_id"),
                folder_id=row_dict.get("folder_id"), order=row_dict.get("order"),
                microproduct_content=row_dict.get("microproduct_content"),
                source_chat_session_id=source_chat_session_id,
                is_standalone=row_dict.get("is_standalone"),
                course_id=row_dict.get("course_id")
            )
        
        # Process shared projects (will override owned if same ID, which is fine)
        for row_data in shared_rows:
            row_dict = dict(row_data)
            project_slug = create_slug(row_dict.get('project_name'))
            source_chat_session_id = row_dict.get("source_chat_session_id")
            if source_chat_session_id:
                source_chat_session_id = str(source_chat_session_id)
            
            all_projects[row_dict["id"]] = ProjectApiResponse(
                id=row_dict["id"], projectName=row_dict["project_name"], projectSlug=project_slug,
                microproduct_name=row_dict.get("microproduct_name"),
                design_template_name=row_dict.get("design_template_name"),
                design_microproduct_type=row_dict.get("design_microproduct_type"),
                created_at=row_dict["created_at"], design_template_id=row_dict.get("design_template_id"),
                folder_id=row_dict.get("folder_id"), order=row_dict.get("order"),
                microproduct_content=row_dict.get("microproduct_content"),
                source_chat_session_id=source_chat_session_id,
                is_standalone=row_dict.get("is_standalone"),
                course_id=row_dict.get("course_id")
            )
    
    # Convert to list and sort
    projects_list = list(all_projects.values())
    projects_list.sort(key=lambda x: (x.order or 0, x.created_at), reverse=True)
    
    return projects_list

@app.get("/api/custom/projects/view/{project_id}", response_model=MicroProductApiResponse, responses={404: {"model": ErrorDetail}})
async def get_project_instance_detail(
    project_id: int, 
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    # Get user identifiers for workspace access
    user_uuid, user_email = await get_user_identifiers_for_workspace(request)
    
    # Check if user owns the project or has workspace access
    select_query = """
        SELECT p.*, dt.template_name as design_template_name, dt.microproduct_type as design_microproduct_type, dt.component_name
        FROM projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.id = $1 AND (
            p.onyx_user_id = $2 
            OR EXISTS (
                SELECT 1 FROM product_access pa
                INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                WHERE pa.product_id = p.id 
                  AND wm.user_id = $3 
                  AND wm.status = 'active'
                  AND pa.access_type IN ('workspace', 'role', 'individual')
                  AND (
                      pa.access_type = 'workspace' 
                      OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                      OR (pa.access_type = 'individual' AND pa.target_id = $3)
                  )
            )
        )
    """
    async with pool.acquire() as conn:
        row = await conn.fetchrow(select_query, project_id, user_uuid, user_email)
    if not row:
        raise HTTPException(status_code=404, detail="Project not found")
    row_dict = dict(row)
    project_instance_name = row_dict.get("microproduct_name") or row_dict.get("project_name")
    project_slug = create_slug(project_instance_name)
    component_name = row_dict.get("component_name")
    details_data = row_dict.get("microproduct_content")
    
    # 🔍 BACKEND VIEW LOGGING: What we retrieved from database for view
    logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Raw details_data type: {type(details_data)}")
    
    # Parse the details_data if it's a JSON string
    parsed_details = None
    if details_data:
        if isinstance(details_data, str):
            try:
                # Parse JSON string to dict
                details_dict = json.loads(details_data)
                # For video products, preserve the original structure without rounding hours
                if component_name == COMPONENT_NAME_VIDEO_PRODUCT:
                    parsed_details = details_dict
                    logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Video product parsed from JSON string (preserving structure): {json.dumps(parsed_details, indent=2)}")
                else:
                    # Round hours to integers before returning for other content types
                    details_dict = round_hours_in_content(details_dict)
                    parsed_details = details_dict
                    logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Parsed from JSON string: {json.dumps(parsed_details, indent=2)}")
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Failed to parse microproduct_content JSON for project {project_id}: {e}")
                parsed_details = None
        else:
            # For video products, preserve the original structure without rounding hours
            if component_name == COMPONENT_NAME_VIDEO_PRODUCT:
                parsed_details = details_data
                logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Video product already dict (preserving structure): {json.dumps(parsed_details, indent=2)}")
            else:
                # Already a dict, just round hours for other content types
                parsed_details = round_hours_in_content(details_data)
                logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Already dict, after round_hours: {json.dumps(parsed_details, indent=2)}")
    
    # 🔍 BACKEND VIEW RESULT LOGGING
    if parsed_details and 'contentBlocks' in parsed_details:
        image_blocks = [block for block in parsed_details['contentBlocks'] if block.get('type') == 'image']
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - Final image blocks for frontend: {json.dumps(image_blocks, indent=2)}")
    else:
        logger.info(f"📋 [BACKEND VIEW] Project {project_id} - No contentBlocks in parsed_details or parsed_details is None")
    
    web_link_path = None
    pdf_link_path = None
    
    # Parse lesson_plan_data if it exists and is a JSON string
    lesson_plan_data = row_dict.get("lesson_plan_data")
    if lesson_plan_data and isinstance(lesson_plan_data, str):
        try:
            lesson_plan_data = json.loads(lesson_plan_data)
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"Failed to parse lesson_plan_data JSON for project {project_id}: {e}")
            lesson_plan_data = None
    
    # 🔍 CRITICAL DEBUG: Log the exact response being sent to frontend
    # For video products and video lesson presentations, ensure we preserve the raw dictionary without Pydantic validation
    if (component_name in [COMPONENT_NAME_VIDEO_PRODUCT, COMPONENT_NAME_VIDEO_LESSON_PRESENTATION, COMPONENT_NAME_SLIDE_DECK]) and parsed_details:
        # Create response with raw video/slide metadata to avoid Pydantic validation issues
        # This prevents the slides array from being corrupted to contentBlocks
        response_data = MicroProductApiResponse(
            name=project_instance_name, slug=project_slug, project_id=project_id,
            design_template_id=row_dict["design_template_id"], component_name=component_name,
            webLinkPath=web_link_path, pdfLinkPath=pdf_link_path, details=parsed_details,
            sourceChatSessionId=row_dict.get("source_chat_session_id"),
            parentProjectName=row_dict.get('project_name'),
            custom_rate=row_dict.get("custom_rate"),
            quality_tier=row_dict.get("quality_tier"),
            is_advanced=row_dict.get("is_advanced"),
            advanced_rates=row_dict.get("advanced_rates"),
            lesson_plan_data=lesson_plan_data
        )
        # Override the details field to ensure it remains as raw dict
        response_data.details = parsed_details
        logger.info(f"🔍 [DATA INTEGRITY] Project {project_id} - Preserved raw dict for {component_name} to prevent slides→contentBlocks corruption")
    else:
        # For other content types, use normal processing
        response_data = MicroProductApiResponse(
            name=project_instance_name, slug=project_slug, project_id=project_id,
            design_template_id=row_dict["design_template_id"], component_name=component_name,
            webLinkPath=web_link_path, pdfLinkPath=pdf_link_path, details=parsed_details,
            sourceChatSessionId=row_dict.get("source_chat_session_id"),
            parentProjectName=row_dict.get('project_name'),
            custom_rate=row_dict.get("custom_rate"),
            quality_tier=row_dict.get("quality_tier"),
            is_advanced=row_dict.get("is_advanced"),
            advanced_rates=row_dict.get("advanced_rates")
        )
    
    # 🔍 CRITICAL DEBUG: For video products and slide-based components, log the exact response being sent
    if component_name == COMPONENT_NAME_VIDEO_PRODUCT:
        logger.info(f"🎬 [CRITICAL DEBUG] Sending response to frontend for Project {project_id}:")
        logger.info(f"🎬 [CRITICAL DEBUG] Response component_name: {response_data.component_name}")
        logger.info(f"🎬 [CRITICAL DEBUG] Response details type: {type(response_data.details)}")
        logger.info(f"🎬 [CRITICAL DEBUG] Response details content: {response_data.details}")
        if hasattr(response_data.details, 'videoUrl'):
            logger.info(f"🎬 [CRITICAL DEBUG] Response has videoUrl: {response_data.details.videoUrl}")
        elif isinstance(response_data.details, dict) and 'videoUrl' in response_data.details:
            logger.info(f"🎬 [CRITICAL DEBUG] ✅ FIXED: Response dict has videoUrl: {response_data.details['videoUrl']}")
            logger.info(f"🎬 [CRITICAL DEBUG] ✅ FIXED: Response dict has videoJobId: {response_data.details.get('videoJobId', 'NOT_FOUND')}")
            logger.info(f"🎬 [CRITICAL DEBUG] ✅ FIXED: Response dict has thumbnailUrl: {response_data.details.get('thumbnailUrl', 'NOT_FOUND')}")
            logger.info(f"🎬 [CRITICAL DEBUG] ✅ FIXED: Video metadata preserved successfully!")
        else:
            logger.info(f"🎬 [CRITICAL DEBUG] ❌ Response has NO videoUrl!")
    elif component_name in [COMPONENT_NAME_VIDEO_LESSON_PRESENTATION, COMPONENT_NAME_SLIDE_DECK]:
        logger.info(f"📊 [CRITICAL DEBUG] Sending slide-based response to frontend for Project {project_id}:")
        logger.info(f"📊 [CRITICAL DEBUG] Response component_name: {response_data.component_name}")
        logger.info(f"📊 [CRITICAL DEBUG] Response details type: {type(response_data.details)}")
        if isinstance(response_data.details, dict):
            logger.info(f"📊 [CRITICAL DEBUG] ✅ Response dict has slides: {'slides' in response_data.details}")
            logger.info(f"📊 [CRITICAL DEBUG] ✅ Response dict has contentBlocks: {'contentBlocks' in response_data.details}")
            if 'slides' in response_data.details:
                logger.info(f"📊 [CRITICAL DEBUG] ✅ FIXED: Slides array preserved with {len(response_data.details['slides'])} slides")
            elif 'contentBlocks' in response_data.details:
                logger.error(f"📊 [CRITICAL DEBUG] ❌ BUG: Data corrupted to contentBlocks instead of slides!")
        else:
            logger.warning(f"📊 [CRITICAL DEBUG] Response details is not a dict: {type(response_data.details)}")
    
    return response_data

@app.get("/api/custom/pdf/folder/{folder_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_folder_as_pdf(
    folder_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download all products in a folder as a single PDF, ordered by type and creation date."""
    try:
        # First, verify the folder exists and belongs to the user
        async with pool.acquire() as conn:
            folder_row = await conn.fetchrow(
                """
                SELECT name FROM project_folders 
                WHERE id = $1 AND onyx_user_id = $2;
                """,
                folder_id, onyx_user_id
            )
        if not folder_row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Folder not found for user.")

        folder_name = folder_row['name']
        
        # Get all projects in the folder, ordered by their position in the folder
        async with pool.acquire() as conn:
            projects = await conn.fetch(
                """
                SELECT p.id, p.project_name, p.microproduct_name, p.microproduct_content,
                       p.created_at, dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                ORDER BY p."order" ASC, p.created_at ASC;
                """,
                folder_id, onyx_user_id
            )
        
        if not projects:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="No projects found in folder.")
        
        # Generate individual PDFs for each project
        pdf_paths = []
        project_names = []
        
        for project in projects:
            project_id = project['id']
            project_name = project['microproduct_name'] or project['project_name']
            content_json = project['microproduct_content']
            component_name = project['design_component_name']
            
            # Skip unsupported project types
            if component_name not in ['TextPresentationDisplay', 'TrainingPlanTable']:
                continue
            
            try:
                # Generate PDF for this project using existing logic
                mp_name_for_pdf_context = project_name
                content_json = project['microproduct_content']
                component_name = project['design_component_name']
                data_for_template_render: Optional[Dict[str, Any]] = None
                pdf_template_file: str

                detected_lang_for_pdf = 'ru'  # Default language
                if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
                    detected_lang_for_pdf = content_json.get('detectedLanguage')
                elif mp_name_for_pdf_context:
                    detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
                
                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

                if component_name == 'TextPresentationDisplay':
                    pdf_template_file = "text_presentation_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        data_for_template_render = json.loads(json.dumps(content_json))
                        if not data_for_template_render.get('detectedLanguage'):
                            data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                        
                        # Log content blocks for debugging image issues
                        content_blocks = data_for_template_render.get('contentBlocks', [])
                        image_blocks = [block for block in content_blocks if block.get('type') == 'image']
                        
                        logger.info(f"🖼️ [PDF GEN] Processing {len(content_blocks)} content blocks, {len(image_blocks)} image blocks")
                        for i, block in enumerate(image_blocks):
                            logger.info(f"🖼️ [PDF GEN] Image block {i}: {json.dumps(block, indent=2)}")
                            if hasattr(block, 'keys'):
                                logger.info(f"🖼️ [PDF GEN] Image block {i} keys: {list(block.keys())}")
                            if 'src' in block:
                                logger.info(f"🖼️ [PDF GEN] Image block {i} src: '{block['src']}' (type: {type(block['src'])})")
                            else:
                                logger.info(f"🚨 [PDF GEN] Image block {i} missing 'src' property!")
                                
                    else:
                        data_for_template_render = {
                            "title": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "contentBlocks": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                elif component_name == 'TrainingPlanTable':
                    pdf_template_file = "training_plan_pdf_template.html"
                    if content_json and isinstance(content_json, dict):
                        try:
                            content_json = round_hours_in_content(content_json)
                            parsed_model = TrainingPlanDetails(**content_json)
                            if parsed_model.detectedLanguage:
                                detected_lang_for_pdf = parsed_model.detectedLanguage
                                current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                            
                            data_for_template_render = {
                                'mainTitle': parsed_model.mainTitle,
                                'sections': parsed_model.sections,
                                'detectedLanguage': detected_lang_for_pdf
                            }
                        except Exception as e:
                            logger.error(f"Error parsing training plan for project {project_id}: {e}")
                            data_for_template_render = {
                                "mainTitle": f"Error: {mp_name_for_pdf_context}",
                                "sections": [],
                                "detectedLanguage": detected_lang_for_pdf
                            }
                    else:
                        data_for_template_render = {
                            "mainTitle": f"Content Unavailable: {mp_name_for_pdf_context}",
                            "sections": [],
                            "detectedLanguage": detected_lang_for_pdf
                        }
                
                else:
                    continue  # Skip unsupported types
                
                if not isinstance(data_for_template_render, dict):
                    data_for_template_render = {"title": "Error", "contentBlocks": [], "detectedLanguage": "en"}
                
                unique_output_filename = f"folder_export_{folder_id}_project_{project_id}_{uuid.uuid4().hex[:8]}.pdf"
                
                context_for_jinja = {
                    'details': data_for_template_render,
                    'locale': current_pdf_locale_strings,
                    'pdf_context': {
                        'static_images_path': os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
                    }
                }
                
                pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
                if os.path.exists(pdf_path):
                    pdf_paths.append(pdf_path)
                    project_names.append(project_name)
                
            except Exception as e:
                logger.error(f"Error generating PDF for project {project_id}: {e}")
                continue
        
        if not pdf_paths:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs could be generated for projects in folder.")
        
        # Combine PDFs into a single file
        try:
            if PdfMerger is None:
                # If PyPDF2 is not available, return the first PDF as a fallback
                logger.warning("PyPDF2 not available, returning first PDF as fallback")
                if pdf_paths:
                    user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    return FileResponse(
                        path=pdf_paths[0],
                        filename=user_friendly_filename,
                        media_type='application/pdf',
                        headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
                    )
                else:
                    raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No PDFs generated and PyPDF2 not available")
            
            merger = PdfMerger()
            
            for pdf_path in pdf_paths:
                merger.append(pdf_path)
            
            combined_pdf_path = f"/tmp/folder_export_{folder_id}_{uuid.uuid4().hex[:8]}.pdf"
            merger.write(combined_pdf_path)
            merger.close()
            
            # Clean up individual PDF files
            for pdf_path in pdf_paths:
                try:
                    os.remove(pdf_path)
                except:
                    pass
            
            user_friendly_filename = f"{create_slug(folder_name)}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            return FileResponse(
                path=combined_pdf_path,
                filename=user_friendly_filename,
                media_type='application/pdf',
                headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
            )
            
        except Exception as e:
            logger.error(f"Error combining PDFs for folder {folder_id}: {e}")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to combine PDFs")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating folder PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate folder PDF: {str(e)[:200]}")
    

# Streaming slide deck PDF generation with progress updates
@app.get("/api/custom/pdf/slide-deck/{project_id}/stream")
async def stream_slide_deck_pdf_generation(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Stream slide deck PDF generation with progress updates"""
    from fastapi.responses import StreamingResponse
    import json
    
    async def generate_with_progress():
        try:
            # Get project data (same as the existing endpoint)
            async with pool.acquire() as conn:
                target_row_dict = await conn.fetchrow(
                    """
                    SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                           dt.component_name as design_component_name
                    FROM projects p
                    LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                    WHERE p.id = $1 AND p.onyx_user_id = $2;
                    """,
                    project_id, onyx_user_id
                )
            
            if not target_row_dict:
                yield f"data: {json.dumps({'error': 'Project not found'})}\n\n"
                return

            component_name = target_row_dict.get("design_component_name")
            if component_name != COMPONENT_NAME_SLIDE_DECK:
                yield f"data: {json.dumps({'error': 'This endpoint is only for slide deck projects'})}\n\n"
                return

            content_json = target_row_dict.get('microproduct_content')
            if not content_json or not isinstance(content_json, dict):
                yield f"data: {json.dumps({'error': 'Invalid slide deck content'})}\n\n"
                return

            # Extract templateVersion from content for version-aware PDF rendering
            deck_template_version = content_json.get('templateVersion') or content_json.get('template_version') or 'v1'
            logger.info(f"🎯 STREAMING PDF - Extracted templateVersion from content: {deck_template_version}")
            
            # Prepare slide deck data
            slide_deck_data = {
                'slides': content_json.get('slides', []),
                'theme': theme or 'dark-purple'
            }

            total_slides = len(slide_deck_data['slides'])
            yield f"data: {json.dumps({'type': 'progress', 'message': f'Starting PDF generation for {total_slides} slides...', 'current': 0, 'total': total_slides})}\n\n"

            mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
            unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
            
            # Generate PDF with regular function and send progress updates
            from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
            
            # Send intermediate progress messages
            yield f"data: {json.dumps({'type': 'progress', 'message': 'Calculating slide dimensions...', 'current': 1, 'total': total_slides})}\n\n"
            
            # Simulate progress for user feedback during long operation
            import asyncio
            
            # Start PDF generation in background and send periodic updates
            # CRITICAL FIX: Pass deck_template_version for version-aware rendering
            pdf_task = asyncio.create_task(generate_slide_deck_pdf_with_dynamic_height(
                slides_data=slide_deck_data['slides'],
                theme=theme,
                output_filename=unique_output_filename,
                use_cache=True,
                deck_template_version=deck_template_version  # ← Pass version for v1/v2 template selection
            ))
            
            # Send progress updates while PDF is generating
            progress_step = 0
            max_steps = total_slides * 2  # Simulate steps for dimension calc + generation
            
            while not pdf_task.done():
                await asyncio.sleep(2)  # Update every 2 seconds
                progress_step += 1
                current_progress = min(progress_step, max_steps - 1)
                
                if progress_step <= total_slides:
                    message = f"Calculating dimensions for slide {progress_step}..."
                else:
                    slide_num = progress_step - total_slides
                    message = f"Generating slide {slide_num}..."
                
                yield f"data: {json.dumps({'type': 'progress', 'message': message, 'current': current_progress, 'total': max_steps})}\n\n"
            
            # Wait for PDF generation to complete
            pdf_path = await pdf_task
            
            # Send final progress update
            yield f"data: {json.dumps({'type': 'progress', 'message': 'PDF generation completed!', 'current': max_steps, 'total': max_steps})}\n\n"
                
            # Final success message with download info - FIXED: Return filename instead of URL that triggers regeneration
            user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
            final_message = {
                'type': 'complete',
                'message': 'PDF generation completed successfully!',
                'download_url': f'/pdf/slide-deck/{project_id}/download/{os.path.basename(pdf_path)}?theme={theme}',
                'filename': user_friendly_pdf_filename
            }
            yield f"data: {json.dumps(final_message)}\n\n"
            
        except Exception as e:
            logger.error(f"Error in streaming PDF generation: {e}", exc_info=True)
            error_message = {
                'type': 'error',
                'message': f'PDF generation failed: {str(e)[:200]}'
            }
            yield f"data: {json.dumps(error_message)}\n\n"

    return StreamingResponse(
        generate_with_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

# New endpoint to serve cached PDFs without regeneration
@app.get("/api/custom/pdf/slide-deck/{project_id}/download/{pdf_filename}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_cached_slide_deck_pdf(
    project_id: int,
    pdf_filename: str,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download cached slide deck PDF without regeneration"""
    try:
        # Verify the project exists and user has access
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        # Construct the path to the cached PDF
        from app.services.pdf_generator import PDF_CACHE_DIR
        pdf_path = PDF_CACHE_DIR / pdf_filename
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="PDF file not found. It may have expired or been deleted.")
        
        # Create user-friendly filename
        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"
        
        return FileResponse(
            path=str(pdf_path), 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error serving cached slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to serve PDF: {str(e)[:200]}")

# Move slide deck route BEFORE the general route to avoid path conflicts
@app.get("/api/custom/pdf/slide-deck/{project_id}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_slide_deck_pdf(
    project_id: int,
    theme: Optional[str] = Query("dark-purple"),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download slide deck as multi-page PDF"""
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        component_name = target_row_dict.get("design_component_name")
        if component_name != COMPONENT_NAME_SLIDE_DECK:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="This endpoint is only for slide deck projects.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        if not content_json or not isinstance(content_json, dict):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slide deck content.")

        # Prepare slide deck data for PDF generation
        slide_deck_data = {
            'lessonTitle': content_json.get('lessonTitle', mp_name_for_pdf_context),
            'slides': content_json.get('slides', []),
            'theme': theme,
            'detectedLanguage': content_json.get('detectedLanguage', 'en')
        }

        # Validate slides structure
        if not isinstance(slide_deck_data['slides'], list):
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid slides structure.")

        logger.info(f"Slide Deck PDF Gen (Project {project_id}): Generating PDF with {len(slide_deck_data['slides'])} slides, theme: {theme}")

        # ✅ NEW: Detailed logging for slide data before PDF generation
        logger.info(f"=== SLIDE DATA ANALYSIS BEFORE PDF GENERATION ===")
        logger.info(f"Project ID: {project_id}")
        logger.info(f"Total slides: {len(slide_deck_data['slides'])}")
        logger.info(f"Theme: {theme}")
        
        # Analyze each slide for big-image-left template
        big_image_left_slides = []
        for i, slide in enumerate(slide_deck_data['slides']):
            if slide.get('templateId') == 'big-image-left':
                big_image_left_slides.append((i, slide))
                logger.info(f"Found big-image-left slide at index {i}")
                
                # Log slide structure
                logger.info(f"  Slide {i} structure:")
                logger.info(f"    templateId: {slide.get('templateId')}")
                logger.info(f"    slideId: {slide.get('slideId')}")
                logger.info(f"    props keys: {list(slide.get('props', {}).keys())}")
                logger.info(f"    metadata keys: {list(slide.get('metadata', {}).keys()) if slide.get('metadata') else 'None'}")
                
                # Log text content
                props = slide.get('props', {})
                logger.info(f"    title: '{props.get('title', 'NOT SET')}'")
                logger.info(f"    subtitle: '{props.get('subtitle', 'NOT SET')}'")
                
                # Log image info without base64 data
                image_path = props.get('imagePath', '')
                if image_path:
                    if image_path.startswith('data:'):
                        logger.info(f"    imagePath: [BASE64 DATA URL - {len(image_path)} characters]")
                    else:
                        logger.info(f"    imagePath: {image_path}")
                else:
                    logger.info(f"    imagePath: NOT SET")
                
                # Log positioning data
                metadata = slide.get('metadata', {})
                element_positions = metadata.get('elementPositions', {})
                logger.info(f"    elementPositions exists: {bool(element_positions)}")
                if element_positions:
                    logger.info(f"    elementPositions keys: {list(element_positions.keys())}")
                    
                    # Check for title and subtitle positions
                    slide_id = slide.get('slideId', 'unknown')
                    title_id = f'draggable-{slide_id}-0'
                    subtitle_id = f'draggable-{slide_id}-1'
                    
                    title_pos = element_positions.get(title_id)
                    subtitle_pos = element_positions.get(subtitle_id)
                    
                    logger.info(f"    title element ID: {title_id}")
                    logger.info(f"    title position: {title_pos}")
                    logger.info(f"    subtitle element ID: {subtitle_id}")
                    logger.info(f"    subtitle position: {subtitle_pos}")
        
        logger.info(f"Total big-image-left slides found: {len(big_image_left_slides)}")
        logger.info(f"=== END SLIDE DATA ANALYSIS ===")

        # Prepare template context
        context_for_jinja = {
            'details': slide_deck_data
        }

        unique_output_filename = f"slide_deck_{project_id}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Generate PDF using the new dynamic height slide deck generation
        from app.services.pdf_generator import generate_slide_deck_pdf_with_dynamic_height
        
        pdf_path = await generate_slide_deck_pdf_with_dynamic_height(
            slides_data=slide_deck_data['slides'],
            theme=theme,
            output_filename=unique_output_filename,
            use_cache=True
        )
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_pdf_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating slide deck PDF for project {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate slide deck PDF: {str(e)[:200]}")


@app.post("/api/custom/pdf/debug/slides", response_class=JSONResponse)
async def debug_slide_generation(
    slides_data: List[Dict[str, Any]],
    theme: Optional[str] = Query("light-modern"),
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Debug endpoint to test individual slide generation and identify problematic slides."""
    try:
        from app.services.pdf_generator import test_all_slides_individually
        
        logger.info(f"Debug slide generation: Testing {len(slides_data)} slides with theme: {theme}")
        
        # Test all slides individually
        summary = await test_all_slides_individually(slides_data, theme)
        
        return JSONResponse(content=summary)
        
    except Exception as e:
        logger.error(f"Error in debug slide generation: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Debug failed: {str(e)[:200]}")


@app.get("/api/custom/pdf/{project_id}/", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf_no_slug(
    project_id: int,
    # all other parameters as in the main function
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    # Just call the main function with a default slug
    return await download_project_instance_pdf(
        project_id=project_id,
        document_name_slug="-",  # or any default value
        parentProjectName=parentProjectName,
        lessonNumber=lessonNumber,
        knowledgeCheck=knowledgeCheck,
        contentAvailability=contentAvailability,
        informationSource=informationSource,
        time=time,
        estCompletionTime=estCompletionTime,
        qualityTier=qualityTier,
        onyx_user_id=onyx_user_id,
        pool=pool,
    )    


@app.get("/api/custom/pdf/{project_id}/{document_name_slug}", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_project_instance_pdf(
    project_id: int,
    document_name_slug: str,
    parentProjectName: Optional[str] = Query(None),
    lessonNumber: Optional[int] = Query(None),
    knowledgeCheck: Optional[str] = Query(None),
    contentAvailability: Optional[str] = Query(None),
    informationSource: Optional[str] = Query(None),
    time: Optional[str] = Query(None),
    estCompletionTime: Optional[str] = Query(None),
    qualityTier: Optional[str] = Query(None),
    quiz: Optional[str] = Query(None),
    onePager: Optional[str] = Query(None),
    videoPresentation: Optional[str] = Query(None),
    lessonPresentation: Optional[str] = Query(None),
    templateType: Optional[str] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    print("OPTIONAL DATA:", parentProjectName, lessonNumber)
    try:
        async with pool.acquire() as conn:
            target_row_dict = await conn.fetchrow(
                """
                SELECT p.project_name, p.microproduct_name, p.microproduct_content,
                       p.lesson_plan_data, dt.component_name as design_component_name
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2;
                """,
                project_id, onyx_user_id
            )
        if not target_row_dict:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found for user.")

        mp_name_for_pdf_context = target_row_dict.get('microproduct_name') or target_row_dict.get('project_name')
        user_friendly_pdf_filename = f"{create_slug(mp_name_for_pdf_context)}_{uuid.uuid4().hex[:8]}.pdf"

        content_json = target_row_dict.get('microproduct_content')
        component_name = target_row_dict.get("design_component_name")
        lesson_plan_data = target_row_dict.get("lesson_plan_data")
        data_for_template_render: Optional[Dict[str, Any]] = None
        pdf_template_file: str
        
        # Debug logging for LessonPlan data
        if component_name == COMPONENT_NAME_LESSON_PLAN:
            logger.info(f"PDF Gen (Proj {project_id}): Raw lesson_plan_data from DB: {lesson_plan_data}")
            if lesson_plan_data:
                logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data type: {type(lesson_plan_data)}")
                if isinstance(lesson_plan_data, dict):
                    logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data keys: {list(lesson_plan_data.keys())}")
                elif isinstance(lesson_plan_data, str):
                    logger.info(f"PDF Gen (Proj {project_id}): lesson_plan_data is string, length: {len(lesson_plan_data)}")
            else:
                logger.warning(f"PDF Gen (Proj {project_id}): No lesson_plan_data found in target_row_dict")

        detected_lang_for_pdf = 'ru'  # Default language
        if isinstance(content_json, dict) and content_json.get('detectedLanguage'):
            detected_lang_for_pdf = content_json.get('detectedLanguage')
        elif mp_name_for_pdf_context: # Fallback if not in content_json
            detected_lang_for_pdf = detect_language(mp_name_for_pdf_context)
        
        # Get the locale strings for the detected language, defaulting to 'en' if not found
        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

        logger.info(f"Project {project_id} PDF Gen: Raw content_json from DB (type: {type(content_json)}). First 1000 chars: {str(content_json)[:1000]}")

        if component_name == COMPONENT_NAME_PDF_LESSON:
            pdf_template_file = "pdf_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                logger.info(f"Project {project_id} PDF Gen (PDF LESSON): Using raw content_json directly for template.")
                data_for_template_render = json.loads(json.dumps(content_json)) 
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model_for_fallback_lang = PdfLessonDetails(**content_json)
                        if parsed_model_for_fallback_lang and parsed_model_for_fallback_lang.detectedLanguage:
                            detected_lang_for_pdf = parsed_model_for_fallback_lang.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                logger.warning(f"Project {project_id} PDF Gen (PDF LESSON): content_json is not a valid dict or is None. Using fallback structure.")
                data_for_template_render = {
                    "lessonTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf}
        elif component_name == COMPONENT_NAME_TEXT_PRESENTATION:
            pdf_template_file = "text_presentation_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
            else:
                data_for_template_render = {
                    "textTitle": f"Content Unavailable/Invalid: {mp_name_for_pdf_context}",
                    "contentBlocks": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_TRAINING_PLAN:
            # Check if templateType is specified to use course outline template
            if templateType == "course-outline":
                pdf_template_file = "course_outline_pdf_template.html"
            else:
                pdf_template_file = "training_plan_pdf_template.html"
            temp_dumped_dict = None
            if content_json and isinstance(content_json, dict):
                try:
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json type: {type(content_json)}")
                    logger.info(f"PDF Gen (Proj {project_id}): Raw content_json keys: {list(content_json.keys()) if isinstance(content_json, dict) else 'Not a dict'}")
                    if 'sections' in content_json:
                        logger.info(f"PDF Gen (Proj {project_id}): sections type: {type(content_json['sections'])}, length: {len(content_json['sections']) if isinstance(content_json['sections'], list) else 'Not a list'}")
                    
                    # Round hours to integers before parsing to prevent float validation errors
                    content_json = round_hours_in_content(content_json)
                    
                    parsed_model = TrainingPlanDetails(**content_json)
                    logger.info(f"PDF Gen (Proj {project_id}): Parsed model sections length: {len(parsed_model.sections)}")
                    
                    if parsed_model.detectedLanguage: 
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    
                    # Calculate completion time for each section
                    for section in parsed_model.sections:
                        total_completion_minutes = 0
                        for lesson in section.lessons:
                            if lesson.completionTime:
                                time_str = str(lesson.completionTime).strip()
                                if time_str and time_str != '':
                                    # Extract numeric part using regex to handle all language units (m, м, хв)
                                    import re
                                    numbers = re.findall(r'\d+', time_str)
                                    if numbers:
                                        try:
                                            # If it contains 'h' (hour indicator), convert to minutes
                                            if 'h' in time_str.lower():
                                                total_completion_minutes += int(numbers[0]) * 60
                                            else:
                                                # For minutes (m, м, хв), just use the number
                                                total_completion_minutes += int(numbers[0])
                                        except (ValueError, IndexError):
                                            total_completion_minutes += 5  # Fallback to 5 minutes
                        
                        # Add the calculated completion time to the section
                        section.totalCompletionTime = total_completion_minutes
                    
                    temp_dumped_dict = parsed_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"PDF Gen (Proj {project_id}): Dumped dict sections length: {len(temp_dumped_dict.get('sections', []))}")
                    data_for_template_render = json.loads(json.dumps(temp_dumped_dict))
                    logger.info(f"PDF Gen (Proj {project_id}): Final data sections length: {len(data_for_template_render.get('sections', []))}")
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for TrainingPlan (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
            if data_for_template_render is None:
                 logger.warning(f"Project {project_id} PDF Gen (TRAINING PLAN): data_for_template_render is None. Using fallback.")
                 data_for_template_render = {"mainTitle": f"Content Error: {mp_name_for_pdf_context}", "sections": [], "detectedLanguage": detected_lang_for_pdf}
            
            current_lang_cfg_main = LANG_CONFIG.get(detected_lang_for_pdf, LANG_CONFIG['ru']) # Using main LANG_CONFIG for units
            data_for_template_render['time_unit_singular'] = current_lang_cfg_main.get('TIME_UNIT_SINGULAR', 'h')
            data_for_template_render['time_unit_decimal_plural'] = current_lang_cfg_main.get('TIME_UNIT_DECIMAL_PLURAL', 'h')
            data_for_template_render['time_unit_general_plural'] = current_lang_cfg_main.get('TIME_UNIT_GENERAL_PLURAL', 'h')
            
            # Extract sources from the training plan data
            sources = []
            if 'sections' in data_for_template_render:
                sources_set = set()
                for section in data_for_template_render['sections']:
                    if 'lessons' in section:
                        for lesson in section['lessons']:
                            if 'source' in lesson and lesson['source']:
                                source_str = lesson['source']
                                # Extract connector name from source string like "Connector Search: notion"
                                import re
                                match = re.match(r'Connector Search:\s*(.+)', source_str, re.IGNORECASE)
                                if match:
                                    sources_set.add(match.group(1))
                                elif 'PDF' in source_str or 'Document' in source_str:
                                    sources_set.add('PDF Document')
                                elif 'text' in source_str.lower() or 'Text' in source_str:
                                    sources_set.add('Create from scratch')
                                else:
                                    sources_set.add(source_str)
                
                # Convert to list of dictionaries with type information
                for source_name in sources_set:
                    if source_name in ['PDF Document', 'Create from scratch']:
                        sources.append({'name': source_name, 'type': 'file'})
                    else:
                        sources.append({'name': source_name, 'type': 'connector'})
            
            data_for_template_render['sources'] = sources
            
            # Add content status for each lesson to show blue icons when content exists
            if 'sections' in data_for_template_render:
                # Get the main title for content matching
                main_title = data_for_template_render.get('mainTitle', '')
                
                for section in data_for_template_render['sections']:
                    if 'lessons' in section:
                        for lesson in section['lessons']:
                            # Initialize content status for each lesson
                            lesson['contentStatus'] = {
                                'presentation': {'exists': False},
                                'onePager': {'exists': False},
                                'quiz': {'exists': False},
                                'videoLesson': {'exists': False}
                            }
                            
                            # Check if content exists for this lesson by querying the database
                            try:
                                # Build expected project name pattern
                                expected_project_name = f"{main_title}: {lesson.get('title', '')}"
                                
                                # Debug: Print the expected project name
                                print(f"Checking content for lesson: {lesson.get('title', '')}")
                                print(f"Expected project name: {expected_project_name}")
                                
                                # Query for existing projects that match this lesson with more flexible matching
                                content_check_query = """
                                SELECT id, project_name, design_microproduct_type, microproduct_type 
                                FROM projects 
                                WHERE onyx_user_id = $1 
                                AND (
                                    project_name = $2 OR 
                                    project_name = $3 OR 
                                    project_name = $4 OR
                                    project_name = $5 OR
                                    project_name LIKE $6 OR
                                    project_name LIKE $7 OR
                                    project_name LIKE $8 OR
                                    project_name LIKE $9
                                )
                                """
                                
                                # Check for different naming patterns
                                legacy_quiz_pattern = f"Quiz - {expected_project_name}"
                                legacy_text_presentation_pattern = f"Text Presentation - {expected_project_name}"
                                legacy_video_pattern = f"Video Lesson - {expected_project_name}"
                                
                                # Add LIKE patterns for more flexible matching
                                like_presentation = f"%{lesson.get('title', '')}%"
                                like_quiz = f"%Quiz%{lesson.get('title', '')}%"
                                like_text = f"%Text Presentation%{lesson.get('title', '')}%"
                                like_video = f"%Video Lesson%{lesson.get('title', '')}%"
                                
                                content_results = await pool.fetch(
                                    content_check_query,
                                    onyx_user_id,
                                    expected_project_name,
                                    legacy_quiz_pattern,
                                    legacy_text_presentation_pattern,
                                    legacy_video_pattern,
                                    like_presentation,
                                    like_quiz,
                                    like_text,
                                    like_video
                                )
                                
                                print(f"Found {len(content_results)} matching projects for lesson: {lesson.get('title', '')}")
                                
                                # Check each matching project to see what type of content it is
                                for project in content_results:
                                    project_name = project.get('project_name', '')
                                    microproduct_type = project.get('design_microproduct_type') or project.get('microproduct_type')
                                    
                                    print(f"Project: {project_name}, Type: {microproduct_type}")
                                    
                                    # Map microproduct types to our content status
                                    if microproduct_type in ['Slide Deck', 'Lesson Presentation']:
                                        lesson['contentStatus']['presentation']['exists'] = True
                                        print(f"Set presentation to True for lesson: {lesson.get('title', '')}")
                                    elif microproduct_type == 'Text Presentation':
                                        lesson['contentStatus']['onePager']['exists'] = True
                                        print(f"Set onePager to True for lesson: {lesson.get('title', '')}")
                                    elif microproduct_type == 'Quiz':
                                        lesson['contentStatus']['quiz']['exists'] = True
                                        print(f"Set quiz to True for lesson: {lesson.get('title', '')}")
                                    elif microproduct_type in ['Video Lesson', 'Video Lesson Presentation']:
                                        lesson['contentStatus']['videoLesson']['exists'] = True
                                        print(f"Set videoLesson to True for lesson: {lesson.get('title', '')}")
                                        
                            except Exception as e:
                                # If there's an error checking content, keep defaults (gray icons)
                                print(f"Error checking content status for lesson {lesson.get('title', '')}: {e}")
                                pass
                            
                            # Alternative approach: Check if we can find any projects with similar names
                            try:
                                # Simple fallback query to find any projects that might match
                                fallback_query = """
                                SELECT project_name, design_microproduct_type, microproduct_type 
                                FROM projects 
                                WHERE onyx_user_id = $1 
                                AND project_name LIKE $2
                                LIMIT 10
                                """
                                
                                lesson_title = lesson.get('title', '')
                                fallback_results = await pool.fetch(
                                    fallback_query,
                                    onyx_user_id,
                                    f"%{lesson_title}%"
                                )
                                
                                if fallback_results:
                                    print(f"Fallback found {len(fallback_results)} projects for lesson: {lesson_title}")
                                    for project in fallback_results:
                                        project_name = project.get('project_name', '')
                                        microproduct_type = project.get('design_microproduct_type') or project.get('microproduct_type')
                                        print(f"Fallback Project: {project_name}, Type: {microproduct_type}")
                                        
                                        # Map microproduct types to our content status
                                        if microproduct_type in ['Slide Deck', 'Lesson Presentation']:
                                            lesson['contentStatus']['presentation']['exists'] = True
                                        elif microproduct_type == 'Text Presentation':
                                            lesson['contentStatus']['onePager']['exists'] = True
                                        elif microproduct_type == 'Quiz':
                                            lesson['contentStatus']['quiz']['exists'] = True
                                        elif microproduct_type in ['Video Lesson', 'Video Lesson Presentation']:
                                            lesson['contentStatus']['videoLesson']['exists'] = True
                                            
                            except Exception as fallback_error:
                                print(f"Fallback query also failed for lesson {lesson.get('title', '')}: {fallback_error}")
                                pass
        elif component_name == COMPONENT_NAME_VIDEO_LESSON: # Updated logic for Video Lesson
            pdf_template_file = "video_lesson_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                data_for_template_render = json.loads(json.dumps(content_json))
                if not data_for_template_render.get('detectedLanguage'):
                    try:
                        parsed_model = VideoLessonData(**content_json)
                        if parsed_model.detectedLanguage:
                            detected_lang_for_pdf = parsed_model.detectedLanguage
                            # Update locale strings if language detection changed
                            current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    except Exception: pass 
                    data_for_template_render['detectedLanguage'] = detected_lang_for_pdf
                else: # If language IS in content_json, ensure locale strings match
                    detected_lang_for_pdf = data_for_template_render.get('detectedLanguage', detected_lang_for_pdf)
                    current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])

            else:
                data_for_template_render = {
                    "mainPresentationTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "slides": [], "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_QUIZ: # Quiz handling
            pdf_template_file = "quiz_pdf_template.html"
            if content_json and isinstance(content_json, dict):
                try:
                    parsed_model = QuizData(**content_json)
                    if parsed_model.detectedLanguage:
                        detected_lang_for_pdf = parsed_model.detectedLanguage
                        # Update locale strings if language detection changed
                        current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS.get(detected_lang_for_pdf, VIDEO_SCRIPT_LANG_STRINGS['en'])
                    data_for_template_render = parsed_model.model_dump(mode='json', exclude_none=True)
                except Exception as e_parse_dump:
                    logger.error(f"Pydantic parsing/dumping failed for Quiz (Proj {project_id}): {e_parse_dump}", exc_info=not IS_PRODUCTION)
                    data_for_template_render = {
                        "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                        "questions": [],
                        "detectedLanguage": detected_lang_for_pdf
                    }
            else:
                data_for_template_render = {
                    "quizTitle": f"Content Error: {mp_name_for_pdf_context}",
                    "questions": [],
                    "detectedLanguage": detected_lang_for_pdf
                }
        elif component_name == COMPONENT_NAME_LESSON_PLAN: # Lesson Plan handling
            pdf_template_file = "lesson_plan_pdf_template.html"
            # Get lesson plan data from the separate lesson_plan_data column
            lesson_plan_data = target_row_dict.get('lesson_plan_data')
            
            # Handle case where lesson_plan_data might be a JSON string
            if lesson_plan_data and isinstance(lesson_plan_data, str):
                try:
                    lesson_plan_data = json.loads(lesson_plan_data)
                    logger.info(f"PDF Gen (Proj {project_id}): Parsed lesson_plan_data from JSON string")
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"PDF Gen (Proj {project_id}): Failed to parse lesson_plan_data JSON: {e}")
                    lesson_plan_data = None
            
            if lesson_plan_data and isinstance(lesson_plan_data, dict):
                data_for_template_render = {
                    "lessonTitle": lesson_plan_data.get('lessonTitle', mp_name_for_pdf_context),
                    "shortDescription": lesson_plan_data.get('shortDescription', ''),
                    "lessonObjectives": lesson_plan_data.get('lessonObjectives', []),
                    "materials": lesson_plan_data.get('materials', []),
                    "contentDevelopmentSpecifications": lesson_plan_data.get('contentDevelopmentSpecifications', []),
                    "suggestedPrompts": lesson_plan_data.get('suggestedPrompts', []),
                    "detectedLanguage": detected_lang_for_pdf
                }
                logger.info(f"PDF Gen (Proj {project_id}): LessonPlan data loaded successfully with {len(lesson_plan_data.get('lessonObjectives', []))} objectives")
            else:
                data_for_template_render = {
                    "lessonTitle": mp_name_for_pdf_context,
                    "shortDescription": "Lesson plan content not available",
                    "lessonObjectives": [],
                    "contentDevelopmentSpecifications": [],
                    "materials": [],
                    "suggestedPrompts": [],
                    "detectedLanguage": detected_lang_for_pdf
                }
                logger.warning(f"PDF Gen (Proj {project_id}): No lesson_plan_data found in database or failed to parse")
        else:
            logger.warning(f"PDF: Unknown component_name '{component_name}' for project {project_id}. Defaulting to simple PDF Lesson structure.")
            pdf_template_file = "pdf_lesson_pdf_template.html" # Or a generic template
            data_for_template_render = {
                "lessonTitle": f"Unknown Content Type: {mp_name_for_pdf_context}",
                "contentBlocks": [{"type":"paragraph", "text":"The content type of this project is not configured for PDF export."}],
                "detectedLanguage": detected_lang_for_pdf
            }

        if not isinstance(data_for_template_render, dict):
             logger.critical(f"Project {project_id} PDF Gen: data_for_template_render is NOT A DICT ({type(data_for_template_render)}) before final context prep.")
             data_for_template_render = {"lessonTitle": "Critical Data Preparation Error", "contentBlocks": [], "detectedLanguage": "en"}
             # Ensure locale is set for critical error case
             current_pdf_locale_strings = VIDEO_SCRIPT_LANG_STRINGS['en']


        if isinstance(data_for_template_render, dict):
            logger.info(f"Project {project_id} PDF Gen: Starting deep inspection of data_for_template_render (to be passed as 'details' in template context)...")
            inspect_list_items_recursively(data_for_template_render.get('contentBlocks', []), "data_for_template_render.contentBlocks")

        unique_output_filename = f"{project_id}_{document_name_slug}_{uuid.uuid4().hex[:12]}.pdf"
        
        # Pass the locale strings to the template context
        static_images_abs_path = os.path.abspath(STATIC_DESIGN_IMAGES_DIR) + '/'
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Static images path: {static_images_abs_path}")
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Path exists: {os.path.exists(static_images_abs_path.rstrip('/'))}")
        
        context_for_jinja = {
            'details': data_for_template_render, 
            'locale': current_pdf_locale_strings,
            'parentProjectName': parentProjectName,
            'lessonNumber': lessonNumber,
            'pdf_context': {
                'static_images_path': static_images_abs_path
            }
        }
        
        # 🔍 PDF CONTEXT LOGGING: What we're passing to the template
        logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Full context keys: {list(context_for_jinja.keys())}")
        
        # Log image blocks without transforming them (let PDF generator handle the transformation)
        if 'details' in context_for_jinja and isinstance(context_for_jinja['details'], dict) and 'contentBlocks' in context_for_jinja['details']:
            content_blocks = context_for_jinja['details']['contentBlocks']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Processing {len(content_blocks)} content blocks")
            
            image_blocks = [block for block in content_blocks if block.get('type') == 'image']
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Found {len(image_blocks)} image blocks")
            for img_block in image_blocks:
                original_src = img_block.get('src', '')
                logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Image block src (before PDF generation): {original_src}")
                if original_src.startswith('/static_design_images/'):
                    filename = original_src.replace('/static_design_images/', '')
                    full_path = static_images_abs_path + filename
                    logger.info(f"📄 [PDF CONTEXT] Project {project_id} - Expected file path: {full_path} (exists: {os.path.exists(full_path)})")
        else:
            logger.info(f"📄 [PDF CONTEXT] Project {project_id} - No contentBlocks found in details")
        
        # Add column visibility settings for Training Plan PDFs
        if component_name == COMPONENT_NAME_TRAINING_PLAN:
            column_visibility = {
                'knowledgeCheck': knowledgeCheck == '1' if knowledgeCheck else True,
                'contentAvailability': contentAvailability == '1' if contentAvailability else True,
                'informationSource': informationSource == '1' if informationSource else True,
                'estCreationTime': time == '1' if time else True,
                'estCompletionTime': estCompletionTime == '1' if estCompletionTime else True,
                'qualityTier': qualityTier == '1' if qualityTier else False,  # Hidden by default
                'quiz': quiz == '1' if quiz else False,
                'onePager': onePager == '1' if onePager else False,
                'videoPresentation': videoPresentation == '1' if videoPresentation else False,
                'lessonPresentation': lessonPresentation == '1' if lessonPresentation else False,
            }
            context_for_jinja['columnVisibility'] = column_visibility

        logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']: {type(context_for_jinja.get('details'))}")
        if isinstance(context_for_jinja.get('details'), dict) and isinstance(context_for_jinja['details'].get('details'), dict):
            final_cb_source = context_for_jinja['details']['details']
            final_cb_type = type(final_cb_source.get('contentBlocks'))
            logger.info(f"Project {project_id} PDF Gen: Type of context_for_jinja['details']['details']['contentBlocks']: {final_cb_type}")
            if isinstance(final_cb_source.get('contentBlocks'), list):
                 for block_idx, block_item_final_check in enumerate(final_cb_source.get('contentBlocks', [])):
                    if isinstance(block_item_final_check, dict) and block_item_final_check.get('type') in ('bullet_list', 'numbered_list'):
                        items_final_check_type = type(block_item_final_check.get('items'))
                        if not isinstance(block_item_final_check.get('items'), list):
                            logger.error(f"Project {project_id} PDF Gen: CRITICAL - 'items' in block_item_final_check for block #{block_idx} is STILL NOT A LIST (type: {items_final_check_type}) just before Jinja render.")
            elif final_cb_type is not None: # if it's not None and not a list
                logger.error(f"Project {project_id} PDF Gen: CRITICAL - context_for_jinja['details']['details']['contentBlocks'] is NOT A LIST (type: {final_cb_type}) just before Jinja render.")

        pdf_path = await generate_pdf_from_html_template(pdf_template_file, context_for_jinja, unique_output_filename)
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        return FileResponse(path=pdf_path, filename=user_friendly_pdf_filename, media_type='application/pdf', headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"})
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in PDF endpoint for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during PDF generation." if IS_PRODUCTION else f"Error during PDF generation: {str(e)[:200]}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.post("/api/custom/projects/delete-multiple", status_code=status.HTTP_200_OK)
async def delete_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided."})

    project_ids_to_trash = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # If scope is 'all', find all associated lesson projects for any Training Plans
            if delete_request.scope == 'all':
                for project_id in delete_request.project_ids:
                    # Fetch outline project name
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM projects WHERE id=$1 AND onyx_user_id=$2",
                        project_id, onyx_user_id
                    )
                    if not row:
                        continue
                    outline_name: str = row["project_name"]
                    # Treat both 'Training Plan' and 'Course Outline' as outline types
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        continue

                    # Select IDs of all projects whose name equals outline_name OR starts with outline_name + ': '
                    pattern = outline_name + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM projects WHERE onyx_user_id=$1 AND (project_name = $2 OR project_name LIKE $3)",
                        onyx_user_id, outline_name, pattern
                    )
                    for lr in lesson_rows:
                        project_ids_to_trash.add(lr["id"])

            if not project_ids_to_trash:
                 return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            # First, fetch all the data we need to move to trash
            projects_to_trash = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(project_ids_to_trash), onyx_user_id)

            if not projects_to_trash:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to move to trash."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_trash:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into trashed_projects with safe values
                    await conn.execute("""
                        INSERT INTO trashed_projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from projects table
                result_status = await conn.execute(
                    "DELETE FROM projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(project_ids_to_trash), onyx_user_id
                )
        
        deleted_count_match = re.search(r"DELETE\s+(\d+)", result_status)
        deleted_count = int(deleted_count_match.group(1)) if deleted_count_match else 0
        
        logger.info(f"User {onyx_user_id} moved IDs {list(project_ids_to_trash)} to trash. Count: {deleted_count}.")
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully moved {deleted_count} project(s) to trash."})

    except Exception as e:
        logger.error(f"Error moving projects to trash for user {onyx_user_id}, IDs {delete_request.project_ids}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while sending projects to trash." if IS_PRODUCTION else f"Database error during trash operation: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Analytics Endpoints ---

@app.get("/api/custom/analytics/dashboard", response_model=Dict[str, Any])
async def get_analytics_dashboard(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get comprehensive analytics dashboard data"""
    import json
    
    print(f"=== DASHBOARD DEBUG: Incoming parameters ===")
    print(f"date_from: {date_from}")
    print(f"date_to: {date_to}")
    print(f"=== END PARAMETERS ===")

    try:
        # DEBUG: Print the latest 10 rows from request_analytics
        async with pool.acquire() as conn:
            debug_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, created_at FROM request_analytics ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: Latest 10 rows from request_analytics ===")
            for row in debug_rows:
                print(dict(row))
            print("=== END DEBUG ===")
            
            # DEBUG: Check specifically for AI parser records
            ai_parser_rows = await conn.fetch(
                "SELECT id, endpoint, method, status_code, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name, created_at FROM request_analytics WHERE is_ai_parser_request = true ORDER BY created_at DESC LIMIT 10"
            )
            print("=== DEBUG: AI Parser records from request_analytics ===")
            for row in ai_parser_rows:
                print(dict(row))
            print(f"Total AI parser records found: {len(ai_parser_rows)}")
            print("=== END AI PARSER DEBUG ===")
            
            # DEBUG: Check if the columns exist and have any data
            column_check = await conn.fetch(
                "SELECT column_name, data_type, is_nullable, column_default FROM information_schema.columns WHERE table_name = 'request_analytics' ORDER BY ordinal_position"
            )
            print("=== DEBUG: All request_analytics columns check ===")
            for row in column_check:
                print(dict(row))
            print("=== END COLUMN CHECK ===")
            
            # DEBUG: Check for any records with non-null ai_parser fields
            any_ai_parser_data = await conn.fetch(
                "SELECT id, endpoint, is_ai_parser_request, ai_parser_tokens, ai_parser_model, ai_parser_project_name FROM request_analytics WHERE is_ai_parser_request IS NOT NULL OR ai_parser_tokens IS NOT NULL OR ai_parser_model IS NOT NULL OR ai_parser_project_name IS NOT NULL ORDER BY created_at DESC LIMIT 5"
            )
            print("=== DEBUG: Any AI parser data ===")
            for row in any_ai_parser_data:
                print(dict(row))
            print(f"Total records with any AI parser data: {len(any_ai_parser_data)}")
            print("=== END ANY AI PARSER DATA ===")
    except Exception as e:
        print(f"DEBUG ERROR: Could not fetch request_analytics: {e}")

    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        print(f"=== DASHBOARD DEBUG: Filter and params ===")
        print(f"where_clause: {where_clause}")
        print(f"params: {params}")
        print(f"=== END FILTER ===")

        async with pool.acquire() as conn:
            # Overall statistics
            stats_query = f"""
                SELECT 
                    COUNT(*) as total_requests,
                    COUNT(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 END) as successful_requests,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as failed_requests,
                    COUNT(CASE WHEN error_message IS NOT NULL THEN 1 END) as error_requests,
                    AVG(response_time_ms) as avg_response_time,
                    MAX(response_time_ms) as max_response_time,
                    MIN(response_time_ms) as min_response_time,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data_transferred,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT endpoint) as unique_endpoints,
                    COUNT(CASE WHEN is_ai_parser_request THEN 1 END) as ai_parser_requests,
                    AVG(ai_parser_tokens) as avg_ai_parser_tokens,
                    MAX(ai_parser_tokens) as max_ai_parser_tokens,
                    MIN(ai_parser_tokens) as min_ai_parser_tokens,
                    SUM(ai_parser_tokens) as total_ai_parser_tokens
                FROM request_analytics
                {where_clause}
            """
            print(f"=== DASHBOARD DEBUG: Stats query ===")
            print(f"Query: {stats_query}")
            print(f"Params: {params}")
            stats_row = await conn.fetchrow(stats_query, *params)
            print(f"Stats result: {dict(stats_row) if stats_row else 'None'}")
            
            # Debug AI parser specific data
            if stats_row:
                print(f"=== AI PARSER DEBUG ===")
                print(f"ai_parser_requests: {stats_row['ai_parser_requests']}")
                print(f"avg_ai_parser_tokens: {stats_row['avg_ai_parser_tokens']}")
                print(f"max_ai_parser_tokens: {stats_row['max_ai_parser_tokens']}")
                print(f"min_ai_parser_tokens: {stats_row['min_ai_parser_tokens']}")
                print(f"total_ai_parser_tokens: {stats_row['total_ai_parser_tokens']}")
                print(f"=== END AI PARSER DEBUG ===")
            
            print(f"=== END STATS ===")
            
            # Status code distribution
            status_query = f"""
                SELECT 
                    status_code,
                    COUNT(*) as count,
                    AVG(response_time_ms) as avg_time
                FROM request_analytics
                {where_clause}
                GROUP BY status_code
                ORDER BY count DESC
            """
            print(f"=== DASHBOARD DEBUG: Status query ===")
            print(f"Query: {status_query}")
            print(f"Params: {params}")
            status_rows = await conn.fetch(status_query, *params)
            print(f"Status rows count: {len(status_rows)}")
            print(f"Status results: {[dict(row) for row in status_rows]}")
            print(f"=== END STATUS ===")
            
            # Top endpoints by request count
            endpoints_query = f"""
                SELECT 
                    endpoint,
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    SUM(COALESCE(request_size_bytes, 0) + COALESCE(response_size_bytes, 0)) as total_data
                FROM request_analytics
                {where_clause}
                GROUP BY endpoint, method
                ORDER BY request_count DESC
                LIMIT 20
            """
            endpoints_rows = await conn.fetch(endpoints_query, *params)
            
            # Top users by request count
            users_query = f"""
                SELECT 
                    user_id,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count,
                    MAX(created_at) as last_request
                FROM request_analytics
                {where_clause}
                {"AND user_id IS NOT NULL" if where_clause else "WHERE user_id IS NOT NULL"}
                GROUP BY user_id
                ORDER BY request_count DESC
                LIMIT 20
            """
            users_rows = await conn.fetch(users_query, *params)
            
            # Hourly distribution
            hourly_query = f"""
                SELECT 
                    EXTRACT(HOUR FROM created_at) as hour,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time
                FROM request_analytics
                {where_clause}
                GROUP BY EXTRACT(HOUR FROM created_at)
                ORDER BY hour
            """
            hourly_rows = await conn.fetch(hourly_query, *params)
            
            # Daily distribution
            daily_query = f"""
                SELECT 
                    DATE(created_at) as date,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY DATE(created_at)
                ORDER BY date DESC
                LIMIT 30
            """
            daily_rows = await conn.fetch(daily_query, *params)
            
            # Method distribution
            method_query = f"""
                SELECT 
                    method,
                    COUNT(*) as request_count,
                    AVG(response_time_ms) as avg_response_time,
                    COUNT(CASE WHEN status_code >= 400 THEN 1 END) as error_count
                FROM request_analytics
                {where_clause}
                GROUP BY method
                ORDER BY request_count DESC
            """
            method_rows = await conn.fetch(method_query, *params)
            
            # Recent errors
            errors_query = f"""
                SELECT 
                    id,
                    endpoint,
                    method,
                    status_code,
                    response_time_ms,
                    error_message,
                    user_id,
                    CASE WHEN position('@' in user_id) > 0 THEN user_id ELSE NULL END AS user_email,
                    (status_code IN (408, 504) OR (error_message ILIKE '%timeout%' OR error_message ILIKE '%timed out%' OR error_message ILIKE '%Timeout%')) AS is_timeout,
                    created_at
                FROM request_analytics
                {where_clause}
                {"AND (error_message IS NOT NULL OR status_code >= 400)" if where_clause else "WHERE error_message IS NOT NULL OR status_code >= 400"}
                ORDER BY created_at DESC
                LIMIT 50
            """
            errors_rows = await conn.fetch(errors_query, *params)
            
            # Performance percentiles
            percentile_query = f"""
                SELECT 
                    percentile_cont(0.5) WITHIN GROUP (ORDER BY response_time_ms) as p50,
                    percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95,
                    percentile_cont(0.99) WITHIN GROUP (ORDER BY response_time_ms) as p99
                FROM request_analytics
                {where_clause}
            """
            percentile_row = await conn.fetchrow(percentile_query, *params)

        response_data = {
            "overview": {
                "total_requests": stats_row["total_requests"],
                "successful_requests": stats_row["successful_requests"],
                "failed_requests": stats_row["failed_requests"],
                "error_requests": stats_row["error_requests"],
                "success_rate": round((stats_row["successful_requests"] / stats_row["total_requests"]) * 100, 2) if stats_row["total_requests"] > 0 else 0,
                "avg_response_time": round(stats_row["avg_response_time"], 2) if stats_row["avg_response_time"] else 0,
                "max_response_time": stats_row["max_response_time"],
                "min_response_time": stats_row["min_response_time"],
                "total_data_transferred": stats_row["total_data_transferred"],
                "unique_users": stats_row["unique_users"],
                "unique_endpoints": stats_row["unique_endpoints"],
                "ai_parser_requests": stats_row["ai_parser_requests"] or 0,
                "avg_ai_parser_tokens": round(stats_row["avg_ai_parser_tokens"], 2) if stats_row["avg_ai_parser_tokens"] else 0,
                "max_ai_parser_tokens": stats_row["max_ai_parser_tokens"] or 0,
                "min_ai_parser_tokens": stats_row["min_ai_parser_tokens"] or 0,
                "total_ai_parser_tokens": stats_row["total_ai_parser_tokens"] or 0
            },
            "status_distribution": [{"status_code": row["status_code"], "count": row["count"], "avg_time": round(row["avg_time"], 2) if row["avg_time"] else 0} for row in status_rows],
            "top_endpoints": [{
                "endpoint": row["endpoint"],
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "error_rate": round((row["error_count"] / row["request_count"]) * 100, 2) if row["request_count"] > 0 else 0,
                "total_data": row["total_data"]
            } for row in endpoints_rows],
            "top_users": [{
                "user_id": row["user_id"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"],
                "last_request": row["last_request"].isoformat() if row["last_request"] else None
            } for row in users_rows],
            "hourly_distribution": [{"hour": int(row["hour"]), "request_count": row["request_count"], "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0} for row in hourly_rows],
            "daily_distribution": [{
                "date": row["date"].isoformat(),
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in daily_rows],
            "method_distribution": [{
                "method": row["method"],
                "request_count": row["request_count"],
                "avg_response_time": round(row["avg_response_time"], 2) if row["avg_response_time"] else 0,
                "error_count": row["error_count"]
            } for row in method_rows],
            "recent_errors": [{
                "id": row["id"],
                "endpoint": row["endpoint"],
                "method": row["method"],
                "status_code": row["status_code"],
                "response_time_ms": row["response_time_ms"],
                "error_message": row["error_message"],
                "user_id": row["user_id"],
                "user_email": row["user_email"],
                "is_timeout": row["is_timeout"],
                "created_at": row["created_at"].isoformat()
            } for row in errors_rows],
            "performance_percentiles": {
                "p50": round(percentile_row["p50"], 2) if percentile_row["p50"] else 0,
                "p95": round(percentile_row["p95"], 2) if percentile_row["p95"] else 0,
                "p99": round(percentile_row["p99"], 2) if percentile_row["p99"] else 0
            }
        }
        
        print(f"=== DASHBOARD DEBUG: Final response ===")
        print(f"Response overview: {response_data['overview']}")
        print(f"Status distribution count: {len(response_data['status_distribution'])}")
        print(f"Top endpoints count: {len(response_data['top_endpoints'])}")
        print(f"=== END FINAL RESPONSE ===")
        
        return response_data
    except Exception as e:
        logger.error(f"Error fetching analytics dashboard: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch analytics data")


@app.get("/api/custom/analytics/requests", response_model=List[RequestAnalytics])
async def get_analytics_requests(
    page: int = Query(1, ge=1, description="Page number"),
    limit: int = Query(50, ge=1, le=1000, description="Items per page"),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    user_id: Optional[str] = Query(None, description="Filter by user ID"),
    min_response_time: Optional[int] = Query(None, description="Minimum response time in ms"),
    max_response_time: Optional[int] = Query(None, description="Maximum response time in ms"),
    has_error: Optional[bool] = Query(None, description="Filter by error status"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get paginated request analytics with filters"""
    try:
        # Build WHERE clause
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if user_id:
            param_count += 1
            conditions.append(f"user_id = ${param_count}")
            params.append(user_id)
        
        if min_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms >= ${param_count}")
            params.append(min_response_time)
        
        if max_response_time is not None:
            param_count += 1
            conditions.append(f"response_time_ms <= ${param_count}")
            params.append(max_response_time)
        
        if has_error is not None:
            if has_error:
                conditions.append("(error_message IS NOT NULL OR status_code >= 400)")
            else:
                conditions.append("(error_message IS NULL AND status_code < 400)")
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""
        
        # Calculate offset
        offset = (page - 1) * limit
        
        # Build query
        query = f"""
            SELECT 
                id, endpoint, method, user_id, status_code, 
                response_time_ms, request_size_bytes, response_size_bytes,
                error_message, created_at
            FROM request_analytics
            {where_clause}
            ORDER BY created_at DESC
            LIMIT ${param_count + 1} OFFSET ${param_count + 2}
        """
        params.extend([limit, offset])
        
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, *params)
            
            return [RequestAnalytics(**dict(row)) for row in rows]
            
    except Exception as e:
        logger.error(f"Error fetching analytics requests: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch request data")


@app.get("/api/custom/analytics/export")
async def export_analytics_data(
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    endpoint: Optional[str] = Query(None, description="Filter by endpoint"),
    method: Optional[str] = Query(None, description="Filter by HTTP method"),
    status_code: Optional[int] = Query(None, description="Filter by status code"),
    format: str = Query("csv", description="Export format (csv or json)"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Export analytics data as CSV or JSON"""
    try:
        # Build comprehensive filter with proper datetime conversion including timezone
        conditions = []
        params = []
        param_count = 0
        
        if date_from:
            param_count += 1
            conditions.append(f"created_at >= ${param_count}")
            start_datetime = datetime.strptime(date_from, '%Y-%m-%d').replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc)
            params.append(start_datetime)
        
        if date_to:
            param_count += 1
            conditions.append(f"created_at <= ${param_count}")
            end_datetime = datetime.strptime(date_to, '%Y-%m-%d').replace(hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc)
            params.append(end_datetime)
        
        if endpoint:
            param_count += 1
            conditions.append(f"endpoint ILIKE ${param_count}")
            params.append(f"%{endpoint}%")
        
        if method:
            param_count += 1
            conditions.append(f"method = ${param_count}")
            params.append(method.upper())
        
        if status_code is not None:
            param_count += 1
            conditions.append(f"status_code = ${param_count}")
            params.append(status_code)
        
        where_clause = "WHERE " + " AND ".join(conditions) if conditions else ""

        async with pool.acquire() as conn:
            query = f"""
                SELECT 
                    id, endpoint, method, user_id, status_code, 
                    response_time_ms, request_size_bytes, response_size_bytes,
                    error_message, created_at
                FROM request_analytics
                {where_clause}
                ORDER BY created_at DESC
            """
            rows = await conn.fetch(query, *params)
            
            if format.lower() == "csv":
                import csv
                import io
                
                output = io.StringIO()
                writer = csv.writer(output)
                
                # Write header
                writer.writerow([
                    "ID", "Endpoint", "Method", "User ID", "Status Code",
                    "Response Time (ms)", "Request Size (bytes)", "Response Size (bytes)",
                    "Error Message", "Created At"
                ])
                
                # Write data
                for row in rows:
                    writer.writerow([
                        row["id"], row["endpoint"], row["method"], row["user_id"],
                        row["status_code"], row["response_time_ms"],
                        row["request_size_bytes"], row["response_size_bytes"],
                        row["error_message"], row["created_at"].isoformat()
                    ])
                
                return StreamingResponse(
                    io.BytesIO(output.getvalue().encode()),
                    media_type="text/csv",
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"}
                )
            else:
                # JSON format
                data = [{
                    "id": row["id"],
                    "endpoint": row["endpoint"],
                    "method": row["method"],
                    "user_id": row["user_id"],
                    "status_code": row["status_code"],
                    "response_time_ms": row["response_time_ms"],
                    "request_size_bytes": row["request_size_bytes"],
                    "response_size_bytes": row["response_size_bytes"],
                    "error_message": row["error_message"],
                    "created_at": row["created_at"].isoformat()
                } for row in rows]
                
                return JSONResponse(
                    content=data,
                    headers={"Content-Disposition": f"attachment; filename=analytics_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"}
                )
                
    except Exception as e:
        logger.error(f"Error exporting analytics data: {e}")
        raise HTTPException(status_code=500, detail="Failed to export analytics data")


@app.get("/api/custom/health")
async def health_check():
    return {"status": "healthy"}

HeadlineBlock.model_rebuild()
ParagraphBlock.model_rebuild()
AlertBlock.model_rebuild()
SectionBreakBlock.model_rebuild()
BulletListBlock.model_rebuild()
NumberedListBlock.model_rebuild()
PdfLessonDetails.model_rebuild()
TextPresentationDetails.model_rebuild()
QuizData.model_rebuild()
ProjectDB.model_rebuild()
MicroProductApiResponse.model_rebuild()
ProjectDetailForEditResponse.model_rebuild()
ProjectUpdateRequest.model_rebuild()
TrainingPlanDetails.model_rebuild()

# ========================= Wizard Course Outline Endpoints =========================

class OutlineWizardPreview(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: full markdown string of the current outline so the assistant can apply
    # targeted changes when the user sends an incremental "edit" prompt.
    originalOutline: Optional[str] = None
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths
    theme: Optional[str] = None  # Selected theme from frontend

class OutlineWizardFinalize(BaseModel):
    prompt: str
    modules: int
    lessonsPerModule: str
    language: str = "en"
    chatSessionId: Optional[str] = None
    editedOutline: Dict[str, Any]
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths
    theme: Optional[str] = None  # Selected theme from frontend
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder


@app.post("/api/custom/products/{project_id}/ensure-json")
async def ensure_product_json(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Ensure the product has a JSON uploaded to Onyx and return its document id.
    Uses the current user's Onyx session to perform the upload if needed.
    """
    logger.info(f"[ensure_product_json] === START for product_id={project_id} ===")
    try:
        # Verify access and fetch product
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        logger.info(f"[ensure_product_json] User: uuid={user_uuid}, email={user_email}")
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT p.id, p.onyx_user_id, p.product_json_onyx_id, p.microproduct_content, p.project_name
                FROM projects p
                WHERE p.id = $1 AND (
                    p.onyx_user_id = $2 OR EXISTS (
                        SELECT 1 FROM product_access pa
                        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                        WHERE pa.product_id = p.id AND wm.user_id = $3 AND wm.status = 'active'
                    )
                )
                """,
                project_id, user_uuid, user_email,
            )
        
        if not row:
            logger.warning(f"[ensure_product_json] Product {project_id} not found or access denied for user {user_uuid}")
            raise HTTPException(status_code=404, detail="Product not found or access denied")

        logger.info(f"[ensure_product_json] Product found: id={row['id']}, name={row.get('project_name')}, existing_onyx_id={row.get('product_json_onyx_id')}")

        if row.get("product_json_onyx_id"):
            logger.info(f"[ensure_product_json] Product {project_id} already has Onyx ID: {row['product_json_onyx_id']}")
            return {"product_json_onyx_id": row["product_json_onyx_id"]}

        # Build JSON bytes
        content = row.get("microproduct_content") or {}
        logger.info(f"[ensure_product_json] Building JSON from content (keys: {list(content.keys()) if isinstance(content, dict) else 'not-dict'})")
        try:
            product_json = json.dumps(content, ensure_ascii=False).encode("utf-8")
            logger.info(f"[ensure_product_json] JSON size: {len(product_json)} bytes")
        except Exception as e:
            logger.error(f"[ensure_product_json] Failed to serialize content: {e}, using empty dict")
            product_json = json.dumps({}, ensure_ascii=False).encode("utf-8")

        # Upload directly to Onyx using current user's cookies
        session_cookie_value = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        if not session_cookie_value:
            logger.error(f"[ensure_product_json] No session cookie found for product {project_id}")
            raise HTTPException(status_code=401, detail="Authentication required")
        cookies = {ONYX_SESSION_COOKIE_NAME: session_cookie_value}
        logger.info(f"[ensure_product_json] Session cookie present: {bool(session_cookie_value)}")

        file_name = f"product_{project_id}.json"
        logger.info(f"[ensure_product_json] Uploading to Onyx: file_name={file_name}, url={ONYX_API_SERVER_URL}")
        
        onyx_id = await upload_product_json_to_onyx(ONYX_API_SERVER_URL, cookies, file_name, product_json)
        logger.info(f"[ensure_product_json] Upload successful, received Onyx ID: {onyx_id}")

        # Persist
        async with pool.acquire() as conn:
            await conn.execute(
                "UPDATE projects SET product_json_onyx_id=$1 WHERE id=$2",
                onyx_id, project_id,
            )
        logger.info(f"[ensure_product_json] Persisted Onyx ID {onyx_id} to product {project_id}")

        logger.info(f"[ensure_product_json] === SUCCESS for product_id={project_id}, onyx_id={onyx_id} ===")
        return {"product_json_onyx_id": onyx_id}
    except HTTPException as he:
        logger.error(f"[ensure_product_json] HTTPException for product {project_id}: {he.status_code} - {he.detail}")
        raise
    except Exception as e:
        logger.error(f"[ensure_product_json] Unexpected error for product {project_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to ensure product JSON: {str(e)}")

_CONTENTBUILDER_PERSONA_CACHE: Optional[int] = None

async def map_smartdrive_paths_to_onyx_files(smartdrive_paths: List[str], user_id: str) -> List[int]:
    """
    Map SmartDrive file paths to corresponding Onyx file IDs.
    Also handles direct Onyx file IDs (numeric strings) from products-as-context feature.
    
    Args:
        smartdrive_paths: List of SmartDrive file paths OR Onyx file IDs (as strings) to map
        user_id: Onyx user ID for context filtering
    
    Returns:
        List of Onyx file IDs that correspond to the SmartDrive paths or direct IDs
    """
    if not smartdrive_paths:
        return []
    
    # Separate direct Onyx IDs from SmartDrive paths
    direct_onyx_ids = []
    actual_paths = []
    
    for item in smartdrive_paths:
        # Check if this is a numeric string (direct Onyx file ID from products-as-context)
        if item.strip().isdigit():
            direct_onyx_ids.append(int(item.strip()))
            logger.info(f"[SMARTDRIVE_MAPPING] Detected direct Onyx file ID: {item}")
        else:
            actual_paths.append(item)
    
    logger.info(f"[SMARTDRIVE_MAPPING] Processing {len(direct_onyx_ids)} direct Onyx IDs and {len(actual_paths)} SmartDrive paths")
    
    # Start with direct Onyx IDs
    onyx_file_ids = direct_onyx_ids.copy()
    
    # Map SmartDrive paths if any
    if actual_paths:
        try:
            pool = await get_db_pool()
            async with pool.acquire() as connection:
                # Query the smartdrive_imports table to find matching Onyx file IDs
                placeholders = ','.join(f'${i+2}' for i in range(len(actual_paths)))
                query = f"""
                    SELECT onyx_file_id, smartdrive_path 
                    FROM smartdrive_imports 
                    WHERE onyx_user_id = $1 
                    AND smartdrive_path IN ({placeholders})
                    AND onyx_file_id IS NOT NULL
                """
                
                params = [user_id] + actual_paths
                rows = await connection.fetch(query, *params)
                
                mapped_file_ids = [row['onyx_file_id'] for row in rows]
                mapped_paths = [row['smartdrive_path'] for row in rows]
                
                onyx_file_ids.extend(mapped_file_ids)
                
                logger.info(f"[SMARTDRIVE_MAPPING] Mapped {len(mapped_file_ids)} file IDs from {len(actual_paths)} SmartDrive paths for user {user_id}")
                logger.info(f"[SMARTDRIVE_MAPPING] Looking for paths: {actual_paths}")
                logger.info(f"[SMARTDRIVE_MAPPING] Found mappings: {[(row['smartdrive_path'], row['onyx_file_id']) for row in rows]}")
                
                # Log any unmapped paths for debugging
                unmapped_paths = [path for path in actual_paths if path not in mapped_paths]
                if unmapped_paths:
                    logger.warning(f"[SMARTDRIVE_MAPPING] Unmapped paths: {unmapped_paths}")
                    
                    # Show what paths ARE available in the database for this user
                    debug_query = "SELECT smartdrive_path FROM smartdrive_imports WHERE onyx_user_id = $1 LIMIT 10"
                    debug_rows = await connection.fetch(debug_query, user_id)
                    available_paths = [row['smartdrive_path'] for row in debug_rows]
                    logger.info(f"[SMARTDRIVE_MAPPING] Sample available paths for user {user_id}: {available_paths[:5]}")
                
        except Exception as e:
            logger.error(f"[SMARTDRIVE_MAPPING] Error mapping SmartDrive paths to Onyx files: {e}", exc_info=True)
    
    logger.info(f"[SMARTDRIVE_MAPPING] Total Onyx file IDs: {len(onyx_file_ids)} (direct: {len(direct_onyx_ids)}, mapped: {len(onyx_file_ids) - len(direct_onyx_ids)})")
    return onyx_file_ids

async def get_contentbuilder_persona_id(cookies: Dict[str, str], use_search_persona: bool = False) -> int:
    """Return persona id of the default ContentBuilder assistant (cached).
    
    Args:
        cookies: Authentication cookies
        use_search_persona: If True, return the Search persona (ID 0) instead of ContentBuilder
    """
    # If Knowledge Base search is requested, use Search persona (ID 0)
    if use_search_persona:
        logger.info(f"[PERSONA_SELECTION] Using Search persona (ID 0) for Knowledge Base search")
        return 0
    
    global _CONTENTBUILDER_PERSONA_CACHE
    if _CONTENTBUILDER_PERSONA_CACHE is not None:
        return _CONTENTBUILDER_PERSONA_CACHE
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.get(f"{ONYX_API_SERVER_URL}/persona", cookies=cookies)
        resp.raise_for_status()
        personas = resp.json()
        # naive: first persona marked is_default_persona and has name 'ContentBuilder'
        for p in personas:
            if p.get("is_default_persona") or "contentbuilder" in p.get("name", "").lower():
                _CONTENTBUILDER_PERSONA_CACHE = p["id"]
                return _CONTENTBUILDER_PERSONA_CACHE
    raise HTTPException(status_code=500, detail="Could not locate ContentBuilder persona")

async def create_onyx_chat_session(persona_id: int, cookies: Dict[str, str]) -> str:
    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.post(
            f"{ONYX_API_SERVER_URL}/chat/create-chat-session",
            json={"persona_id": persona_id, "description": None},
            cookies=cookies,
        )
        resp.raise_for_status()
        data = resp.json()
        return data.get("chat_session_id") or data.get("chatSessionId")

async def stream_chat_message(chat_session_id: str, message: str, cookies: Dict[str, str], enable_search: bool = False) -> str:
    """Send message via Onyx and return the full answer, handling both streaming and non-streaming responses."""
    logger.info(f"[stream_chat_message] chat_id={chat_session_id} len(message)={len(message)} enable_search={enable_search}")

    # Use longer timeout for Knowledge Base searches
    timeout_duration = 600.0 if enable_search else 300.0
    async with httpx.AsyncClient(timeout=timeout_duration) as client:
        # Enable search when needed for Knowledge Base searches
        retrieval_options = {
            "run_search": "always" if enable_search else "never",
            "real_time": False,
        }
        payload = {
            "chat_session_id": chat_session_id,
            "message": message,
            "parent_message_id": None,
            "file_descriptors": [],
            "user_file_ids": [],
            "user_folder_ids": [],
            "prompt_id": None,
            "search_doc_ids": None,
            "retrieval_options": retrieval_options,
            "stream_response": False,
        }
        # Prefer the non-streaming simplified endpoint if available (much faster and avoids nginx timeouts)
        simple_url = f"{ONYX_API_SERVER_URL}/chat/send-message-simple-api"
        logger.debug(f"[stream_chat_message] POST {simple_url} (preferred) ...")
        try:
            resp = await client.post(simple_url, json=payload, cookies=cookies)
            if resp.status_code == 404:
                raise HTTPStatusError("simple api not found", request=resp.request, response=resp)
        except HTTPStatusError:
            logger.debug("[stream_chat_message] simple-api not available, falling back to generic endpoint")
            # Fallback to the generic endpoint (may stream)
            resp = await client.post(
                f"{ONYX_API_SERVER_URL}/chat/send-message",
                json=payload,
                cookies=cookies,
            )
        logger.info(f"[stream_chat_message] Response status={resp.status_code} ctype={resp.headers.get('content-type')}")
        resp.raise_for_status()
        # Depending on deployment, Onyx may return SSE stream or JSON.
        ctype = resp.headers.get("content-type", "")
        if ctype.startswith("text/event-stream"):
            logger.info(f"[stream_chat_message] Processing streaming response...")
            full_answer = ""
            line_count = 0
            done_received = False
            
            async for line in resp.aiter_lines():
                line_count += 1
                if not line:
                    continue
                    
                if not line.startswith("data: "):
                    logger.debug(f"[stream_chat_message] Skipping non-data line: {line[:100]}")
                    continue
                    
                payload_text = line.removeprefix("data: ").strip()
                if payload_text == "[DONE]":
                    logger.info(f"[stream_chat_message] Received [DONE] signal after {line_count} lines")
                    done_received = True
                    break
                    
                try:
                    packet = json.loads(payload_text)
                except Exception as e:
                    logger.debug(f"[stream_chat_message] Failed to parse JSON: {payload_text[:100]} - {e}")
                    continue
                    
                if packet.get("answer_piece"):
                    answer_piece = packet["answer_piece"]
                    full_answer += answer_piece
                    if len(full_answer) % 500 == 0:  # Log progress every 500 chars
                        logger.debug(f"[stream_chat_message] Accumulated {len(full_answer)} chars so far...")
                        
            logger.info(f"[stream_chat_message] Streaming completed. Total chars: {len(full_answer)}, Lines processed: {line_count}, Done received: {done_received}")
            return full_answer
        # Fallback JSON response
        try:
            data = resp.json()
            return data.get("answer") or data.get("answer_citationless") or ""
        except Exception:
            return resp.text.strip()

# ------------ utility to parse markdown outline (very simple) -------------

def _parse_outline_markdown(md: str) -> List[Dict[str, Any]]:
    """Parse the markdown outline produced by the assistant into a lightweight
    list-of-modules representation expected by the wizard UI.

    Enhanced to handle various markdown formats and create intelligent module divisions.
    """
    logger.info(f"[PARSE_OUTLINE] Starting parse with input length: {len(md)}")
    logger.info(f"[PARSE_OUTLINE] Input preview: {md[:200]}{'...' if len(md) > 200 else ''}")
    
    modules: List[Dict[str, Any]] = []
    current: Optional[Dict[str, Any]] = None

    list_item_regex = re.compile(r"^(?:- |\* |\d+\.)")
    _buf: List[str] = []  # buffer for current lesson lines

    def flush_current_lesson(buf: List[str]) -> Optional[str]:
        """Combine buffered lines into a single lesson string."""
        if not buf:
            return None
        return "\n".join(buf)

    lines_processed = 0
    for raw_line in md.splitlines():
        lines_processed += 1
        if not raw_line.strip():
            continue  # skip empty lines

        indent = len(raw_line) - len(raw_line.lstrip())
        line = raw_line.lstrip()

        # Enhanced module detection - look for ## headers OR ### headers OR "Module" patterns
        is_module_header = (
            line.startswith("## ") or 
            (line.startswith("# ") and "module" in line.lower()) or
            line.startswith("**Module") or
            re.match(r"^Module\s+\d+", line, re.IGNORECASE)
        )
        
        if is_module_header:
            # flush any buffered lesson into previous module before switching
            if current:
                last_lesson = flush_current_lesson(_buf)
                if last_lesson:
                    current["lessons"].append(last_lesson)
                _buf = []

            # Extract title from various formats
            title_part = line.lstrip("#* ").strip()
            if ":" in title_part:
                title_part = title_part.split(":", 1)[-1].strip()
            if title_part.lower().startswith("module"):
                # Keep the "Module X:" format if present
                pass
            
            current = {
                "id": f"mod{len(modules) + 1}",
                "title": title_part,
                "totalHours": 0.0,
                "lessons": [],
            }
            modules.append(current)
            logger.debug(f"[PARSE_OUTLINE] Found module: {title_part}")
            continue

        # Lesson detection – only consider top-level list items (indent == 0)
        if current:
            # Note: Total Time lines are now auto-calculated from lesson creation times
            # We still capture them for backward compatibility but will recalculate
            m_time = re.match(r"(?:Total Time|Общее время|Загальний час)\s*:\s*([0-9]+(?:\.[0-9]+)?)", line, re.IGNORECASE)
            if m_time:
                try:
                    # Store the original value for backward compatibility, but we'll recalculate
                    current["originalTotalHours"] = float(m_time.group(1))
                except ValueError:
                    pass  # leave default 0.0 if parsing fails

            if indent == 0 and list_item_regex.match(line):
                # Starting a new top-level lesson → flush previous buffer
                ls_string = flush_current_lesson(_buf) if '_buf' in locals() else None
                if ls_string:
                    current["lessons"].append(ls_string)
                _buf = []  # reset buffer for new lesson

                lesson_title = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if lesson_title.startswith("**") and "**" in lesson_title[2:]:
                    lesson_title = lesson_title.split("**", 2)[1].strip()
                _buf.append(lesson_title)
                logger.debug(f"[PARSE_OUTLINE] Found lesson: {lesson_title}")
                continue
            elif current.get('lessons') is not None and '_buf' in locals():
                # inside a lesson details block (indented)
                if indent > 0:
                    _buf.append(line)
                continue

    # flush buffer after loop to whichever module is active
    if current:
        last_lesson = flush_current_lesson(_buf)
        if last_lesson:
            current["lessons"].append(last_lesson)

    logger.info(f"[PARSE_OUTLINE] After main parsing: {len(modules)} modules found, {lines_processed} lines processed")

    # Enhanced fallback when no module headings present
    if not modules:
        logger.warning(f"[PARSE_OUTLINE] No modules found, using intelligent fallback parsing")
        
        # Collect all lessons first
        all_lessons = []
        for raw_line in md.splitlines():
            if not raw_line.strip():
                continue
            indent = len(raw_line) - len(raw_line.lstrip())
            line = raw_line.lstrip()
            if indent == 0 and list_item_regex.match(line):
                txt = re.sub(r"^(?:- |\* |\d+\.\s*)", "", line).strip()
                if txt.startswith("**") and "**" in txt[2:]:
                    txt = txt.split("**", 2)[1].strip()
                all_lessons.append(txt)
        
        # If we have lessons, try to intelligently divide them into modules
        if all_lessons:
            logger.info(f"[PARSE_OUTLINE] Found {len(all_lessons)} lessons for intelligent division")
            
            # Try to determine intended module count from lesson separators or natural breaks
            separator_indices = []
            for i, lesson in enumerate(all_lessons):
                if "---" in lesson or lesson.strip() == "---":
                    separator_indices.append(i)
            
            if separator_indices:
                # Use separator-based division
                logger.info(f"[PARSE_OUTLINE] Using separator-based division with {len(separator_indices)} separators")
                start_idx = 0
                for module_num, sep_idx in enumerate(separator_indices + [len(all_lessons)], 1):
                    if start_idx < sep_idx:
                        module_lessons = [l for l in all_lessons[start_idx:sep_idx] if "---" not in l]
                        if module_lessons:  # Only create module if it has lessons
                            module = {
                                "id": f"mod{module_num}",
                                "title": f"Module {module_num}",
                                "totalHours": 0.0,
                                "lessons": module_lessons
                            }
                            modules.append(module)
                    start_idx = sep_idx + 1
            else:
                # Intelligently divide lessons into reasonable groups (3-5 lessons per module)
                target_lessons_per_module = 4
                num_modules = max(1, min(6, (len(all_lessons) + target_lessons_per_module - 1) // target_lessons_per_module))
                lessons_per_module = len(all_lessons) // num_modules
                remainder = len(all_lessons) % num_modules
                
                logger.info(f"[PARSE_OUTLINE] Dividing {len(all_lessons)} lessons into {num_modules} modules (~{lessons_per_module} lessons each)")
                
                start_idx = 0
                for module_num in range(1, num_modules + 1):
                    # Add one extra lesson to some modules to handle remainder
                    current_module_size = lessons_per_module + (1 if module_num <= remainder else 0)
                    end_idx = start_idx + current_module_size
                    
                    module_lessons = all_lessons[start_idx:end_idx]
                    if module_lessons:  # Only create module if it has lessons
                        module = {
                            "id": f"mod{module_num}",
                            "title": f"Module {module_num}",
                            "totalHours": 0.0,
                            "lessons": module_lessons
                        }
                        modules.append(module)
                    start_idx = end_idx
        
        # Last resort fallback - create single module with all content
        if not modules:
            logger.warning(f"[PARSE_OUTLINE] No lessons found, dumping all non-empty lines")
            tmp_module = {"id": "mod1", "title": "Course Content", "lessons": [], "totalHours": 0.0}
            tmp_module["lessons"] = [l.strip() for l in md.splitlines() if l.strip()]
            modules.append(tmp_module)
        
        logger.info(f"[PARSE_OUTLINE] Intelligent fallback created {len(modules)} modules")

    logger.info(f"[PARSE_OUTLINE] Final result: {len(modules)} modules")
    
    # Auto-calculate total creation time for each module by summing lesson creation times
    for i, module in enumerate(modules):
        logger.info(f"[PARSE_OUTLINE] Module {i+1}: '{module.get('title', 'No title')}' with {len(module.get('lessons', []))} lessons")
        
        # Calculate total creation time from lesson creation times
        total_creation_hours = 0.0
        lessons = module.get('lessons', [])
        
        for lesson in lessons:
            if isinstance(lesson, str):
                # Parse lesson details from markdown format
                lesson_lines = lesson.split('\n')
                for line in lesson_lines:
                    # Look for Time field in markdown format: "- **Time**: 17h" or "- **Время**: 17h" or "- **Час**: 17h"
                    time_match = re.search(r'^\s*-\s*\*\*(?:Time|Время|Час)\*\*:\s*([0-9]+(?:\.[0-9]+)?)h?\s*$', line.strip())
                    if time_match:
                        try:
                            hours = float(time_match.group(1))
                            total_creation_hours += hours
                            logger.debug(f"[PARSE_OUTLINE] Found lesson time: {hours}h")
                        except (ValueError, TypeError):
                            logger.warning(f"[PARSE_OUTLINE] Could not parse lesson time from line: {line}")
        
        # Set the auto-calculated total hours
        module['totalHours'] = total_creation_hours
        logger.info(f"[PARSE_OUTLINE] Module {i+1} auto-calculated totalHours: {total_creation_hours}")

    return modules

# ----------------------- ENDPOINTS ---------------------------------------

@app.post("/api/custom/course-outline/preview")
async def wizard_outline_preview(payload: OutlineWizardPreview, request: Request):
    # EXTENSIVE DEBUG LOGGING: Log all incoming parameters
    logger.info(f"🔍 [STEP 6] Backend received request with payload attributes:")
    for attr in ['prompt', 'modules', 'lessonsPerModule', 'language', 'fromConnectors', 'connectorIds', 'connectorSources', 'selectedFiles', 'fromFiles', 'fileIds', 'folderIds', 'fromText', 'userText', 'fromKnowledgeBase']:
        value = getattr(payload, attr, 'NOT_SET')
        logger.info(f"🔍 [STEP 6] payload.{attr} = {value}")
    
    logger.info(f"🔍 [STEP 6] Raw request body size: {len(await request.body())} bytes")
    logger.info(f"[PREVIEW_START] Course outline preview initiated")
    logger.info(f"[PREVIEW_PARAMS] prompt='{payload.prompt[:50]}...' modules={payload.modules} lessonsPerModule={payload.lessonsPerModule} lang={payload.language}")
    logger.info(f"[PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    # EXTENSIVE DEBUG: Check all connector-related attributes
    logger.info(f"🔍 [CRITICAL] payload.fromConnectors = {getattr(payload, 'fromConnectors', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.connectorIds = {getattr(payload, 'connectorIds', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.connectorSources = {getattr(payload, 'connectorSources', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] payload.selectedFiles = {getattr(payload, 'selectedFiles', 'MISSING')}")
    logger.info(f"🔍 [CRITICAL] HasAttr selectedFiles: {hasattr(payload, 'selectedFiles')}")
    logger.info(f"🔍 [CRITICAL] All payload attributes: {[attr for attr in dir(payload) if not attr.startswith('_')]}")
    logger.info(f"[PREVIEW_PARAMS] chatSessionId={payload.chatSessionId}")
    logger.info(f"[PREVIEW_PARAMS] originalOutline length={len(payload.originalOutline) if payload.originalOutline else 0}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")
    logger.info(f"[PREVIEW_AUTH] Cookie value: {cookies[ONYX_SESSION_COOKIE_NAME][:20] if cookies[ONYX_SESSION_COOKIE_NAME] else 'None'}...")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[PREVIEW_CHAT] Creating new chat session")
        try:
            logger.info(f"[PREVIEW_CHAT] Attempting to get contentbuilder persona ID")
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            logger.info(f"[PREVIEW_CHAT] Attempting to create Onyx chat session")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[PREVIEW_CHAT_ERROR] Failed to create chat session: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    logger.info(f"[PREVIEW_PAYLOAD] Building wizard payload")
    wiz_payload = {
        "product": "Course Outline",
        "prompt": payload.prompt,
        "language": payload.language,
    }
    logger.info(f"[PREVIEW_PAYLOAD] Base payload created with product={wiz_payload['product']}, language={wiz_payload['language']}")

    # Add file context if provided
    if payload.fromFiles:
        logger.info(f"[PREVIEW_PAYLOAD] Adding file context: fromFiles=True")
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
            logger.info(f"[PREVIEW_PAYLOAD] Added folderIds: {payload.folderIds}")
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds
            logger.info(f"[PREVIEW_PAYLOAD] Added fileIds: {payload.fileIds}")

    # Add connector context if provided
    if payload.fromConnectors:
        logger.info(f"[PREVIEW_PAYLOAD] Adding connector context: fromConnectors=True")
        wiz_payload["fromConnectors"] = True
        if payload.connectorIds:
            wiz_payload["connectorIds"] = payload.connectorIds
            logger.info(f"[PREVIEW_PAYLOAD] Added connectorIds: {payload.connectorIds}")
        if payload.connectorSources:
            wiz_payload["connectorSources"] = payload.connectorSources
            logger.info(f"[PREVIEW_PAYLOAD] Added connectorSources: {payload.connectorSources}")
        if payload.selectedFiles:
            wiz_payload["selectedFiles"] = payload.selectedFiles
            logger.info(f"[PREVIEW_PAYLOAD] Added selectedFiles: {payload.selectedFiles}")

    # Add text context if provided - use virtual file system for large texts to prevent AI memory issues
    if payload.fromText and payload.userText:
        logger.info(f"[PREVIEW_PAYLOAD] Adding text context: fromText=True, textMode={payload.textMode}")
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"[PREVIEW_PAYLOAD] Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        # Check if we're using hybrid approach (files present) or direct approach (text-only)
        if should_use_hybrid_approach(payload):
            # Hybrid approach: create virtual files for text (existing behavior for file-based scenarios)
            if text_length > LARGE_TEXT_THRESHOLD:
                logger.info(f"[PREVIEW_PAYLOAD] Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system for hybrid approach")
                try:
                    logger.info(f"[PREVIEW_PAYLOAD] Attempting to create virtual file for large text")
                    virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                    wiz_payload["virtualFileId"] = virtual_file_id
                    wiz_payload["textCompressed"] = False
                    logger.info(f"[PREVIEW_PAYLOAD] Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
                except Exception as e:
                    logger.error(f"[PREVIEW_PAYLOAD] Failed to create virtual file for large text: {e}", exc_info=True)
                    # Fallback to compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"[PREVIEW_PAYLOAD] Fallback to compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Use compression for hybrid approach with medium/small text
                if text_length > TEXT_SIZE_THRESHOLD:
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"[PREVIEW_PAYLOAD] Using compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
                else:
                    wiz_payload["userText"] = payload.userText
                    wiz_payload["textCompressed"] = False
        else:
            # Direct approach: send text directly in wizard request (no file conversion)
            logger.info(f"[PREVIEW_PAYLOAD] ✅ Using DIRECT approach: sending text directly in wizard request ({text_length} chars)")
            
            # For very large texts, use compression to reduce payload size
            if text_length > TEXT_SIZE_THRESHOLD:
                compressed_text = compress_text(payload.userText)
                wiz_payload["userText"] = compressed_text
                wiz_payload["textCompressed"] = True
                logger.info(f"[PREVIEW_PAYLOAD] Compressed text for direct wizard request ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Send text directly without compression
                wiz_payload["userText"] = payload.userText
                wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"[PREVIEW_PAYLOAD] Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        logger.info(f"[PREVIEW_PAYLOAD] Adding Knowledge Base context: fromKnowledgeBase=True")
        wiz_payload["fromKnowledgeBase"] = True

    if payload.originalOutline:
        logger.info(f"[PREVIEW_PAYLOAD] Adding originalOutline ({len(payload.originalOutline)} chars)")
        wiz_payload["originalOutline"] = payload.originalOutline
    else:
        logger.info(f"[PREVIEW_PAYLOAD] Adding module configuration: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
        wiz_payload.update({
            "modules": payload.modules,
            "lessonsPerModule": payload.lessonsPerModule,
        })

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        logger.info(f"[PREVIEW_PAYLOAD] Decompressing text for assistant")
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"[PREVIEW_PAYLOAD] Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"[PREVIEW_PAYLOAD] Failed to decompress text: {e}", exc_info=True)
            # Continue with original text if decompression fails
    
    logger.info(f"[PREVIEW_PAYLOAD] Final payload keys: {list(wiz_payload.keys())}")
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
    # Force JSON-ONLY preview output for Course Outline to enable immediate parsed preview
    try:
        json_preview_instructions = f"""
CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Course Outline preview, strictly following this example structure:
{DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
"""
        wizard_message = wizard_message + "\n" + json_preview_instructions
    except Exception as e:
        logger.warning(f"[PREVIEW_JSON_INSTR] Failed to append JSON-only preview instructions: {e}")
    logger.info(f"[PREVIEW_PAYLOAD] Created wizard message ({len(wizard_message)} chars)")

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[PREVIEW_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    if payload.selectedFiles:
                        # Combined context: connectors + SmartDrive files
                        logger.info(f"[HYBRID_CONTEXT] Extracting COMBINED context from connectors: {payload.connectorSources} and SmartDrive files: {payload.selectedFiles}")
                        
                        # Extract connector context
                        connector_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                        
                        # Map SmartDrive paths to Onyx file IDs with proper normalization
                        raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                        
                        # Normalize paths to handle URL encoding and character variations
                        smartdrive_file_paths = []
                        for path in raw_paths:
                            # Handle URL encoding
                            try:
                                from urllib.parse import unquote
                                normalized_path = unquote(path)
                            except:
                                normalized_path = path
                            
                            # Handle `+` character variations (some systems use `+` in filenames)
                            # Try both with and without `+` to match database records
                            smartdrive_file_paths.append(normalized_path)
                            if '+' in normalized_path:
                                smartdrive_file_paths.append(normalized_path.replace('+', ''))
                            
                        onyx_user_id = await get_current_onyx_user_id(request)
                        
                        # DEBUG: Log the mapping attempt
                        logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                        logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                        logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                        
                        file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                        
                        if file_ids:
                            logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                            # Extract file context and combine with connector context
                            file_context_from_smartdrive = await extract_file_context_from_onyx(file_ids, [], cookies)
                            
                            # Combine both contexts
                            file_context = f"{connector_context}\n\n=== ADDITIONAL CONTEXT FROM SELECTED FILES ===\n\n{file_context_from_smartdrive}"
                        else:
                            logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths, using only connector context")
                            file_context = connector_context
                    else:
                        # For connector-based filtering only, extract context from specific connectors
                        logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                        file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromConnectors and payload.selectedFiles:
                    # SmartDrive files only (no connectors)
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from SmartDrive files only: {payload.selectedFiles}")
                    
                    # Map SmartDrive paths to Onyx file IDs
                    raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                    
                    # Normalize paths to handle URL encoding and character variations
                    smartdrive_file_paths = []
                    for path in raw_paths:
                        # Try multiple variations to match database records
                        from urllib.parse import unquote, quote
                        import re
                        
                        candidates = []
                        # Base variants
                        candidates.append(path)
                        try:
                            decoded_path = unquote(path)
                            candidates.append(decoded_path)
                        except:
                            decoded_path = path
                        try:
                            encoded_path = quote(path, safe='/')
                            candidates.append(encoded_path)
                        except:
                            pass
                        if ' ' in path:
                            candidates.append(path.replace(' ', '%20'))
                        if '%20' in path:
                            candidates.append(path.replace('%20', ' '))
                        
                        # Derived variants: trim spaces before dot and collapse multiple spaces
                        derived = []
                        for c in list(candidates):
                            trimmed_dot = re.sub(r"\s+\.", ".", c)
                            if trimmed_dot != c:
                                derived.append(trimmed_dot)
                            collapsed = re.sub(r"\s{2,}", " ", c)
                            if collapsed != c:
                                derived.append(collapsed)
                        candidates.extend(derived)
                        
                        # Encode derived variants as well
                        for c in list(candidates):
                            try:
                                enc = quote(c, safe='/')
                                candidates.append(enc)
                            except:
                                pass
                        
                        # Deduplicate while preserving order
                        seen = set()
                        for c in candidates:
                            if c and c not in seen:
                                seen.add(c)
                                smartdrive_file_paths.append(c)
                    
                    onyx_user_id = await get_current_onyx_user_id(request)
                    
                    # DEBUG: Log the mapping attempt
                    logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                    logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                    logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                    
                    file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                    
                    if file_ids:
                        logger.info(f"[HYBRID_CONTEXT] Successfully mapped {len(file_ids)} SmartDrive files to Onyx file IDs: {file_ids}")
                        # Extract context from the mapped file IDs
                        file_context = await extract_file_context_from_onyx(file_ids, [], cookies)
                    else:
                        logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths: {smartdrive_file_paths}")
                        file_context = f"Selected files: {', '.join(raw_paths)}\nNote: These files are from SmartDrive but could not be mapped to indexed content. Please ensure the files have been properly imported and indexed."
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Course Outline"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        
                        # Extract live progress updates using robust regex-based approach
                        progress_updates = extract_live_progress(assistant_reply, chat_id)
                        if progress_updates:
                            logger.info(f"[LIVE_STREAM_DEBUG] Found {len(progress_updates)} new updates")
                            
                            # Send only new incremental markdown content, not the entire structure
                            new_markdown_content = ""
                            for update in progress_updates:
                                # Keep JSON update for logs/debugging
                                yield (json.dumps(update) + "\n").encode()
                                logger.info(f"[LIVE_STREAM] Sent {update['type']}: {update['title']}")
                                
                                # Build only the new markdown content for this update
                                if update.get('type') == 'module':
                                    new_markdown_content += f"## {update['title']}\n"
                                elif update.get('type') == 'lesson':
                                    new_markdown_content += f"- {update['title']}\n"
                            
                            # Send only the new markdown content as incremental delta
                            if new_markdown_content:
                                yield (json.dumps({"type": "delta", "text": new_markdown_content}) + "\n").encode()
                                logger.info(f"[LIVE_STREAM_MD] Sent incremental markdown ({len(new_markdown_content)} chars): {repr(new_markdown_content[:100])}")
                        
                        # Send simple test updates to verify streaming works
                        if chunks_received % 50 == 0:
                            test_update = {"type": "module", "title": f"Test Module {chunks_received//50}", "id": f"test{chunks_received//50}"}
                            yield (json.dumps(test_update) + "\n").encode()
                            logger.info(f"[STREAM_TEST] Sent test module for chunk {chunks_received}")
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache full raw outline for later finalize step
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

                if not assistant_reply.strip():
                    logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
                    error_packet = {"type": "error", "message": "No content received from AI service"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    return

                # Try JSON-first parsing for immediate structured preview
                def _extract_json_text(s: str) -> str:
                    try:
                        start = s.find('{')
                        end = s.rfind('}')
                        if start != -1 and end != -1 and start < end:
                            return s[start:end+1]
                        return s
                    except Exception:
                        return s

                modules_preview = []
                try:
                    json_text = _extract_json_text(assistant_reply)
                    parsed = json.loads(json_text)
                    sections = parsed.get('sections', []) if isinstance(parsed, dict) else []
                    for i, sec in enumerate(sections):
                        title = (sec.get('title') if isinstance(sec, dict) else str(sec)) or ''
                        lessons_src = sec.get('lessons', []) if isinstance(sec, dict) else []
                        lessons = []
                        for ls in lessons_src:
                            if isinstance(ls, dict):
                                lesson_title = ls.get('title') or ''
                            else:
                                lesson_title = str(ls)
                            
                            # Clean lesson title (remove "Lesson X.Y:" prefix)
                            import re
                            cleaned_lesson_title = re.sub(r'^Lesson\s+\d+\.\d+:\s*', '', lesson_title).strip()
                            lessons.append(cleaned_lesson_title)
                        modules_preview.append({
                            "id": f"mod{i+1}",
                            "title": title,
                            "totalHours": (sec.get('totalHours') if isinstance(sec, dict) else 0.0) or 0.0,
                            "lessons": lessons,
                        })
                    logger.info(f"[PREVIEW_JSON_PARSE] Parsed modules from JSON: {len(modules_preview)}")
                except Exception as e:
                    logger.warning(f"[PREVIEW_JSON_PARSE] Failed to parse JSON preview ({e}); falling back to markdown parser")
                    logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
                    modules_preview = _parse_outline_markdown(assistant_reply)
                    
                    # Validate the parsed result meets basic requirements
                    validation_passed = True
                    validation_messages = []
                    # Check if we have reasonable number of modules (not just 1 with many lessons)
                    if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                        validation_passed = False
                        validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
                    # Check if we have expected module count (if specified in payload)
                    expected_modules = getattr(payload, 'modules', None)
                    if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                        validation_passed = False
                        validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
                    if not validation_passed:
                        logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                        logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
                    else:
                        logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
                
                # Send completion packet with the parsed outline
                logger.info(f"[PREVIEW_DONE] Creating completion packet")
                done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
                yield (json.dumps(done_packet) + "\n").encode()
                logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[PREVIEW_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[PREVIEW_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            # Force JSON-ONLY preview output for Course Outline in direct OpenAI path
            enhanced_wizard_message = wizard_message + """

CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Course Outline preview, strictly following this example structure:
""" + DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM + """
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
"""
            
            try:
                logger.info(f"[OPENAI_STREAM_DEBUG] Starting to iterate over chunks")
                async for chunk_data in stream_openai_response(enhanced_wizard_message):
                    logger.info(f"[OPENAI_STREAM_DEBUG] Received chunk: {chunk_data.get('type', 'unknown')}")
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.info(f"[OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        
                                                # Extract live progress updates using the same robust approach as hybrid
                        progress_updates = extract_live_progress(assistant_reply, chat_id)
                        if progress_updates:
                            logger.info(f"[LIVE_STREAM_DEBUG] Found {len(progress_updates)} new updates")
                            
                            # Send only new incremental markdown content, not the entire structure
                            new_markdown_content = ""
                            for update in progress_updates:
                                # Keep JSON update for logs/debugging
                                yield (json.dumps(update) + "\n").encode()
                                logger.info(f"[LIVE_STREAM] Sent {update['type']}: {update['title']}")
                                
                                # Build only the new markdown content for this update
                                if update.get('type') == 'module':
                                    new_markdown_content += f"## {update['title']}\n"
                                elif update.get('type') == 'lesson':
                                    new_markdown_content += f"- {update['title']}\n"
                            
                            # Send only the new markdown content as incremental delta
                            if new_markdown_content:
                                yield (json.dumps({"type": "delta", "text": new_markdown_content}) + "\n").encode()
                                logger.info(f"[LIVE_STREAM_MD] Sent incremental markdown ({len(new_markdown_content)} chars): {repr(new_markdown_content[:100])}")
                        
                        # Send test update every 100 chunks to verify streaming works
                        if chunks_received % 100 == 0:
                            test_update = {"type": "test", "message": f"Processing chunk {chunks_received}"}
                            yield (json.dumps(test_update) + "\n").encode()
                            logger.info(f"[STREAM_TEST] Sent test update for chunk {chunks_received}")
                        
                        # Raw delta fallback disabled - we now send structured incremental markdown
                    elif chunk_data["type"] == "error":
                        logger.error(f"[OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            except Exception as e:
                logger.error(f"[OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        if not assistant_reply.strip():
            logger.error(f"[PREVIEW_STREAM] CRITICAL: assistant_reply is empty or whitespace only!")
            error_packet = {"type": "error", "message": "No content received from AI service"}
            yield (json.dumps(error_packet) + "\n").encode()
            return

        # Try JSON-first parsing for immediate structured preview
        def _extract_json_text(s: str) -> str:
            try:
                start = s.find('{')
                end = s.rfind('}')
                if start != -1 and end != -1 and start < end:
                    return s[start:end+1]
                return s
            except Exception:
                return s

        modules_preview = []
        try:
            json_text = _extract_json_text(assistant_reply)
            parsed = json.loads(json_text)
            sections = parsed.get('sections', []) if isinstance(parsed, dict) else []
            for i, sec in enumerate(sections):
                title = (sec.get('title') if isinstance(sec, dict) else str(sec)) or ''
                lessons_src = sec.get('lessons', []) if isinstance(sec, dict) else []
                lessons = []
                for ls in lessons_src:
                    if isinstance(ls, dict):
                        lesson_title = ls.get('title') or ''
                    else:
                        lesson_title = str(ls)
                    
                    # Clean lesson title (remove "Lesson X.Y:" prefix)
                    import re
                    cleaned_lesson_title = re.sub(r'^Lesson\s+\d+\.\d+:\s*', '', lesson_title).strip()
                    lessons.append(cleaned_lesson_title)
                modules_preview.append({
                    "id": f"mod{i+1}",
                    "title": title,
                    "totalHours": (sec.get('totalHours') if isinstance(sec, dict) else 0.0) or 0.0,
                    "lessons": lessons,
                })
            logger.info(f"[PREVIEW_JSON_PARSE] Parsed modules from JSON: {len(modules_preview)}")
        except Exception as e:
            logger.warning(f"[PREVIEW_JSON_PARSE] Failed to parse JSON preview ({e}); falling back to markdown parser")
            logger.info(f"[PREVIEW_PARSING] Starting markdown parsing of {len(assistant_reply)} chars")
            modules_preview = _parse_outline_markdown(assistant_reply)
            
            # Validate the parsed result meets basic requirements
            validation_passed = True
            validation_messages = []
            # Check if we have reasonable number of modules (not just 1 with many lessons)
            if len(modules_preview) == 1 and len(modules_preview[0].get('lessons', [])) > 8:
                validation_passed = False
                validation_messages.append(f"Single module with {len(modules_preview[0].get('lessons', []))} lessons detected")
            # Check if we have expected module count (if specified in payload)
            expected_modules = getattr(payload, 'modules', None)
            if expected_modules and abs(len(modules_preview) - expected_modules) > 1:  # Allow 1 module difference
                validation_passed = False
                validation_messages.append(f"Expected ~{expected_modules} modules, got {len(modules_preview)}")
            if not validation_passed:
                logger.warning(f"[PREVIEW_VALIDATION] Outline structure validation failed: {'; '.join(validation_messages)}")
                logger.warning(f"[PREVIEW_VALIDATION] Raw content preview for debugging: {assistant_reply[:500]}{'...' if len(assistant_reply) > 500 else ''}")
            else:
                logger.info(f"[PREVIEW_VALIDATION] Outline structure validation passed")
        
                # Send completion packet with the parsed outline
        logger.info(f"[PREVIEW_DONE] Creating completion packet")
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        yield (json.dumps(done_packet) + "\n").encode()
        logger.info(f"[PREVIEW_STREAM] Sent completion packet with {len(modules_preview)} modules")
        return
                

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )

async def _ensure_training_plan_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_TRAINING_PLAN)
        if row:
            return row["id"]
        # create minimal template
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Training Plan", DEFAULT_TRAINING_PLAN_JSON_EXAMPLE_FOR_LLM, "Training Plan", COMPONENT_NAME_TRAINING_PLAN
        )
        return row["id"]

# After you get the parsed content from the AI parser, insert it like this:
async def insert_ai_audit_onepager_to_db(
    pool: asyncpg.Pool,
    onyx_user_id: str,
    project_name: str,
    microproduct_content: dict,
    chat_session_id: str = None
) -> int:
    """Insert AI-audit one-pager into database with correct template and component"""
    
    # First, ensure we have a Text Presentation template
    template_id = await _ensure_text_presentation_template(pool)
    
    insert_query = """
    INSERT INTO projects (
        onyx_user_id, project_name, product_type, microproduct_type,
        microproduct_name, microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, NOW(), $9)
    RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id;
    """
    
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            insert_query,
            onyx_user_id,
            project_name,
            "AI Audit",  # product_type
            "AI Audit",  # microproduct_type
            project_name,  # microproduct_name
            microproduct_content,  # parsed content from AI parser
            template_id,  # design_template_id (from _ensure_text_presentation_template)
            chat_session_id,  # source_chat_session_id
            None,  # folder_id - AI audit doesn't support folder assignment yet
        )
    
    if not row:
        raise HTTPException(status_code=500, detail="Failed to create AI-audit one-pager project entry.")
    
    return row["id"]


async def create_audit_folder(pool, onyx_user_id, company_name):
    async with pool.acquire() as conn:
        query = """
        INSERT INTO project_folders (onyx_user_id, name)
        VALUES ($1, $2)
        RETURNING id;
        """
        row = await conn.fetchrow(query, onyx_user_id, f"AI-Аудит: {company_name}")
        return row["id"]
    

async def assign_projects_to_folder(pool, folder_id, project_ids):
    async with pool.acquire() as conn:
        await conn.executemany(
            "UPDATE projects SET folder_id = $1 WHERE id = $2",
            [(folder_id, pid) for pid in project_ids]
        )


def set_progress(job_id, message):
    AI_AUDIT_PROGRESS.setdefault(job_id, []).append(message)


@app.get("/api/custom/ai-audit/progress")
async def get_audit_progress(jobId: str):
    return {"messages": AI_AUDIT_PROGRESS.get(jobId, [])}


async def scrape_company_data_from_website(company_website: str, language: str = "ru") -> AiAuditScrapedData:
    """
    Scrape company website to extract all necessary data for AI audit.
    Returns structured data that can be used in prompts.
    """
    try:
        logger.info(f"🌐 [WEBSITE SCRAPING] Starting to scrape: {company_website}")
        
        # Use the research abstraction (OpenAI web search preferred, SerpAPI fallback)
        # For website-only scraping, we need to extract domain name for the search
        from urllib.parse import urlparse
        parsed_url = urlparse(company_website)
        domain_name = parsed_url.netloc.replace('www.', '')
        logger.info(f"🌐 [WEBSITE SCRAPING] Using domain name for search: {domain_name}")
        website_content = await company_research(domain_name, "", company_website, language)
        logger.info(f"🌐 [WEBSITE SCRAPING] Received content length: {len(website_content)} characters")
        
        # Extract company name from website content
        company_name = await extract_company_name_from_website_content(website_content, company_website)
        
        # Extract company description from website content
        company_description = await extract_company_description_from_website_content(website_content, company_website, language)
        
        # Extract other company data using AI analysis
        company_data = await extract_company_metadata_from_website(website_content, company_website)
        
        scraped_data = AiAuditScrapedData(
            companyName=company_name,
            companyDesc=company_description,
            employees=company_data.get("employees", "Unknown"),
            franchise=company_data.get("franchise", "Unknown"),
            onboardingProblems=company_data.get("onboardingProblems", "To be analyzed from website content"),
            documents=company_data.get("documents", ["Other"]),
            documentsOther=company_data.get("documentsOther", "To be determined from website analysis"),
            priorities=company_data.get("priorities", ["Other"]),
            priorityOther=company_data.get("priorityOther", "To be determined from website analysis")
        )
        
        logger.info(f"🌐 [WEBSITE SCRAPING] Successfully scraped data for: {company_name}")
        return scraped_data
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error scraping website {company_website}: {e}")
        # Return fallback data if scraping fails
        return AiAuditScrapedData(
            companyName="Company Name",
            companyDesc="Company Description",
            employees="Unknown",
            franchise="Unknown",
            onboardingProblems="To be analyzed from website content",
            documents=["Other"],
            documentsOther="To be determined from website analysis",
            priorities=["Other"],
            priorityOther="To be determined from website analysis"
        )

async def extract_company_name_from_website_content(website_content: str, company_website: str) -> str:
    """Extract company name from website content using AI."""
    try:
        prompt = f"""
        Извлеки точное название компании из предоставленного контента веб-сайта.
        
        ВЕБ-САЙТ: {company_website}
        КОНТЕНТ ВЕБ-САЙТА:
        {website_content}
        
        ИНСТРУКЦИИ:
        - Найди официальное название компании
        - Верни только название компании, без дополнительной информации
        - Если не можешь определить название, верни "Company Name"
        
        ОТВЕТ (только название компании):
        """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        company_name = response_text.strip()
        if not company_name or company_name.lower() in ["unknown", "неизвестно", "not found"]:
            company_name = "Company Name"
            
        logger.info(f"🏢 [WEBSITE SCRAPING] Extracted company name: {company_name}")
        return company_name
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company name: {e}")
        return "Company Name"

async def extract_company_description_from_website_content(website_content: str, company_website: str, language: str = "ru") -> str:
    """Extract company description from website content using AI."""
    try:
        if language == "en":
            prompt = f"""
            Create a brief company description based on the website content.

            WEBSITE: {company_website}
            WEBSITE CONTENT:
            {website_content}

            INSTRUCTIONS:
            - Create description in style: "Company providing services in [main services]"
            - Use only information from the website
            - Description should be maximally brief (ONLY 1 sentence)
            - DO NOT add additional details or examples
            - Generate ALL content EXCLUSIVELY in English
            - If you cannot determine description, return "Company Description"

            RESPONSE (company description only):
            """
        elif language == "es":
            prompt = f"""
            Crea una breve descripción de la empresa basada en el contenido del sitio web.

            SITIO WEB: {company_website}
            CONTENIDO DEL SITIO WEB:
            {website_content}

            INSTRUCCIONES:
            - Crea descripción en estilo: "Empresa que proporciona servicios en [servicios principales]"
            - Usa solo información del sitio web
            - La descripción debe ser máxima breve (SOLO 1 oración)
            - NO agregues detalles adicionales o ejemplos
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            - Si no puedes determinar la descripción, devuelve "Descripción de la Empresa"

            RESPUESTA (solo descripción de la empresa):
            """
        elif language == "ua":
            prompt = f"""
            Створіть короткий опис компанії на основі вмісту веб-сайту.

            ВЕБ-САЙТ: {company_website}
            ВМІСТ ВЕБ-САЙТУ:
            {website_content}

            ІНСТРУКЦІЇ:
            - Створіть опис у стилі: "Компанія, що надає послуги в галузі [основні послуги]"
            - Використовуйте лише інформацію з веб-сайту
            - Опис має бути максимально коротким (ЛИШЕ 1 речення)
            - НЕ додавайте додаткові деталі або приклади
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            - Якщо не можете визначити опис, поверніть "Опис компанії"

            ВІДПОВІДЬ (лише опис компанії):
            """
        else:
            prompt = f"""
            Создай краткое описание компании на основе контента веб-сайта.

            ВЕБ-САЙТ: {company_website}
            КОНТЕНТ ВЕБ-САЙТА:
            {website_content}

            ИНСТРУКЦИИ:
            - Создай описание в стиле: "Компания, предоставляющая услуги по [основные услуги]"
            - Используй только информацию с веб-сайта
            - Описание должно быть максимально кратким (ТОЛЬКО 1 предложение)
            - НЕ добавляй дополнительные детали или примеры
            - Если не можешь определить описание, верни "Company Description"

            ОТВЕТ (только описание компании):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        company_description = response_text.strip()
        if not company_description or company_description.lower() in ["unknown", "неизвестно", "not found"]:
            company_description = "Company Description"
            
        logger.info(f"📝 [WEBSITE SCRAPING] Extracted company description: {company_description}")
        return company_description
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company description: {e}")
        return "Company Description"

async def extract_company_metadata_from_website(website_content: str, company_website: str) -> dict:
    """Extract additional company metadata from website content using AI."""
    try:
        prompt = f"""
        Проанализируй контент веб-сайта и извлеки следующую информацию о компании:
        
        ВЕБ-САЙТ: {company_website}
        КОНТЕНТ ВЕБ-САЙТА:
        {website_content}
        
        ИНСТРУКЦИИ:
        - Определи примерное количество сотрудников (если указано)
        - Определи, является ли компания франшизой или планирует открывать филиалы
        - Определи основные проблемы с онбордингом (если упоминаются)
        - Определи типы документов, которые использует компания
        - Определи приоритеты компании в области HR
        
        ФОРМАТ ОТВЕТА (только JSON):
        {{
            "employees": "количество сотрудников или Unknown",
            "franchise": "Yes/No/Unknown",
            "onboardingProblems": "основные проблемы или To be analyzed from website content",
            "documents": ["список типов документов или [\"Other\"]"],
            "documentsOther": "дополнительные документы или To be determined from website analysis",
            "priorities": ["список приоритетов или [\"Other\"]"],
            "priorityOther": "дополнительные приоритеты или To be determined from website analysis"
        }}
        
        ОТВЕТ (только JSON):
        """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response
        try:
            company_data = json.loads(response_text.strip())
            logger.info(f"📊 [WEBSITE SCRAPING] Extracted company metadata: {company_data}")
            return company_data
        except json.JSONDecodeError:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse JSON, using defaults")
            return {
                "employees": "Unknown",
                "franchise": "Unknown",
                "onboardingProblems": "To be analyzed from website content",
                "documents": ["Other"],
                "documentsOther": "To be determined from website analysis",
                "priorities": ["Other"],
                "priorityOther": "To be determined from website analysis"
            }
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting company metadata: {e}")
        return {
            "employees": "Unknown",
            "franchise": "Unknown",
            "onboardingProblems": "To be analyzed from website content",
            "documents": ["Other"],
            "documentsOther": "To be determined from website analysis",
            "priorities": ["Other"],
            "priorityOther": "To be determined from website analysis"
        }

async def create_audit_onepager(duckduckgo_summary, example_text_path, payload, language="ru"):
    try:
        with open(example_text_path, encoding="utf-8") as f:
            example_text = f.read()
    except Exception as e:
        logger.error(f"[AI-Audit] Error reading example: {e}")
        example_text = "(Example not found)"
    if not duckduckgo_summary or duckduckgo_summary.strip() == "" or duckduckgo_summary.strip().startswith("(Нет релевантных данных"):
        duck_info = "(DuckDuckGo не дал информации. Используй только анкету.)"
    else:
        duck_info = duckduckgo_summary
    # Language-specific instructions
    if language == "en":
        language_instruction = """
    CRITICAL LANGUAGE REQUIREMENT:
    - Generate ALL content EXCLUSIVELY in English
    - Use English terminology and professional business language
    - Maintain the same structure and formatting as the example
    - Translate all section headers, labels, and text to English
    - Use English business terminology for all concepts
    """
        system_message = "You are a professional AI assistant for generating training one-pager documents in English. Strictly follow ContentBuilder.ai rules and generate content exclusively in English."
    else:
        language_instruction = ""
        system_message = "Ты профессиональный AI-ассистент для генерации обучающих one-pager документов. Строго следуй правилам ContentBuilder.ai."

    prompt = f"""
    Сгенерируй AI-аудит (one-pager) для компании, используя ВСЮ информацию из анкеты пользователя и результаты интернет-исследования (DuckDuckGo).
    {language_instruction}

    ТВОЯ ЗАДАЧА:
    - СКОПИРУЙ ПРИМЕР НИЖЕ МАКСИМАЛЬНО ТОЧНО, ДОСЛОВНО.
    - Используй те же секции, тот же порядок, ту же длину, те же заголовки, те же таблицы, те же списки, те же абзацы, то же форматирование.
    - Если в примере есть таблица — в твоём ответе тоже должна быть таблица с тем же количеством строк и столбцов.
    - Если в примере 5 секций — в твоём ответе тоже должно быть 5 секций с теми же названиями и в том же порядке.
    - ЗАМЕНИ только данные, относящиеся к компании, на новые из анкеты и поиска.
    - НЕ сокращай, НЕ добавляй новых секций, НЕ меняй структуру, НЕ меняй форматирование, НЕ меняй количество строк, НЕ меняй количество столбцов в таблицах.
    - Если не уверен — лучше скопируй больше из примера, чем меньше.
    - Твой ответ должен быть на 90% буквальной копией примера, только с новыми данными.
    - Если DuckDuckGo не дал информации, используй только анкету.
    - Если в примере есть таблица, твоя таблица должна быть с тем же количеством строк и столбцов, только с новыми данными.
    - Если в примере есть абзац, твой ответ должен содержать такой же абзац на том же месте.
    - Если в примере есть список, твой ответ должен содержать такой же список с тем же количеством пунктов.
    - Не меняй ни одну структуру, даже если кажется, что это не подходит — просто замени данные.

    ---
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {payload.companyName}
    - Описание компании: {payload.companyDesc}
    - Сайт компании: {payload.companyWebsite}
    - Количество сотрудников: {payload.employees}
    - Франшиза: {payload.franchise}
    - Проблемы онбординга: {payload.onboardingProblems}
    - Документы: {', '.join(payload.documents)} {payload.documentsOther}
    - Приоритеты: {', '.join(payload.priorities)} {payload.priorityOther}

    ---
    РЕЗУЛЬТАТЫ ИНТЕРНЕТ-ИССЛЕДОВАНИЯ (DuckDuckGo):
    {duck_info}

    ---
    СКОПИРУЙ ПРИМЕР НИЖЕ, ЗАМЕНИВ ТОЛЬКО ДАННЫЕ О КОМПАНИИ:
    {example_text}

    Ответь только текстом one-pager по этим правилам, без пояснений.
    """
    logger.info(f"[AI-Audit] Final prompt (first 500 chars): {prompt[:500]}")
    client = get_openai_client()
    try:
        # Set a longer timeout for the OpenAI call
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4096,
            temperature=0.2,
            timeout=httpx.Timeout(180.0)  # 180 seconds
        )
    except Exception as e:
        logger.error(f"[AI-Audit] OpenAI generation error: {e}", exc_info=True)
        return {"error": f"Ошибка генерации AI-аудита: {e}"}
    
    result = response.choices[0].message.content
    logger.info(f"[AI-Audit] OpenAI result (first 500 chars): {result[:500]}")

    with open("custom_assistants/content_builder_ai.txt", encoding="utf-8") as f:
        assistant_instructions = f.read()

    # Compose the parsing prompt
    parsing_prompt = (
        f"{assistant_instructions}\n\n"
        "WIZARD_REQUEST\n"
        + json.dumps({
            "product": "Text Presentation",
            "microproduct": "One-Pager",
            "prompt": "Приведи этот текст к нужному формату one-pager для ContentBuilder.ai",
            "language": "ru",
            "fromText": True,
            "textMode": "context",
            "userText": result,
            "strict": True,
            "parseMode": "onepager"
        }, ensure_ascii=False)
    )

    # Call OpenAI again (use gpt-4o-mini or your preferred model)
    parsed_response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Ты профессиональный AI-ассистент для парсинга продуктов ContentBuilder.ai."},
            {"role": "user", "content": parsing_prompt}
        ],
        max_tokens=4096,
        temperature=0.1,
        timeout=httpx.Timeout(120.0)
    )
    parsed_markdown = parsed_response.choices[0].message.content

    parsed_json = await parse_ai_response_with_llm(
        ai_response=parsed_markdown,
        project_name=payload.companyName,
        target_model=TextPresentationDetails,  # or your one-pager model
        default_error_model_instance=TextPresentationDetails(textTitle="Parse error", contentBlocks=[]),
        dynamic_instructions=f"""
        You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
        This product is for general text like introductions, goal descriptions, etc.
        Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

        **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

        **Global Fields:**
        1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
            - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
            - Extract ONLY the title part (the last part after the last "**")
            - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
            - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
            - Do NOT include the course name or "Text Presentation" label in the title
            - If no clear pattern is found, use the first meaningful title or heading
        2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
        3.  `detectedLanguage` (string): e.g., "en", "ru".

        **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

        1.  **`type: "headline"`**
            * `level` (integer):
                * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
            * `text` (string): Headline text.
            * `iconName` (string, optional): Based on level and context as described above.
            * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

        2.  **`type: "paragraph"`**
            * `text` (string): Full paragraph text.
            * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

        3.  **`type: "bullet_list"`**
            * `items` (array of `ListItem`): Can be strings or other nested content blocks.
            * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

        4.  **`type: "numbered_list"`**
            * `items` (array of `ListItem`):
                * Can be simple strings for basic numbered points.
                * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                    * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                    * The `items` of this *inner* `bullet_list` should then be:
                        1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                        2. A `paragraph` block (for the main descriptive text).
                        3. Optionally, another `paragraph` block with `isRecommendation: true`.
                * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

        5.  **`type: "table"`**
            * `headers` (array of strings): The column headers for the table.
            * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
            * `caption` (string, optional): A short description or title for the table, if present in the source text.
            * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


        6.  **`type: "alert"`**
            *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
            *   `title` (string, optional): The title of the alert.
            *   `text` (string): The body text of the alert.
            *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

        7.  **`type: "section_break"`**
            * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

        **General Parsing Rules & Icon Names:**
        * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
        * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
        * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
        * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
        * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
        * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

        Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
            • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
            • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
            • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

        Return ONLY the JSON object.
        """,
        target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
    )
    return parsed_json


@app.post("/api/custom/ai-audit/generate")
async def generate_ai_audit_onepager(payload: AiAuditQuestionnaireRequest, request: Request, background_tasks: BackgroundTasks, pool: asyncpg.Pool = Depends(get_db_pool)):
    job_id = str(uuid.uuid4())
    set_progress(job_id, "Starting AI-Audit generation...")
    background_tasks.add_task(_run_audit_generation, payload, request, pool, job_id)
    return {"jobId": job_id}


@app.post("/api/custom/ai-audit/landing-page/generate")
async def generate_ai_audit_landing_page(payload: AiAuditQuestionnaireRequest, request: Request, background_tasks: BackgroundTasks, pool: asyncpg.Pool = Depends(get_db_pool)):
    job_id = str(uuid.uuid4())
    set_progress(job_id, "Starting AI-Audit landing page generation...")
    background_tasks.add_task(_run_landing_page_generation, payload, request, pool, job_id)
    return {"jobId": job_id}


@app.get("/api/custom/ai-audit/landing-page/{project_id}")
async def get_ai_audit_landing_page_data(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Get the dynamic landing page data for a specific AI audit project.
    """
    try:
        # 📊 LOG: Data retrieval request received
        logger.info(f"📥 [AUDIT DATA FLOW] Landing page data request for project ID: {project_id}")
        
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get the project data
        query = """
        SELECT microproduct_content, microproduct_name 
        FROM projects 
        WHERE id = $1 AND onyx_user_id = $2
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, project_id, onyx_user_id)
            
        if not row:
            logger.error(f"❌ [AUDIT DATA FLOW] Project {project_id} not found for user {onyx_user_id}")
            raise HTTPException(status_code=404, detail="Project not found")
        
        content = row["microproduct_content"]
        project_name = row["microproduct_name"]
        
        # 🎯 CRITICAL INSTRUMENTATION: Initial database read - what's actually in the DB
        logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] ==========================================")
        logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Project {project_id} - Raw database content retrieved")
        logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Content type: {type(content)}")
        logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Content is dict: {isinstance(content, dict)}")
        
        if content and isinstance(content, dict):
            logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] All keys in database: {list(content.keys())}")
            logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Has courseOutlineTableHeaders: {'courseOutlineTableHeaders' in content}")
            
            if 'courseOutlineTableHeaders' in content:
                logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] ✅ courseOutlineTableHeaders EXISTS in database!")
                logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Raw value: {content['courseOutlineTableHeaders']}")
                logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Type: {type(content['courseOutlineTableHeaders'])}")
            else:
                logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] ❌ courseOutlineTableHeaders NOT in database")
                logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] Available keys: {list(content.keys())}")
        else:
            logger.error(f"🎯 [TABLE HEADER INITIAL DB READ] ❌ Content is not a dict or is None!")
        
        logger.info(f"🎯 [TABLE HEADER INITIAL DB READ] ==========================================")
        
        # 📊 DETAILED LOGGING: Language preference in retrieved data
        language_from_db = content.get("language", "NOT_FOUND") if content else "NO_CONTENT"
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Retrieved from database - language: '{language_from_db}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Retrieved from database - content type: {type(content)}")
        
        # 📊 LOG: Raw data retrieved from database
        logger.info(f"💾 [AUDIT DATA FLOW] Retrieved project data from database:")
        logger.info(f"💾 [AUDIT DATA FLOW] - Project name: '{project_name}'")
        logger.info(f"💾 [AUDIT DATA FLOW] - Content keys: {list(content.keys()) if content else 'None'}")
        
        # Extract the dynamic data
        company_name = content.get("companyName", "Unknown Company")
        company_description = content.get("companyDescription", "Company description not available")
        
        # 📊 LOG: Extracted dynamic data
        logger.info(f"🔍 [AUDIT DATA FLOW] Extracted dynamic data:")
        logger.info(f"🔍 [AUDIT DATA FLOW] - Company name: '{company_name}'")
        logger.info(f"🔍 [AUDIT DATA FLOW] - Company description: '{company_description}'")
        
        # Extract job positions from the landing page data
        job_positions = content.get("jobPositions", [])
        
        # 📊 LOG: Job positions extraction process
        logger.info(f"💼 [AUDIT DATA FLOW] Starting job positions extraction:")
        logger.info(f"💼 [AUDIT DATA FLOW] - Job positions in content: {len(job_positions)} positions")
        
        if job_positions:
            # 📊 LOG: Job positions found in landing page data
            logger.info(f"💼 [AUDIT DATA FLOW] Job positions found in landing page data:")
            for i, position in enumerate(job_positions):
                logger.info(f"💼 [AUDIT DATA FLOW] - Position {i+1}: {position}")
        else:
            logger.info(f"💼 [AUDIT DATA FLOW] No job positions in landing page data, using default positions")
            # Fallback to default positions if none found
            job_positions = [
                {"title": "HVAC Technician", "description": "Installation and maintenance of heating, ventilation, and air conditioning systems", "icon": "👷"},
                {"title": "Electrician", "description": "Installation and maintenance of electrical systems", "icon": "⚡"},
                {"title": "Project Manager", "description": "Overseeing projects and coordinating teams", "icon": "📋"}
            ]
        
        # Extract workforce crisis data from the landing page data
        workforce_crisis = content.get("workforceCrisis", {})
        
        # 📊 LOG: Workforce crisis data extraction
        logger.info(f"📊 [AUDIT DATA FLOW] Workforce crisis data extraction:")
        logger.info(f"📊 [AUDIT DATA FLOW] - Workforce crisis data: {workforce_crisis}")
        
        # Extract course outline modules from the landing page data
        course_outline_modules = content.get("courseOutlineModules", [])
        
        # 📊 LOG: Course outline modules extraction
        logger.info(f"📚 [AUDIT DATA FLOW] Course outline modules extraction:")
        logger.info(f"📚 [AUDIT DATA FLOW] - Course outline modules count: {len(course_outline_modules)}")
        for i, module_title in enumerate(course_outline_modules):
            logger.info(f"📚 [AUDIT DATA FLOW] - Module {i+1}: {module_title}")
        
        # Extract course templates from the landing page data
        course_templates = content.get("courseTemplates", [])
        
        # 📊 LOG: Course templates extraction
        logger.info(f"🎓 [AUDIT DATA FLOW] Course templates extraction:")
        logger.info(f"🎓 [AUDIT DATA FLOW] - Course templates count: {len(course_templates)}")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [AUDIT DATA FLOW] - Template {i+1}: {template.get('title', 'Unknown')}")
        
        # 🎯 CRITICAL INSTRUMENTATION: Extract table headers from database
        course_outline_table_headers = content.get("courseOutlineTableHeaders", None)
        
        logger.info(f"🎯 [TABLE HEADER DB READ] ==========================================")
        logger.info(f"🎯 [TABLE HEADER DB READ] Project {project_id} - Reading table headers from database")
        logger.info(f"🎯 [TABLE HEADER DB READ] Database content keys: {list(content.keys())}")
        logger.info(f"🎯 [TABLE HEADER DB READ] Has courseOutlineTableHeaders in DB: {'courseOutlineTableHeaders' in content}")
        
        if course_outline_table_headers:
            logger.info(f"🎯 [TABLE HEADER DB READ] ✅ courseOutlineTableHeaders FOUND in database!")
            logger.info(f"🎯 [TABLE HEADER DB READ] Retrieved data: {json.dumps(course_outline_table_headers, indent=2)}")
            logger.info(f"🎯 [TABLE HEADER DB READ] - Lessons: '{course_outline_table_headers.get('lessons', 'NOT SET')}'")
            logger.info(f"🎯 [TABLE HEADER DB READ] - Assessment: '{course_outline_table_headers.get('assessment', 'NOT SET')}'")
            logger.info(f"🎯 [TABLE HEADER DB READ] - Duration: '{course_outline_table_headers.get('duration', 'NOT SET')}'")
            logger.info(f"🎯 [TABLE HEADER DB READ] This data WILL BE sent to frontend")
        else:
            logger.info(f"🎯 [TABLE HEADER DB READ] ❌ courseOutlineTableHeaders NOT FOUND in database")
            logger.info(f"🎯 [TABLE HEADER DB READ] Frontend will use default localized values")
        logger.info(f"🎯 [TABLE HEADER DB READ] ==========================================")
        
        # 📊 LOG: Final response data structure
        response_data = {
            "projectId": project_id,
            "projectName": project_name,
            "companyName": company_name,
            "companyDescription": company_description,
            "jobPositions": job_positions,
            "workforceCrisis": workforce_crisis,
            "courseOutlineModules": course_outline_modules,
            "courseTemplates": course_templates,
            "language": content.get("language", "ru"),  # 🔧 FIX: Include language parameter in response
            "courseOutlineTableHeaders": course_outline_table_headers  # 🔧 CRITICAL FIX: Include table headers in response
        }
        
        logger.info(f"📤 [AUDIT DATA FLOW] Final response data:")
        logger.info(f"📤 [AUDIT DATA FLOW] - Project ID: {response_data['projectId']}")
        logger.info(f"📤 [AUDIT DATA FLOW] - Project Name: '{response_data['projectName']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Company Name: '{response_data['companyName']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Company Description: '{response_data['companyDescription']}'")
        logger.info(f"📤 [AUDIT DATA FLOW] - Job Positions Count: {len(response_data['jobPositions'])}")
        logger.info(f"📤 [AUDIT DATA FLOW] - Workforce Crisis Data: {response_data['workforceCrisis']}")
        
        # 🎯 CRITICAL LOGGING: Confirm table headers in response
        logger.info(f"🎯 [TABLE HEADER RESPONSE] ==========================================")
        logger.info(f"🎯 [TABLE HEADER RESPONSE] Project {project_id} - Including courseOutlineTableHeaders in response")
        logger.info(f"🎯 [TABLE HEADER RESPONSE] Response includes courseOutlineTableHeaders: {'courseOutlineTableHeaders' in response_data}")
        if response_data.get('courseOutlineTableHeaders'):
            logger.info(f"🎯 [TABLE HEADER RESPONSE] ✅ Sending table headers to frontend!")
            logger.info(f"🎯 [TABLE HEADER RESPONSE] Data: {json.dumps(response_data['courseOutlineTableHeaders'], indent=2)}")
        else:
            logger.info(f"🎯 [TABLE HEADER RESPONSE] ❌ No table headers to send (using None)")
        logger.info(f"🎯 [TABLE HEADER RESPONSE] ==========================================")
        
        # 📊 DETAILED LOGGING: Language parameter in response
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Response data - language: '{response_data['language']}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Response data keys: {list(response_data.keys())}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting landing page data: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")



# Audit sharing models
class ShareAuditRequest(BaseModel):
    expires_in_days: Optional[int] = 30  # Default 30 days expiration

class ShareAuditResponse(BaseModel):
    share_token: str
    public_url: str
    expires_at: datetime

@app.post("/api/custom/audits/{audit_id}/share")
async def share_audit(
    audit_id: int, 
    request_data: ShareAuditRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
) -> ShareAuditResponse:
    """
    Generate a share token for an audit project, making it publicly accessible.
    """
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Verify the audit belongs to the user and is an audit project
        query = """
        SELECT id, project_name, microproduct_content 
        FROM projects 
        WHERE id = $1 AND onyx_user_id = $2 
        AND (project_name LIKE 'AI-Аудит%' OR project_name LIKE '%Landing Page%')
        """
        
        async with pool.acquire() as conn:
            audit = await conn.fetchrow(query, audit_id, onyx_user_id)
            
        if not audit:
            raise HTTPException(status_code=404, detail="Audit not found or access denied")
        
        # Generate secure share token
        share_token = str(uuid.uuid4())
        
        # Calculate expiration date
        expires_at = datetime.now(timezone.utc)
        if request_data.expires_in_days:
            from datetime import timedelta
            expires_at += timedelta(days=request_data.expires_in_days)
        else:
            from datetime import timedelta
            expires_at += timedelta(days=30)  # Default 30 days
        
        # Update the project with sharing information
        update_query = """
        UPDATE projects 
        SET share_token = $1, is_public = TRUE, shared_at = NOW(), expires_at = $2
        WHERE id = $3
        """
        
        async with pool.acquire() as conn:
            await conn.execute(update_query, share_token, expires_at, audit_id)
        
        # Generate public URL - use the correct public domain for sharing
        # Check if we have a public domain override, otherwise detect from request
        public_domain = os.environ.get("PUBLIC_FRONTEND_URL")
        
        if not public_domain:
            # Try to detect the public domain from the request headers
            host = request.headers.get("host", "")
            if "dev4.contentbuilder.ai" in host:
                public_domain = "https://dev4.contentbuilder.ai/custom-projects-ui"
            elif host and not host.startswith("custom_frontend"):
                # Use the host from the request with https
                protocol = "https" if request.headers.get("x-forwarded-proto") == "https" else "http"
                public_domain = f"{protocol}://{host}"
                if "/custom-projects-ui" not in public_domain:
                    public_domain += "/custom-projects-ui"
            else:
                # Fallback to environment variable or localhost
                frontend_domain = os.environ.get("CUSTOM_FRONTEND_URL", "http://localhost:3001")
                public_domain = frontend_domain
        
        public_url = f"{public_domain}/public/audit/{share_token}"
        
        logger.info(f"🔗 [AUDIT SHARING] Created share token for audit {audit_id}: {share_token}")
        
        return ShareAuditResponse(
            share_token=share_token,
            public_url=public_url,
            expires_at=expires_at
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error sharing audit: {e}")
        raise HTTPException(status_code=500, detail="Failed to share audit")

@app.get("/api/custom/public/audits/{share_token}")
async def get_public_audit(
    share_token: str,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Get audit data by share token for public access (no authentication required).
    """
    try:
        # Query for public audit by share token
        query = """
        SELECT id, project_name, microproduct_content, shared_at, expires_at, is_public
        FROM projects 
        WHERE share_token = $1 AND is_public = TRUE
        """
        
        async with pool.acquire() as conn:
            audit = await conn.fetchrow(query, share_token)
            
        if not audit:
            raise HTTPException(status_code=404, detail="Shared audit not found")
        
        # Check if the share has expired
        if audit["expires_at"] and audit["expires_at"] < datetime.now(timezone.utc):
            raise HTTPException(status_code=410, detail="Shared audit link has expired")
        
        content = audit["microproduct_content"]
        project_name = audit["project_name"]
        
        # Extract the dynamic data similar to the private endpoint
        company_name = content.get("companyName", "Unknown Company")
        company_description = content.get("companyDescription", "Company description not available")
        job_positions = content.get("jobPositions", [])
        workforce_crisis = content.get("workforceCrisis", {})
        course_outline_modules = content.get("courseOutlineModules", [])
        course_templates = content.get("courseTemplates", [])
        course_outline_table_headers = content.get("courseOutlineTableHeaders", None)  # 🔧 CRITICAL FIX: Extract table headers
        
        # 🎯 INSTRUMENTATION: Log table headers for public audits
        logger.info(f"🎯 [PUBLIC AUDIT TABLE HEADERS] Project {audit['id']} - courseOutlineTableHeaders: {course_outline_table_headers}")
        
        # Return the same structure as the private endpoint but without sensitive info
        response_data = {
            "projectId": audit["id"],
            "projectName": project_name,
            "companyName": company_name,
            "companyDescription": company_description,
            "jobPositions": job_positions,
            "workforceCrisis": workforce_crisis,
            "courseOutlineModules": course_outline_modules,
            "courseTemplates": course_templates,
            "language": content.get("language", "ru"),
            "courseOutlineTableHeaders": course_outline_table_headers,  # 🔧 CRITICAL FIX: Include table headers in response
            "isPublicView": True,  # Flag to indicate this is a public view
            "sharedAt": audit["shared_at"].isoformat() if audit["shared_at"] else None
        }
        
        logger.info(f"🌐 [PUBLIC AUDIT ACCESS] Served public audit with token: {share_token}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting public audit: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve shared audit")


async def _run_audit_generation(payload, request, pool, job_id):
    try:
        set_progress(job_id, "Scraping company website...")
        # Scrape company data from website
        scraped_data = await scrape_company_data_from_website(payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit] Scraped company data: {scraped_data.companyName}")
        
        set_progress(job_id, "Researching additional company info...")
        # Get additional research data using scraped company name and description
        duckduckgo_summary = await company_research(scraped_data.companyName, scraped_data.companyDesc, payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit] Research summary: {duckduckgo_summary[:300]}")

        set_progress(job_id, "Generating first one-pager...")
        # Create a combined payload with scraped data for the prompt
        combined_payload = type('CombinedPayload', (), {
            'companyName': scraped_data.companyName,
            'companyDesc': scraped_data.companyDesc,
            'companyWebsite': payload.companyWebsite,
            'employees': scraped_data.employees,
            'franchise': scraped_data.franchise,
            'onboardingProblems': scraped_data.onboardingProblems,
            'documents': scraped_data.documents,
            'documentsOther': scraped_data.documentsOther,
            'priorities': scraped_data.priorities,
            'priorityOther': scraped_data.priorityOther
        })()
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/First-one-pager.txt", combined_payload, payload.language)

        onyx_user_id = await get_current_onyx_user_id(request)

        # After you get the parsed content from the AI parser:
        project_id = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {scraped_data.companyName}",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id}")

        set_progress(job_id, "Researching open positions...")
        positions = extract_open_positions_from_table(parsed_json)

        results = []
        for position in positions:
            set_progress(job_id, f"Generating onboarding for '{position.get('Позиция', 'New Position')}'")
            project = await generate_and_finalize_course_outline_for_position(
                scraped_data.companyName, position, onyx_user_id, pool, request
            )
            results.append(project)

        logger.info(f"[AI-Audit] Created {len(results)} course outlines for positions")

        set_progress(job_id, "Generating closing one-pager...")
        parsed_json = await create_audit_onepager(duckduckgo_summary, "custom_assistants/AI-Audit/Second-one-pager.txt", combined_payload)

        # After you get the parsed content from the AI parser:
        project_id_2 = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит: {scraped_data.companyName} (2)",
            microproduct_content=parsed_json.model_dump(mode='json', exclude_none=True),
            chat_session_id=None
        )

        logger.info(f"[AI-Audit] Successfully created project with ID: {project_id_2}")

        set_progress(job_id, "Finalizing and saving to folder...")
        all_project_ids = [project_id] + [p.id for p in results] + [project_id_2]

        # 1. Create a new folder
        folder_id = await create_audit_folder(pool, onyx_user_id, scraped_data.companyName)

        # 2. Assign all projects to this folder
        await assign_projects_to_folder(pool, folder_id, all_project_ids)

        set_progress(job_id, "AI-Audit complete!")
        logger.info(f"[AI-Audit] Finished the AI-Audit Generation")
        return {
            "id": project_id,
            "id_2": project_id_2,
            "name": f"AI-Аудит: {scraped_data.companyName}",
            "folderId": folder_id
        }
    
    except Exception as e:
        set_progress(job_id, f"Error: {str(e)}")


async def extract_company_name_from_data(duckduckgo_summary: str, payload) -> str:
    """
    Extract the company name from scraped data using AI.
    Returns only the company name as a string.
    """
    prompt = f"""
    Извлеки ТОЛЬКО название компании из предоставленных данных.
    
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {getattr(payload, 'companyName', 'Company Name')}
    - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
    - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
    
    ДАННЫЕ ИЗ ИНТЕРНЕТА:
    {duckduckgo_summary}
    
    ТВОЯ ЗАДАЧА:
    - Верни ТОЛЬКО название компании
    - Используй наиболее точное и официальное название
    - Если есть несколько вариантов, выбери самый короткий и официальный
    - НЕ добавляй никаких дополнительных слов или объяснений
    - НЕ используй кавычки или другие символы
    
    ОТВЕТ (только название компании):
    """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        company_name = response_text.strip()
        if not company_name:
            company_name = getattr(payload, 'companyName', 'Company Name')  # Fallback to original name
        
        logger.info(f"[AI-Audit Landing Page] Extracted company name: {company_name}")
        return company_name
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting company name: {e}")
        return getattr(payload, 'companyName', 'Company Name')  # Fallback to original name


async def generate_company_description_from_data(duckduckgo_summary: str, payload) -> str:
    """
    Generate a company description from scraped data using AI.
    Returns a concise description similar to the original subtitle format.
    """
    prompt = f"""
    Создай краткое описание компании в стиле: "Компания предоставляющий услуги по [основные услуги]. [дополнительная информация о компании]"
    
    ДАННЫЕ АНКЕТЫ:
    - Название компании: {getattr(payload, 'companyName', 'Company Name')}
    - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
    - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
    
    ДАННЫЕ ИЗ ИНТЕРНЕТА:
    {duckduckgo_summary}
    
    ТВОЯ ЗАДАЧА:
    - Создай описание в том же стиле, что и пример: "Компания предоставляющий услуги по установке и обслуживанию систем HVAC, электрики, солнечных панелей, а также бытовой и коммерческой техники. Обеспечивая полный цикл инженерных решений"
    - Используй информацию из интернета для определения основных услуг компании
    - Сделай описание кратким (1-2 предложения)
    - Начни с "Компания предоставляющий услуги по"
    - НЕ добавляй кавычки или другие символы
    - Пиши на русском языке
    
    ОТВЕТ (только описание компании):
    """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        company_description = response_text.strip()
        if not company_description:
            company_description = getattr(payload, 'companyDesc', 'Company Description')  # Fallback to original description
        
        logger.info(f"[AI-Audit Landing Page] Generated company description: {company_description}")
        return company_description
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating company description: {e}")
        return getattr(payload, 'companyDesc', 'Company Description')  # Fallback to original description


async def generate_ai_image_for_job_position(job_title: str, company_name: str) -> str:
    """
    Generate an AI image for a specific job position using Google Gemini.
    """
    try:
        # Create a professional prompt for the job position with enhanced framing
        prompt = f"""A professional photograph of a {job_title} actively working at {company_name}. 
    
        SCENE: The person is engaged in their typical work activities in an authentic workplace environment appropriate for a {job_title}. Show them using professional tools, equipment, or technology relevant to their role. The composition should capture both the person (from waist up or full body) and their work environment.

        ACTIVITY: Include specific work processes - for example:
        - If barista: preparing coffee, operating espresso machine, arranging cups
        - If programmer: coding at multiple monitors, reviewing code, collaborating with team
        - If mechanic: working on equipment, using tools, diagnostic work
        - If teacher: conducting lesson, using whiteboard, interacting with materials
        - If sales representative: presenting products, meeting with clients, demonstrating features
        - If nurse: caring for patients, using medical equipment, documenting care

        ENVIRONMENT: Authentic workplace setting that matches the {job_title} role - not just a generic office. Include relevant background elements, tools, equipment, and work materials that tell the story of what this person does.

        STYLE: High-quality professional photography with good lighting that shows both the person and their work context. The person should be wearing appropriate work attire/uniform for their specific role.

        COMPOSITION: Environmental portrait style that captures the essence of the job, not just a headshot."""

        # Use wider dimensions for course template images to better fit the container
        width, height = 1792, 1024
        
        # Create the request
        request = AIImageGenerationRequest(
            prompt=prompt,
            width=width,
            height=height,
            quality="standard",
            style="vivid",
            model="gemini-2.5-flash-image-preview"
        )
        
        # Generate the image
        result = await generate_ai_image(request)
        
        logger.info(f"🎨 [COURSE IMAGE] Generated image for {job_title}: {result['file_path']}")
        return result['file_path']
        
    except Exception as e:
        logger.error(f"❌ [COURSE IMAGE] Error generating image for {job_title}: {e}")
        # Return a fallback image path
        return f"/custom-projects-ui/images/audit-section-5-job-1-mobile.png"

async def generate_course_description_for_position(job_title: str, company_name: str, duckduckgo_summary: str, language: str = "ru") -> str:
    """
    Generate a concise course description for a specific job position.
    """
    try:
        if language == "en":
            prompt = f"""Create a brief course description for the position "{job_title}" at {company_name}.

COMPANY DATA:
{duckduckgo_summary}

CRITICAL REQUIREMENTS:
- Description must be VERY short - maximum 80 characters (not 100!)
- Use ONLY simple format: "Training in [skills] for [short position name]"
- Avoid long words and unnecessary details
- DO NOT use complex constructions

GOOD EXAMPLES (short):
- "Training in data analysis and visualization for analyst."
- "Training in system design for engineer."
- "Training in sales techniques for manager."

BAD EXAMPLES (too long):
- "Training in effective sales strategies and customer relationship management for sales manager"
- "Training in effective communication and problem solving for customer service specialists"

SHORTENING RULES:
- "sales manager" → "manager"
- "customer service specialist" → "consultant"
- "marketing specialist" → "marketer"
- "data analyst" → "analyst"

RESPONSE (course description only, maximum 80 characters):"""
        elif language == "es":
            prompt = f"""Crea una breve descripción del curso para la posición "{job_title}" en {company_name}.

DATOS DE LA EMPRESA:
{duckduckgo_summary}

REQUISITOS CRÍTICOS:
- La descripción debe ser MUY corta - máximo 80 caracteres (¡no 100!)
- Usa SOLO formato simple: "Capacitación en [habilidades] para [nombre corto de posición]"
- Evita palabras largas y detalles innecesarios
- NO uses construcciones complejas

BUENOS EJEMPLOS (cortos):
- "Capacitación en análisis de datos y visualización para analista."
- "Capacitación en diseño de sistemas para ingeniero."
- "Capacitación en técnicas de ventas para gerente."

MALOS EJEMPLOS (muy largos):
- "Capacitación en estrategias efectivas de ventas y gestión de relaciones con clientes para gerente de ventas"
- "Capacitación en comunicación efectiva y resolución de problemas para especialistas en atención al cliente"

REGLAS DE ABREVIACIÓN:
- "gerente de ventas" → "gerente"
- "especialista en atención al cliente" → "consultor"
- "especialista en marketing" → "marketero"
- "analista de datos" → "analista"

RESPUESTA (solo descripción del curso, máximo 80 caracteres):"""
        elif language == "ua":
            prompt = f"""Створіть короткий опис курсу для посади "{job_title}" в компанії {company_name}.

ДАНІ ПРО КОМПАНІЮ:
{duckduckgo_summary}

КРИТИЧНІ ВИМОГИ:
- Опис має бути ДУЖЕ коротким - максимум 80 символів (не 100!)
- Використовуйте ЛИШЕ простий формат: "Навчання [навичкам] для [скорочена назва посади]"
- Уникайте довгих слів та зайвих деталей
- НЕ використовуйте складні конструкції

ХОРОШІ ПРИКЛАДИ (короткі):
- "Навчання аналізу даних та візуалізації для аналітика."
- "Навчання проектуванню систем для інженера."
- "Навчання продажам для менеджера."

ПОГАНІ ПРИКЛАДИ (занадто довгі):
- "Навчання ефективним стратегіям продажів та управлінню клієнтськими відносинами для менеджера"
- "Навчання ефективному спілкуванню та вирішенню проблем для фахівців з обслуговування"

ПРАВИЛА СКОРОЧЕННЯ:
- "менеджер з продажів" → "менеджера"
- "фахівець з обслуговування клієнтів" → "консультанта"
- "спеціаліст з маркетингу" → "маркетолога"
- "аналізатор даних" → "аналітика"

ВІДПОВІДЬ (тільки опис курсу, максимум 80 символів):"""
        else:  # Russian
            prompt = f"""Создай краткое описание курса обучения для позиции "{job_title}" в компании {company_name}.

ДАННЫЕ О КОМПАНИИ:
{duckduckgo_summary}

КРИТИЧЕСКИЕ ТРЕБОВАНИЯ:
- Описание должно быть ОЧЕНЬ коротким - максимум 80 символов (не 100!)
- Используй ТОЛЬКО простой формат: "Обучение [навыкам] для [сокращенное название позиции]"
- Избегай длинных слов и лишних деталей
- НЕ используй сложные конструкции

ХОРОШИЕ ПРИМЕРЫ (короткие):
- "Обучение анализу данных и визуализации для аналитика."
- "Обучение проектированию систем для инженера."
- "Обучение продажам для менеджера."

ПЛОХИЕ ПРИМЕРЫ (слишком длинные):
- "Обучение эффективным стратегиям продаж и управлению клиентскими отношениями для менеджера"
- "Обучение эффективному общению и решению проблем для специалистов по обслуживанию"

ПРАВИЛА СОКРАЩЕНИЯ:
- "менеджер по продажам" → "менеджера"
- "специалист по обслуживанию клиентов" → "консультанта"
- "специалист по маркетингу" → "маркетолога"
- "аналитик данных" → "аналитика"

ОТВЕТ (только описание курса, максимум 80 символов):"""
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean up the response
        description = response_text.strip()
        if len(description) > 80:
            description = description[:77] + "..."
            
        return description
        
    except Exception as e:
        logger.error(f"❌ [COURSE DESCRIPTION] Error generating course description for {job_title}: {e}")
        if language == "en":
            return f"Training in key skills for {job_title} position."
        elif language == "es":
            return f"Capacitación en habilidades clave para la posición {job_title}."
        elif language == "ua":
            return f"Навчання ключовим навичкам для посади {job_title}."
        else:  # Russian
            return f"Обучение ключевым навыкам для позиции {job_title}."

async def generate_course_outline_for_landing_page(duckduckgo_summary: str, job_positions: list, payload, language: str = "ru") -> list:
    """
    Generate course outline data for the landing page modules section.
    Returns a list of modules with titles and lessons extracted from the first job position's course outline.
    """
    try:
        if not job_positions:
            logger.warning("[COURSE OUTLINE] No job positions available for course outline generation")
            return []
        
        # Use the first job position for course outline generation
        first_position = job_positions[0]
        position_title = first_position.get('title', 'Сотрудник')
        
        logger.info(f"[COURSE OUTLINE] Generating course outline for position: {position_title}")
        
        # Build the prompt for course outline generation
        if language == "en":
            prompt = f"""Create a detailed course outline 'Onboarding for {position_title}' for new employees in this position at '{getattr(payload, 'companyName', 'Company Name')}'.

COMPANY CONTEXT:
- Company Name: {getattr(payload, 'companyName', 'Company Name')}
- Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
- Position: {position_title}
- Additional company information: {duckduckgo_summary}

COURSE REQUIREMENTS:
- The course should be specific to company {getattr(payload, 'companyName', 'Company Name')} and position {position_title}
- Content should reflect real tasks and responsibilities of this position in this company
- Consider industry specifics and corporate culture
- Create EXACTLY 4 modules with UNIQUE names
- Each module should have FROM 5 TO 7 lessons
- Module and lesson names should be CREATIVE and DIVERSE
- Avoid repetitive formulations
- Each lesson should be specific and practical for this position
- DO NOT add module numbers in titles (e.g., 'Module 1:', 'Module 2:', etc.)
- Use only descriptive module names without prefixes
- Generate ALL content EXCLUSIVELY in English

RESPONSE FORMAT (JSON only):
[
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}},
    {{"title": "Module Title", "lessons": ["Lesson 1", "Lesson 2", "Lesson 3", "Lesson 4", "Lesson 5"]}}
]

RESPONSE (JSON only):"""
        elif language == "es":
            prompt = f"""Crea un esquema detallado del curso 'Incorporación para {position_title}' para nuevos empleados en esta posición en '{getattr(payload, 'companyName', 'Company Name')}'.

CONTEXTO DE LA EMPRESA:
- Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
- Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
- Posición: {position_title}
- Información adicional de la empresa: {duckduckgo_summary}

REQUISITOS DEL CURSO:
- El curso debe ser específico para la empresa {getattr(payload, 'companyName', 'Company Name')} y la posición {position_title}
- El contenido debe reflejar las tareas y responsabilidades reales de esta posición en esta empresa
- Considera las especificidades de la industria y la cultura corporativa
- Crea EXACTAMENTE 4 módulos con nombres ÚNICOS
- Cada módulo debe tener DE 5 A 7 lecciones
- Los nombres de módulos y lecciones deben ser CREATIVOS y DIVERSOS
- Evita formulaciones repetitivas
- Cada lección debe ser específica y práctica para esta posición
- NO agregues números de módulos en los títulos (ej., 'Módulo 1:', 'Módulo 2:', etc.)
- Usa solo nombres descriptivos de módulos sin prefijos
- Genera TODO el contenido EXCLUSIVAMENTE en español

FORMATO DE RESPUESTA (solo JSON):
[
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}},
    {{"title": "Título del Módulo", "lessons": ["Lección 1", "Lección 2", "Lección 3", "Lección 4", "Lección 5"]}}
]

RESPUESTA (solo JSON):"""
        elif language == "ua":
            prompt = f"""Створіть детальний план курсу 'Онбординг для посади {position_title}' для нових співробітників на цій посаді в компанії '{getattr(payload, 'companyName', 'Company Name')}'.

КОНТЕКСТ КОМПАНІЇ:
- Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
- Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
- Посада: {position_title}
- Додаткова інформація про компанію: {duckduckgo_summary}

ВИМОГИ ДО КУРСУ:
- Курс повинен бути специфічним для компанії {getattr(payload, 'companyName', 'Company Name')} та посади {position_title}
- Зміст повинен відображати реальні завдання та обов'язки цієї посади в цій компанії
- Враховуйте специфіку галузі та корпоративну культуру
- Створіть РІВНО 4 модулі з УНІКАЛЬНИМИ назвами
- У кожному модулі має бути ВІД 5 ДО 7 уроків
- Назви модулів та уроків мають бути КРЕАТИВНИМИ та РІЗНОМАНІТНИМИ
- Уникайте повторюваних формулювань
- Кожен урок має бути конкретним та практичним для цієї посади
- НЕ додавайте номери модулів у назви (наприклад, 'Модуль 1:', 'Модуль 2:' тощо)
- Використовуйте лише описові назви модулів без префіксів
- Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою

ФОРМАТ ВІДПОВІДІ (тільки JSON):
[
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}},
    {{"title": "Назва модуля", "lessons": ["Урок 1", "Урок 2", "Урок 3", "Урок 4", "Урок 5"]}}
]

ВІДПОВІДЬ (тільки JSON):"""
        else:
            wizard_request = {
                "product": "Course Outline",
                "prompt": (
                    f"Создай детальный курс аутлайн 'Онбординг для должности {position_title}' для новых сотрудников этой должности в компании '{getattr(payload, 'companyName', 'Company Name')}'. \n"
                    f"КОНТЕКСТ КОМПАНИИ:\n"
                    f"- Название компании: {getattr(payload, 'companyName', 'Company Name')}\n"
                    f"- Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}\n"
                    f"- Должность: {position_title}\n"
                    f"- Дополнительная информация о компании: {duckduckgo_summary}\n\n"
                    f"ТРЕБОВАНИЯ К КУРСУ:\n"
                    f"- Курс должен быть специфичным для компании {getattr(payload, 'companyName', 'Company Name')} и должности {position_title}\n"
                    f"- Содержание должно отражать реальные задачи и обязанности этой должности в данной компании\n"
                    f"- Учитывай специфику отрасли и корпоративную культуру компании\n"
                    f"- Создай РОВНО 4 модуля с УНИКАЛЬНЫМИ названиями\n"
                    f"- В каждом модуле должно быть ОТ 5 ДО 7 уроков\n"
                    f"- Названия модулей и уроков должны быть КРЕАТИВНЫМИ и РАЗНООБРАЗНЫМИ\n"
                    f"- Избегай повторяющихся формулировок\n"
                    f"- Каждый урок должен быть конкретным и практичным для данной должности\n"
                    f"- НЕ добавляй номера модулей в названия (например, 'Модуль 1:', 'Модуль 2:' и т.д.)\n"
                    f"- Используй только описательные названия модулей без префиксов\n"
                ),
                "modules": 4,
                "lessonsPerModule": "5-7",
                "language": language
            }
        
        # Generate the course outline
        outline_text = await stream_openai_response_direct(prompt, model=LLM_DEFAULT_MODEL)
        
        # Parse the outline text to extract modules with lessons
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = outline_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            parsed_outline = json.loads(cleaned_response)
            
            if not isinstance(parsed_outline, list):
                raise ValueError("Response is not a list")
                
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"[COURSE OUTLINE] Failed to parse JSON response: {e}")
            logger.warning(f"[COURSE OUTLINE] Raw response was: '{outline_text}'")
            # Fall back to default modules
            parsed_outline = []
        
        # Extract modules with lessons
        course_modules = []
        for i, module in enumerate(parsed_outline):
            if i < 4:  # Limit to 4 modules as per UI design
                module_data = {
                    "title": module.get('title', f'Модуль {i+1}'),
                    "lessons": module.get('lessons', [])
                }
                course_modules.append(module_data)
                logger.info(f"[COURSE OUTLINE] Module {i+1}: {module_data['title']} with {len(module_data['lessons'])} lessons")
                for j, lesson in enumerate(module_data['lessons']):
                    logger.info(f"[COURSE OUTLINE] - Lesson {j+1}: {lesson}")
        
        # Ensure we have exactly 4 modules (pad with default modules if needed)
        while len(course_modules) < 4:
            if language == "en":
                course_modules.append({
                    "title": f'Module {len(course_modules) + 1}',
                    "lessons": []
                })
            elif language == "es":
                course_modules.append({
                    "title": f'Módulo {len(course_modules) + 1}',
                    "lessons": []
                })
            elif language == "ua":
                course_modules.append({
                    "title": f'Модуль {len(course_modules) + 1}',
                    "lessons": []
                })
            else:  # Russian
                course_modules.append({
                    "title": f'Модуль {len(course_modules) + 1}',
                    "lessons": []
                })
        
        logger.info(f"[COURSE OUTLINE] Generated {len(course_modules)} modules with lessons for landing page")
        return course_modules
        
    except Exception as e:
        logger.error(f"[COURSE OUTLINE] Error generating course outline for landing page: {e}")
        # Return default modules as fallback
        if language == "en":
            return [
                {
                    "title": "Company Introduction and Corporate Culture",
                    "lessons": ["Company Overview", "Corporate Values and Standards", "Organizational Structure", "Policies and Procedures", "Communication Systems"]
                },
                {
                    "title": "Work Fundamentals and Professional Skills",
                    "lessons": ["Technical Job Requirements", "Work Processes and Procedures", "Tools and Systems", "Work Quality and Standards", "Safety and Compliance"]
                },
                {
                    "title": "Team and Customer Interaction",
                    "lessons": ["Teamwork", "Customer Service", "Conflict Management", "Effective Communication", "Feedback and Development"]
                },
                {
                    "title": "Development and Career Growth",
                    "lessons": ["Goal Setting", "Development Planning", "Performance Evaluation", "Growth Opportunities", "Continuous Learning"]
                }
            ]
        elif language == "es":
            return [
                {
                    "title": "Introducción a la Empresa y Cultura Corporativa",
                    "lessons": ["Visión General de la Empresa", "Valores y Estándares Corporativos", "Estructura Organizacional", "Políticas y Procedimientos", "Sistemas de Comunicación"]
                },
                {
                    "title": "Fundamentos del Trabajo y Habilidades Profesionales",
                    "lessons": ["Requisitos Técnicos del Puesto", "Procesos y Procedimientos de Trabajo", "Herramientas y Sistemas", "Calidad del Trabajo y Estándares", "Seguridad y Cumplimiento"]
                },
                {
                    "title": "Interacción con el Equipo y Clientes",
                    "lessons": ["Trabajo en Equipo", "Servicio al Cliente", "Gestión de Conflictos", "Comunicación Efectiva", "Retroalimentación y Desarrollo"]
                },
                {
                    "title": "Desarrollo y Crecimiento Profesional",
                    "lessons": ["Establecimiento de Objetivos", "Planificación del Desarrollo", "Evaluación del Rendimiento", "Oportunidades de Crecimiento", "Aprendizaje Continuo"]
                }
            ]
        elif language == "ua":
            return [
                {
                    "title": "Введення в компанію та корпоративну культуру",
                    "lessons": ["Огляд компанії", "Корпоративні цінності та стандарти", "Організаційна структура", "Політики та процедури", "Системи комунікації"]
                },
                {
                    "title": "Основи роботи та професійні навички",
                    "lessons": ["Технічні вимоги до посади", "Робочі процеси та процедури", "Інструменти та системи", "Якість роботи та стандарти", "Безпека та відповідність"]
                },
                {
                    "title": "Взаємодія з командою та клієнтами",
                    "lessons": ["Робота в команді", "Обслуговування клієнтів", "Управління конфліктами", "Ефективна комунікація", "Зворотний зв'язок та розвиток"]
                },
                {
                    "title": "Розвиток та кар'єрне зростання",
                    "lessons": ["Постановка цілей", "Планування розвитку", "Оцінка продуктивності", "Можливості зростання", "Безперервне навчання"]
                }
            ]
        else:
            return [
                {
                    "title": "Введение в компанию и корпоративную культуру",
                    "lessons": ["Знакомство с компанией", "Корпоративные ценности и стандарты", "Организационная структура", "Политики и процедуры", "Системы коммуникации"]
                },
                {
                    "title": "Основы работы и профессиональные навыки",
                    "lessons": ["Технические требования к должности", "Рабочие процессы и процедуры", "Инструменты и системы", "Качество работы и стандарты", "Безопасность и соответствие"]
                },
                {
                    "title": "Взаимодействие с командой и клиентами",
                    "lessons": ["Работа в команде", "Обслуживание клиентов", "Управление конфликтами", "Эффективная коммуникация", "Обратная связь и развитие"]
                },
                {
                    "title": "Развитие и карьерный рост",
                    "lessons": ["Постановка целей", "Планирование развития", "Оценка производительности", "Возможности роста", "Непрерывное обучение"]
                }
            ]


async def generate_course_templates(duckduckgo_summary: str, job_positions: list, payload, course_outline_modules: list = None, language: str = "ru") -> list:
    """
    Generate course templates by combining real job positions with AI-generated positions.
    Returns exactly 6 course templates with dynamic content.
    """
    try:
        logger.info(f"🎓 [COURSE TEMPLATES] Starting course templates generation")
        logger.info(f"🎓 [COURSE TEMPLATES] Real job positions: {len(job_positions)}")
        
        # Calculate total modules and lessons from course outline
        total_modules = 0
        total_lessons = 0
        if course_outline_modules:
            total_modules = len(course_outline_modules)
            total_lessons = sum(len(module.get('lessons', [])) for module in course_outline_modules)
            logger.info(f"🎓 [COURSE TEMPLATES] Course outline data: {total_modules} modules, {total_lessons} lessons")
        
        # Start with real job positions
        course_templates = []
        
        # Add real job positions first
        for i, position in enumerate(job_positions[:6]):  # Take up to 6 real positions
            job_title = position.get("title", f"Position {i+1}")
            
            # Generate proper course description for scraped positions
            course_description = await generate_course_description_for_position(
                job_title, 
                getattr(payload, 'companyName', 'Company Name'), 
                duckduckgo_summary,
                language
            )
            
            # Generate AI image for the job position
            logger.info(f"🎨 [COURSE TEMPLATES] Generating AI image for position: {job_title}")
            ai_image_path = await generate_ai_image_for_job_position(
                job_title,
                getattr(payload, 'companyName', 'Company Name')
            )
            logger.info(f"🎨 [COURSE TEMPLATES] Generated AI image path: {ai_image_path}")
            
            course_template = {
                "title": job_title,
                "description": course_description,
                "modules": total_modules if total_modules > 0 else random.randint(4, 6),
                "lessons": total_lessons if total_lessons > 0 else random.randint(15, 30),
                "rating": "5.0",
                "image": ai_image_path
            }
            course_templates.append(course_template)
        
        # If we need more positions to reach 6, generate them with AI
        if len(course_templates) < 6:
            needed_positions = 6 - len(course_templates)
            logger.info(f"🎓 [COURSE TEMPLATES] Generating {needed_positions} additional positions with AI")
            
            additional_positions = await generate_additional_positions(duckduckgo_summary, needed_positions, payload, getattr(payload, 'language', 'ru'))
            
            for i, position in enumerate(additional_positions):
                job_title = position.get("title", f"Generated Position {i+1}")
                
                # Generate AI image for the AI-generated position
                logger.info(f"🎨 [COURSE TEMPLATES] Generating AI image for AI-generated position: {job_title}")
                ai_image_path = await generate_ai_image_for_job_position(
                    job_title,
                    getattr(payload, 'companyName', 'Company Name')
                )
                logger.info(f"🎨 [COURSE TEMPLATES] Generated AI image path for AI-generated position: {ai_image_path}")
                
                course_template = {
                    "title": job_title,
                    "description": position.get("description", "Описание курса для данной позиции."),
                    "modules": total_modules if total_modules > 0 else random.randint(4, 6),
                    "lessons": total_lessons if total_lessons > 0 else random.randint(15, 30),
                    "rating": "5.0",
                    "image": ai_image_path
                }
                course_templates.append(course_template)
        
        logger.info(f"🎓 [COURSE TEMPLATES] Generated {len(course_templates)} course templates")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [COURSE TEMPLATES] - Template {i+1}: {template['title']}")
        
        return course_templates
        
    except Exception as e:
        logger.error(f"❌ [COURSE TEMPLATES] Error generating course templates: {e}")
        # Fallback to default templates
        return [
            {
                "title": "HVAC Installer",
                "description": "Обучение установке, обслуживанию и ремонту систем HVAC оборудования.",
                "modules": 5,
                "lessons": 25,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-1-mobile.png"
            },
            {
                "title": "Electrician", 
                "description": "Обучение монтажу, подключению и обслуживанию электрических систем.",
                "modules": 5,
                "lessons": 22,
                "rating": "4.6",
                "image": "/custom-projects-ui/images/audit-section-5-job-2-mobile.png"
            },
            {
                "title": "Service Technician",
                "description": "Обучение диагностике, техническому обслуживанию и проверке оборудования.",
                "modules": 5,
                "lessons": 18,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-3-mobile.png"
            },
            {
                "title": "Project Manager",
                "description": "Обучение планированию, организации и контролю проектов.",
                "modules": 5,
                "lessons": 14,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-4-mobile.png"
            },
            {
                "title": "Field Operations Manager",
                "description": "Обучение управлению процессами и координации полевых команд.",
                "modules": 5,
                "lessons": 22,
                "rating": "4.6",
                "image": "/custom-projects-ui/images/audit-section-5-job-5-desktop.png"
            },
            {
                "title": "Slide Deck Specialist",
                "description": "Обучение созданию презентаций и визуальных обучающих материалов.",
                "modules": 5,
                "lessons": 18,
                "rating": "5.0",
                "image": "/custom-projects-ui/images/audit-section-5-job-6-desktop.png"
            }
        ]


async def generate_additional_positions(duckduckgo_summary: str, count: int, payload, language: str = "ru") -> list:
    """
    Generate additional job positions using AI based on company industry and context.
    """
    try:
        # 📊 DETAILED LOGGING: Language parameter in additional positions generation
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - language: '{language}'")
        # Determine language for logging
        language_name = "English" if language == "en" else "Spanish" if language == "es" else "Ukrainian" if language == "ua" else "Russian"
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - will use {language_name} prompts")
        
        if language == "en":
            prompt = f"""
            Analyze the company data and generate {count} additional logical positions for training courses.
            
            QUESTIONNAIRE DATA:
            - Company name: {getattr(payload, 'companyName', 'Company Name')}
            - Company description: {getattr(payload, 'companyDesc', 'Company Description')}
            - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            INTERNET DATA:
            {duckduckgo_summary}
            
            INSTRUCTIONS:
            - Generate {count} logical positions that fit this company and industry
            - Each position should be realistic and suitable for training courses
            - Positions should complement existing vacancies
            - Course description should be BRIEF (maximum 100 characters)
            - Use format: "Training [key skills/processes] for [position]"
            - Return data in JSON format: [{{"title": "Position Title", "description": "Brief training course description"}}]
            - Generate ALL content EXCLUSIVELY in English
            
            EXAMPLES OF POSITIONS AND DESCRIPTIONS:
            - {{"title": "Customer Support", "description": "Training in customer service and problem solving."}}
            - {{"title": "Marketing Specialist", "description": "Training in marketing fundamentals and product promotion."}}
            - {{"title": "Logistics Coordinator", "description": "Training in supply chain management and logistics."}}
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Analiza los datos de la empresa y genera {count} posiciones lógicas adicionales para cursos de capacitación.
            
            DATOS DEL CUESTIONARIO:
            - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
            - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
            - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            DATOS DE INTERNET:
            {duckduckgo_summary}
            
            INSTRUCCIONES:
            - Genera {count} posiciones lógicas que se ajusten a esta empresa e industria
            - Cada posición debe ser realista y adecuada para cursos de capacitación
            - Las posiciones deben complementar las vacantes existentes
            - La descripción del curso debe ser BREVE (máximo 100 caracteres)
            - Usa el formato: "Capacitación en [habilidades/procesos clave] para [posición]"
            - Devuelve los datos en formato JSON: [{{"title": "Título de la Posición", "description": "Breve descripción del curso de capacitación"}}]
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            EJEMPLOS DE POSICIONES Y DESCRIPCIONES:
            - {{"title": "Atención al Cliente", "description": "Capacitación en servicio al cliente y resolución de problemas."}}
            - {{"title": "Especialista en Marketing", "description": "Capacitación en fundamentos de marketing y promoción de productos."}}
            - {{"title": "Coordinador de Logística", "description": "Capacitación en gestión de cadena de suministro y logística."}}
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Проаналізуйте дані компанії та згенеруйте {count} додаткових логічних позицій для курсів навчання.
            
            ДАНІ АНКЕТИ:
            - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
            - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
            - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            ДАНІ З ІНТЕРНЕТУ:
            {duckduckgo_summary}
            
            ІНСТРУКЦІЇ:
            - Згенеруйте {count} логічних позицій, які підходять для цієї компанії та галузі
            - Кожна позиція повинна бути реалістичною та підходящою для курсу навчання
            - Позиції повинні доповнювати існуючі вакансії
            - Опис курсу повинен бути КОРОТКИМ (максимум 100 символів)
            - Використовуйте формат: "Навчання [ключовим навичкам/процесам] для [позиції]"
            - Поверніть дані у форматі JSON: [{{"title": "Назва позиції", "description": "Короткий опис курсу навчання"}}]
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ПРИКЛАДИ ПОЗИЦІЙ ТА ОПИСІВ:
            - {{"title": "Спеціаліст з обслуговування клієнтів", "description": "Навчання роботі з клієнтами та вирішенню проблем."}}
            - {{"title": "Спеціаліст з маркетингу", "description": "Навчання основам маркетингу та просування товарів."}}
            - {{"title": "Координатор логістики", "description": "Навчання управлінню постачанням та логістикою."}}
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:
            prompt = f"""
            Проанализируй данные компании и сгенерируй {count} дополнительных логических позиций для курсов обучения.
            
            ДАННЫЕ АНКЕТЫ:
            - Название компании: {getattr(payload, 'companyName', 'Company Name')}
            - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
            - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
            
            ДАННЫЕ ИЗ ИНТЕРНЕТА:
            {duckduckgo_summary}
            
            ИНСТРУКЦИИ:
            - Сгенерируй {count} логических позиций, которые подходят для данной компании и отрасли
            - Каждая позиция должна быть реалистичной и подходящей для курса обучения
            - Позиции должны дополнять уже существующие вакансии
            - Описание курса должно быть КРАТКИМ (максимум 100 символов)
            - Используй формат: "Обучение [ключевым навыкам/процессам] для [позиции]"
            - Верни данные в формате JSON: [{{"title": "Название позиции", "description": "Краткое описание курса обучения"}}]
            
            ПРИМЕРЫ ПОЗИЦИЙ И ОПИСАНИЙ:
            - {{"title": "Customer Support", "description": "Обучение работе с клиентами и решению проблем."}}
            - {{"title": "Marketing Specialist", "description": "Обучение основам маркетинга и продвижения товаров."}}
            - {{"title": "Logistics Coordinator", "description": "Обучение управлению поставками и логистикой."}}
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Log the raw response for debugging
        logger.info(f"[COURSE TEMPLATES] Raw additional positions response: '{response_text}'")
        
        # 📊 DETAILED LOGGING: Language parameter in response
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - raw response length: {len(response_text)}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] generate_additional_positions - language used: {language_name}")
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            additional_positions = json.loads(cleaned_response)
            
            if not isinstance(additional_positions, list):
                raise ValueError("Response is not a list")
            
            logger.info(f"[COURSE TEMPLATES] Successfully parsed {len(additional_positions)} additional positions")
            return additional_positions
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"[COURSE TEMPLATES] JSON parsing error: {e}")
            logger.error(f"[COURSE TEMPLATES] Raw response was: '{response_text}'")
            # Fallback to default positions based on language
            if language == "en":
                fallback_positions = [
                    {"title": "Customer Support", "description": "Training in customer service and problem solving."},
                    {"title": "Marketing Specialist", "description": "Training in marketing strategies and promotion."},
                    {"title": "Logistics Coordinator", "description": "Training in logistics and supply chain management."},
                    {"title": "Quality Assurance", "description": "Training in quality control and testing."}
                ]
            elif language == "es":
                fallback_positions = [
                    {"title": "Atención al Cliente", "description": "Capacitación en servicio al cliente y resolución de problemas."},
                    {"title": "Especialista en Marketing", "description": "Capacitación en estrategias de marketing y promoción."},
                    {"title": "Coordinador de Logística", "description": "Capacitación en logística y gestión de cadena de suministro."},
                    {"title": "Control de Calidad", "description": "Capacitación en control de calidad y pruebas."}
                ]
            elif language == "ua":
                fallback_positions = [
                    {"title": "Спеціаліст з обслуговування клієнтів", "description": "Навчання роботі з клієнтами та вирішенню проблем."},
                    {"title": "Спеціаліст з маркетингу", "description": "Навчання маркетинговим стратегіям та просуванню."},
                    {"title": "Координатор логістики", "description": "Навчання логістиці та управлінню постачанням."},
                    {"title": "Контроль якості", "description": "Навчання контролю якості та тестуванню."}
                ]
            else:  # Russian
                fallback_positions = [
                    {"title": "Customer Support", "description": "Обучение работе с клиентами и решению их проблем."},
                    {"title": "Marketing Specialist", "description": "Обучение маркетинговым стратегиям и продвижению."},
                    {"title": "Logistics Coordinator", "description": "Обучение управлению логистическими процессами."},
                    {"title": "Quality Assurance", "description": "Обучение контролю качества и тестированию."}
                ]
            return fallback_positions[:count]
            
    except Exception as e:
        logger.error(f"❌ [COURSE TEMPLATES] Error generating additional positions: {e}")
        return []


async def generate_workforce_crisis_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate workforce crisis data including industry, burnout, turnover, losses, search time, and chart data.
    Returns a dictionary with all the dynamic values for the "Кадровый кризис" section.
    """
    try:
        # Generate all workforce crisis data in parallel for efficiency
        industry_task = extract_company_industry(duckduckgo_summary, payload, language)
        burnout_task = extract_burnout_data(duckduckgo_summary, payload, language)
        turnover_task = extract_turnover_data(duckduckgo_summary, payload, language)
        losses_task = extract_losses_data(duckduckgo_summary, payload, language)
        search_time_task = extract_search_time_data(duckduckgo_summary, payload, language)
        chart_data_task = extract_personnel_shortage_chart_data(duckduckgo_summary, payload, language)
        yearly_shortage_task = extract_yearly_shortage_data(duckduckgo_summary, payload, language)
        
        # Wait for all tasks to complete
        industry, burnout, turnover, losses, search_time, chart_data, yearly_shortage = await asyncio.gather(
            industry_task, burnout_task, turnover_task, losses_task, search_time_task, chart_data_task, yearly_shortage_task
        )
        
        # Get grammatically correct industry text variants
        industry_forms = get_industry_text_variants(industry)
        
        workforce_crisis_data = {
            "industry": industry,
            "industryForms": industry_forms,  # Add grammatically correct forms
            "burnout": burnout,
            "turnover": turnover,
            "losses": losses,
            "searchTime": search_time,
            "chartData": chart_data,
            "yearlyShortage": yearly_shortage
        }
        
        logger.info(f"[AI-Audit Landing Page] Generated workforce crisis data: {workforce_crisis_data}")
        return workforce_crisis_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating workforce crisis data: {e}")
        # Return default values as fallback with grammatically correct forms
        industry_forms = get_industry_text_variants("hvac")
        return {
            "industry": "hvac",
            "industryForms": industry_forms,
            "burnout": {"months": "14", "industryName": "HVAC-компаниях"},
            "turnover": {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}},
            "losses": {"amount": "$10К–$18К"},
            "searchTime": {"days": "30–60"},
            "chartData": {
                "industry": "hvac",
                "chartData": [
                    {"month": "Январь", "shortage": 150},
                    {"month": "Февраль", "shortage": 165},
                    {"month": "Март", "shortage": 180},
                    {"month": "Апрель", "shortage": 195},
                    {"month": "Май", "shortage": 210},
                    {"month": "Июнь", "shortage": 225},
                    {"month": "Июль", "shortage": 240},
                    {"month": "Август", "shortage": 255},
                    {"month": "Сентябрь", "shortage": 270},
                    {"month": "Октябрь", "shortage": 285},
                    {"month": "Ноябрь", "shortage": 300},
                    {"month": "Декабрь", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "рост",
                "description": f"Постоянный рост дефицита квалифицированных кадров {industry_forms['crisis_in']}"
            },
            "yearlyShortage": {
                "yearlyShortage": 80000,
                "industry": "hvac",
                "description": f"Типичный дефицит квалифицированных кадров {industry_forms['of_industry']}"
            }
        }


def get_industry_text_variants(industry_name: str) -> dict:
    """Generate grammatically correct industry references for Russian text"""
    
    # Normalize industry name to lowercase
    industry = industry_name.lower().strip()
    
    # Define proper grammatical forms for common industries
    industry_forms = {
        "автомобильная промышленность": {
            "in_sector": "в автомобильном секторе",
            "in_industry": "в автомобильной отрасли",
            "crisis_in": "в автомобильной отрасли",
            "shortage_in": "в автомобильном секторе",
            "of_industry": "автомобильной отрасли"
        },
        "информационные технологии": {
            "in_sector": "в IT-секторе", 
            "in_industry": "в IT-отрасли",
            "crisis_in": "в сфере информационных технологий",
            "shortage_in": "в IT-секторе",
            "of_industry": "IT-отрасли"
        },
        "it": {
            "in_sector": "в IT-секторе", 
            "in_industry": "в IT-отрасли",
            "crisis_in": "в сфере информационных технологий",
            "shortage_in": "в IT-секторе",
            "of_industry": "IT-отрасли"
        },
        "строительство": {
            "in_sector": "в строительном секторе",
            "in_industry": "в строительной отрасли", 
            "crisis_in": "в строительной отрасли",
            "shortage_in": "в строительном секторе",
            "of_industry": "строительной отрасли"
        },
        "медицина": {
            "in_sector": "в медицинском секторе",
            "in_industry": "в медицинской отрасли",
            "crisis_in": "в сфере здравоохранения", 
            "shortage_in": "в медицинском секторе",
            "of_industry": "медицинской отрасли"
        },
        "здравоохранение": {
            "in_sector": "в медицинском секторе",
            "in_industry": "в сфере здравоохранения",
            "crisis_in": "в сфере здравоохранения", 
            "shortage_in": "в медицинском секторе",
            "of_industry": "сферы здравоохранения"
        },
        "образование": {
            "in_sector": "в образовательном секторе",
            "in_industry": "в сфере образования",
            "crisis_in": "в сфере образования", 
            "shortage_in": "в образовательном секторе",
            "of_industry": "сферы образования"
        },
        "hvac": {
            "in_sector": "в HVAC-секторе",
            "in_industry": "в HVAC-отрасли",
            "crisis_in": "в HVAC-отрасли", 
            "shortage_in": "в HVAC-секторе",
            "of_industry": "HVAC-отрасли"
        },
        "производство": {
            "in_sector": "в производственном секторе",
            "in_industry": "в производственной отрасли",
            "crisis_in": "в производственной отрасли", 
            "shortage_in": "в производственном секторе",
            "of_industry": "производственной отрасли"
        },
        "торговля": {
            "in_sector": "в торговом секторе",
            "in_industry": "в торговой отрасли",
            "crisis_in": "в торговой отрасли", 
            "shortage_in": "в торговом секторе",
            "of_industry": "торговой отрасли"
        },
        "финансы": {
            "in_sector": "в финансовом секторе",
            "in_industry": "в финансовой отрасли",
            "crisis_in": "в финансовой отрасли", 
            "shortage_in": "в финансовом секторе",
            "of_industry": "финансовой отрасли"
        }
    }
    
    # Default fallback for unknown industries
    default_forms = {
        "in_sector": f"в {industry} секторе",
        "in_industry": f"в {industry} отрасли", 
        "crisis_in": f"в {industry} отрасли",
        "shortage_in": f"в {industry} секторе",
        "of_industry": f"{industry} отрасли"
    }
    
    return industry_forms.get(industry, default_forms)


async def extract_company_industry(duckduckgo_summary: str, payload, language: str = "ru") -> str:
    """
    Extract the company's primary industry from scraped data.
    """
    if language == "en":
        prompt = f"""
        Determine the company's primary industry based on the provided data.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Determine the company's primary industry
        - Return the industry name in lowercase
        - Use standard industry names from the list:
          * automotive industry
          * information technology (or IT)
          * construction
          * healthcare
          * education
          * manufacturing
          * retail
          * finance
          * HVAC
        - If you cannot determine, return "general services"
        - Generate ALL content EXCLUSIVELY in English
        
        RESPONSE (only industry name in lowercase):
        """
    elif language == "es":
        prompt = f"""
        Determina la industria principal de la empresa basándote en los datos proporcionados.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Determina la industria principal de la empresa
        - Devuelve el nombre de la industria en minúsculas
        - Usa nombres estándar de industrias de la lista:
          * industria automotriz
          * tecnología de la información (o TI)
          * construcción
          * salud
          * educación
          * manufactura
          * retail
          * finanzas
          * HVAC
        - Si no puedes determinar, devuelve "servicios generales"
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        RESPUESTA (solo nombre de la industria en minúsculas):
        """
    elif language == "ua":
        prompt = f"""
        Визначте основну галузь компанії на основі наданих даних.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Визначте основну галузь діяльності компанії
        - Поверніть назву галузі в називному відмінку, малими літерами
        - Використовуйте стандартні назви галузей зі списку:
          * автомобільна промисловість
          * інформаційні технології (або IT)
          * будівництво
          * охорона здоров'я
          * освіта
          * виробництво
          * торгівля
          * фінанси
          * HVAC
        - Якщо не можете визначити, поверніть "загальні послуги"
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ВІДПОВІДЬ (лише назва галузі в називному відмінку, малими літерами):
        """
    else:
        prompt = f"""
        Определи основную отрасль/индустрию компании на основе предоставленных данных.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Определи основную отрасль деятельности компании
        - Верни название отрасли в именительном падеже, строчными буквами
        - Используй стандартные названия отраслей из списка:
          * автомобильная промышленность
          * информационные технологии (или IT)
          * строительство
          * медицина (или здравоохранение)
          * образование
          * производство
          * торговля
          * финансы
          * HVAC
        - Если не можешь определить, верни "общие услуги"
        
        ОТВЕТ (только название отрасли в именительном падеже, строчными буквами):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        industry = response_text.strip().lower()
        if not industry:
            industry = "общие услуги"
        
        logger.info(f"[AI-Audit Landing Page] Extracted industry: {industry}")
        return industry
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting industry: {e}")
        return "HVAC"


async def extract_burnout_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract burnout statistics from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine employee burnout statistics in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Determine the company's industry based on the data
        - Find information about average employee tenure in this industry
        - If no data is available, use typical values for the industry
        - Return ONLY valid JSON without additional text
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - For IT companies: {{"months": "18", "industryName": "IT companies"}}
        - For e-commerce: {{"months": "16", "industryName": "e-commerce companies"}}
        - For construction: {{"months": "12", "industryName": "construction companies"}}
        - For HVAC: {{"months": "14", "industryName": "HVAC companies"}}
        
        IMPORTANT: Respond ONLY with a valid JSON object, without additional text or explanations.
        """
    elif language == "es":
        prompt = f"""
        Analiza los datos y determina las estadísticas de agotamiento de empleados en la industria de la empresa.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Determina la industria de la empresa basándote en los datos
        - Encuentra información sobre la duración promedio de empleados en esta industria
        - Si no hay datos disponibles, usa valores típicos para la industria
        - Devuelve SOLO JSON válido sin texto adicional
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        EJEMPLOS:
        - Para empresas IT: {{"months": "18", "industryName": "empresas de TI"}}
        - Para e-commerce: {{"months": "16", "industryName": "empresas de comercio electrónico"}}
        - Para construcción: {{"months": "12", "industryName": "empresas de construcción"}}
        - Para HVAC: {{"months": "14", "industryName": "empresas HVAC"}}
        
        IMPORTANTE: Responde SOLO con un objeto JSON válido, sin texto adicional o explicaciones.
        """
    elif language == "ua":
        prompt = f"""
        Проаналізуйте дані та визначте статистику вигорання співробітників у галузі компанії.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Визначте галузь компанії на основі даних
        - Знайдіть інформацію про середню тривалість роботи співробітників у цій галузі
        - Якщо даних немає, використовуйте типові значення для галузі
        - Поверніть ЛИШЕ валідний JSON без додаткового тексту
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ПРИКЛАДИ:
        - Для IT-компаній: {{"months": "18", "industryName": "IT-компаніях"}}
        - Для e-commerce: {{"months": "16", "industryName": "e-commerce-компаніях"}}
        - Для будівництва: {{"months": "12", "industryName": "будівельних компаніях"}}
        - Для HVAC: {{"months": "14", "industryName": "HVAC-компаніях"}}
        
        ВАЖЛИВО: Відповідайте ЛИШЕ валідним JSON об'єктом, без додаткового тексту або пояснень.
        """
    else:
        prompt = f"""
        Проанализируй данные и определи статистику выгорания сотрудников в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Определи отрасль компании на основе данных
        - Найди информацию о средней продолжительности работы сотрудников в этой отрасли
        - Если данных нет, используй типичные значения для отрасли
        - Верни ТОЛЬКО валидный JSON без дополнительного текста
        
        ПРИМЕРЫ:
        - Для IT-компании: {{"months": "18", "industryName": "IT-компаниях"}}
        - Для маркетплейса: {{"months": "16", "industryName": "e-commerce-компаниях"}}
        - Для строительства: {{"months": "12", "industryName": "строительных компаниях"}}
        - Для HVAC: {{"months": "14", "industryName": "HVAC-компаниях"}}
        
        ВАЖНО: Отвечай ТОЛЬКО валидным JSON объектом, без дополнительного текста или объяснений.
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Log the raw response for debugging
        logger.info(f"[AI-Audit Landing Page] Raw burnout response: '{response_text}'")
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            burnout_data = json.loads(cleaned_response)
            if "months" not in burnout_data or "industryName" not in burnout_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            burnout_data = {"months": "14", "industryName": "HVAC-компаниях"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted burnout data: {burnout_data}")
        return burnout_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting burnout data: {e}")
        return {"months": "14", "industryName": "HVAC-компаниях"}


async def extract_turnover_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract turnover statistics from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine employee turnover statistics in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about employee turnover in the industry (% of employees leaving per year)
        - Find information about early departures (% of employees leaving in the first months)
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"percentage": "percentage per year", "earlyExit": {{"percentage": "percentage", "months": "months"}}}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Construction: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        RESPONSE (JSON only):
        """
    elif language == "es":
        prompt = f"""
        Analiza los datos y determina las estadísticas de rotación de empleados en la industria de la empresa.
        
        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        DATOS DE INTERNET:
        {duckduckgo_summary}
        
        INSTRUCCIONES:
        - Encuentra información sobre la rotación de empleados en la industria (% de empleados que se van por año)
        - Encuentra información sobre salidas tempranas (% de empleados que se van en los primeros meses)
        - Si no hay datos disponibles, usa valores típicos para la industria
        - Devuelve datos en formato JSON: {{"percentage": "porcentaje por año", "earlyExit": {{"percentage": "porcentaje", "months": "meses"}}}}
        - Genera TODO el contenido EXCLUSIVAMENTE en español
        
        EJEMPLOS:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Construcción: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        RESPUESTA (solo JSON):
        """
    elif language == "ua":
        prompt = f"""
        Проаналізуйте дані та визначте статистику плинності кадрів у галузі компанії.
        
        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}
        
        ІНСТРУКЦІЇ:
        - Знайдіть інформацію про плинність кадрів у галузі (% звільнень на рік)
        - Знайдіть інформацію про ранні звільнення (% звільнень у перші місяці)
        - Якщо даних немає, використовуйте типові значення для галузі
        - Поверніть дані у форматі JSON: {{"percentage": "відсоток на рік", "earlyExit": {{"percentage": "відсоток", "months": "місяці"}}}}
        - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
        
        ПРИКЛАДИ:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Будівництво: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        ВІДПОВІДЬ (лише JSON):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи статистику текучести кадров в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о текучести кадров в отрасли (% увольнений в год)
        - Найди информацию о ранних увольнениях (% увольнений в первые месяцы)
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"percentage": "процент в год", "earlyExit": {{"percentage": "процент", "months": "месяцы"}}}}
        
        ПРИМЕРЫ:
        - HVAC: {{"percentage": "85", "earlyExit": {{"percentage": "45", "months": "3"}}}}
        - IT: {{"percentage": "60", "earlyExit": {{"percentage": "30", "months": "6"}}}}
        - Строительство: {{"percentage": "90", "earlyExit": {{"percentage": "50", "months": "2"}}}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            turnover_data = json.loads(cleaned_response)
            if "percentage" not in turnover_data or "earlyExit" not in turnover_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Turnover JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            turnover_data = {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}}
        
        logger.info(f"[AI-Audit Landing Page] Extracted turnover data: {turnover_data}")
        return turnover_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting turnover data: {e}")
        return {"percentage": "85", "earlyExit": {"percentage": "45", "months": "3"}}


async def extract_losses_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract financial losses data from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the company's financial losses for unfilled positions.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about financial losses per year for unfilled positions
        - Consider lost profits, overtime, and downtime
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"amount": "amount in dollars"}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"amount": "$10K–$18K"}}
        - IT: {{"amount": "$15K–$25K"}}
        - Construction: {{"amount": "$8K–$15K"}}
        - Healthcare: {{"amount": "$20K–$35K"}}
        
        RESPONSE (JSON only):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи финансовые потери компании при незакрытой позиции.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о финансовых потерях при незакрытой позиции в год
        - Учитывай упущенную прибыль, переработки и простои
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"amount": "сумма в долларах"}}
        
        ПРИМЕРЫ:
        - HVAC: {{"amount": "$10К–$18К"}}
        - IT: {{"amount": "$15К–$25К"}}
        - Строительство: {{"amount": "$8К–$15К"}}
        - Медицина: {{"amount": "$20К–$35К"}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            losses_data = json.loads(cleaned_response)
            if "amount" not in losses_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Losses JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            losses_data = {"amount": "$10К–$18К"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted losses data: {losses_data}")
        return losses_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting losses data: {e}")
        return {"amount": "$10К–$18К"}


async def extract_search_time_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Extract candidate search time data from scraped data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the average candidate search time in the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        - Find information about average candidate search time in the industry
        - If no data is available, use typical values for the industry
        - Return data in JSON format: {{"days": "day range"}}
        - Generate ALL content EXCLUSIVELY in English
        
        EXAMPLES:
        - HVAC: {{"days": "30–60"}}
        - IT: {{"days": "45–90"}}
        - Construction: {{"days": "20–45"}}
        - Healthcare: {{"days": "60–120"}}
        
        RESPONSE (JSON only):
        """
    else:
        prompt = f"""
        Проанализируй данные и определи среднее время поиска кандидата в отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        - Найди информацию о среднем времени поиска кандидата в отрасли
        - Если данных нет, используй типичные значения для отрасли
        - Верни данные в формате JSON: {{"days": "диапазон дней"}}
        
        ПРИМЕРЫ:
        - HVAC: {{"days": "30–60"}}
        - IT: {{"days": "45–90"}}
        - Строительство: {{"days": "20–45"}}
        - Медицина: {{"days": "60–120"}}
        
        ОТВЕТ (только JSON):
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Try to parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            search_time_data = json.loads(cleaned_response)
            if "days" not in search_time_data:
                raise ValueError("Missing required fields")
        except (json.JSONDecodeError, ValueError) as e:
            # Log the parsing error for debugging
            logger.error(f"[AI-Audit Landing Page] Search time JSON parsing error: {e}")
            logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
            logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
            # Fallback to default values
            search_time_data = {"days": "30–60"}
        
        logger.info(f"[AI-Audit Landing Page] Extracted search time data: {search_time_data}")
        return search_time_data
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error extracting search time data: {e}")
        return {"days": "30–60"}


async def extract_personnel_shortage_chart_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate structured dataset for the "Shortage of qualified personnel" chart.
    Returns a dictionary with 12 months of personnel shortage data.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and generate a structured dataset for the "Shortage of qualified personnel" chart for the last 12 months.

        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}

        INTERNET DATA:
        {duckduckgo_summary}

        INSTRUCTIONS:
        1. Determine the INDUSTRY as a whole (not the specific company) based on the provided data
        2. Analyze personnel shortage for the ENTIRE INDUSTRY, not just the specified company
        3. Generate realistic data with natural fluctuations (NOT linear growth)
        4. Consider industry specifics: seasonality, economic cycles, market events
        5. The shortage scale should match the industry size (large industries = large numbers)
        6. Generate ALL content EXCLUSIVELY in English

        REALISM REQUIREMENTS:
        - FORBIDDEN: perfectly linear increase every month
        - MANDATORY: include monthly fluctuations (some months may show decrease)
        - Seasonal factors: consider industry specifics (e.g., construction - peak in summer, automotive - decrease in August due to vacations)
        - Scale should reflect industry size (manufacturing, IT, finance = thousands of specialists)
        - Include 2-3 months with slight decrease in indicators

        RESPONSE FORMAT:
        Return ONLY a valid JSON object in the following format:
        {{
            "industry": "industry name (not company)",
            "chartData": [
                {{"month": "January", "shortage": 2800}},
                {{"month": "February", "shortage": 2650}},
                {{"month": "March", "shortage": 3100}},
                {{"month": "April", "shortage": 3450}},
                {{"month": "May", "shortage": 3200}},
                {{"month": "June", "shortage": 3800}},
                {{"month": "July", "shortage": 4100}},
                {{"month": "August", "shortage": 3600}},
                {{"month": "September", "shortage": 3900}},
                {{"month": "October", "shortage": 4200}},
                {{"month": "November", "shortage": 3950}},
                {{"month": "December", "shortage": 4300}}
            ],
            "totalShortage": [sum of all shortage values],
            "trend": "growth/stability/decline",
            "description": "Brief description of personnel shortage trend in the industry with mention of key factors"
        }}

        NEGATIVE EXAMPLE (DON'T do this):
        - Data: 100, 110, 120, 130, 140... (too linear and small scale)
        - Focus only on one company instead of industry

        MANDATORY CHECKS:
        - Use only English month names
        - Shortage values - whole numbers corresponding to industry size
        - Minimum 2 months should show decrease compared to previous
        - industry should be industry name, not company name
        - Consider real industry scale when generating numbers
        """
    elif language == "es":
        prompt = f"""
        Analiza los datos y genera un conjunto de datos estructurado para el gráfico "Escasez de personal calificado" de los últimos 12 meses.

        DATOS DE LA EMPRESA:
        - Nombre de la empresa: {getattr(payload, 'companyName', 'Company Name')}
        - Descripción de la empresa: {getattr(payload, 'companyDesc', 'Company Description')}
        - Sitio web: {getattr(payload, 'companyWebsite', 'Company Website')}

        DATOS DE INTERNET:
        {duckduckgo_summary}

        INSTRUCCIONES:
        1. Determina la INDUSTRIA en general (no la empresa específica) basándote en los datos proporcionados
        2. Analiza la escasez de personal para TODA LA INDUSTRIA, no solo para la empresa especificada
        3. Genera datos realistas con fluctuaciones naturales (NO crecimiento lineal)
        4. Considera especificidades de la industria: estacionalidad, ciclos económicos, eventos del mercado
        5. La escala de escasez debe coincidir con el tamaño de la industria (industrias grandes = números grandes)
        6. Genera TODO el contenido EXCLUSIVAMENTE en español

        REQUISITOS DE REALISMO:
        - PROHIBIDO: aumento perfectamente lineal cada mes
        - OBLIGATORIO: incluir fluctuaciones mensuales (algunos meses pueden mostrar disminución)
        - Factores estacionales: considera especificidades de la industria (ej., construcción - pico en verano, automotriz - disminución en agosto por vacaciones)
        - La escala debe reflejar el tamaño de la industria (manufactura, IT, finanzas = miles de especialistas)
        - Incluir 2-3 meses con ligera disminución en los indicadores

        FORMATO DE RESPUESTA:
        Devuelve SOLO un objeto JSON válido en el siguiente formato:
        {{
            "industry": "nombre de la industria (no empresa)",
            "chartData": [
                {{"month": "Enero", "shortage": 2800}},
                {{"month": "Febrero", "shortage": 2650}},
                {{"month": "Marzo", "shortage": 3100}},
                {{"month": "Abril", "shortage": 3450}},
                {{"month": "Mayo", "shortage": 3200}},
                {{"month": "Junio", "shortage": 3800}},
                {{"month": "Julio", "shortage": 4100}},
                {{"month": "Agosto", "shortage": 3600}},
                {{"month": "Septiembre", "shortage": 3900}},
                {{"month": "Octubre", "shortage": 4200}},
                {{"month": "Noviembre", "shortage": 3950}},
                {{"month": "Diciembre", "shortage": 4300}}
            ],
            "totalShortage": [suma de todos los valores de shortage],
            "trend": "crecimiento/estabilidad/declive",
            "description": "Breve descripción de la tendencia de escasez de personal en la industria con mención de factores clave"
        }}

        EJEMPLO NEGATIVO (NO hagas esto):
        - Datos: 100, 110, 120, 130, 140... (demasiado lineal y escala pequeña)
        - Enfocarse solo en una empresa en lugar de la industria

        VERIFICACIONES OBLIGATORIAS:
        - Usa solo nombres de meses en español
        - Valores de shortage - números enteros correspondientes al tamaño de la industria
        - Mínimo 2 meses deben mostrar disminución comparado con el anterior
        - industry debe ser nombre de la industria, no de la empresa
        - Considera la escala real de la industria al generar números
        """
    elif language == "ua":
        prompt = f"""
        Проаналізуйте дані та згенеруйте структурований набір даних для графіка "Дефіцит кваліфікованих кадрів" за останні 12 місяців.

        ДАНІ КОМПАНІЇ:
        - Назва компанії: {getattr(payload, 'companyName', 'Company Name')}
        - Опис компанії: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}

        ДАНІ З ІНТЕРНЕТУ:
        {duckduckgo_summary}

        ІНСТРУКЦІЇ:
        1. Визначте ГАЛУЗЬ в цілому (не конкретну компанію) на основі наданих даних
        2. Проаналізуйте дефіцит кадрів для ВСІЄЇ ГАЛУЗІ, а не тільки для вказаної компанії
        3. Згенеруйте реалістичні дані з природними коливаннями (НЕ лінійне зростання)
        4. Врахуйте галузеву специфіку: сезонність, економічні цикли, ринкові події
        5. Масштаб дефіциту повинен відповідати розміру галузі (великі галузі = великі числа)
        6. Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою

        ВИМОГИ ДО РЕАЛІЗМУ:
        - ЗАБОРОНЕНО: ідеально лінійне збільшення щомісяця
        - ОБОВ'ЯЗКОВО: включіть місячні коливання (деякі місяці можуть показувати зниження)
        - Сезонні фактори: врахуйте специфіку галузі (наприклад, будівництво - пік влітку, автопром - зниження в серпні через відпустки)
        - Масштаб повинен відображати розмір галузі (машинобудування, IT, фінанси = тисячі спеціалістів)
        - Включіть 2-3 місяці з незначним зниженням показників

        ФОРМАТ ВІДПОВІДІ:
        Поверніть ЛИШЕ валідний JSON об'єкт у наступному форматі:
        {{
            "industry": "назва галузі (не компанії)",
            "chartData": [
                {{"month": "Січень", "shortage": 2800}},
                {{"month": "Лютий", "shortage": 2650}},
                {{"month": "Березень", "shortage": 3100}},
                {{"month": "Квітень", "shortage": 3450}},
                {{"month": "Травень", "shortage": 3200}},
                {{"month": "Червень", "shortage": 3800}},
                {{"month": "Липень", "shortage": 4100}},
                {{"month": "Серпень", "shortage": 3600}},
                {{"month": "Вересень", "shortage": 3900}},
                {{"month": "Жовтень", "shortage": 4200}},
                {{"month": "Листопад", "shortage": 3950}},
                {{"month": "Грудень", "shortage": 4300}}
            ],
            "totalShortage": [сума всіх значень shortage],
            "trend": "зростання/стабільність/зниження",
            "description": "Короткий опис тренду дефіциту кадрів у галузі з згадкою ключових факторів"
        }}

        НЕГАТИВНИЙ ПРИКЛАД (НЕ робіть так):
        - Дані: 100, 110, 120, 130, 140... (занадто лінійно і малий масштаб)
        - Фокус тільки на одній компанії замість галузі

        ОБОВ'ЯЗКОВІ ПЕРЕВІРКИ:
        - Використовуйте лише українські назви місяців
        - Значення shortage - цілі числа, що відповідають розміру галузі
        - Мінімум 2 місяці повинні показувати зниження порівняно з попереднім
        - industry повинно бути назвою галузі, а не компанії
        - Враховуйте реальний масштаб галузі при генерації чисел
        """
    else:
        prompt = f"""
        Проанализируй данные и сгенерируй структурированный набор данных для графика "Дефицит квалифицированных кадров" за последние 12 месяцев.

        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}

        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}

        ИНСТРУКЦИИ:
        1. Определи ОТРАСЛЬ в целом (не конкретную компанию) на основе предоставленных данных
        2. Анализируй дефицит кадров для ВСЕЙ ОТРАСЛИ, а не только для указанной компании
        3. Сгенерируй реалистичные данные с естественными колебаниями (НЕ линейный рост)
        4. Учти отраслевую специфику: сезонность, экономические циклы, рыночные события
        5. Масштаб дефицита должен соответствовать размеру отрасли (крупные отрасли = большие числа)

        ТРЕБОВАНИЯ К РЕАЛИСТИЧНОСТИ:
        - ЗАПРЕЩЕНО: идеально линейное увеличение каждый месяц
        - ОБЯЗАТЕЛЬНО: включи месячные колебания (некоторые месяцы могут показывать снижение)
        - Сезонные факторы: учти специфику отрасли (например, строительство - пик летом, автопром - снижение в августе из-за отпусков)
        - Масштаб должен отражать размер отрасли (машиностроение, IT, финансы = тысячи специалистов)
        - Включи 2-3 месяца с незначительным снижением показателей

        ФОРМАТ ОТВЕТА:
        Верни ТОЛЬКО валидный JSON объект в следующем формате:
        {{
            "industry": "название отрасли (не компании)",
            "chartData": [
                {{"month": "Январь", "shortage": 2800}},
                {{"month": "Февраль", "shortage": 2650}},
                {{"month": "Март", "shortage": 3100}},
                {{"month": "Апрель", "shortage": 3450}},
                {{"month": "Май", "shortage": 3200}},
                {{"month": "Июнь", "shortage": 3800}},
                {{"month": "Июль", "shortage": 4100}},
                {{"month": "Август", "shortage": 3600}},
                {{"month": "Сентябрь", "shortage": 3900}},
                {{"month": "Октябрь", "shortage": 4200}},
                {{"month": "Ноябрь", "shortage": 3950}},
                {{"month": "Декабрь", "shortage": 4300}}
            ],
            "totalShortage": [сумма всех shortage],
            "trend": "рост/стабильность/снижение",
            "description": "Краткое описание тренда дефицита кадров в отрасли с упоминанием ключевых факторов. Используй правильные падежи: 'в [отрасль] отрасли' или 'в [отрасль] секторе'"
        }}

        ОТРИЦАТЕЛЬНЫЙ ПРИМЕР (НЕ делай так):
        - Данные: 100, 110, 120, 130, 140... (слишком линейно и маленький масштаб)
        - Фокус только на одной компании вместо отрасли

        ОБЯЗАТЕЛЬНЫЕ ПРОВЕРКИ:
        - Используй только русские названия месяцев
        - Значения shortage - целые числа, соответствующие размеру отрасли
        - Минимум 2 месяца должны показывать снижение по сравнению с предыдущим
        - industry должно быть названием отрасли, а не компании
        - Учитывай реальный масштаб отрасли при генерации чисел
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean and parse the response
        cleaned_response = response_text.strip()
        if cleaned_response.startswith('```json'):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith('```'):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        
        # Parse JSON response
        chart_data = json.loads(cleaned_response)
        
        # Validate the structure
        if not isinstance(chart_data, dict) or 'chartData' not in chart_data:
            raise ValueError("Invalid chart data structure")
        
        if not isinstance(chart_data['chartData'], list) or len(chart_data['chartData']) != 12:
            raise ValueError("Chart data must contain exactly 12 months")
        
        # Log the generated data for verification
        logger.info(f"[AI-Audit Landing Page] Generated personnel shortage chart data:")
        logger.info(f"[AI-Audit Landing Page] - Industry: {chart_data.get('industry', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Total shortage: {chart_data.get('totalShortage', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Trend: {chart_data.get('trend', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Chart data points: {len(chart_data.get('chartData', []))}")
        
        # Log each month's data for detailed verification
        for i, month_data in enumerate(chart_data.get('chartData', [])):
            logger.info(f"[AI-Audit Landing Page] - Month {i+1}: {month_data.get('month', 'Unknown')} - {month_data.get('shortage', 'Unknown')} specialists")
        
        return chart_data
        
    except json.JSONDecodeError as e:
        logger.error(f"[AI-Audit Landing Page] Chart data JSON parsing error: {e}")
        logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
        logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
        # Fallback to default values based on language
        if language == "en":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "January", "shortage": 150},
                    {"month": "February", "shortage": 165},
                    {"month": "March", "shortage": 180},
                    {"month": "April", "shortage": 195},
                    {"month": "May", "shortage": 210},
                    {"month": "June", "shortage": 225},
                    {"month": "July", "shortage": 240},
                    {"month": "August", "shortage": 255},
                    {"month": "September", "shortage": 270},
                    {"month": "October", "shortage": 285},
                    {"month": "November", "shortage": 300},
                    {"month": "December", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "growth",
                "description": "Continuous growth in qualified personnel shortage in HVAC industry"
            }
        elif language == "es":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Enero", "shortage": 150},
                    {"month": "Febrero", "shortage": 165},
                    {"month": "Marzo", "shortage": 180},
                    {"month": "Abril", "shortage": 195},
                    {"month": "Mayo", "shortage": 210},
                    {"month": "Junio", "shortage": 225},
                    {"month": "Julio", "shortage": 240},
                    {"month": "Agosto", "shortage": 255},
                    {"month": "Septiembre", "shortage": 270},
                    {"month": "Octubre", "shortage": 285},
                    {"month": "Noviembre", "shortage": 300},
                    {"month": "Diciembre", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "crecimiento",
                "description": "Crecimiento continuo en la escasez de personal calificado en la industria HVAC"
            }
        elif language == "ua":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Січень", "shortage": 150},
                    {"month": "Лютий", "shortage": 165},
                    {"month": "Березень", "shortage": 180},
                    {"month": "Квітень", "shortage": 195},
                    {"month": "Травень", "shortage": 210},
                    {"month": "Червень", "shortage": 225},
                    {"month": "Липень", "shortage": 240},
                    {"month": "Серпень", "shortage": 255},
                    {"month": "Вересень", "shortage": 270},
                    {"month": "Жовтень", "shortage": 285},
                    {"month": "Листопад", "shortage": 300},
                    {"month": "Грудень", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "зростання",
                "description": "Постійне зростання дефіциту кваліфікованих кадрів у галузі HVAC"
            }
        else:  # Russian
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Январь", "shortage": 150},
                    {"month": "Февраль", "shortage": 165},
                    {"month": "Март", "shortage": 180},
                    {"month": "Апрель", "shortage": 195},
                    {"month": "Май", "shortage": 210},
                    {"month": "Июнь", "shortage": 225},
                    {"month": "Июль", "shortage": 240},
                    {"month": "Август", "shortage": 255},
                    {"month": "Сентябрь", "shortage": 270},
                    {"month": "Октябрь", "shortage": 285},
                    {"month": "Ноябрь", "shortage": 300},
                    {"month": "Декабрь", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "рост",
                "description": "Постоянный рост дефицита квалифицированных кадров в HVAC-отрасли"
            }
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating chart data: {e}")
        # Fallback to default values based on language
        if language == "en":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "January", "shortage": 150},
                    {"month": "February", "shortage": 165},
                    {"month": "March", "shortage": 180},
                    {"month": "April", "shortage": 195},
                    {"month": "May", "shortage": 210},
                    {"month": "June", "shortage": 225},
                    {"month": "July", "shortage": 240},
                    {"month": "August", "shortage": 255},
                    {"month": "September", "shortage": 270},
                    {"month": "October", "shortage": 285},
                    {"month": "November", "shortage": 300},
                    {"month": "December", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "growth",
                "description": "Continuous growth in qualified personnel shortage in HVAC industry"
            }
        elif language == "es":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Enero", "shortage": 150},
                    {"month": "Febrero", "shortage": 165},
                    {"month": "Marzo", "shortage": 180},
                    {"month": "Abril", "shortage": 195},
                    {"month": "Mayo", "shortage": 210},
                    {"month": "Junio", "shortage": 225},
                    {"month": "Julio", "shortage": 240},
                    {"month": "Agosto", "shortage": 255},
                    {"month": "Septiembre", "shortage": 270},
                    {"month": "Octubre", "shortage": 285},
                    {"month": "Noviembre", "shortage": 300},
                    {"month": "Diciembre", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "crecimiento",
                "description": "Crecimiento continuo en la escasez de personal calificado en la industria HVAC"
            }
        elif language == "ua":
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Січень", "shortage": 150},
                    {"month": "Лютий", "shortage": 165},
                    {"month": "Березень", "shortage": 180},
                    {"month": "Квітень", "shortage": 195},
                    {"month": "Травень", "shortage": 210},
                    {"month": "Червень", "shortage": 225},
                    {"month": "Липень", "shortage": 240},
                    {"month": "Серпень", "shortage": 255},
                    {"month": "Вересень", "shortage": 270},
                    {"month": "Жовтень", "shortage": 285},
                    {"month": "Листопад", "shortage": 300},
                    {"month": "Грудень", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "зростання",
                "description": "Постійне зростання дефіциту кваліфікованих кадрів у галузі HVAC"
            }
        else:  # Russian
            return {
                "industry": "HVAC",
                "chartData": [
                    {"month": "Январь", "shortage": 150},
                    {"month": "Февраль", "shortage": 165},
                    {"month": "Март", "shortage": 180},
                    {"month": "Апрель", "shortage": 195},
                    {"month": "Май", "shortage": 210},
                    {"month": "Июнь", "shortage": 225},
                    {"month": "Июль", "shortage": 240},
                    {"month": "Август", "shortage": 255},
                    {"month": "Сентябрь", "shortage": 270},
                    {"month": "Октябрь", "shortage": 285},
                    {"month": "Ноябрь", "shortage": 300},
                    {"month": "Декабрь", "shortage": 315}
                ],
                "totalShortage": 2775,
                "trend": "рост",
                "description": "Постоянный рост дефицита квалифицированных кадров в HVAC-отрасли"
            }


async def extract_yearly_shortage_data(duckduckgo_summary: str, payload, language: str = "ru") -> dict:
    """
    Generate a single yearly shortage number for the company's specific industry.
    Returns a dictionary with the annual shortage count.
    """
    if language == "en":
        prompt = f"""
        Analyze the data and determine the exact number of missing qualified specialists per year for the company's industry.
        
        COMPANY DATA:
        - Company Name: {getattr(payload, 'companyName', 'Company Name')}
        - Company Description: {getattr(payload, 'companyDesc', 'Company Description')}
        - Website: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        INTERNET DATA:
        {duckduckgo_summary}
        
        INSTRUCTIONS:
        1. Determine the company's industry based on the provided data
        2. Calculate a realistic number of missing specialists per year for this industry
        3. Consider industry size, growth rates, and current personnel shortage
        4. The number should be realistic and justified for this industry
        5. Consider regional characteristics and industry scale
        6. Generate ALL content EXCLUSIVELY in English
        
        REQUIREMENTS:
        - Return ONLY one number (number of missing specialists per year)
        - The number should be whole
        - The number should be realistic for the industry
        - Consider industry scale (local, regional, national)
        
        RESPONSE FORMAT:
        Return ONLY a valid JSON object in the following format:
        {{
            "yearlyShortage": 80000,
            "industry": "industry name",
            "description": "Brief justification of the number"
        }}
        
        EXAMPLES FOR DIFFERENT INDUSTRIES:
        - HVAC: 45000-80000 specialists per year
        - IT: 120000-200000 specialists per year  
        - Construction: 60000-100000 specialists per year
        - Healthcare: 80000-150000 specialists per year
        - Manufacturing: 70000-120000 specialists per year
        
        IMPORTANT: 
        - The number should be realistic for the industry
        - Consider industry size and scale
        - Include justification in description
        """
    else:
        prompt = f"""
        Проанализируй данные и определи точное количество недостающих квалифицированных специалистов в год для отрасли компании.
        
        ДАННЫЕ АНКЕТЫ:
        - Название компании: {getattr(payload, 'companyName', 'Company Name')}
        - Описание компании: {getattr(payload, 'companyDesc', 'Company Description')}
        - Веб-сайт: {getattr(payload, 'companyWebsite', 'Company Website')}
        
        ДАННЫЕ ИЗ ИНТЕРНЕТА:
        {duckduckgo_summary}
        
        ИНСТРУКЦИИ:
        1. Определи отрасль компании на основе предоставленных данных
        2. Рассчитай реалистичное количество недостающих специалистов в год для данной отрасли
        3. Учти размер отрасли, темпы роста и текущий дефицит кадров
        4. Число должно быть реалистичным и обоснованным для данной отрасли
        5. Учти региональные особенности и масштаб отрасли
        
        ТРЕБОВАНИЯ:
        - Верни ТОЛЬКО одно число (количество недостающих специалистов в год)
        - Число должно быть целым
        - Число должно быть реалистичным для отрасли
        - Учти масштаб отрасли (локальная, региональная, национальная)
        
        ФОРМАТ ОТВЕТА:
        Верни ТОЛЬКО валидный JSON объект в следующем формате:
        {{
            "yearlyShortage": 80000,
            "industry": "название отрасли",
            "description": "Краткое обоснование числа. Используй правильные падежи: 'в [отрасль] отрасли' или 'в [отрасль] секторе'"
        }}
        
        ПРИМЕРЫ ДЛЯ РАЗНЫХ ОТРАСЛЕЙ:
        - HVAC: 45000-80000 специалистов в год
        - IT: 120000-200000 специалистов в год  
        - Строительство: 60000-100000 специалистов в год
        - Медицина: 80000-150000 специалистов в год
        - Производство: 70000-120000 специалистов в год
        
        ВАЖНО: 
        - Число должно быть реалистичным для отрасли
        - Учти размер и масштаб отрасли
        - Включи обоснование в description
        """
    
    try:
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Clean and parse the response
        cleaned_response = response_text.strip()
        if cleaned_response.startswith('```json'):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith('```'):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        
        # Parse JSON response
        yearly_data = json.loads(cleaned_response)
        
        # Validate the structure
        if not isinstance(yearly_data, dict) or 'yearlyShortage' not in yearly_data:
            raise ValueError("Invalid yearly shortage data structure")
        
        if not isinstance(yearly_data['yearlyShortage'], int) or yearly_data['yearlyShortage'] <= 0:
            raise ValueError("Yearly shortage must be a positive integer")
        
        # Log the generated data for verification
        logger.info(f"[AI-Audit Landing Page] Generated yearly shortage data:")
        logger.info(f"[AI-Audit Landing Page] - Industry: {yearly_data.get('industry', 'Unknown')}")
        logger.info(f"[AI-Audit Landing Page] - Yearly Shortage: {yearly_data.get('yearlyShortage', 'Unknown')} specialists")
        logger.info(f"[AI-Audit Landing Page] - Description: {yearly_data.get('description', 'No description')}")
        
        return yearly_data
        
    except json.JSONDecodeError as e:
        logger.error(f"[AI-Audit Landing Page] Yearly shortage JSON parsing error: {e}")
        logger.error(f"[AI-Audit Landing Page] Raw response was: '{response_text}'")
        logger.error(f"[AI-Audit Landing Page] Cleaned response was: '{cleaned_response}'")
        # Fallback to default values
        return {
            "yearlyShortage": 80000,
            "industry": "HVAC",
            "description": "Типичный дефицит квалифицированных кадров в HVAC-отрасли"
        }
        
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error generating yearly shortage data: {e}")
        return {
            "yearlyShortage": 80000,
            "industry": "HVAC", 
            "description": "Типичный дефицит квалифицированных кадров в HVAC-отрасли"
        }


async def _run_landing_page_generation(payload, request, pool, job_id):
    try:
        # 📊 DETAILED LOGGING: Language preference received in backend
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received payload: {payload.model_dump()}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received language: {payload.language}")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Backend received companyWebsite: {payload.companyWebsite}")
        
        # 📊 LOG: Initial payload received
        logger.info(f"🔍 [AUDIT DATA FLOW] Starting landing page generation for job {job_id}")
        logger.info(f"📥 [AUDIT DATA FLOW] Initial payload: {payload.model_dump()}")
        
        set_progress(job_id, "Scraping company website...")
        # Scrape company data from website
        scraped_data = await scrape_company_data_from_website(payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit Landing Page] Scraped company data: {scraped_data.companyName}")
        
        set_progress(job_id, "Researching additional company info...")
        # Get additional research data using scraped company name and description
        duckduckgo_summary = await company_research(scraped_data.companyName, scraped_data.companyDesc, payload.companyWebsite, payload.language)
        logger.info(f"[AI-Audit Landing Page] Research summary: {duckduckgo_summary[:300]}")
        
        # 📊 LOG: Scraped data received
        logger.info(f"🌐 [AUDIT DATA FLOW] Scraped data length: {len(duckduckgo_summary)} characters")
        logger.info(f"🌐 [AUDIT DATA FLOW] Scraped data preview: {duckduckgo_summary[:500]}...")

        set_progress(job_id, "Using scraped company name...")
        company_name = scraped_data.companyName
        
        # 📊 LOG: Company name generated
        logger.info(f"🏢 [AUDIT DATA FLOW] Generated company name: '{company_name}'")

        set_progress(job_id, "Using scraped company description...")
        company_description = scraped_data.companyDesc

        # 📊 LOG: Company description generated
        logger.info(f"📝 [AUDIT DATA FLOW] Generated company description: '{company_description}'")

        set_progress(job_id, "Generating job positions from scraped data...")
        # Create a combined payload with scraped data for job positions generation
        combined_payload = type('CombinedPayload', (), {
            'companyName': scraped_data.companyName,
            'companyDesc': scraped_data.companyDesc,
            'companyWebsite': payload.companyWebsite,
            'employees': scraped_data.employees,
            'franchise': scraped_data.franchise,
            'onboardingProblems': scraped_data.onboardingProblems,
            'documents': scraped_data.documents,
            'documentsOther': scraped_data.documentsOther,
            'priorities': scraped_data.priorities,
            'priorityOther': scraped_data.priorityOther
        })()
        # Generate job positions using the same logic as the old audit
        job_positions = await generate_job_positions_from_scraped_data(duckduckgo_summary, combined_payload, company_name, payload.language)
        
        # 📊 LOG: Job positions generated
        logger.info(f"💼 [AUDIT DATA FLOW] Generated {len(job_positions)} job positions")
        for i, position in enumerate(job_positions):
            logger.info(f"💼 [AUDIT DATA FLOW] - Position {i+1}: {position}")

        set_progress(job_id, "Generating workforce crisis data...")
        # Generate workforce crisis data for the "Кадровый кризис" section
        workforce_crisis_data = await generate_workforce_crisis_data(duckduckgo_summary, combined_payload, payload.language)
        
        # 📊 LOG: Workforce crisis data generated
        logger.info(f"📊 [AUDIT DATA FLOW] Generated workforce crisis data: {workforce_crisis_data}")

        set_progress(job_id, "Generating course outline...")
        # Generate course outline for the "План обучения" section
        course_outline_modules = await generate_course_outline_for_landing_page(duckduckgo_summary, job_positions, combined_payload, payload.language)
        
        # 📊 LOG: Course outline generated
        logger.info(f"📚 [AUDIT DATA FLOW] Generated course outline with {len(course_outline_modules)} modules")
        for i, module_title in enumerate(course_outline_modules):
            logger.info(f"📚 [AUDIT DATA FLOW] - Module {i+1}: {module_title}")

        set_progress(job_id, "Generating course templates...")
        # Generate course templates for the "Готовые шаблоны курсов" section
        course_templates = await generate_course_templates(duckduckgo_summary, job_positions, combined_payload, course_outline_modules, payload.language)
        
        # 📊 LOG: Course templates generated
        logger.info(f"🎓 [AUDIT DATA FLOW] Generated {len(course_templates)} course templates")
        for i, template in enumerate(course_templates):
            logger.info(f"🎓 [AUDIT DATA FLOW] - Template {i+1}: {template['title']}")

        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Create the landing page content with dynamic data
        landing_page_data = {
            "companyName": company_name,
            "companyDescription": company_description,
            "jobPositions": job_positions,
            "workforceCrisis": workforce_crisis_data,
            "courseOutlineModules": course_outline_modules,
            "courseTemplates": course_templates,
            "language": payload.language,
            "originalPayload": payload.model_dump()
        }
        
        # 📊 DETAILED LOGGING: Language preference in landing page data
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Landing page data - language: '{landing_page_data['language']}'")
        logger.info(f"🔍 [LANGUAGE FLOW DEBUG] Landing page data - payload.language: '{payload.language}'")
        
        # 📊 LOG: Landing page data structure created
        logger.info(f"📦 [AUDIT DATA FLOW] Landing page data structure created:")
        logger.info(f"📦 [AUDIT DATA FLOW] - companyName: '{landing_page_data['companyName']}'")
        logger.info(f"📦 [AUDIT DATA FLOW] - companyDescription: '{landing_page_data['companyDescription']}'")
        logger.info(f"📦 [AUDIT DATA FLOW] - originalPayload keys: {list(landing_page_data['originalPayload'].keys())}")

        # Save as a product
        project_id = await insert_ai_audit_onepager_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=f"AI-Аудит Landing Page: {company_name}",
            microproduct_content=landing_page_data,
            chat_session_id=None
        )

        logger.info(f"[AI-Audit Landing Page] Successfully created project with ID: {project_id}")
        
        # 📊 LOG: Project saved to database
        logger.info(f"💾 [AUDIT DATA FLOW] Project saved to database with ID: {project_id}")
        logger.info(f"💾 [AUDIT DATA FLOW] Project name: 'AI-Аудит Landing Page: {company_name}'")
        
        # 🔧 FIX: Assign landing page to existing audit folder or create new one
        # First, try to find an existing audit folder for this company
        async with pool.acquire() as conn:
            existing_folder_query = """
            SELECT pf.id 
            FROM project_folders pf
            JOIN projects p ON pf.id = p.folder_id
            WHERE pf.onyx_user_id = $1 
            AND p.microproduct_name LIKE 'AI-Аудит: %'
            AND p.microproduct_name LIKE $2
            LIMIT 1
            """
            existing_folder = await conn.fetchrow(existing_folder_query, onyx_user_id, f"%{company_name}%")
            
            if existing_folder:
                # Assign to existing folder
                folder_id = existing_folder["id"]
                await conn.execute("UPDATE projects SET folder_id = $1 WHERE id = $2", folder_id, project_id)
                logger.info(f"🔧 [AUDIT DATA FLOW] Assigned landing page to existing folder: {folder_id}")
            else:
                # Create new folder and assign
                folder_id = await create_audit_folder(pool, onyx_user_id, company_name)
                await conn.execute("UPDATE projects SET folder_id = $1 WHERE id = $2", folder_id, project_id)
                logger.info(f"🔧 [AUDIT DATA FLOW] Created new folder and assigned landing page: {folder_id}")

        set_progress(job_id, "Landing page complete!")
        logger.info(f"[AI-Audit Landing Page] Finished the Landing Page Generation")
        
        # 📊 LOG: Final response data
        final_response = {
            "id": project_id,
            "name": f"AI-Аудит Landing Page: {company_name}",
            "companyName": company_name,
            "companyDescription": company_description
        }
        logger.info(f"📤 [AUDIT DATA FLOW] Final response data: {final_response}")
    
        return final_response
    except Exception as e:
        logger.error(f"[AI-Audit Landing Page] Error: {e}")
        set_progress(job_id, f"Error: {str(e)}")


# Event Poster Save as Product Endpoint - Exact same approach as AI Audit
@app.post("/api/custom/event-poster/save-as-product")
async def save_event_poster_as_product(payload: dict, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Save event poster as a product using exact same approach as AI audit"""
    try:
        logger.info(f"💾 [EVENT_POSTER_SAVE] Starting save as product")
        logger.info(f"💾 [EVENT_POSTER_SAVE] Payload: {payload}")
        
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Create project name from event name
        event_name = payload.get('eventName', 'Event Poster')
        project_name = f"Event Poster: {event_name}"
        
        # Save as a product using exact same approach as AI audit
        project_id = await insert_event_poster_to_db(
            pool=pool,
            onyx_user_id=onyx_user_id,
            project_name=project_name,
            microproduct_content=payload,
            chat_session_id=None
        )

        logger.info(f"💾 [EVENT_POSTER_SAVE] Successfully created project with ID: {project_id}")
        
        return {
            "id": project_id,
            "name": project_name,
            "message": "Event poster saved successfully!"
        }
        
    except Exception as e:
        logger.error(f"💾 [EVENT_POSTER_SAVE] Error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save event poster: {str(e)}")


async def insert_event_poster_to_db(
    pool: asyncpg.Pool,
    onyx_user_id: str,
    project_name: str,
    microproduct_content: dict,
    chat_session_id: str = None
) -> int:
    """Insert event poster into database - exact same approach as AI audit"""
    
    # First, ensure we have a Text Presentation template (same as audit)
    template_id = await _ensure_text_presentation_template(pool)
    
    insert_query = """
    INSERT INTO projects (
        onyx_user_id, project_name, product_type, microproduct_type,
        microproduct_name, microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, NOW(), $9)
    RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                microproduct_content, design_template_id, source_chat_session_id, created_at, folder_id;
    """
    
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            insert_query,
            onyx_user_id,
            project_name,
            "Event Poster",  # product_type
            "Event Poster",  # microproduct_type
            project_name,  # microproduct_name
            microproduct_content,  # event poster data
            template_id,  # design_template_id (from _ensure_text_presentation_template)
            chat_session_id,  # source_chat_session_id
            None,  # folder_id - no folder assignment for now
        )
    
    if not row:
        raise HTTPException(status_code=500, detail="Failed to create event poster project entry.")
    
    return row["id"]


# Event Poster Update Endpoint - Auto-save functionality 
@app.put("/api/custom/event-poster/update/{project_id}")
async def update_event_poster_data(project_id: int, payload: dict, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Update event poster data (auto-save functionality).
    """
    try:
        logger.info(f"🔄 [EVENT_POSTER_UPDATE] Auto-save request for project ID: {project_id}")
        
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get the microProductContent from payload
        microproduct_content = payload.get('microProductContent')
        if not microproduct_content:
            raise HTTPException(status_code=400, detail="microProductContent is required")
        
        logger.info(f"🔄 [EVENT_POSTER_UPDATE] Updating content: {list(microproduct_content.keys()) if microproduct_content else 'None'}")
        
        # Update the project data (no updated_at column in projects table)
        update_query = """
        UPDATE projects 
        SET microproduct_content = $1
        WHERE id = $2 AND onyx_user_id = $3
        RETURNING id, microproduct_content, microproduct_name
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(update_query, microproduct_content, project_id, onyx_user_id)
            
        if not row:
            logger.error(f"❌ [EVENT_POSTER_UPDATE] Project {project_id} not found for user {onyx_user_id}")
            raise HTTPException(status_code=404, detail="Event poster project not found or no permission")
        
        logger.info(f"✅ [EVENT_POSTER_UPDATE] Successfully updated project {project_id}")
        
        # Return the updated data
        return {
            "success": True,
            "projectId": project_id,
            "microproduct_content": row["microproduct_content"],
            "project_name": row["microproduct_name"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [EVENT_POSTER_UPDATE] Error updating data: {e}")
        raise HTTPException(status_code=500, detail="Failed to update event poster data")


# Event Poster Data Fetch Endpoint - Exact same approach as AI Audit Landing Page
@app.get("/api/custom/event-poster/{project_id}")
async def get_event_poster_data(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Get the event poster data for a specific project (same approach as AI audit landing page).
    """
    try:
        logger.info(f"📥 [EVENT_POSTER_DATA] Data request for project ID: {project_id}")
        
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get the project data (same query as AI audit)
        query = """
        SELECT microproduct_content, microproduct_name 
        FROM projects 
        WHERE id = $1 AND onyx_user_id = $2
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, project_id, onyx_user_id)
            
        if not row:
            logger.error(f"❌ [EVENT_POSTER_DATA] Project {project_id} not found for user {onyx_user_id}")
            raise HTTPException(status_code=404, detail="Event poster project not found")
        
        content = row["microproduct_content"]
        project_name = row["microproduct_name"]
        
        logger.info(f"💾 [EVENT_POSTER_DATA] Retrieved project data from database:")
        logger.info(f"💾 [EVENT_POSTER_DATA] - Project name: '{project_name}'")
        logger.info(f"💾 [EVENT_POSTER_DATA] - Content keys: {list(content.keys()) if content else 'None'}")
        logger.info(f"💾 [EVENT_POSTER_DATA] - Content structure: {content}")
        
        # Return the event poster data directly (same structure as stored)
        return_data = {
            "projectId": project_id,
            "projectName": project_name,
            "eventData": content
        }
        
        logger.info(f"💾 [EVENT_POSTER_DATA] Returning data: {return_data}")
        
        return return_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [EVENT_POSTER_DATA] Error fetching data: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch event poster data")


def extract_open_positions_from_table(parsed_json):
    """
    Extracts open positions from a TableBlock in parsed_json.contentBlocks.
    Returns a list of dicts, one per position, with keys matching the table headers.
    Removes trailing '*' from header keys.
    """
    def clean_key(key):
        # Remove all trailing and leading '*' and whitespace
        return key.strip().rstrip("*").lstrip("*").strip()

    for block in getattr(parsed_json, "contentBlocks", []):
        if getattr(block, "type", None) == "table":
            headers = getattr(block, "headers", [])
            rows = getattr(block, "rows", [])
            # Normalize header names for easier matching
            header_map = {clean_key(h).lower(): i for i, h in enumerate(headers)}
            print("HEADER MAP:", header_map)
            if "позиция" in header_map:
                positions = []
                for row in rows:
                    position = {clean_key(headers[i]): row[i] for i in range(min(len(headers), len(row)))}
                    positions.append(position)

                return positions
    return []


async def generate_company_specific_fallback_positions(company_name: str, language: str = "ru") -> list:
    """Generate company-specific fallback positions when no real positions are found."""
    try:
        if language == "en":
            prompt = f"""
            Create a list of 3-5 logical positions for the company {company_name}.
            
            INSTRUCTIONS:
            - Create positions that logically fit this company
            - Use realistic job titles
            - Add a brief description for each position
            - Generate ALL content EXCLUSIVELY in English
            
            RESPONSE FORMAT (JSON only):
            [
                {{"Position": "position title 1", "Description": "brief description"}},
                {{"Position": "position title 2", "Description": "brief description"}},
                ...
            ]
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Crea una lista de 3-5 posiciones lógicas para la empresa {company_name}.
            
            INSTRUCCIONES:
            - Crea posiciones que se ajusten lógicamente a esta empresa
            - Usa títulos de trabajo realistas
            - Agrega una descripción breve para cada posición
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            FORMATO DE RESPUESTA (solo JSON):
            [
                {{"Position": "título de posición 1", "Description": "descripción breve"}},
                {{"Position": "título de posición 2", "Description": "descripción breve"}},
                ...
            ]
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Створіть список з 3-5 логічних позицій для компанії {company_name}.
            
            ІНСТРУКЦІЇ:
            - Створіть позиції, які логічно підходять для цієї компанії
            - Використовуйте реалістичні назви посад
            - Додайте короткий опис для кожної позиції
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ФОРМАТ ВІДПОВІДІ (тільки JSON):
            [
                {{"Position": "назва позиції 1", "Description": "короткий опис"}},
                {{"Position": "назва позиції 2", "Description": "короткий опис"}},
                ...
            ]
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:  # Russian
            prompt = f"""
            Создай список из 3-5 логичных должностей для компании {company_name}.
            
            ИНСТРУКЦИИ:
            - Создай позиции, которые логично подходят для данной компании
            - Используй реалистичные названия должностей
            - Добавь краткое описание для каждой позиции
            
            ФОРМАТ ОТВЕТА (только JSON):
            [
                {{"Position": "название позиции 1", "Description": "краткое описание"}},
                {{"Position": "название позиции 2", "Description": "краткое описание"}},
                ...
            ]
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            positions = json.loads(cleaned_response)
            formatted_positions = []
            for position in positions:
                # Handle different field names based on language
                title = position.get("Position", position.get("Позиция", "Position"))
                description = position.get("Description", position.get("Описание", f"Open position at {company_name}"))
                formatted_positions.append({
                    "title": title,
                    "description": description,
                    "icon": "👷"
                })
            logger.info(f"💼 [WEBSITE SCRAPING] Generated {len(formatted_positions)} company-specific fallback positions")
            return formatted_positions
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse fallback positions JSON: {e}")
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Raw response was: '{response_text}'")
            # Language-specific generic fallback
            if language == "en":
                return [
                    {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
                    {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
                    {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"}
                ]
            elif language == "es":
                return [
                    {"title": "Representante de Ventas", "description": f"Ventas y desarrollo comercial en {company_name}", "icon": "💼"},
                    {"title": "Atención al Cliente", "description": f"Servicio al cliente y soporte en {company_name}", "icon": "🎧"},
                    {"title": "Gerente de Operaciones", "description": f"Gestión de operaciones y procesos en {company_name}", "icon": "⚙️"}
                ]
            elif language == "ua":
                return [
                    {"title": "Представник з продажів", "description": f"Продажі та розвиток бізнесу в {company_name}", "icon": "💼"},
                    {"title": "Служба підтримки клієнтів", "description": f"Обслуговування клієнтів та підтримка в {company_name}", "icon": "🎧"},
                    {"title": "Менеджер операцій", "description": f"Управління операціями та процесами в {company_name}", "icon": "⚙️"}
                ]
            else:  # Russian
                return [
                    {"title": "Менеджер по продажам", "description": f"Продажи и развитие бизнеса в {company_name}", "icon": "💼"},
                    {"title": "Служба поддержки", "description": f"Обслуживание клиентов и поддержка в {company_name}", "icon": "🎧"},
                    {"title": "Менеджер операций", "description": f"Управление операциями и процессами в {company_name}", "icon": "⚙️"}
                ]
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error generating fallback positions: {e}")
        return [
            {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
            {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
            {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"}
        ]

async def extract_job_positions_from_website_content(website_content: str, company_name: str, language: str = "ru") -> list:
    """Extract job positions directly from website content using AI."""
    try:
        if language == "en":
            prompt = f"""
            Analyze the website content and extract a list of open job positions for the company.
            
            COMPANY: {company_name}
            WEBSITE CONTENT:
            {website_content}
            
            INSTRUCTIONS:
            - Find all mentions of job openings, positions, career opportunities
            - Extract specific position titles (e.g., "Sales Manager", "Mechanical Engineer", "Marketing Specialist")
            - If no specific vacancies are found, create logical positions for this company
            - Return maximum 8 real positions
            - Generate ALL content EXCLUSIVELY in English
            
            RESPONSE FORMAT (JSON only):
            [
                {{"Position": "position title 1", "Description": "brief description"}},
                {{"Position": "position title 2", "Description": "brief description"}},
                ...
            ]
            
            RESPONSE (JSON only):
            """
        elif language == "es":
            prompt = f"""
            Analiza el contenido del sitio web y extrae una lista de puestos de trabajo abiertos para la empresa.
            
            EMPRESA: {company_name}
            CONTENIDO DEL SITIO WEB:
            {website_content}
            
            INSTRUCCIONES:
            - Encuentra todas las menciones de ofertas de trabajo, posiciones, oportunidades de carrera
            - Extrae títulos de posiciones específicas (ej: "Gerente de Ventas", "Ingeniero Mecánico", "Especialista en Marketing")
            - Si no se encuentran vacantes específicas, crea posiciones lógicas para esta empresa
            - Devuelve máximo 8 posiciones reales
            - Genera TODO el contenido EXCLUSIVAMENTE en español
            
            FORMATO DE RESPUESTA (solo JSON):
            [
                {{"Position": "título de posición 1", "Description": "descripción breve"}},
                {{"Position": "título de posición 2", "Description": "descripción breve"}},
                ...
            ]
            
            RESPUESTA (solo JSON):
            """
        elif language == "ua":
            prompt = f"""
            Проаналізуйте вміст веб-сайту та витягніть список відкритих вакансій для компанії.
            
            КОМПАНІЯ: {company_name}
            ВМІСТ ВЕБ-САЙТУ:
            {website_content}
            
            ІНСТРУКЦІЇ:
            - Знайдіть усі згадки про вакансії, посади, кар'єрні можливості
            - Витягніть назви конкретних посад (наприклад: "Менеджер з продажів", "Інженер-механік", "Спеціаліст з маркетингу")
            - Якщо конкретних вакансій не знайдено, створіть логічні позиції для цієї компанії
            - Поверніть максимум 8 реальних позицій
            - Генеруйте ВЕСЬ контент ВИКЛЮЧНО українською мовою
            
            ФОРМАТ ВІДПОВІДІ (тільки JSON):
            [
                {{"Position": "назва позиції 1", "Description": "короткий опис"}},
                {{"Position": "назва позиції 2", "Description": "короткий опис"}},
                ...
            ]
            
            ВІДПОВІДЬ (тільки JSON):
            """
        else:
            prompt = f"""
            Проанализируй контент веб-сайта и извлеки список открытых вакансий компании.
            
            КОМПАНИЯ: {company_name}
            КОНТЕНТ ВЕБ-САЙТА:
            {website_content}
            
            ИНСТРУКЦИИ:
            - Найди все упоминания вакансий, должностей, карьерных возможностей
            - Извлеки названия конкретных позиций (например: "Менеджер по продажам", "Инженер-механик", "Специалист по маркетингу")
            - Если конкретных вакансий нет, создай логичные позиции для данной компании
            - Верни максимум 8 реальных позиций
            
            ФОРМАТ ОТВЕТА (только JSON):
            [
                {{"Позиция": "название позиции 1", "Описание": "краткое описание"}},
                {{"Позиция": "название позиции 2", "Описание": "краткое описание"}},
                ...
            ]
            
            ОТВЕТ (только JSON):
            """
        
        response_text = await stream_openai_response_direct(
            prompt=prompt,
            model=LLM_DEFAULT_MODEL
        )
        
        # Parse JSON response - handle markdown-wrapped JSON
        try:
            # Clean the response text - remove markdown code blocks if present
            cleaned_response = response_text.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove ```
            cleaned_response = cleaned_response.strip()
            
            job_positions = json.loads(cleaned_response)
            
            if not isinstance(job_positions, list):
                raise ValueError("Response is not a list")
            
            logger.info(f"💼 [WEBSITE SCRAPING] Extracted {len(job_positions)} job positions from website")
            return job_positions
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Failed to parse job positions JSON: {e}")
            logger.warning(f"⚠️ [WEBSITE SCRAPING] Raw response was: '{response_text}'")
            return []
        
    except Exception as e:
        logger.error(f"❌ [WEBSITE SCRAPING] Error extracting job positions: {e}")
        return []

async def generate_job_positions_from_scraped_data(duckduckgo_summary: str, payload, company_name: str, language: str = "ru") -> list:
    """
    Generates job positions directly from scraped website content using AI.
    More efficient than generating a full audit one-pager.
    Ensures exactly 11 vacancies are returned by generating additional ones if needed.
    """
    try:
        # 📊 LOG: Starting job positions generation
        logger.info(f"🔍 [AUDIT DATA FLOW] generate_job_positions_from_scraped_data called")
        logger.info(f"🔍 [AUDIT DATA FLOW] Scraped data length: {len(duckduckgo_summary)} characters")
        
        # Extract job positions directly from scraped content using AI
        job_positions = await extract_job_positions_from_website_content(duckduckgo_summary, company_name, language)
        
        # Convert to the format expected by the frontend
        formatted_positions = []
        for position in job_positions:
            # Get the position title and description - handle different field names based on language
            position_title = position.get("Position", position.get("Позиция", "Position"))
            position_description = position.get("Description", position.get("Описание", f"Open position at {company_name}"))
            formatted_positions.append({
                "title": position_title,
                "description": position_description,
                "icon": "👷"  # Default icon
            })
        
        # 📊 LOG: Job positions extracted and formatted
        logger.info(f"🔍 [AUDIT DATA FLOW] Extracted {len(job_positions)} raw positions")
        logger.info(f"🔍 [AUDIT DATA FLOW] Formatted {len(formatted_positions)} positions for frontend")
        
        # If no positions found, use company-specific fallback
        if not formatted_positions:
            logger.info(f"🔍 [AUDIT DATA FLOW] No positions found, using company-specific fallback")
            # Generate company-specific fallback positions
            fallback_positions = await generate_company_specific_fallback_positions(company_name, language)
            formatted_positions = fallback_positions
        
        # Ensure exactly 11 vacancies by generating additional ones if needed
        target_count = 11
        if len(formatted_positions) < target_count:
            needed_positions = target_count - len(formatted_positions)
            logger.info(f"🔍 [AUDIT DATA FLOW] Need {needed_positions} additional positions to reach {target_count} total")
            
            # Generate additional positions using the same logic as course templates
            additional_positions = await generate_additional_positions(duckduckgo_summary, needed_positions, payload, language)
            
            # Convert additional positions to the expected format
            for position in additional_positions:
                formatted_positions.append({
                    "title": position.get("title", "Generated Position"),
                    "description": position.get("description", f"Open position at {company_name}"),
                    "icon": "👷"  # Default icon
                })
            
            logger.info(f"🔍 [AUDIT DATA FLOW] Added {len(additional_positions)} additional positions")
        
        # Ensure we don't exceed 11 positions
        if len(formatted_positions) > target_count:
            formatted_positions = formatted_positions[:target_count]
            logger.info(f"🔍 [AUDIT DATA FLOW] Trimmed positions to exactly {target_count}")
        
        logger.info(f"🔍 [AUDIT DATA FLOW] Final result: {len(formatted_positions)} positions")
        return formatted_positions
        
    except Exception as e:
        logger.error(f"❌ [AUDIT DATA FLOW] Error generating job positions: {e}")
        # Return company-specific fallback positions
        try:
            fallback_positions = await generate_company_specific_fallback_positions(company_name, language)
            # Ensure we have exactly 11 positions
            while len(fallback_positions) < 11:
                fallback_positions.append({
                    "title": f"Position {len(fallback_positions) + 1}",
                    "description": f"Open position at {company_name}",
                    "icon": "👷"
                })
            return fallback_positions[:11]
        except Exception as fallback_error:
            logger.error(f"❌ [AUDIT DATA FLOW] Error generating fallback positions: {fallback_error}")
            # Ultimate fallback - generic positions
            return [
                {"title": "Sales Representative", "description": f"Sales and business development at {company_name}", "icon": "💼"},
                {"title": "Customer Support", "description": f"Customer service and support at {company_name}", "icon": "🎧"},
                {"title": "Operations Manager", "description": f"Operations and process management at {company_name}", "icon": "⚙️"},
                {"title": "Marketing Specialist", "description": f"Marketing strategies at {company_name}", "icon": "📢"},
                {"title": "Quality Assurance", "description": f"Quality control at {company_name}", "icon": "✅"},
                {"title": "Technical Support", "description": f"Technical assistance at {company_name}", "icon": "🔧"},
                {"title": "Project Manager", "description": f"Project coordination at {company_name}", "icon": "📋"},
                {"title": "Logistics Coordinator", "description": f"Supply chain management at {company_name}", "icon": "📦"},
                {"title": "HR Specialist", "description": f"Human resources at {company_name}", "icon": "👥"},
                {"title": "Finance Analyst", "description": f"Financial analysis at {company_name}", "icon": "💰"},
                {"title": "IT Administrator", "description": f"IT systems management at {company_name}", "icon": "💻"}
            ]


def extract_job_positions_from_content(content):
    """
    Extracts job positions from the AI audit content.
    Returns a list of job position objects with title and description.
    """
    # 📊 LOG: Job positions extraction function called
    logger.info(f"🔍 [AUDIT DATA FLOW] extract_job_positions_from_content called")
    logger.info(f"🔍 [AUDIT DATA FLOW] Content type: {type(content)}")
    logger.info(f"🔍 [AUDIT DATA FLOW] Content keys: {list(content.keys()) if isinstance(content, dict) else 'Not a dict'}")
    
    job_positions = []
    
    if not content or not isinstance(content, dict):
        logger.info(f"🔍 [AUDIT DATA FLOW] No valid content provided, returning empty list")
        return job_positions
    
    # Look for contentBlocks in the content
    content_blocks = content.get("contentBlocks", [])
    logger.info(f"🔍 [AUDIT DATA FLOW] Found {len(content_blocks)} content blocks")
    
    for i, block in enumerate(content_blocks):
        if block.get("type") == "table":
            headers = block.get("headers", [])
            rows = block.get("rows", [])
            
            logger.info(f"🔍 [AUDIT DATA FLOW] Table {i+1}: {len(headers)} headers, {len(rows)} rows")
            logger.info(f"🔍 [AUDIT DATA FLOW] Headers: {headers}")
            
            # Check if this is a job positions table
            if any("позиция" in str(header).lower() for header in headers):
                logger.info(f"🔍 [AUDIT DATA FLOW] Found job positions table!")
                for j, row in enumerate(rows):
                    if len(row) > 0:
                        position_title = str(row[0]).strip() if row[0] else "Position"
                        # Create a simple job position object
                        position = {
                            "title": position_title,
                            "description": f"Open position at the company",
                            "icon": "👷"  # Default icon
                        }
                        job_positions.append(position)
                        logger.info(f"🔍 [AUDIT DATA FLOW] Added position {j+1}: {position}")
    
    # If no positions found in tables, return some default positions
    if not job_positions:
        logger.info(f"🔍 [AUDIT DATA FLOW] No positions found in content, using default positions")
        job_positions = [
            {"title": "HVAC Technician", "description": "Installation and maintenance of heating, ventilation, and air conditioning systems", "icon": "👷"},
            {"title": "Electrician", "description": "Installation and maintenance of electrical systems", "icon": "⚡"},
            {"title": "Project Manager", "description": "Overseeing projects and coordinating teams", "icon": "📋"}
        ]
        logger.info(f"🔍 [AUDIT DATA FLOW] Using {len(job_positions)} default positions")
    
    logger.info(f"🔍 [AUDIT DATA FLOW] Returning {len(job_positions)} job positions")
    return job_positions


async def generate_and_finalize_course_outline_for_position(
    company_name: str,
    position: dict,
    onyx_user_id: str,
    pool,
    request: Request,
    language: str = "ru"):
    # 1. Build the prompt for the LLM
    wizard_request = {
        "product": "Course Outline",
        "prompt": (
            f"Создай курс аутлайн 'Онбординг для должности {position['Позиция']}' для новых сотрудников этой должности в компании '{company_name}'. \n"
            f"Структура должна охватывать все аспекты работы сотрудника на этой должности в данной среде. Не включай аспекты работы других должностей, только то, что касается должности '{position['Позиция']}'. \n"
        ),
        "modules": 4,
        "lessonsPerModule": "5-7",
        "language": language
    }
    # Convert to JSON string for the LLM
    prompt = json.dumps(wizard_request, ensure_ascii=False)

    outline_text = ""
    async for chunk_data in stream_openai_response(prompt):
        if chunk_data.get("type") == "delta":
            outline_text += chunk_data["text"]
        elif chunk_data.get("type") == "error":
            raise Exception(f"OpenAI error: {chunk_data['text']}")


    # 4. Finalize/save the project (reuse add_project_to_custom_db)
    template_id = await _ensure_training_plan_template(pool)

    project_data = ProjectCreateRequest(
        projectName=f"Онбординг: {position['Позиция']}",
        design_template_id=template_id,
        microProductName=f"Онбординг: {position['Позиция']}",
        aiResponse=outline_text,
        chatSessionId=None
    )
    project_db_candidate = await add_project_to_custom_db(
        project_data=project_data,
        onyx_user_id=onyx_user_id,
        pool=pool
    )

    try:
        async with pool.acquire() as conn:
            # Convert Pydantic model to dictionary for processing
            content = project_db_candidate.microproduct_content.model_dump(mode='json', exclude_none=True) if project_db_candidate.microproduct_content else {}
            
            if isinstance(content, dict) and content.get("sections"):
                sections = content["sections"]
                updated_sections = []
                
                for section in sections:
                    if isinstance(section, dict) and section.get("lessons"):
                        # Ensure each lesson has proper hours value and completionTime (default to 1 hour and 5m if missing)
                        updated_lessons = []
                        for lesson in section["lessons"]:
                            if isinstance(lesson, dict):
                                # Set default hours if missing or zero
                                if lesson.get("hours", 0) == 0:
                                    lesson["hours"] = 1
                                # Set default completionTime if missing
                                if not lesson.get("completionTime"):
                                    lesson["completionTime"] = "5m"
                                # Ensure all required lesson fields are present
                                lesson.setdefault("check", {"type": "none", "text": ""})
                                # Set content coverage based on source
                                source = lesson.get("source", "Create from scratch")
                                content_available = {"type": "yes", "text": "0%"} if source == "Create from scratch" else {"type": "yes", "text": "0%"}
                                lesson.setdefault("contentAvailable", content_available)
                                lesson.setdefault("source", source)
                                # Populate recommended content types if missing
                                try:
                                    existing_flags = {
                                        "presentation": False,
                                        "one-pager": False,
                                        "quiz": False,
                                        "video-lesson": False,
                                    }
                                    recommendations = analyze_lesson_content_recommendations(
                                        lesson.get("title", ""),
                                        lesson.get("quality_tier") or section.get("quality_tier") or content.get("quality_tier"),
                                        existing_flags
                                    )
                                    lesson.setdefault("recommended_content_types", recommendations)
                                    # Update completionTime from recommendations
                                    try:
                                        lesson["completionTime"] = compute_completion_time_from_recommendations(recommendations.get("primary", []))
                                        # Also generate completion_breakdown for advanced mode support
                                        primary = recommendations.get("primary", [])
                                        ranges = {
                                            'one-pager': (2,3),
                                            'presentation': (5,10),
                                            'quiz': (5,7),
                                            'video-lesson': (2,5),
                                        }
                                        breakdown = {}
                                        total_m = 0
                                        for p in primary:
                                            r = ranges.get(p)
                                            if r:
                                                mid = int(round((r[0]+r[1])/2))
                                                breakdown[p] = mid
                                                total_m += mid
                                        if total_m > 0:
                                            lesson['completion_breakdown'] = breakdown
                                    except Exception:
                                        lesson.setdefault("completionTime", "5m")
                                except Exception:
                                    pass
                                updated_lessons.append(lesson)
                            else:
                                # If lesson is just a string, convert to proper structure
                                updated_lessons.append({
                                    "title": str(lesson),
                                    "check": {"type": "none", "text": ""},
                                    "contentAvailable": {"type": "yes", "text": "0%"},
                                    "source": "Create from scratch",
                                    "hours": 1,
                                    "recommended_content_types": analyze_lesson_content_recommendations(str(lesson), content.get("quality_tier"), {"presentation": False, "one-pager": False, "quiz": False, "video-lesson": False})
                                })
                        
                        # Calculate total hours from lesson hours
                        total_hours = sum(lesson.get("hours", 0) for lesson in updated_lessons)
                        
                        # Update section with calculated total hours and set autoCalculateHours to true
                        updated_section = {
                            **section,
                            "lessons": updated_lessons,
                            "totalHours": total_hours,
                            "autoCalculateHours": True
                        }
                        # Ensure section has proper ID if missing
                        if not updated_section.get("id"):
                            updated_section["id"] = f"№{len(updated_sections) + 1}"
                        updated_sections.append(updated_section)
                    else:
                        updated_sections.append(section)
                
                # Update the project with recalculated totals and ensure mainTitle and detectedLanguage
                if updated_sections:
                    updated_content = {
                        **content, 
                        "sections": updated_sections,
                        "mainTitle": content.get("mainTitle") or f"Онбординг: {position['Позиция']}",
                        "detectedLanguage": content.get("detectedLanguage") or language
                    }
                    await conn.execute(
                        """
                        UPDATE projects
                        SET microproduct_content = $1::jsonb
                        WHERE id = $2
                        """,
                        json.dumps(updated_content), project_db_candidate.id
                    )
                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
    except Exception as e:
        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")


    return project_db_candidate


@app.post("/api/custom/course-outline/finalize")
async def wizard_outline_finalize(payload: OutlineWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Get user ID and deduct credits for course outline finalization
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        credits_needed = calculate_product_credits("course_outline")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Course outline finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for course outline finalization")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for course outline: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    # Ensure we have a chat session id (needed both for cache lookup and possible assistant fallback)
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        # Check if this is a Knowledge Base search request
        use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
        persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Helper: check whether the user made ANY changes (structure or content)
    def _any_changes_made(orig_modules: List[Dict[str, Any]], edited: Dict[str, Any]) -> bool:
        try:
            edited_sections = edited.get("sections") or edited.get("modules") or []
            
            # Debug logging to understand the data structures
            logger.info(f"Comparing changes: orig_modules count={len(orig_modules)}, edited_sections count={len(edited_sections)}")
            
            # Check structural changes first (modules/lessons added/removed)
            if len(orig_modules) != len(edited_sections):
                logger.info(f"Structural change detected: module count changed from {len(orig_modules)} to {len(edited_sections)}")
                return True
            
            # Check for content changes (titles modified)
            for i, (o, e) in enumerate(zip(orig_modules, edited_sections)):
                # Compare module titles
                orig_title = str(o.get("title", "")).strip()
                edited_title = str(e.get("title", "")).strip() if isinstance(e, dict) else str(e).strip()
                
                logger.debug(f"Module {i}: comparing titles '{orig_title}' vs '{edited_title}'")
                if orig_title != edited_title:
                    logger.info(f"Module title change detected at index {i}: '{orig_title}' -> '{edited_title}'")
                    return True
                
                # Compare lesson structure and content
                orig_lessons = o.get("lessons", [])
                edited_lessons = e.get("lessons", []) if isinstance(e, dict) else []
                
                if len(orig_lessons) != len(edited_lessons):
                    logger.info(f"Lesson count change detected in module {i}: {len(orig_lessons)} -> {len(edited_lessons)}")
                    return True
                
                # Compare individual lesson titles
                for j, (ol, el) in enumerate(zip(orig_lessons, edited_lessons)):
                    # Handle different lesson formats
                    if isinstance(ol, dict):
                        orig_lesson = str(ol.get("title", ol.get("name", ""))).strip()
                    else:
                        orig_lesson = str(ol).strip()
                    
                    if isinstance(el, dict):
                        edited_lesson = str(el.get("title", el.get("name", ""))).strip()
                    else:
                        edited_lesson = str(el).strip()
                    
                    logger.debug(f"Module {i}, Lesson {j}: comparing '{orig_lesson}' vs '{edited_lesson}'")
                    if orig_lesson != edited_lesson:
                        logger.info(f"Lesson change detected in module {i}, lesson {j}: '{orig_lesson}' -> '{edited_lesson}'")
                        return True
            
            logger.info("No changes detected - outline is identical")
            return False
        except Exception as e:
            # On any parsing issue assume changes were made so we use assistant
            logger.warning(f"Error during change detection (assuming changes made): {e}")
            return True



    # ---------- 1) Decide strategy ----------
    raw_outline_cached = OUTLINE_PREVIEW_CACHE.get(chat_id)
    
    # Debug cache lookup
    logger.info(f"DEBUG: Cache lookup for chat_id='{chat_id}', found cached outline: {bool(raw_outline_cached)}")
    if raw_outline_cached:
        logger.info(f"DEBUG: Cached outline preview (first 200 chars): {raw_outline_cached[:200]}...")
    else:
        logger.info(f"DEBUG: Available cache keys: {list(OUTLINE_PREVIEW_CACHE.keys())}")
    
    if raw_outline_cached:
        # Parse cached preview - try JSON first, fallback to markdown
        try:
            # Try to parse as JSON (new format)
            cached_json = json.loads(raw_outline_cached.strip())
            if isinstance(cached_json, dict) and "sections" in cached_json:
                # Convert JSON sections to modules format for comparison
                parsed_orig = []
                for section in cached_json["sections"]:
                    parsed_orig.append({
                        "id": section.get("id", ""),
                        "title": section.get("title", ""),
                        "lessons": [re.sub(r'^Lesson\s+\d+\.\d+:\s*', '', lesson.get("title", "")).strip() for lesson in section.get("lessons", [])],
                        "totalHours": section.get("totalHours", 0)
                    })
                logger.info(f"[FINALIZE_CACHE] Parsed {len(parsed_orig)} modules from JSON preview")
            else:
                # Fallback to markdown parsing
                parsed_orig = _parse_outline_markdown(raw_outline_cached)
                logger.info(f"[FINALIZE_CACHE] Used markdown fallback, parsed {len(parsed_orig)} modules")
        except (json.JSONDecodeError, KeyError) as e:
            # Fallback to markdown parsing for old format
            parsed_orig = _parse_outline_markdown(raw_outline_cached)
            logger.info(f"[FINALIZE_CACHE] JSON parse failed ({e}), used markdown fallback: {len(parsed_orig)} modules")
        
        # Debug: Log the data structures being compared
        logger.info(f"DEBUG: parsed_orig structure: {json.dumps(parsed_orig, indent=2)[:500]}...")
        logger.info(f"DEBUG: payload.editedOutline structure: {json.dumps(payload.editedOutline, indent=2)[:500]}...")
        
        any_changes = _any_changes_made(parsed_orig, payload.editedOutline)
        
        if not any_changes:
            # NO CHANGES: Use direct parser path (fastest)
            use_direct_parser = True
            use_assistant_then_parser = False
            logger.info("No changes detected - using direct parser path")
        else:
            # CHANGES DETECTED: Use assistant first, then parser
            use_direct_parser = False
            use_assistant_then_parser = True
            logger.info("Changes detected - using assistant + parser path")
    else:
        # No cached data available - use assistant + parser path
        use_direct_parser = False
        use_assistant_then_parser = True
        logger.info("No cached outline - using assistant + parser path")

    # ---------- 2) DIRECT PARSER PATH: No changes made, use cached data directly ----------
    if use_direct_parser:
        direct_path_project_id = None  # Track project ID for cleanup if needed
        try:
            # Use cached outline directly since no changes were made
            template_id = await _ensure_training_plan_template(pool)
            
            # Extract project name from JSON or markdown
            project_name_detected = None
            try:
                # Try JSON first
                cached_json = json.loads(raw_outline_cached.strip())
                if isinstance(cached_json, dict) and "mainTitle" in cached_json:
                    project_name_detected = cached_json["mainTitle"]
                    logger.info(f"[DIRECT_PATH] Extracted project name from JSON: {project_name_detected}")
            except (json.JSONDecodeError, KeyError):
                pass
            
            # Fallback to markdown extraction or payload prompt
            if not project_name_detected:
                project_name_detected = _extract_project_name_from_markdown(raw_outline_cached) or payload.prompt
                logger.info(f"[DIRECT_PATH] Using fallback project name: {project_name_detected}")
            
            logger.info(f"Direct parser path: Using cached outline with {len(raw_outline_cached)} characters")
            
            # Build source context from payload
            source_context_type, source_context_data = build_source_context(payload)
            
            project_request = ProjectCreateRequest(
                projectName=project_name_detected,
                design_template_id=template_id,
                microProductName=None,
                aiResponse=raw_outline_cached,
                chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                folder_id=int(payload.folderId) if payload.folderId else None,
                source_context_type=source_context_type,
                source_context_data=source_context_data,
            )
            onyx_user_id = await get_current_onyx_user_id(request)

            project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
            direct_path_project_id = project_db_candidate.id  # Store for potential cleanup
            
            logger.info(f"Direct parser path: Created project {direct_path_project_id}")
            logger.info(f"Direct parser path: Project content type: {type(project_db_candidate.microproduct_content)}")
            
            # Check if content was parsed successfully
            content_valid = False
            if project_db_candidate.microproduct_content:
                if hasattr(project_db_candidate.microproduct_content, "sections"):
                    sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                    content_valid = len(sections) > 0
                    logger.info(f"Direct parser path: Found {len(sections)} sections in parsed content")
                else:
                    logger.warning(f"Direct parser path: Content does not have sections attribute")
            else:
                logger.warning(f"Direct parser path: microproduct_content is None")

            # --- Patch theme into DB if provided (only for TrainingPlan components) ---
            if payload.theme and content_valid:
                async with pool.acquire() as conn:
                    design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                    if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                        await conn.execute(
                            """
                            UPDATE projects
                            SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                            WHERE id = $2
                            """,
                            payload.theme, project_db_candidate.id
                        )
                        row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                        if row_patch and row_patch["microproduct_content"] is not None:
                            project_db_candidate.microproduct_content = row_patch["microproduct_content"]

            # --- Recalculate module total hours after creation ---
            if content_valid and project_db_candidate.microproduct_content:
                try:
                    async with pool.acquire() as conn:
                        content = project_db_candidate.microproduct_content
                        if isinstance(content, dict) and content.get("sections"):
                            sections = content["sections"]
                            updated_sections = []
                            
                            for section in sections:
                                if isinstance(section, dict) and section.get("lessons"):
                                    # Calculate total hours from lesson hours
                                    total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                    # Update section with calculated total hours and set autoCalculateHours to true
                                    updated_section = {
                                        **section,
                                        "totalHours": total_hours,
                                        "autoCalculateHours": True
                                    }
                                    updated_sections.append(updated_section)
                                else:
                                    updated_sections.append(section)
                            
                            # Update the project with recalculated totals
                            if updated_sections:
                                updated_content = {**content, "sections": updated_sections}
                                await conn.execute(
                                    """
                                    UPDATE projects
                                    SET microproduct_content = $1::jsonb
                                    WHERE id = $2
                                    """,
                                    json.dumps(updated_content), project_db_candidate.id
                                )
                                logger.info(f"Direct parser path: Recalculated module total hours for project {project_db_candidate.id}")
                except Exception as e:
                    logger.warning(f"Direct parser path: Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

            # Success when we have valid parsed content
            if content_valid:
                logger.info(f"Direct parser path successful for project {direct_path_project_id}")
                logger.debug(f'Full content for project {direct_path_project_id}: {project_db_candidate.microproduct_content}')
                return JSONResponse(content={"type": "done", "id": project_db_candidate.id})
            else:
                # Direct parser path validation failed - clean up the created project and fall back to assistant
                logger.warning(f"Direct parser path validation failed for project {direct_path_project_id} - LLM parsing likely failed")
                logger.warning(f"Content details: {project_db_candidate.microproduct_content}")
                try:
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
                
                # Fall back to assistant path
                logger.info("Falling back to assistant + parser path due to direct parser failure")
                use_direct_parser = False
                use_assistant_then_parser = True
                
        except Exception as direct_e:
            # Clean up any project created during direct parser path failure
            if direct_path_project_id:
                logger.warning(f"Direct parser path failed with project {direct_path_project_id}, attempting cleanup...")
                try:
                    onyx_user_id = await get_current_onyx_user_id(request)
                    async with pool.acquire() as conn:
                        await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", direct_path_project_id, onyx_user_id)
                    logger.info(f"Successfully cleaned up failed direct parser project {direct_path_project_id}")
                except Exception as cleanup_e:
                    logger.error(f"Failed to cleanup direct parser project {direct_path_project_id}: {cleanup_e}")
            
            logger.error(f"Direct parser path failed with error: {direct_e}")
            
            # If another concurrent request already started creation we patiently wait for it instead of kicking off assistant again
            if isinstance(direct_e, HTTPException) and direct_e.status_code == status.HTTP_429_TOO_MANY_REQUESTS:
                logger.info("wizard_outline_finalize detected in-progress creation. Waiting for completion…")
                max_wait_sec = 900  # 15 minutes
                poll_every_sec = 1
                waited = 0
                while waited < max_wait_sec:
                    async with pool.acquire() as conn:
                        if chat_id:
                            # Prefer locating the project by the wizard chat_session_id (unique identifier per outline wizard run)
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                                uuid.UUID(chat_id),
                            )
                        else:
                            # Fallback to the previous behaviour when we have no chat_id information available
                            row = await conn.fetchrow(
                                "SELECT id, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2 ORDER BY created_at DESC LIMIT 1",
                                onyx_user_id,
                                payload.prompt,
                            )
                    if row and row["microproduct_content"] is not None:
                        return JSONResponse(content={"type": "done", "id": row["id"]})
                    await asyncio.sleep(poll_every_sec)
                    waited += poll_every_sec
                logger.warning("wizard_outline_finalize waited too long for existing creation – giving up")
            else:
                logger.warning(f"wizard_outline_finalize direct parser path failed – will use assistant path. Details: {direct_e}")
            
            # Fall back to assistant path
            use_direct_parser = False
            use_assistant_then_parser = True

    # ---------- 3) ASSISTANT + PARSER PATH: Process changes with assistant, then parse ----------
    if use_assistant_then_parser:
        # Before starting assistant path, check if a project was already created successfully for this session
        if chat_id:
            try:
                async with pool.acquire() as conn:
                    existing_row = await conn.fetchrow(
                        "SELECT id, microproduct_content FROM projects WHERE source_chat_session_id = $1 ORDER BY created_at DESC LIMIT 1",
                        uuid.UUID(chat_id),
                    )
                    if existing_row and existing_row["microproduct_content"] is not None:
                        # Check if the existing project has valid content
                        try:
                            content = existing_row["microproduct_content"]
                            if isinstance(content, dict) and content.get("sections"):
                                logger.info(f"Found existing valid project {existing_row['id']} for chat session, returning it")
                                return JSONResponse(content={"type": "done", "id": existing_row["id"]})
                        except Exception:
                            pass  # Continue with assistant path if content validation fails
            except Exception as e:
                logger.warning(f"Failed to check for existing project: {e}")
        
        # Build wizard payload for assistant path - different structure for finalization
        # CRITICAL: Don't send modules/lessonsPerModule during finalization as they conflict
        # with user edits and cause the AI to ignore the actual edited structure
        wiz_payload = {
            "product": "Course Outline",
            "action": "finalize",
            "prompt": payload.prompt,
            "language": payload.language,
            "editedOutline": payload.editedOutline,
        }
        
        # Only add structural parameters if no user edits exist (fallback case)
        edited_sections = payload.editedOutline.get("sections", payload.editedOutline.get("modules", [])) if payload.editedOutline else []
        user_edit_module_count = len(edited_sections)
        
        if not payload.editedOutline or user_edit_module_count == 0:
            logger.info(f"[FINALIZE_PAYLOAD] No user edits found, adding structural parameters as fallback: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}")
            wiz_payload["modules"] = payload.modules
            wiz_payload["lessonsPerModule"] = payload.lessonsPerModule
        else:
            logger.info(f"[FINALIZE_PAYLOAD] User edits present ({user_edit_module_count} modules) - omitting conflicting structural parameters (original: modules={payload.modules}, lessonsPerModule={payload.lessonsPerModule}) to preserve user structure")
            # Log the first few modules to understand the structure
            for i, section in enumerate(edited_sections[:3]):
                if isinstance(section, dict):
                    section_title = section.get("title", "Unknown")
                    section_lessons = len(section.get("lessons", []))
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: '{section_title}' ({section_lessons} lessons)")
                else:
                    logger.info(f"[FINALIZE_PAYLOAD] User edit module {i+1}: {type(section)} - {str(section)[:50]}...")
            if user_edit_module_count > 3:
                logger.info(f"[FINALIZE_PAYLOAD] ... and {user_edit_module_count - 3} more modules")

        # Add file context if provided
        if payload.fromFiles:
            wiz_payload["fromFiles"] = True
            if payload.folderIds:
                wiz_payload["folderIds"] = payload.folderIds
            if payload.fileIds:
                wiz_payload["fileIds"] = payload.fileIds

        # Add text context if provided
        if payload.fromText and payload.userText:
            wiz_payload["fromText"] = True
            wiz_payload["textMode"] = payload.textMode
            wiz_payload["userText"] = payload.userText

        wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)
        logger.info(f"[FINALIZE_PAYLOAD] Final wizard message structure: {list(wiz_payload.keys())}")
        logger.info(f"[FINALIZE_PAYLOAD] Wizard message length: {len(wizard_message)} chars")

        async def streamer():
            assistant_reply: str = ""
            last_send = asyncio.get_event_loop().time()
            chunks_received = 0

            # Use longer timeout for large text processing to prevent AI memory issues
            timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
            logger.info(f"[FINALIZE_OPENAI_STREAM] Starting OpenAI finalization streamer with timeout: {timeout_duration} seconds")
            logger.info(f"[FINALIZE_OPENAI_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
            
            try:
                # Use OpenAI streaming for finalization instead of Onyx
                logger.info(f"[FINALIZE_OPENAI_STREAM] ✅ USING OPENAI DIRECT STREAMING for finalization")
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[FINALIZE_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[FINALIZE_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return

                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[FINALIZE_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[FINALIZE_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
            except Exception as e:
                logger.error(f"[FINALIZE_OPENAI_STREAM_ERROR] Error in OpenAI finalization streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

            # Cache full raw outline for later finalize step
            if chat_id:
                OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

            # Create the project using the assistant response
            try:
                template_id = await _ensure_training_plan_template(pool)
                project_name_detected = _extract_project_name_from_markdown(assistant_reply) or payload.prompt
                
                logger.info(f"Assistant + parser path: Creating project with {len(assistant_reply)} characters")
                
                # Build source context from payload
                source_context_type, source_context_data = build_source_context(payload)
                
                project_request = ProjectCreateRequest(
                    projectName=project_name_detected,
                    design_template_id=template_id,
                    microProductName=None,
                    aiResponse=assistant_reply,
                    chatSessionId=uuid.UUID(chat_id) if chat_id else None,
                    folder_id=int(payload.folderId) if payload.folderId else None,
                    source_context_type=source_context_type,
                    source_context_data=source_context_data,
                )
                onyx_user_id = await get_current_onyx_user_id(request)

                project_db_candidate = await add_project_to_custom_db(project_request, onyx_user_id, pool)  # type: ignore[arg-type]
                
                logger.info(f"Assistant + parser path: Created project {project_db_candidate.id}")
                
                # Check if content was parsed successfully
                content_valid = False
                if project_db_candidate.microproduct_content:
                    if hasattr(project_db_candidate.microproduct_content, "sections"):
                        sections = getattr(project_db_candidate.microproduct_content, "sections", [])
                        content_valid = len(sections) > 0
                        logger.info(f"Assistant + parser path: Found {len(sections)} sections in parsed content")
                    else:
                        logger.warning(f"Assistant + parser path: Content does not have sections attribute")
                else:
                    logger.warning(f"Assistant + parser path: microproduct_content is None")

                # --- Patch theme into DB if provided (only for TrainingPlan components) ---
                if payload.theme and content_valid:
                    async with pool.acquire() as conn:
                        design_template = await conn.fetchrow("SELECT component_name FROM design_templates WHERE id = $1", template_id)
                        if design_template and design_template.get("component_name") == COMPONENT_NAME_TRAINING_PLAN:
                            await conn.execute(
                                """
                                UPDATE projects
                                SET microproduct_content = jsonb_set(COALESCE(microproduct_content::jsonb, '{}'), '{theme}', to_jsonb($1::text), true)
                                WHERE id = $2
                                """,
                                payload.theme, project_db_candidate.id
                            )
                            row_patch = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id = $1", project_db_candidate.id)
                            if row_patch and row_patch["microproduct_content"] is not None:
                                project_db_candidate.microproduct_content = row_patch["microproduct_content"]

                # --- Recalculate module total hours after creation ---
                if content_valid and project_db_candidate.microproduct_content:
                    try:
                        async with pool.acquire() as conn:
                            content = project_db_candidate.microproduct_content
                            if isinstance(content, dict) and content.get("sections"):
                                sections = content["sections"]
                                updated_sections = []
                                
                                for section in sections:
                                    if isinstance(section, dict) and section.get("lessons"):
                                        # Calculate total hours from lesson hours
                                        total_hours = sum(lesson.get("hours", 0) for lesson in section["lessons"])
                                        # Update section with calculated total hours and set autoCalculateHours to true
                                        updated_section = {
                                            **section,
                                            "totalHours": total_hours,
                                            "autoCalculateHours": True
                                        }
                                        updated_sections.append(updated_section)
                                    else:
                                        updated_sections.append(section)
                                
                                # Update the project with recalculated totals
                                if updated_sections:
                                    updated_content = {**content, "sections": updated_sections}
                                    await conn.execute(
                                        """
                                        UPDATE projects
                                        SET microproduct_content = $1::jsonb
                                        WHERE id = $2
                                        """,
                                        json.dumps(updated_content), project_db_candidate.id
                                    )
                                    logger.info(f"Recalculated module total hours for project {project_db_candidate.id}")
                    except Exception as e:
                        logger.warning(f"Failed to recalculate module total hours for project {project_db_candidate.id}: {e}")

                if content_valid:
                    logger.info(f"Assistant + parser path successful for project {project_db_candidate.id}")
                    # Send completion packet with the project ID
                    done_packet = {"type": "done", "id": project_db_candidate.id}
                    yield (json.dumps(done_packet) + "\n").encode()
                else:
                    logger.error(f"Assistant + parser path: Project {project_db_candidate.id} created but content validation failed")
                    # Clean up the failed project
                    try:
                        async with pool.acquire() as conn:
                            await conn.execute("DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2", project_db_candidate.id, onyx_user_id)
                        logger.info(f"Successfully cleaned up failed assistant + parser project {project_db_candidate.id}")
                    except Exception as cleanup_e:
                        logger.error(f"Failed to cleanup assistant + parser project {project_db_candidate.id}: {cleanup_e}")
                    
                    # Send error packet
                    error_packet = {"type": "error", "message": "Failed to parse the generated outline"}
                    yield (json.dumps(error_packet) + "\n").encode()
                    
            except Exception as create_e:
                logger.error(f"Assistant + parser path: Failed to create project: {create_e}")
                # Send error packet
                error_packet = {"type": "error", "message": f"Failed to create project: {str(create_e)}"}
                yield (json.dumps(error_packet) + "\n").encode()

        return StreamingResponse(streamer(), media_type="application/json")

@app.post("/api/custom/course-outline/init-chat")
async def init_course_outline_chat(request: Request):
    """Pre-create Chat Session & persona so subsequent preview calls are faster."""
    cookies = request.cookies
    # For init-chat, we'll use the default ContentBuilder persona
    # The actual persona selection will happen in the preview endpoint based on the request payload
    persona_id = await get_contentbuilder_persona_id(cookies)
    chat_id = await create_onyx_chat_session(persona_id, cookies)
    return {"personaId": persona_id, "chatSessionId": chat_id}

# ======================= End Wizard Section ==============================

# === Wizard Outline helpers & cache ===
from collections import OrderedDict
from typing import OrderedDict as OrderedDictType

class LRUCache:
    def __init__(self, maxsize: int = 100):
        self.maxsize = maxsize
        self.cache: OrderedDictType[str, str] = OrderedDict()
    
    def get(self, key: str, default=None):
        if key in self.cache:
            # Move to end (most recent)
            self.cache.move_to_end(key)
            return self.cache[key]
        return default
    
    def __setitem__(self, key: str, value: str):
        if key in self.cache:
            # Update existing key
            self.cache[key] = value
            self.cache.move_to_end(key)
        else:
            # Add new key
            self.cache[key] = value
            if len(self.cache) > self.maxsize:
                # Remove oldest item
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
    
    def __contains__(self, key: str) -> bool:
        return key in self.cache
    
    def __delitem__(self, key: str):
        if key in self.cache:
            del self.cache[key]

OUTLINE_PREVIEW_CACHE = LRUCache(100)  # chat_session_id -> raw markdown outline
QUIZ_PREVIEW_CACHE = LRUCache(100)  # chat_session_id -> raw quiz content

# Global tracking for live streaming progress to avoid duplicates
LIVE_STREAM_TRACKING: Dict[str, Dict[str, set]] = {}  # chat_id -> {"modules": set(), "lessons": set()}

def extract_live_progress(assistant_reply: str, chat_id: str):
    """Extract modules and lessons from streaming JSON response and yield progress updates."""
    import re
    
    # Initialize tracking for this chat session
    if chat_id not in LIVE_STREAM_TRACKING:
        LIVE_STREAM_TRACKING[chat_id] = {"modules": set(), "lessons": set(), "last_position": 0, "markdown_structure": ""}
    
    sent_modules = LIVE_STREAM_TRACKING[chat_id]["modules"]
    sent_lessons = LIVE_STREAM_TRACKING[chat_id]["lessons"]
    last_position = LIVE_STREAM_TRACKING[chat_id]["last_position"]
    
    progress_updates = []
    
    try:
        # Only process new content since last position to avoid re-processing
        new_content = assistant_reply[last_position:]
        LIVE_STREAM_TRACKING[chat_id]["last_position"] = len(assistant_reply)
        
        # Debug logging
        logger.info(f"[LIVE_PROGRESS_DEBUG] Processing {len(new_content)} new chars (total: {len(assistant_reply)})")
        
        if not new_content.strip():
            return progress_updates
        
        # Look for module patterns in FULL response (not just new content)
        # This is key for incremental JSON building where patterns span multiple chunks
        module_pattern = r'"id":\s*"(№\d+)"[^}]*?"title":\s*"([^"]+)"'
        module_matches = re.findall(module_pattern, assistant_reply)
        
        # Convert to (title, id) format for consistency
        all_module_matches = [(title, module_id) for module_id, title in module_matches]
        
        for title, module_id in all_module_matches:
            module_key = f"{module_id}:{title}"
            if module_key not in sent_modules and title.strip():
                sent_modules.add(module_key)
                progress_updates.append({
                    "type": "module",
                    "title": title.strip(),
                    "id": module_id
                })
                logger.info(f"[LIVE_PROGRESS] Found new module: {title}")
        
        # Parse lessons by finding them within their specific module context
        # Use a more robust approach that tracks module context as we parse
        
        # First, create a mapping of all modules we know about
        module_map = {}  # module_id -> module_title
        for title, module_id in all_module_matches:
            module_map[module_id] = title
        
        # Find all lesson patterns with context about their position
        lesson_pattern_with_context = r'"title":\s*"Lesson\s+\d+\.\d+:\s*([^"]+)"'
        
        # Split the response by modules to find lessons in each module
        # Look for module starts: "id": "№X"
        module_splits = re.split(r'"id":\s*"(№\d+)"', assistant_reply)
        
        logger.info(f"[LIVE_PROGRESS_DEBUG] Module splits: {len(module_splits)} sections, module_map: {module_map}")
        
        current_module_id = None
        current_module_title = "Unknown Module"
        
        for i, section in enumerate(module_splits):
            if i % 2 == 1:  # Odd indices are module IDs
                current_module_id = section
                current_module_title = module_map.get(current_module_id, "Unknown Module")
            elif i % 2 == 0 and current_module_id:  # Even indices after a module ID are module content
                # Look for lessons in this module section
                lesson_matches = re.findall(lesson_pattern_with_context, section)
                logger.info(f"[LIVE_PROGRESS_DEBUG] Module {current_module_id} ({current_module_title}): found {len(lesson_matches)} lessons in section")
                
                for lesson_title in lesson_matches:
                    cleaned_title = lesson_title.strip()
                    lesson_key = f"{current_module_id}:{current_module_title}:{cleaned_title}"
                    
                    if lesson_key not in sent_lessons and cleaned_title:
                        sent_lessons.add(lesson_key)
                        progress_updates.append({
                            "type": "lesson",
                            "title": cleaned_title,
                            "module": current_module_title,
                            "module_id": current_module_id
                        })
                        logger.info(f"[LIVE_PROGRESS] Found new lesson in {current_module_title}: {cleaned_title}")
    
    except Exception as e:
        logger.debug(f"[LIVE_PROGRESS_EXTRACT] Error extracting progress: {e}")
    
    return progress_updates

def _apply_title_edits_to_outline(original_md: str, edited_outline: Dict[str, Any]) -> str:
    """Return a markdown outline that reflects the *structure* provided in
    `edited_outline` (modules & lessons) while preserving the original header.

    Instead of trying to patch-in titles at the old positions, we rebuild each
    module's lesson list from scratch. This guarantees correctness even when
    lessons were inserted, removed or reordered in the UI.
    """

    # ---- 1. Normalise `edited_outline` ----
    sections: Optional[List[Any]] = None
    if isinstance(edited_outline, dict):
        sections = edited_outline.get("sections") or edited_outline.get("modules")
    elif isinstance(edited_outline, list):
        sections = edited_outline

    if not sections:
        return original_md  # nothing to merge -> return original untouched

    # ---- 2. Preserve the very first non-empty line (usually Universal Header) ----
    header_line = None
    for line in original_md.splitlines():
        if line.strip():
            header_line = line.rstrip()
            break

    out_lines: List[str] = []
    if header_line:
        out_lines.append(header_line)
        out_lines.append("")  # spacer line to match original formatting

    # ---- 3. Rebuild modules & lessons ----
    for idx, sec in enumerate(sections):
        # Module title
        title = sec.get("title") if isinstance(sec, dict) else str(sec)
        out_lines.append(f"## Module {idx + 1}: {title.strip()}")

        # Lessons
        lessons_list: List[Any] = []
        if isinstance(sec, dict):
            lessons_list = sec.get("lessons", []) or []
        elif isinstance(sec, list):
            lessons_list = sec

        for ls in lessons_list:
            ls_raw = ls.get("title") if isinstance(ls, dict) else str(ls)
            if not isinstance(ls_raw, str):
                ls_raw = str(ls_raw)

            segments = ls_raw.split("\n")
            main_line = segments[0].strip()
            out_lines.append(f"- **{main_line}**")

            for extra in segments[1:]:
                extra = extra.rstrip()
                if extra:
                    out_lines.append(f"  {extra}")

        out_lines.append("")  # blank line between modules for readability

    return "\n".join(out_lines).rstrip()  # drop trailing newline

# ------------------- Utility: extract project name from AI markdown header -------------------

_HEADER_RE = re.compile(r"^\*\*(?P<name>[^*]+)\*\*\s*:\s*\*\*.+")


def _extract_project_name_from_markdown(md: str) -> Optional[str]:
    """Return the first **Project Name** element found in the Universal Product Header.

    The header line looks like:
        **Project Name** : **Course Outline** : **Course Outline**
    We return "Project Name" (stripped).
    """
    if not md:
        return None
    first_line = md.splitlines()[0].strip()
    m = _HEADER_RE.match(first_line)
    if m:
        return m.group("name").strip()
    return None

# --- PDF Lesson helper and wizard endpoints ---

# Ensure a design template for PDF Lesson exists, return its ID
async def _ensure_pdf_lesson_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_PDF_LESSON)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "PDF Lesson", DEFAULT_PDF_LESSON_JSON_EXAMPLE_FOR_LLM, "PDF Lesson", COMPONENT_NAME_PDF_LESSON,
        )
        return row["id"]

# Ensure a design template for Slide Deck exists, return its ID
async def _ensure_slide_deck_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_SLIDE_DECK)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Slide Deck", DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM, "Slide Deck", COMPONENT_NAME_SLIDE_DECK,
        )
        return row["id"]


# Ensure a design template for Video Lesson Presentation exists, return its ID
async def _ensure_video_lesson_presentation_template(pool: asyncpg.Pool) -> int:
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT id FROM design_templates WHERE component_name = $1 LIMIT 1", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION)
        if row:
            return row["id"]
        row = await conn.fetchrow(
            """
            INSERT INTO design_templates (template_name, template_structuring_prompt, microproduct_type, component_name)
            VALUES ($1, $2, $3, $4) RETURNING id;
            """,
            "Video Lesson Presentation", DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM, "Video Lesson Presentation", COMPONENT_NAME_VIDEO_LESSON_PRESENTATION,
        )
        return row["id"]


# Ensure a design template for Text Presentation exists, return its ID
async def _ensure_text_presentation_template(pool: asyncpg.Pool) -> int:
    """Ensure text presentation template exists and return its ID"""
    try:
        # Check if text presentation template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Text Presentation' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create text presentation template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Text Presentation Template",
            "Create a comprehensive text presentation with clear structure, engaging content, and professional formatting.",
            "Text Presentation",
            COMPONENT_NAME_TEXT_PRESENTATION,
            "/text-presentation.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring text presentation template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure text presentation template")


# -------- Lesson Presentation (PDF Lesson) Wizard ---------

class LessonWizardPreview(BaseModel):
    outlineProjectId: Optional[int] = None  # Parent Training Plan project id
    lessonTitle: Optional[str] = None      # Specific lesson to generate, optional when prompt-based
    lengthRange: Optional[str] = None      # e.g. "400-500 words"
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths


class LessonWizardFinalize(BaseModel):
    outlineProjectId: Optional[int] = None
    lessonTitle: str
    lengthRange: Optional[str] = None
    aiResponse: str                        # User-edited markdown / plain text
    prompt: str
    chatSessionId: Optional[str] = None
    slidesCount: Optional[int] = 5         # Number of slides to generate
    productType: Optional[str] = "lesson_presentation"  # "lesson_presentation" or "video_lesson_presentation"
    theme: Optional[str] = None            # Selected theme for presentation
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: user edits tracking
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    editedSlides: Optional[List[Dict[str, Any]]] = None
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths


@app.post("/api/custom/lesson-presentation/preview")
async def wizard_lesson_preview(payload: LessonWizardPreview, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    if not cookies[ONYX_SESSION_COOKIE_NAME]:
        raise HTTPException(status_code=401, detail="Not authenticated")

    # Ensure chat session
    if payload.chatSessionId:
        chat_id = payload.chatSessionId
    else:
        # Check if this is a Knowledge Base search request
        use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
        persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
        chat_id = await create_onyx_chat_session(persona_id, cookies)

    # Build wizard request for assistant persona
    is_video_lesson = payload.productType == "video_lesson_presentation"
    wizard_dict: Dict[str, Any] = {
        "product": "Video Lesson Slides Deck" if is_video_lesson else "Slides Deck",
        "action": "preview",
        "language": payload.language,
        "slidesCount": payload.slidesCount or 5,
        "generateVoiceover": is_video_lesson,  # Flag to indicate voiceover generation
        "theme": payload.theme or "dark-purple",  # Use selected theme or default
    }
    if payload.outlineProjectId is not None:
        wizard_dict["outlineProjectId"] = payload.outlineProjectId
        
        # Fetch outline name to include in wizard request
        try:
            # Get current user ID to fetch the outline
            onyx_user_id = await get_current_onyx_user_id(request)
            
            # Fetch outline name from database
            async with pool.acquire() as conn:
                outline_row = await conn.fetchrow(
                    "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                    payload.outlineProjectId, onyx_user_id
                )
                if outline_row:
                    wizard_dict["outlineName"] = outline_row["project_name"]
        except Exception as e:
            logger.warning(f"Failed to fetch outline name for project {payload.outlineProjectId}: {e}")
            # Continue without outline name - not critical for preview
            
    if payload.lessonTitle:
        wizard_dict["lessonTitle"] = payload.lessonTitle
    if payload.prompt:
        wizard_dict["prompt"] = payload.prompt

    wizard_dict["importantRules"] = "IMPORTANT: DO NOT CREATE CONCLUSION SLIDES. ONLY CREATE EDUCATIONAL SLIDES. DO NOT CREATE SLIDES WITH TITLES LIKE 'Conclusion', 'Summary', 'Wrap-Up', 'Thank You', 'Further Reading', 'Additional Resources', 'Questions', 'Open Floor for Questions', 'Feedback'. DO NOT MAKE SECOND SLIDE BE A TITLE SLIDE. DO NOT USE 'content-slide' SLIDES"
    wizard_dict["importantRules"] += """
CRITICAL FORMATTING REQUIREMENTS FOR VIDEO LESSON PRESENTATION:
1. After the Universal Product Header (**[Project Name]** : **Video Lesson Slides Deck** : **[Lesson Title]**), add exactly TWO blank lines
2. Each slide MUST use this exact format: **Slide N: [Descriptive Title]** `[slide-type]`
3. Use "---" separators between slides
5. NEVER use markdown headers (##, ###) for slide titles - ONLY use **Slide N: Title** format
6. Ensure slides are numbered sequentially: Slide 1, Slide 2, Slide 3, etc.
    """
    
    # Add file context if provided
    if payload.fromFiles:
        wizard_dict["fromFiles"] = True
        if payload.folderIds:
            wizard_dict["folderIds"] = payload.folderIds
        if payload.fileIds:
            wizard_dict["fileIds"] = payload.fileIds

    # Add connector context if provided
    if payload.fromConnectors:
        wizard_dict["fromConnectors"] = True
        if payload.connectorIds:
            wizard_dict["connectorIds"] = payload.connectorIds
        if payload.connectorSources:
            wizard_dict["connectorSources"] = payload.connectorSources

    # Add text context if provided - use compression for large texts
    if payload.fromText and payload.userText:
        wizard_dict["fromText"] = True
        wizard_dict["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        # Check if we're using hybrid approach (files present) or direct approach (text-only)
        if should_use_hybrid_approach(payload):
            # Hybrid approach: for lesson presentations, we still send text directly but with compression
            logger.info(f"Using hybrid approach for lesson presentation with text ({text_length} chars)")
            if text_length > TEXT_SIZE_THRESHOLD:
                compressed_text = compress_text(payload.userText)
                wizard_dict["userText"] = compressed_text
                wizard_dict["textCompressed"] = True
                logger.info(f"Using compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
            else:
                wizard_dict["userText"] = payload.userText
                wizard_dict["textCompressed"] = False
        else:
            # Direct approach: send text directly in wizard request (no file conversion)
            logger.info(f"✅ Using DIRECT approach: sending text directly in wizard request ({text_length} chars)")
            
            if text_length > TEXT_SIZE_THRESHOLD:
                compressed_text = compress_text(payload.userText)
                wizard_dict["userText"] = compressed_text
                wizard_dict["textCompressed"] = True
                logger.info(f"Compressed text for direct wizard request ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Send text directly without compression
                wizard_dict["userText"] = payload.userText
                wizard_dict["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wizard_dict["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for lesson generation")

    # Decompress text if it was compressed
    if wizard_dict.get("textCompressed") and wizard_dict.get("userText"):
        try:
            decompressed_text = decompress_text(wizard_dict["userText"])
            wizard_dict["userText"] = decompressed_text
            wizard_dict["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed lesson text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress lesson text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wizard_dict) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."
    
    # Force JSON-ONLY preview output for Presentation to enable immediate parsed preview (like Course Outline)
    try:
        # Get the appropriate JSON example based on whether this is a video lesson
        is_video_lesson = payload.productType == "video_lesson_presentation"
        if is_video_lesson:
            json_example = DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM
        else:
            # Use multiple diverse examples to encourage template variety
            import random
            import os
            
            # Load additional example files
            example_files = [
                DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM,
            ]
            
            # Try to load additional example files
            try:
                with open('NEW_SLIDE_DECK_JSON_EXAMPLE_2.json', 'r', encoding='utf-8') as f:
                    example_files.append(f.read())
            except:
                pass
                
            try:
                with open('NEW_SLIDE_DECK_JSON_EXAMPLE_3.json', 'r', encoding='utf-8') as f:
                    example_files.append(f.read())
            except:
                pass
            
            # Randomly select one example to reduce over-reliance on a single pattern
            json_example = random.choice(example_files)
        

        json_preview_instructions = ""  
        if not is_video_lesson:
            json_preview_instructions += f"""

CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Presentation preview, strictly following this example structure:
{json_example}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
This enables immediate parsing without additional LLM calls during finalization.

MANDATORY PREVIEW UI REQUIREMENT:
- EVERY slide MUST include "previewKeyPoints": [...] field at the root level (same level as slideId, slideNumber, etc).
- Include 4-6 content-rich bullets (10–18 words each), specific and informative.
- These previewKeyPoints are for preview only and will be ignored/stripped on save.
- Example format: "previewKeyPoints": ["Comprehensive overview of digital marketing fundamentals", "Target audience analysis and segmentation strategies", ...]

CRITICAL SCHEMA AND CONTENT RULES (MUST MATCH FINAL FORMAT):
- **MANDATORY SLIDE COUNT**: You MUST generate EXACTLY the requested number of slides. This is NON-NEGOTIABLE. If 20 slides are requested, the output MUST contain precisely 20 slides in the slides[] array. If 15 slides are requested, generate exactly 15. Count your slides before finishing to ensure you have the exact number.
- Use component-based slides with exact fields: slideId, slideNumber, slideTitle, templateId, props{', voiceoverText' if is_video_lesson else ''}.
- The root must include lessonTitle, slides[], currentSlideId (optional), detectedLanguage; { 'hasVoiceover: true (MANDATORY)' if is_video_lesson else 'hasVoiceover is not required' }.
- Generate sequential slideNumber values (1..N) and descriptive slideId values (e.g., "slide_3_topic").
- Preserve original language across all text.

CRITICAL TABLE RULE:
- If prompt/content implies tabular comparison (e.g., table, comparison, vs, side by side, data comparison, statistics, performance table, табличные данные), you MUST use table-dark or table-light with JSON props: tableData.headers[] and tableData.rows[]; NEVER markdown tables.

CONTENT DENSITY AND LEARNING REQUIREMENTS:
- MAXIMIZE educational value: each slide should teach substantial concepts, not just overview points.
- Bullet points must be EXTREMELY comprehensive (60-100 words each), explaining HOW, WHY, WHEN, and WHERE with specific examples, tools, methodologies, step-by-step processes, common pitfalls, and actionable insights.
- Process steps must be detailed (30-50 words each), including context, prerequisites, expected outcomes, and practical implementation guidance.
- Big-numbers slides MUST have meaningful descriptions explaining the significance of each statistic.
- Include concrete examples, real-world applications, specific tools/technologies, and measurable outcomes in every slide.
- Ensure learners gain deep understanding of the topic after reading the complete presentation.

General Rules:
- Do NOT duplicate title and subtitle content; keep them distinct.
- **CRITICAL**: Generate EXACTLY the requested number of slides (slidesCount parameter). Do NOT generate fewer slides. If 20 slides are requested, you MUST create all 20 slides with substantial educational content. Add more detailed content to reach the exact count if needed.
- STRICTLY NO closing/inspirational slides — do not generate: thank you, next steps, resources, looking ahead, embracing [anything], wrap-up, conclusion, summary, what's next, future directions, acknowledgments. Focus ONLY on educational content slides.
- BANNED AGENDA SLIDES: Do NOT generate "What We'll Cover", "Training Agenda", "Learning Objectives", or similar overview slides. Start directly with educational content. Do not end with title slides or resources slides; end on substantive content.
- Localization: auxiliary keywords like Recommendation/Conclusion must match content language when used within props text.

MANDATORY TEMPLATE DIVERSITY (CRITICAL - AVOID REPETITION):
- You MUST use a wide variety of templates from the full catalog below. DO NOT repeat the same templates.
- For 10-15 slides, use each template AT MOST ONCE, preferring: title-slide (1), bullet-points-right (max 2), two-column (1), process-steps (1), four-box-grid (1), timeline (1), big-numbers (1), challenges-solutions (1), big-image variants (1-2), metrics-analytics (1), market-share OR pie-chart-infographics (1), table variants (1), pyramid (1).
- Prioritize templates that best express your content; avoid defaulting to bullet-points-right for everything.
- Use specialty templates like metrics-analytics, pie-chart-infographics, event-list, pyramid, market-share when content fits.

PROFESSIONAL IMAGE GENERATION GUIDELINES (AUTHENTIC WORKPLACE PHOTOGRAPHY):
Create professional photographs showing people actively working in authentic environments relevant to the slide topic.

IMAGE PROMPT STRUCTURE - Use this exact format:
"A professional photograph of [PROFESSIONAL ROLE] actively working [WORKPLACE CONTEXT]. 

SCENE: The person is engaged in their typical work activities in an authentic workplace environment appropriate for [ROLE]. Show them using professional tools, equipment, or technology relevant to their role. The composition should capture both the person (from waist up or full body) and their work environment.

ACTIVITY: Include specific work processes that match the slide content - for example:
- If data science: analyzing data on multiple monitors, coding machine learning models, presenting findings to team
- If marketing: reviewing campaign analytics dashboards, creating content, strategizing with team over brand materials
- If software development: coding at workstation with multiple monitors, debugging, collaborating in code review
- If business analysis: examining financial data, creating presentations, meeting with stakeholders over reports
- If project management: coordinating with team, reviewing project timelines, facilitating planning sessions
- If design: working in creative software, sketching concepts, reviewing designs with colleagues

ENVIRONMENT: Authentic workplace setting that matches the role - not just a generic office. Include relevant background elements, tools, equipment, and work materials that tell the story of what this person does professionally.

STYLE: High-quality professional photography with good natural lighting that shows both the person and their work context. The person should be wearing appropriate professional attire for their specific role.

COMPOSITION: Environmental portrait style that captures the essence of the work, showing the professional genuinely engaged in their tasks within their authentic work environment."

CORE PRINCIPLES:
1. SHOW REAL WORK ACTIVITIES: People must be actively doing their jobs, not posing or looking at camera
2. AUTHENTIC ENVIRONMENTS: Real workplaces with actual tools, equipment, and materials visible
3. SPECIFIC TO SLIDE CONTENT: Match the professional role and activity to the slide's actual topic
4. NO STOCK PHOTO CLICHÉS: Avoid handshakes, pointing at charts, lightbulbs, chess pieces, staged meetings
5. ENVIRONMENTAL PORTRAITS: Capture both the person and their workspace to tell the full professional story
6. NATURAL MOMENTS: Show genuine work activities, not overly staged or posed scenarios

EXAMPLES FOR COMMON SLIDE TYPES:
- Business Strategy: "Business strategists collaborating around conference table with laptops, market analysis documents, and whiteboards showing strategic frameworks visible in background"
- Data Analysis: "Data analysts working at multi-monitor workstations with data visualizations, dashboards, and statistical reports displayed, taking notes and discussing insights"
- Technology: "Software engineers coding at workstations with multiple monitors showing code editors and terminal windows, reviewing pull requests and debugging together"
- Marketing: "Marketing professionals analyzing campaign data on screens, reviewing creative assets, planning content calendar with brand materials visible"
- Finance: "Financial analysts examining market data on trading terminals, reviewing financial models on spreadsheets, discussing investment strategies"

Always specify: realistic workplace, professional attire, authentic tools/equipment, natural lighting, environmental portrait composition.

🚨 FINAL REMINDER - EXACT SLIDE COUNT IS MANDATORY 🚨
Before you start generating slides, note the slidesCount parameter. You MUST generate that EXACT number of slides.
If slidesCount=20, generate 20 slides. If slidesCount=15, generate 15 slides. NO EXCEPTIONS.
After generation, verify your slides[] array has the correct length. This is a critical requirement.

Template Catalog with required props and usage:
- title-slide: title, subtitle, [author], [date]
  • Usage: ONLY for the first slide of the presentation; opening/section title with heading and short subtitle.
- big-image-left: title, subtitle, imagePrompt, [imageAlt], [imageUrl], [imageSize]
  • Usage: DO NOT USE except for first slide. Use other templates instead.
- big-image-top: title, subtitle, imagePrompt, [imageAlt], [imageUrl], [imageSize]
  • Usage: hero image across top; explanatory text below.
- bullet-points-right: title, bullets[] or (title+subtitle+bullets[]), imagePrompt, [imageAlt], [bulletStyle], [maxColumns]
  • Usage: key takeaways with bullets on left and image area on right; supports brief intro text. Do not use the deprecated bullet-points template. In examples, write each bullet as 2–3 sentences with concrete details.
- two-column: title, leftTitle, leftContent, rightTitle, rightContent, [leftImagePrompt], [rightImagePrompt]
  • Usage: compare/contrast or split content; balanced two columns. CRITICAL: leftContent and rightContent must be plain text (NO bullet points •), exactly 1-2 sentences each.
- process-steps: title, steps[]
  • Usage: sequential workflow; 3–5 labeled steps in a row.
- four-box-grid: title, boxes[] (heading,text or title,content)
  • Usage: 2×2 grid of highlights; four concise boxes.
- timeline: title, events[] (date,title,description)
  • Usage: chronological milestones; left-to-right progression. Do not use event-list.
- big-numbers: title, subtitle, steps[] (EXACTLY 3 items: value,label,description - NEVER use "numbers" key)
  • Usage: three headline metrics; large values with descriptive labels and MANDATORY descriptions explaining significance. The subtitle should provide context for the metrics (2-3 sentences). Each step's description should be 2–3 concise sentences.
- pyramid: title, steps[] (EXACTLY: heading,number - NOT levels, NOT description)
  • Usage: hierarchical structure; 3-5 level pyramid visual. Each step must have "heading" (the text) and "number" (like "01", "02", etc). Do NOT include "levels" or "description" fields.
- challenges-solutions: title, challengesTitle, solutionsTitle, challenges[] (strings), solutions[] (strings)
  • Usage: problem/solution mapping; two facing columns. Each challenge/solution should be 5-6 words maximum for clean display.
- metrics-analytics: title, metrics[] (number,text)
  • Usage: EXACTLY 5-6 numbered analytics points; connected layout. Use ONLY when you have specific, meaningful KPIs, measurements, or operational metrics with concrete numbers (percentages, counts, times, etc). Each metric MUST have context explaining what the number means. DO NOT use for generic lists. DO NOT convert to bullet-points.
- market-share: title, [subtitle], chartData[] (label,description,percentage,color,year), [bottomText]
  • Usage: bar/ratio comparison for market distribution or category breakdown. CRITICAL: percentage values MUST represent actual percentages (0-100) that together sum to approximately 100%, or represent meaningful standalone metrics like "market share: 35%", "adoption rate: 67%". DO NOT use arbitrary numbers. The subtitle and description fields MUST clearly explain what the percentages represent (e.g., "Market share by vendor", "Customer segmentation by size", "Technology adoption rates").
- [Removed] comparison-slide is deprecated. Use table-light or table-dark with tableData.headers[] and tableData.rows[][] instead.
- table-dark: title, tableData: headers[],rows[][]
  • Usage: comparison matrix with checkbox columns (like feature comparison tables). Structure: The FIRST column in each row is the row label (e.g., "Retention Rate", "Conversion Rate"). All subsequent columns are checkbox cells - use "✓" for checked, "" (empty string) or "✗" for unchecked. Headers should describe what each column represents (e.g., "Before Mapping", "After Mapping"). Example row: ["Customer Satisfaction", "✓", "", "✓"] where first cell is the label and others are checkboxes.
- table-light: title, tableData: headers[],rows[][]
  • Usage: dense tabular data (light theme) with consistent column structure. Each row must have EXACTLY the same number of elements as headers[] (not headers.length + 1). Example: if headers has 3 items, each row must also have exactly 3 items.
- pie-chart-infographics: title, chartData.segments[] (label,value,color), monthlyData[], [chartSize], [colors]
  • Usage: distribution breakdown showing proportional parts of a whole. CRITICAL: segment values MUST sum to 100 (representing percentages) or represent actual quantities that are part of a total. Each segment needs label (category name), value (the number/percentage), and color (hex code like #2563eb). The title MUST clearly explain what distribution is being shown (e.g., "Budget Allocation by Department", "Customer Demographics by Age Group").

CRITICAL TEMPLATE DIVERSITY ENFORCEMENT:
- Each template should appear AT MOST ONCE per presentation. Avoid template repetition at all costs.
- When you have 5-8 metrics/KPIs, use metrics-analytics template (DO NOT convert to bullet-points-right).
- For tabular data, always use table-dark or table-light templates (DO NOT use markdown tables).
- Prioritize variety: use different templates for different content types to maintain visual interest.
- Select templates based on content structure, not convenience. Challenge yourself to use diverse templates.
- NEVER use big-image-left or big-image-top templates after slide 1. Use bullet-points-right, four-box-grid, or other content-rich templates instead.
"""
        else:
            json_preview_instructions += f"""





CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Presentation preview, strictly following this example structure:
{json_example}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.
This enables immediate parsing without additional LLM calls during finalization.

MANDATORY PREVIEW UI REQUIREMENT:
- EVERY slide MUST include "previewKeyPoints": [...] field at the root level (same level as slideId, slideNumber, etc).
- Include 4-6 content-rich bullets (10–18 words each), specific and informative.
- These previewKeyPoints are for preview only and will be ignored/stripped on save.
- Example format: "previewKeyPoints": ["Comprehensive overview of digital marketing fundamentals", "Target audience analysis and segmentation strategies", ...]

CRITICAL SCHEMA AND CONTENT RULES (MUST MATCH FINAL FORMAT):
- **MANDATORY SLIDE COUNT**: You MUST generate EXACTLY the requested number of slides. This is NON-NEGOTIABLE. If 20 slides are requested, the output MUST contain precisely 20 slides in the slides[] array. If 15 slides are requested, generate exactly 15. Count your slides before finishing to ensure you have the exact number.
- Use component-based slides with exact fields: slideId, slideNumber, slideTitle, templateId, props{', voiceoverText' if is_video_lesson else ''}.
- The root must include lessonTitle, slides[], currentSlideId (optional), detectedLanguage; { 'hasVoiceover: true (MANDATORY)' if is_video_lesson else 'hasVoiceover is not required' }.
- Generate sequential slideNumber values (1..N) and descriptive slideId values (e.g., "slide_3_topic").
- Preserve original language across all text.

CRITICAL TABLE RULE:
- If prompt/content implies tabular comparison (e.g., table, comparison, vs, side by side, data comparison, statistics, performance table, табличные данные), you MUST use table-dark or table-light with JSON props: tableData.headers[] and tableData.rows[]; NEVER markdown tables.

CONTENT DENSITY AND LEARNING REQUIREMENTS:
- MAXIMIZE educational value: each slide should teach substantial concepts, not just overview points.
- Bullet points must be EXTREMELY comprehensive (60-100 words each), explaining HOW, WHY, WHEN, and WHERE with specific examples, tools, methodologies, step-by-step processes, common pitfalls, and actionable insights.
- Process steps must be detailed (30-50 words each), including context, prerequisites, expected outcomes, and practical implementation guidance.
- Big-numbers slides MUST have meaningful descriptions explaining the significance of each statistic.
- Include concrete examples, real-world applications, specific tools/technologies, and measurable outcomes in every slide.
- Ensure learners gain deep understanding of the topic after reading the complete presentation.

General Rules:
- Do NOT duplicate title and subtitle content; keep them distinct.
- **CRITICAL**: Generate EXACTLY the requested number of slides (slidesCount parameter). Do NOT generate fewer slides. If 20 slides are requested, you MUST create all 20 slides with substantial educational content. Add more detailed content to reach the exact count if needed.
- STRICTLY NO closing/inspirational slides — do not generate: thank you, next steps, resources, looking ahead, embracing [anything], wrap-up, conclusion, summary, what's next, future directions, acknowledgments. Focus ONLY on educational content slides.
- BANNED AGENDA SLIDES: Do NOT generate "What We'll Cover", "Training Agenda", "Learning Objectives", or similar overview slides. Start directly with educational content.
- Localization: auxiliary keywords like Recommendation/Conclusion must match content language when used within props text.



PROFESSIONAL IMAGE SELECTION GUIDELINES (CRITICAL FOR RELEVANCE):
Based on presentation design best practices, follow these rules for selecting appropriate images:

1. RELEVANCE OVER AESTHETICS: Images must directly support and enhance your slide's message, not just be decorative.
   - For business concepts: Use workplace scenarios, professional environments, real business activities
   - For technical topics: Show actual tools, interfaces, workflows, or realistic work environments
   - For data/analytics: Use realistic data visualization scenarios, not abstract concepts
   - For processes: Show people actually performing the process or realistic workflow environments

2. AVOID OVERUSED STOCK PHOTO CLICHÉS:
   - NO: Handshakes, chess pieces, lightbulbs, arrows hitting targets, people pointing at charts
   - NO: Overly staged business meetings, fake-looking "diverse teams" in conference rooms
   - NO: Generic "success" imagery (mountains, climbing, finish lines)
   - YES: Authentic workplace moments, realistic technology use, genuine professional interactions

3. CONTEXT-SPECIFIC IMAGE SELECTION:
   - Marketing slides: Real marketing campaigns, authentic customer interactions, actual marketing tools in use
   - Technology slides: Real developers coding, authentic tech environments, actual software interfaces
   - Finance slides: Real financial professionals at work, authentic trading floors, actual financial data analysis
   - Education slides: Real learning environments, authentic teaching moments, actual educational technology

4. REALISTIC WORKPLACE SCENES:
   - Show people actually using the tools/concepts being discussed
   - Include authentic details: real computer screens, actual work materials, genuine work environments
   - Avoid posed or overly perfect scenarios; prefer candid, realistic moments
   - Include diverse but authentic representation without forced staging

5. VISUAL METAPHORS THAT WORK:
   - Use concrete, relatable metaphors that enhance understanding
   - Construction/building for development processes, gardens for growth concepts
   - Transportation for journey/progress concepts, but make them specific and realistic
   - Avoid abstract or overused metaphors; prefer specific, actionable imagery



🚨 FINAL REMINDER - EXACT SLIDE COUNT IS MANDATORY 🚨
Before you start generating slides, note the slidesCount parameter. You MUST generate that EXACT number of slides.
If slidesCount=20, generate 20 slides. If slidesCount=15, generate 15 slides. NO EXCEPTIONS.
After generation, verify your slides[] array has the correct length. This is a critical requirement.

EXCLUSIVE VIDEO LESSON TEMPLATE CATALOG (ONLY 5 TEMPLATES ALLOWED):

- course-overview-slide: title, subtitle, imagePath, [imageAlt], [logoPath], [pageNumber]
  • Purpose: Opening slide for course introduction with strong visual impact
  • Structure: Split-panel design with title/subtitle on gradient background and large avatar display
  • Required props: title (main heading), subtitle (course description and learning objectives)
  • Visual elements: imagePath (professional avatar/instructor image), logoPath (course branding)
  • Usage: MUST be used as the first slide to welcome learners and set course expectations
  • Content guidelines: Title should be welcoming and engaging; subtitle should outline what learners will achieve

- impact-statements-slide: title, statements[] (array of {{number, description}}), profileImagePath, [pageNumber], [logoNew]
  • Purpose: Showcase key statistics, metrics, or impact data with visual emphasis
  • Structure: Three prominent cards displaying numerical achievements with descriptive context
  • Required props: title (section heading), statements (EXACTLY 3 items with 'number' field like "95%" or "3x" and 'description' field explaining significance)
  • Visual elements: profileImagePath (avatar reinforcing credibility), logoNew (branding element)
  • Usage: Present compelling data, success rates, performance metrics, or quantifiable outcomes
  • Content guidelines: Numbers should be impactful (percentages, multipliers, large numbers); descriptions should explain real-world meaning

- phishing-definition-slide: title, definitions[] (array of strings), profileImagePath, [rightImagePath], [pageNumber], [logoPath]
  • Purpose: Present multiple key definitions, concepts, or educational points in organized list format
  • Structure: Left panel with title and definition points; right panel with full avatar image
  • Required props: title (main topic heading), definitions (array of 3-6 detailed definition strings)
  • Visual elements: profileImagePath (instructor/expert avatar), rightImagePath (supporting visual illustration)
  • Usage: Define critical terminology, explain key concepts, list important principles or guidelines
  • Content guidelines: Each definition should be comprehensive (2-3 sentences); use clear, educational language; maintain consistent depth across all definitions

- soft-skills-assessment-slide: title, tips[] (array of {{text, isHighlighted}}), profileImagePath, [logoPath], [logoText], [pageNumber]
  • Purpose: Highlight exactly two critical tips, recommendations, or assessment criteria with different visual emphasis
  • Structure: Large prominent title, avatar display, and two tip cards with contrasting styles (one highlighted, one standard)
  • Required props: title (assessment or tip category), tips (EXACTLY 2 items with 'text' field containing the tip and 'isHighlighted' boolean)
  • Visual elements: profileImagePath (expert/instructor image), logoPath (branding), logoText (contextual label like "Assessment Guide")
  • Usage: Present key success tips, critical assessment criteria, important recommendations, or strategic guidance
  • Content guidelines: First tip (isHighlighted: true) should be most critical; second tip provides complementary guidance; each tip should be actionable and specific

- work-life-balance-slide: title, content, imagePath, [logoPath], [pageNumber]
  • Purpose: Deliver comprehensive narrative content, conclusions, or detailed explanations
  • Structure: Content-rich slide with gradient background, visual arch design, and avatar display for lengthy text
  • Required props: title (conclusion or section heading), content (2-4 paragraphs of detailed narrative text)
  • Visual elements: imagePath (relevant thematic or conclusion image), logoPath (branding)
  • Usage: MUST be used as conclusion slide; also suitable for detailed explanations requiring substantial text
  • Content guidelines: Content should synthesize key learnings, provide actionable next steps, or deliver comprehensive explanations; maintain professional, encouraging tone

MANDATORY 5-SLIDE VIDEO LESSON STRUCTURE (CRITICAL - EXACT ORDER REQUIRED):
- Video lessons MUST contain EXACTLY 5 slides using the 5 templates in this specific order:
  1. FIRST SLIDE: course-overview-slide (Welcome and course introduction)
  2. SECOND SLIDE: impact-statements-slide (Key statistics and impact metrics)
  3. THIRD SLIDE: phishing-definition-slide (Core definitions and concepts)
  4. FOURTH SLIDE: soft-skills-assessment-slide (Critical tips and recommendations)
  5. FIFTH SLIDE: work-life-balance-slide (Conclusion and next steps)
- NO template repetition allowed - each template used EXACTLY ONCE
- NO additional slides beyond these 5 - maintain strict 5-slide structure
- NO substitutions - you must use these exact 5 templates in this exact order
- This structure ensures comprehensive coverage: Introduction → Data → Education → Application → Conclusion

VIDEO LESSON SPECIFIC REQUIREMENTS:
- Every slide MUST include voiceoverText with 2-4 sentences of conversational explanation that expands on the visual content.
- Use inclusive language ("we", "you", "let's"), smooth transitions, and approximately 30–60 seconds speaking time per slide.
- The root object MUST include hasVoiceover: true.
"""
        wizard_message = wizard_message + json_preview_instructions
        logger.info(f"[PRESENTATION_PREVIEW] Added JSON-only preview instructions for {'video lesson' if is_video_lesson else 'slide deck'}")
    except Exception as e:
        logger.warning(f"[PRESENTATION_PREVIEW_JSON_INSTR] Failed to append JSON-only preview instructions: {e}")

    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wizard_dict.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[LESSON_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    if payload.selectedFiles:
                        # Combined context: connectors + SmartDrive files
                        logger.info(f"[HYBRID_CONTEXT] Extracting COMBINED context from connectors: {payload.connectorSources} and SmartDrive files: {payload.selectedFiles}")
                        
                        # Extract connector context
                        connector_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                        
                        # Map SmartDrive paths to Onyx file IDs with proper normalization
                        raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                        
                        # Normalize paths to handle URL encoding and character variations
                        smartdrive_file_paths = []
                        for path in raw_paths:
                            # Handle URL encoding
                            try:
                                from urllib.parse import unquote
                                normalized_path = unquote(path)
                            except:
                                normalized_path = path
                            
                            # Handle `+` character variations (some systems use `+` in filenames)
                            # Try both with and without `+` to match database records
                            smartdrive_file_paths.append(normalized_path)
                            if '+' in normalized_path:
                                smartdrive_file_paths.append(normalized_path.replace('+', ''))
                            
                        onyx_user_id = await get_current_onyx_user_id(request)
                        
                        # DEBUG: Log the mapping attempt
                        logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                        logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                        logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                        
                        file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                        
                        if file_ids:
                            logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                            # Extract file context and combine with connector context
                            file_context_from_smartdrive = await extract_file_context_from_onyx(file_ids, [], cookies)
                            
                            # Combine both contexts
                            file_context = f"{connector_context}\n\n=== ADDITIONAL CONTEXT FROM SELECTED FILES ===\n\n{file_context_from_smartdrive}"
                        else:
                            logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths, using only connector context")
                            file_context = connector_context
                    else:
                        # For connector-based filtering only, extract context from specific connectors
                        logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                        file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromConnectors and payload.selectedFiles:
                    # SmartDrive files only (no connectors)
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from SmartDrive files only: {payload.selectedFiles}")
                    
                    # Map SmartDrive paths to Onyx file IDs
                    raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                    
                    # Normalize paths to handle URL encoding and character variations
                    smartdrive_file_paths = []
                    for path in raw_paths:
                        # Try multiple variations to match database records
                        from urllib.parse import unquote, quote
                        import re
                        
                        candidates = []
                        # Base variants
                        candidates.append(path)
                        try:
                            decoded_path = unquote(path)
                            candidates.append(decoded_path)
                        except:
                            decoded_path = path
                        try:
                            encoded_path = quote(path, safe='/')
                            candidates.append(encoded_path)
                        except:
                            pass
                        
                        # Character normalization variants
                        for candidate in list(candidates):
                            # Handle spaces encoded as + or %20
                            if '+' in candidate:
                                candidates.append(candidate.replace('+', ' '))
                                candidates.append(candidate.replace('+', '%20'))
                            if '%20' in candidate:
                                candidates.append(candidate.replace('%20', ' '))
                                candidates.append(candidate.replace('%20', '+'))
                            if ' ' in candidate:
                                candidates.append(candidate.replace(' ', '+'))
                                candidates.append(candidate.replace(' ', '%20'))
                        
                        # Remove duplicates while preserving order
                        seen = set()
                        for candidate in candidates:
                            if candidate not in seen:
                                smartdrive_file_paths.append(candidate)
                                seen.add(candidate)
                    
                    onyx_user_id = await get_current_onyx_user_id(request)
                    
                    # DEBUG: Log the mapping attempt
                    logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                    logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                    logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                    
                    file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                    
                    if file_ids:
                        logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                        # Extract file context from SmartDrive files
                        file_context = await extract_file_context_from_onyx(file_ids, [], cookies)
                    else:
                        logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths")
                        file_context = ""
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wizard_dict.get("virtualFileId"):
                        file_ids_list.append(wizard_dict["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wizard_dict['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                chunks_received = 0
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Video Lesson Presentation" if is_video_lesson else "Lesson Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[LESSON_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[LESSON_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                chunks_received = 0
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[LESSON_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[LESSON_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                        now = asyncio.get_event_loop().time()
                        if now - last_send > 8:
                            yield b" "
                            last_send = now
                        logger.debug(f"[LESSON_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[LESSON_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                
                # Cache for potential finalize step if needed
                if chat_id:
                    OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
                    logger.info(f"[LESSON_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")
                
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[LESSON_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")
        # Send completion packet with the parsed outline.
        done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}

        # 🔍 CRITICAL DEBUG: Log the raw AI response before parser processing
        logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Raw AI response for video lesson presentation:")
        logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Response length: {len(assistant_reply)} characters")
        logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Full response content:")
        logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] {assistant_reply}")
        
        # Try to extract and log JSON structure if present
        try:
            import re
            json_match = re.search(r'\{.*\}', assistant_reply, re.DOTALL)
            if json_match:
                json_text = json_match.group()
                parsed_json = json.loads(json_text)
                logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Extracted JSON structure:")
                logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] JSON keys: {list(parsed_json.keys()) if isinstance(parsed_json, dict) else 'Not a dict'}")
                if isinstance(parsed_json, dict) and 'slides' in parsed_json:
                    slides = parsed_json['slides']
                    actual_count = len(slides)
                    logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Number of slides: {actual_count}")
                    
                    # Validate slide count matches request
                    requested_count = wizard_dict.get('slidesCount', 5)
                    if actual_count != requested_count:
                        logger.warning(f"⚠️ [SLIDE_COUNT_MISMATCH] AI generated {actual_count} slides but {requested_count} were requested! This is a critical issue that needs attention.")
                    else:
                        logger.info(f"✅ [SLIDE_COUNT_MATCH] AI correctly generated {actual_count} slides as requested.")
                    
                    for i, slide in enumerate(slides):
                        if isinstance(slide, dict):
                            template_id = slide.get('templateId', 'NO_TEMPLATE_ID')
                            slide_title = slide.get('slideTitle', 'NO_TITLE')
                            logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Slide {i+1}: templateId='{template_id}', slideTitle='{slide_title}'")
                            if 'voiceoverText' in slide:
                                voiceover = slide['voiceoverText']
                                logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Slide {i+1} voiceover: {voiceover[:100]}{'...' if len(voiceover) > 100 else ''}")
                else:
                    logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] No 'slides' key found in JSON")
            else:
                logger.info(f"🔍 [VIDEO_LESSON_AI_RESPONSE] No JSON structure found in response")
        except Exception as e:
            logger.warning(f"🔍 [VIDEO_LESSON_AI_RESPONSE] Failed to parse JSON from response: {e}")

        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="text/plain")


@app.post("/api/custom/lesson-presentation/finalize")
async def wizard_lesson_finalize(payload: LessonWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    logger.info(f"Finalizing lesson presentation: {payload.lessonTitle}")
    
    # Validate required fields early
    if not payload.lessonTitle or not payload.lessonTitle.strip():
        raise HTTPException(status_code=400, detail="Lesson title is required")
    
    if not payload.aiResponse or not payload.aiResponse.strip():
        raise HTTPException(status_code=400, detail="AI response content is required")

    # NEW: Regenerate changed slides if user made edits; produce updated JSON
    regenerated_json: Optional[Dict[str, Any]] = None
    try:
        if getattr(payload, 'hasUserEdits', False) and getattr(payload, 'originalContent', None) and getattr(payload, 'editedSlides', None):
            orig = json.loads(payload.originalContent)  # type: ignore[arg-type]
            if isinstance(orig, dict) and isinstance(orig.get("slides"), list):
                slides = orig["slides"]
                is_video_lesson_local = payload.productType == "video_lesson_presentation"
                for edit in payload.editedSlides:  # type: ignore[attr-defined]
                    try:
                        slide_num = int(edit.get("slideNumber"))
                        # locate slide
                        idx = None
                        for i, s in enumerate(slides):
                            n = s.get("slideNumber") if isinstance(s, dict) else None
                            if (n or i + 1) == slide_num:
                                idx = i; break
                        if idx is None:
                            continue
                        target = slides[idx]
                        new_title = edit.get("newTitle")
                        new_points = edit.get("previewKeyPoints")
                        regen_payload = {
                            "product": "Video Lesson Slides Deck" if is_video_lesson_local else "Slides Deck",
                            "action": "regenerate-slide",
                            "language": "en",
                            "regeneration": {
                                "slideNumber": slide_num,
                                "prioritizedTopics": new_points if isinstance(new_points, list) else None,
                                "newTitle": new_title if isinstance(new_title, str) else None,
                                "theme": payload.theme,
                            },
                            "context": {"lessonTitle": payload.lessonTitle}
                        }
                        json_example = DEFAULT_VIDEO_LESSON_JSON_EXAMPLE_FOR_LLM if is_video_lesson_local else DEFAULT_SLIDE_DECK_JSON_EXAMPLE_FOR_LLM
                        wizard_message = (
                            "WIZARD_REQUEST\n" + json.dumps(regen_payload) +
                            "\nCRITICAL: This is an edit. Regenerate ONLY this slide so that its content fully matches the updated title and prioritized topics. Prioritize previewKeyPoints over the title if both are present.\n"
                            "Follow the SAME rules and JSON schema as initial generation (component-based slides with appropriate templateId and props).\n"
                            "Use the EXACT prop structure shown in these examples for each template:\n" + 
                            json_example + "\n" +
                            "You MUST output ONLY a single JSON object of the slide with fields: slideId, slideNumber, slideTitle, templateId, props" + (", voiceoverText" if is_video_lesson_local else "") + ".\n"
                            "Do NOT include code fences, markdown, or commentary. Return JSON object only.\n"
                        )
                        # Collect once-off response
                        regenerated_text = ""
                        async for chunk in stream_openai_response(wizard_message):
                            if chunk.get("type") == "delta":
                                regenerated_text += chunk.get("text", "")
                        cleaned = regenerated_text.strip()
                        if cleaned.startswith("```"):
                            cleaned = cleaned.strip('`')
                            cleaned = cleaned.replace("json", "", 1).strip()
                        new_slide_obj = json.loads(cleaned)
                        new_slide_obj["slideNumber"] = slide_num
                        slides[idx] = new_slide_obj
                    except Exception as regen_err:
                        logger.warning(f"[REGEN_SLIDE] Failed to regenerate slide {edit}: {regen_err}")
                regenerated_json = orig
    except Exception as e:
        logger.warning(f"[REGEN_EDITED_SLIDES] Skipping edits processing due to error: {e}")

    # Parse AI response to determine slide count for credit calculation
    try:
        slides_data = json.loads((json.dumps(regenerated_json) if regenerated_json else payload.aiResponse))
        credits_needed = calculate_product_credits("lesson_presentation", slides_data)
    except:
        # If parsing fails, use default credit cost
        credits_needed = calculate_product_credits("lesson_presentation")

    # Get user ID and deduct credits for lesson presentation
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Lesson presentation finalization")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for lesson presentation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for lesson presentation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")

    try:
        # Determine if this is a video lesson presentation
        is_video_lesson = payload.productType == "video_lesson_presentation"
        
        # Get the appropriate template with retry mechanism
        max_retries = 3
        template_id = None
        for attempt in range(max_retries):
            try:
                if is_video_lesson:
                    template_id = await _ensure_video_lesson_presentation_template(pool)
                else:
                    template_id = await _ensure_slide_deck_template(pool)
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"Failed to get template after {max_retries} attempts: {e}")
                    raise HTTPException(status_code=500, detail="Unable to initialize template")
                await asyncio.sleep(0.5)  # Brief delay before retry

        if not template_id:
            raise HTTPException(status_code=500, detail="Template initialization failed")

        # Get user ID
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = payload.lessonTitle.strip() if payload.lessonTitle else "Video Lesson Presentation"
        if payload.outlineProjectId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineProjectId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lessonTitle.strip()}"
            except Exception as e:
                logger.warning(f"Failed to fetch outline name for lesson naming: {e}")
                # Continue with plain lesson title if outline fetch fails

        # Build source context from payload
        source_context_type, source_context_data = build_source_context(payload)
        
        # Create project data
        project_data = ProjectCreateRequest(
            projectName=project_name,
            design_template_id=template_id,
            microProductName=project_name,
            aiResponse=(json.dumps(regenerated_json) if regenerated_json else payload.aiResponse.strip()),
            chatSessionId=payload.chatSessionId,
            outlineId=payload.outlineProjectId,  # Pass outlineId for consistent naming
            folder_id=int(payload.folderId) if payload.folderId else None,  # Add folder assignment
            theme=payload.theme,  # Pass selected theme
            source_context_type=source_context_type,
            source_context_data=source_context_data
        )
        
        # Create project with proper error handling
        try:
            created_project = await add_project_to_custom_db(project_data, onyx_user_id, pool)
        except HTTPException as e:
            # Re-raise HTTP exceptions as-is
            raise e
        except Exception as e:
            logger.error(f"Failed to create project: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Failed to create lesson project")

        # Validate the created project
        if not created_project or not created_project.id:
            logger.error("Project creation returned invalid result")
            raise HTTPException(status_code=500, detail="Project creation failed - invalid response")

        logger.info(f"Successfully finalized lesson presentation with project ID: {created_project.id}")

        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if row:
                    logger.info(f"[LESSON_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved presentation JSON for project {created_project.id}: {log_e}")

        # Return simple JSON response (not streaming for now)
        return {
            "id": created_project.id,
            "projectName": created_project.project_name,
            "message": "Lesson presentation finalized successfully"
        }
    
    except HTTPException:
        # Re-raise HTTP exceptions without modification
        raise
    except Exception as e:
        logger.error(f"Unexpected error in lesson finalization: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail="An unexpected error occurred during finalization"
        )

# --- Delete single project endpoint ---
@app.delete("/api/custom/projects/{project_id}", status_code=204)
async def delete_project(
    project_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Delete a single project by ID.
    """
    try:
        async with pool.acquire() as conn:
            # Check if project exists and belongs to user
            project_row = await conn.fetchrow(
                "SELECT id FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
            if not project_row:
                raise HTTPException(
                    status_code=404,
                    detail="Project not found"
                )
            
            # Delete the project
            await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
        logger.info(f"Successfully deleted project {project_id}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error deleting project {project_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred while deleting the project"
        )

# --- New endpoint: list trashed projects for user ---

def fix_product_descriptions(lesson_plan_data, logger):
    """
    Ensures all product_description fields are strings, not nested objects.
    """
    if "contentDevelopmentSpecifications" not in lesson_plan_data:
        return
    
    logger.info("🔧 Fixing product descriptions to ensure string format...")
    
    for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
        if block.get("type") == "product" and "product_description" in block:
            description = block["product_description"]
            
            if not isinstance(description, str):
                logger.warning(f"Block {i}: product_description is {type(description)}, converting to string")
                
                if isinstance(description, dict):
                    # Flatten dictionary to comprehensive string
                    text_parts = []
                    
                    def flatten_dict(d, prefix=""):
                        for key, value in d.items():
                            if isinstance(value, dict):
                                flatten_dict(value, f"{prefix}{key}: ")
                            elif isinstance(value, list):
                                text_parts.append(f"{prefix}{key}: {', '.join(map(str, value))}")
                            else:
                                text_parts.append(f"{prefix}{key}: {value}")
                    
                    flatten_dict(description)
                    flattened_description = ". ".join(text_parts) + "."
                    
                    # Ensure comprehensive content
                    if len(flattened_description) < 250:
                        flattened_description += " Additional specifications include technical quality standards, accessibility compliance (WCAG), target audience considerations, assessment criteria, and implementation guidelines for professional content development."
                    
                    block["product_description"] = flattened_description
                    logger.info(f"✅ Block {i}: Flattened to {len(flattened_description)} chars")
                    
                elif isinstance(description, list):
                    # Join list items
                    flattened_description = ". ".join(map(str, description)) + "."
                    block["product_description"] = flattened_description
                    logger.info(f"✅ Block {i}: Joined list to string")
                    
                else:
                    # Convert any other type
                    block["product_description"] = str(description)
                    logger.info(f"✅ Block {i}: Converted to string")
            else:
                logger.info(f"✅ Block {i}: product_description is already a string ({len(description)} chars)")

@app.post("/api/custom/lesson-plan/generate", response_model=LessonPlanResponse)
async def generate_lesson_plan(
    payload: LessonPlanGenerationRequest, 
    request: Request, 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Generate a lesson plan directly from a course outline using the hybrid approach.
    """
    logger.info(f"Generating lesson plan for outline project {payload.outlineProjectId}")
    
    try:
        # Get user ID
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Retrieve the source context from the course outline project
        async with pool.acquire() as conn:
            outline_row = await conn.fetchrow(
                """
                SELECT id, project_name, source_context_type, source_context_data, 
                       microproduct_content, microproduct_type
                FROM projects 
                WHERE id = $1 AND onyx_user_id = $2
                """,
                payload.outlineProjectId, onyx_user_id
            )
            
            if not outline_row:
                raise HTTPException(
                    status_code=404, 
                    detail="Course outline project not found"
                )
            
            if outline_row["microproduct_type"] not in ["Training Plan", "Course Outline"]:
                raise HTTPException(
                    status_code=400, 
                    detail="Specified project is not a course outline"
                )
        
        # Extract source context
        source_context_type = outline_row["source_context_type"]
        source_context_data = outline_row["source_context_data"]
        
        # Prepare context for OpenAI
        context_for_openai = ""
        
        if source_context_type == "files" and source_context_data:
            # Extract file context using hybrid approach
            file_ids = source_context_data.get("file_ids", [])
            folder_ids = source_context_data.get("folder_ids", [])
            
            if file_ids or folder_ids:
                # Get cookies from request for Onyx API calls
                cookies = dict(request.cookies)
                
                # Extract context using the existing hybrid approach
                file_context = await extract_file_context_from_onyx(
                    file_ids, folder_ids, cookies
                )
                
                # Build context string for OpenAI
                if file_context.get("file_summaries"):
                    context_for_openai += "File Content:\n" + "\n".join(file_context["file_summaries"]) + "\n\n"
                if file_context.get("key_topics"):
                    context_for_openai += "Key Topics:\n" + ", ".join(file_context["key_topics"]) + "\n\n"
        
        elif source_context_type == "connectors" and source_context_data:
            # Extract connector context
            connector_ids = source_context_data.get("connector_ids", [])
            connector_sources = source_context_data.get("connector_sources", [])
            
            if connector_ids:
                context_for_openai += f"Connector Sources: {', '.join(connector_sources)}\n\n"
        
        elif source_context_type == "text" and source_context_data:
            # Extract text context
            user_text = source_context_data.get("user_text", "")
            if user_text:
                context_for_openai += f"Source Text:\n{user_text}\n\n"
        
        elif source_context_type == "knowledge_base" and source_context_data:
            # Extract knowledge base context
            search_query = source_context_data.get("search_query", "")
            if search_query:
                context_for_openai += f"Knowledge Base Query: {search_query}\n\n"
        
        # Extract specific lesson data and course outline content
        lesson_completion_time = "6m"  # Default fallback
        lesson_context = ""
        
        if outline_row["microproduct_content"]:
            try:
                outline_content = outline_row["microproduct_content"]
                if isinstance(outline_content, dict):
                    # Extract relevant information from outline
                    if "sections" in outline_content:
                        sections_text = []
                        found_lesson = False
                        
                        for section in outline_content["sections"]:
                            if isinstance(section, dict):
                                section_title = section.get("title", "")
                                section_content = section.get("content", "")
                                
                                # Check if this section matches the module name
                                if section_title and payload.moduleName.lower() in section_title.lower():
                                    # Look for the specific lesson in this section
                                    if "lessons" in section and isinstance(section["lessons"], list):
                                        for lesson in section["lessons"]:
                                            if isinstance(lesson, dict):
                                                lesson_title = lesson.get("title", "")
                                                if lesson_title and payload.lessonTitle.lower() in lesson_title.lower():
                                                    # Found the specific lesson - extract its completion time and individual product times
                                                    lesson_completion_time = lesson.get("completionTime", "6m")
                                                    
                                                    # Extract individual product completion times if available
                                                    individual_completion_times = {}
                                                    if lesson.get("completionTimes"):
                                                        completion_times_data = lesson["completionTimes"]
                                                        if isinstance(completion_times_data, dict):
                                                            # Map frontend naming to backend naming
                                                            if 'presentation' in completion_times_data:
                                                                individual_completion_times['presentation'] = completion_times_data['presentation']
                                                            if 'onePager' in completion_times_data:
                                                                individual_completion_times['one-pager'] = completion_times_data['onePager']
                                                            if 'quiz' in completion_times_data:
                                                                individual_completion_times['quiz'] = completion_times_data['quiz']
                                                            if 'videoLesson' in completion_times_data:
                                                                individual_completion_times['video-lesson'] = completion_times_data['videoLesson']
                                                    
                                                    lesson_context = f"Lesson Context: {lesson_title}"
                                                    if lesson.get("content"):
                                                        lesson_context += f" - {lesson.get('content')}"
                                                    found_lesson = True
                                                    
                                                    if individual_completion_times:
                                                        logger.info(f"Found lesson '{lesson_title}' with individual completion times: {individual_completion_times}")
                                                    else:
                                                        logger.info(f"Found lesson '{lesson_title}' with total completion time: {lesson_completion_time}")
                                                    break
                                    if found_lesson:
                                        break
                                
                                # Add general section context
                                if section_title and section_content:
                                    sections_text.append(f"{section_title}: {section_content}")
                        
                        if sections_text:
                            context_for_openai += "Course Outline Content:\n" + "\n".join(sections_text) + "\n\n"
                        
                        if lesson_context:
                            context_for_openai += lesson_context + "\n\n"
                            
            except Exception as e:
                logger.warning(f"Failed to parse outline content: {e}")
                
        # Use individual completion times if available, otherwise use total lesson time
        if individual_completion_times:
            logger.info(f"Using individual product completion times: {individual_completion_times}")
        else:
            logger.info(f"Using total lesson completion time: {lesson_completion_time} for lesson plan generation")
        
        # Helper function to extract minutes from time string
        def extract_minutes_from_time(time_str):
            try:
                if not time_str:
                    return 6  # Default fallback
                # Extract numeric part from strings like "6m", "2h", etc.
                numeric_part = ''.join(filter(str.isdigit, str(time_str)))
                if not numeric_part:
                    return 6  # Default fallback
                minutes = int(numeric_part)
                if 'h' in str(time_str).lower():
                    minutes *= 60  # Convert hours to minutes
                return minutes
            except:
                return 6  # Default fallback
        
        # Prepare timing information for products using individual times if available
        timing_info = "Product Timing Guidelines:\n"
        
        # Calculate video lesson duration
        if individual_completion_times and 'video-lesson' in individual_completion_times:
            video_minutes = extract_minutes_from_time(individual_completion_times['video-lesson'])
            timing_info += f"- Video Lesson Duration: {video_minutes} minutes (from individual completion time)\n"
        else:
            # Fallback to calculated duration from total lesson time
            total_minutes = extract_minutes_from_time(lesson_completion_time)
            video_duration = max(2, min(5, total_minutes // 2))
            timing_info += f"- Video Lesson Duration: Approximately {video_duration} minutes (calculated from lesson completion time)\n"
        
        # Calculate presentation length
        if individual_completion_times and 'presentation' in individual_completion_times:
            presentation_minutes = extract_minutes_from_time(individual_completion_times['presentation'])
            # Convert presentation time to slide count (rough estimate: 1 slide per minute + buffer)
            presentation_slides = max(8, min(20, presentation_minutes + 3))
            timing_info += f"- Presentation Length: Approximately {presentation_slides} slides (based on {presentation_minutes}min completion time)\n"
        else:
            # Fallback to calculated slides from total lesson time
            total_minutes = extract_minutes_from_time(lesson_completion_time)
            presentation_slides = max(8, min(15, total_minutes + 2))
            timing_info += f"- Presentation Length: Approximately {presentation_slides} slides (calculated from lesson completion time)\n"
        
        # Calculate quiz length
        if individual_completion_times and 'quiz' in individual_completion_times:
            quiz_minutes = extract_minutes_from_time(individual_completion_times['quiz'])
            # Convert quiz time to question count (rough estimate: 1-2 questions per minute)
            quiz_questions = max(5, min(15, quiz_minutes * 2))
            timing_info += f"- Quiz Length: Approximately {quiz_questions} questions (based on {quiz_minutes}min completion time)\n"
        else:
            timing_info += "- Quiz Length: 8-12 questions (standard range)\n"
        
        # One-pager timing
        if individual_completion_times and 'one-pager' in individual_completion_times:
            onepager_minutes = extract_minutes_from_time(individual_completion_times['one-pager'])
            timing_info += f"- One-Pager: Single comprehensive page (based on {onepager_minutes}min completion time)\n"
        else:
            timing_info += "- One-Pager: Single comprehensive page\n"
        

        
        # Create specific prompts based on recommended products
        has_video = any('video' in product.lower() for product in payload.recommendedProducts)
        has_presentation = any('presentation' in product.lower() for product in payload.recommendedProducts)
        
        prompts_instruction = "AI TOOL PROMPTS: Create ready-to-use prompts for AI content creation tools (like Synthesia, Gamma, etc.) "
        if has_video and has_presentation:
            prompts_instruction += "Provide exactly 2 specific prompts - one for video lesson creation and one for presentation creation. Each prompt should be detailed and actionable."
        elif has_video:
            prompts_instruction += "Provide exactly 1 specific prompt for video lesson creation. The prompt should be detailed and actionable."
        elif has_presentation:
            prompts_instruction += "Provide exactly 1 specific prompt for presentation creation. The prompt should be detailed and actionable."
        else:
            prompts_instruction += "Provide 2-3 specific content creation prompts for the recommended product types. Each prompt should be detailed and actionable."

        # Extract source materials from the course outline context
        source_materials = []
        
        if source_context_type == "files" and source_context_data:
            file_ids = source_context_data.get("file_ids", [])
            folder_ids = source_context_data.get("folder_ids", [])
            
            # Get cookies from request for Onyx API calls
            cookies = dict(request.cookies)
            
            # Fetch actual file names
            if file_ids:
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        # Get the file system to find files by ID
                        response = await client.get(
                            f"{ONYX_API_SERVER_URL}/user/file-system",
                            cookies=cookies
                        )
                        response.raise_for_status()
                        folders_data = response.json()
                        
                        # Extract file names from the folder structure
                        file_names = {}
                        for folder in folders_data:
                            if 'files' in folder:
                                for file_info in folder['files']:
                                    if file_info.get('id') in file_ids:
                                        file_names[file_info['id']] = file_info.get('name', f'Document {file_info["id"]}')
                        
                        # Add files with their actual names
                        for file_id in file_ids:
                            file_name = file_names.get(file_id, f'Document {file_id}')
                            source_materials.append(file_name)
                            
                except Exception as e:
                    logger.error(f"Error fetching file names: {e}")
                    # Fallback to generic names
                    source_materials.extend([f"Document {file_id}" for file_id in file_ids])
            
            # Fetch actual folder names
            if folder_ids:
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        # Get the file system to find folders by ID
                        response = await client.get(
                            f"{ONYX_API_SERVER_URL}/user/file-system",
                            cookies=cookies
                        )
                        response.raise_for_status()
                        folders_data = response.json()
                        
                        # Extract folder names
                        folder_names = {}
                        for folder in folders_data:
                            if folder.get('id') in folder_ids:
                                folder_names[folder['id']] = folder.get('name', f'Folder {folder["id"]}')
                        
                        # Add folders with their actual names
                        for folder_id in folder_ids:
                            folder_name = folder_names.get(folder_id, f'Folder {folder_id}')
                            source_materials.append(f"{folder_name} (Folder)")
                            
                except Exception as e:
                    logger.error(f"Error fetching folder names: {e}")
                    # Fallback to generic names
                    source_materials.extend([f"Folder {folder_id}" for folder_id in folder_ids])
                
        elif source_context_type == "connectors" and source_context_data:
            connector_sources = source_context_data.get("connector_sources", [])
            if connector_sources:
                source_materials.extend([f"Connector: {source}" for source in connector_sources])
                
        elif source_context_type == "text" and source_context_data:
            source_materials.append("Custom Text Input")
            
        elif source_context_type == "knowledge_base" and source_context_data:
            search_query = source_context_data.get("search_query", "")
            if search_query:
                source_materials.append(f"Knowledge Base Query: {search_query}")
        
        # If no source materials found, use general knowledge
        if not source_materials:
            source_materials = ["General Knowledge"]

        print("context_for_openai:", context_for_openai)
        
        # Prepare OpenAI prompt
        openai_prompt = f"""
You are an expert instructional designer and educational content developer. Based on the following source context, create a comprehensive lesson plan that serves as a complete task specification for Content Developers to create high-quality educational materials.

Source Context:
{context_for_openai}

Lesson Information:
- Lesson Title: {payload.lessonTitle}
- Module Name: {payload.moduleName}
- Lesson Number: {payload.lessonNumber}
- Lesson Completion Time: {lesson_completion_time}
- Recommended Products: {', '.join(payload.recommendedProducts)}
- CRITICAL: Use these EXACT product names in product blocks within contentDevelopmentSpecifications: {payload.recommendedProducts}

{timing_info}

Create a detailed lesson plan following instructional design best practices:

LESSON OBJECTIVES: Write 3-5 specific, measurable learning objectives using Bloom's Taxonomy action verbs. Each objective should specify what learners will be able to DO after completing the lesson (not what will be taught to them). Include the performance/behavior, conditions, and success criteria where applicable.

SHORT DESCRIPTION: Write a compelling 2-3 sentence description that clearly communicates the lesson's value proposition to learners. Focus on practical outcomes and real-world applications they will gain, not just topics covered.

CONTENT DEVELOPMENT SPECIFICATIONS: Create a flowing, structured lesson format that combines educational text blocks with product specifications. This section should tell a complete story about the lesson topic, with product blocks seamlessly integrated. Structure as follows:

TEXT BLOCKS: Create 3-5 educational text blocks with:
- block_title: A clear, engaging title (e.g., "Understanding the Fundamentals", "Key Implementation Strategies", "Best Practices for Success")
- block_content: Rich educational content that should contain ONE of the following formats:
  * Plain text paragraphs only (for explanatory content)
  * Bullet lists only (using -) for key points and benefits
  * Numbered lists only (using 1.) for sequential steps or processes
  * Mixed format: Brief intro text followed by a list (if context requires both)

CRITICAL CASE STUDY REQUIREMENT: If any text block includes a case study, it MUST be a real, specific case study with actual details - including real company names, specific outcomes, actual dates/timeframes, and concrete results. NEVER use placeholder text like "Company X", "a major corporation", "recent study", or generic examples. Research and provide actual case studies with verifiable details.

PRODUCT BLOCKS: For each recommended product, create a product block with:
- product_name: Exact name from recommendedProducts list
- product_description: SIMPLE CONTENT OUTLINE ONLY. This should be a clear, concise description of WHAT TOPICS AND CONTENT should be covered in this product. Write as a single string that serves as a content roadmap for developers. Include:

CONTENT TOPICS TO COVER (as a simple list format):
- Main topics that must be addressed (3-5 key areas)
- Important subtopics within each main area
- Key concepts and terminology to explain
- Essential examples or case studies to include
- Practical applications to demonstrate

EXAMPLE FORMAT: "This [product type] should cover the following topics: Topic 1 including subtopic A and subtopic B, Topic 2 with focus on concept X and concept Y, Topic 3 demonstrating practical application Z. Key terminology to explain includes [terms]. Essential examples should include [specific example]. The content should help learners understand [main learning outcome]."

DO NOT INCLUDE: Detailed instructions, technical specs, formatting requirements, or step-by-step creation guidance. Keep it focused on WHAT content should be covered, not HOW to create it.

INTEGRATION PATTERN: Alternate between text blocks and product blocks to create educational flow:
- Start with 1-2 text blocks introducing the topic
- Insert first product block
- Add randomly 1-2 text blocks expanding on concepts (vary the count)
- Insert next product block (if applicable)
- Continue pattern with random 1-2 text blocks between each product block
- End with a text block for conclusion

IMPORTANT: Vary the number of text blocks between products (sometimes 1, sometimes 2) to create natural flow. Each text block should contain EITHER plain text paragraphs OR bullet/numbered lists, not necessarily both.

The content should flow naturally, building knowledge progressively while seamlessly incorporating product specifications that support the learning journey.

MATERIALS: List the actual source materials used to create this lesson plan:

Source Materials Used:
{', '.join(source_materials)}

AI TOOL PROMPTS: Create exactly one COMPLETE, COPY-PASTE READY prompt for each recommended product type. These prompts should be fully formed instructions that users can copy directly into AI tools (like ChatGPT, Claude, Synthesia, Gamma, etc.) without any modification. Each prompt must be self-contained and immediately executable.

CRITICAL REQUIREMENTS:
- Create exactly {len(payload.recommendedProducts)} prompts (one for each recommended product)
- Each prompt must be 200-400 words and READY TO USE as-is
- Fill in ALL placeholder values with actual lesson-specific content
- Prompts should be complete sentences that can be copied and pasted directly into AI tools

PRODUCT TYPE FORMATS:

IMPORTANT: Replace ALL bracketed placeholders with actual lesson-specific information. The final prompts should contain NO brackets or placeholders.

FOR VIDEO LESSONS:
Create prompts following this pattern (fill in all specific details):
"Create a professional training video for [specific target audience with experience level]. This is the [specific lesson context], titled [actual lesson title]. The video should welcome learners with [specific opening approach], explain that the main goal is [actual learning objective from the lesson plan], cover [specific topics from the lesson's product block content], and provide [specific overview elements]. The tone should be [appropriate tone for audience], and the duration should be around [X] minutes based on the lesson timing."

FOR PRESENTATIONS:
Create prompts following this pattern (fill in all specific details):
"Create a professional educational presentation for [specific target audience]. This is the [lesson context] for the unit on [topic area], titled '[actual lesson title].' The presentation should [specific opening approach], explain that the main goal is to [actual learning objective], and provide [specific content breakdown from lesson topics]. The presentation must cover [list specific topics from product block]. The tone should be [appropriate tone], with [visual style description]. The presentation should be around [X-Y] slides based on timing. For each slide, please generate concise on-slide text and provide detailed speaker notes to guide the teacher."

FOR QUIZZES:
Create prompts following this pattern (fill in all specific details):
"Create a [specific number]-question multiple-choice quiz for [specific target audience] to assess their understanding of the [lesson context], '[actual lesson title].' The quiz's primary goal is to test the students' knowledge of [specific concepts from lesson objectives]. It should cover [specific content areas from product block]. The questions should be clear, direct, and multiple-choice, with four distinct answer options. The tone should be educational and straightforward. For each question, provide one correct answer and three plausible but incorrect distractors. Additionally, include a brief rationale for each answer option explaining why it is correct or incorrect, and a hint for each question that guides students toward the correct concept without giving away the answer."

FOR ONE-PAGERS:
Create prompts following this pattern (fill in all specific details):
"Create a professional e-learning document for [specific target audience]. This document should act as a [specific document purpose] for the lesson titled '[actual lesson title].' The document should [specific content organization requirements]. It must cover [specific topics from the lesson's product block content]. The tone should be [appropriate tone]. The information should be highly organized for scannability, using headings, bullet points, and bold text to highlight key information. The final document should be approximately [specific length] long."

CRITICAL DISTINCTION - PRODUCT BLOCKS vs PROMPTS:
- PRODUCT BLOCKS = Simple content outline describing WHAT topics should be covered (example: "This video should cover topic A, topic B, and topic C with focus on practical applications")
- AI TOOL PROMPTS = Complete, copy-paste ready instructions for AI tools (example: "Create a professional training video for senior project managers. This is an advanced lesson on risk management, titled 'Advanced Risk Assessment Techniques.' The video should...")

CONTENT REQUIREMENTS FOR ALL PROMPTS:
- Target audience must be specific (not generic)
- Learning objectives must be measurable and clear
- Content must include specific topics and subtopics from the lesson context
- Examples and case studies must be detailed and relevant
- Tone and style must be appropriate for the audience and context
- Fill in ALL bracketed placeholders with actual lesson-specific information
- Final prompts should be ready to copy-paste into AI tools without modification

CRITICAL REQUIREMENT: 
- ONLY include products that are explicitly listed in the recommendedProducts array: {payload.recommendedProducts}
- Use the EXACT product names from the recommendedProducts list in product blocks within contentDevelopmentSpecifications
- Do NOT add any products that are not in the recommendedProducts array
- Do NOT change the spelling or format of product names from the recommendedProducts list

Focus on creating actionable specifications that enable Content Developers to produce effective, engaging educational materials.

Return your response as a valid JSON object with this exact structure. PAY SPECIAL ATTENTION to product_description - it must be a SINGLE STRING, not nested objects:

{{
  "lessonTitle": "string",
  "lessonObjectives": ["string"],
  "shortDescription": "string",
  "contentDevelopmentSpecifications": [
    {{
      "type": "text",
      "block_title": "string",
      "block_content": "string (can include bullet lists with - or numbered lists with 1.)"
    }},
    {{
      "type": "product",
      "product_name": "exact name from recommendedProducts",
      "product_description": "This must be a single string with all specifications in one paragraph. Example: Create a comprehensive 12-slide presentation targeting intermediate project managers with 3-5 years experience. Content must cover RAG methodology definitions, common applications in project contexts, specific limitations including data quality issues and implementation challenges, real-world case studies from software development and construction industries. Technical specifications: 16:9 aspect ratio, 1920x1080 resolution, corporate branding with blue/white color scheme, Arial font 24pt minimum for titles and 18pt for content. Each slide should include speaker notes with detailed talking points. Accessibility requirements: high contrast text (4.5:1 ratio), alt text for all images, screen reader compatibility. Structure: opening slide with agenda, 2 slides for definitions, 4 slides for applications, 3 slides for limitations with examples, 2 slides for case studies, closing slide with key takeaways. Include interactive elements like polls or discussion questions every 4 slides to maintain engagement."
    }}
  ],
  "materials": ["string"],
  "suggestedPrompts": ["string"]
}}

CRITICAL: The product_description field shown above is an example of the single string format required. Do NOT use nested objects like {{"contentSpecifications": {{}}, "technicalSpecs": {{}}}}. Everything must be in one comprehensive string.

Ensure the JSON is valid and follows the exact structure specified.
"""
        
        # Generate lesson plan using OpenAI
        openai_client = get_openai_client()
        
        response = await openai_client.chat.completions.create(
            model=LLM_DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": "You are an expert educational content creator. Always respond with valid JSON. Create extremely detailed, comprehensive content specifications."},
                {"role": "user", "content": openai_prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        # Parse OpenAI response
        ai_response = response.choices[0].message.content.strip()
        
        # Strip markdown code blocks if present
        if ai_response.startswith('```json'):
            ai_response = ai_response[7:]  # Remove ```json
        elif ai_response.startswith('```'):
            ai_response = ai_response[3:]   # Remove ```
        
        if ai_response.endswith('```'):
            ai_response = ai_response[:-3]  # Remove trailing ```
        
        ai_response = ai_response.strip()  # Clean up any remaining whitespace
        
        # Strip markdown code blocks if present (similar to existing LLM parsing logic)
        ai_response = re.sub(r"^```json\s*|\s*```$", "", ai_response.strip(), flags=re.MULTILINE)
        ai_response = re.sub(r"^```(?:json)?\s*|\s*```$", "", ai_response, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        try:
            lesson_plan_data = json.loads(ai_response)
            
            # CRITICAL FIX: Ensure product_description is always a string
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        if not isinstance(description, str):
                            logger.warning(f"Block {i}: Converting {type(description)} to string")
                            if isinstance(description, dict):
                                # Flatten nested structure to string
                                parts = []
                                def flatten_obj(obj, prefix=""):
                                    if isinstance(obj, dict):
                                        for k, v in obj.items():
                                            if isinstance(v, dict):
                                                flatten_obj(v, f"{prefix}{k}: ")
                                            elif isinstance(v, list):
                                                parts.append(f"{prefix}{k}: {', '.join(map(str, v))}")
                                            else:
                                                parts.append(f"{prefix}{k}: {v}")
                                    else:
                                        parts.append(str(obj))
                                flatten_obj(description)
                                flattened = ". ".join(parts) + "."
                                if len(flattened) < 200:
                                    flattened += " This includes comprehensive specifications, technical requirements, and quality standards."
                                block["product_description"] = flattened
                                logger.info(f"✅ Block {i}: Flattened to {len(flattened)} chars")
                            else:
                                block["product_description"] = str(description)
                                logger.info(f"✅ Block {i}: Converted to string")
            # Validate the structure
            required_fields = ["lessonTitle", "lessonObjectives", "shortDescription", "contentDevelopmentSpecifications", "materials", "suggestedPrompts"]
            for field in required_fields:
                if field not in lesson_plan_data:
                    raise ValueError(f"Missing required field: {field}")
            
            # COMPREHENSIVE FIX: Ensure all product descriptions are strings
            logger.info("Validating and fixing product descriptions...")
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for i, block in enumerate(lesson_plan_data["contentDevelopmentSpecifications"]):
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        
                        if not isinstance(description, str):
                            logger.warning(f"Block {i}: Converting {type(description)} to string for product_description")
                            
                            if isinstance(description, dict):
                                # Convert dictionary to comprehensive string
                                parts = []
                                for key, value in description.items():
                                    if isinstance(value, dict):
                                        nested_parts = [f"{k}: {v}" for k, v in value.items()]
                                        parts.append(f"{key.upper()}: {'. '.join(nested_parts)}")
                                    elif isinstance(value, list):
                                        parts.append(f"{key.upper()}: {', '.join(map(str, value))}")
                                    else:
                                        parts.append(f"{key}: {value}")
                                
                                flattened = ". ".join(parts)
                                if len(flattened) < 200:
                                    flattened += ". This content should include comprehensive specifications, technical requirements, accessibility standards, and quality criteria."
                                
                                block["product_description"] = flattened
                                logger.info(f"Block {i}: Flattened to {len(flattened)} character string")
                                
                            else:
                                # Convert other types to string
                                block["product_description"] = str(description)
                                logger.info(f"Block {i}: Converted {type(description)} to string")
            
            # Fix product descriptions that might be nested objects instead of strings
            if "contentDevelopmentSpecifications" in lesson_plan_data:
                for block in lesson_plan_data["contentDevelopmentSpecifications"]:
                    if block.get("type") == "product" and "product_description" in block:
                        description = block["product_description"]
                        if isinstance(description, dict):
                            # AI returned nested structure - flatten it to a comprehensive string
                            logger.warning(f"AI returned nested structure for product_description, flattening to string")
                            flattened_description = ""
                            
                            # Extract content specifications
                            if "contentSpecifications" in description:
                                content_specs = description["contentSpecifications"]
                                if isinstance(content_specs, dict):
                                    flattened_description += "CONTENT SPECIFICATIONS: "
                                    for key, value in content_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(content_specs, list):
                                    flattened_description += "CONTENT SPECIFICATIONS: " + ". ".join(map(str, content_specs)) + ". "
                            
                            # Extract technical specifications
                            if "technicalSpecifications" in description:
                                tech_specs = description["technicalSpecifications"]
                                flattened_description += "TECHNICAL SPECIFICATIONS: "
                                if isinstance(tech_specs, dict):
                                    for key, value in tech_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(tech_specs, list):
                                    flattened_description += ". ".join(map(str, tech_specs)) + ". "
                            
                            # Extract target audience specifications
                            if "targetAudienceAdaptations" in description:
                                audience_specs = description["targetAudienceAdaptations"]
                                flattened_description += "TARGET AUDIENCE: "
                                if isinstance(audience_specs, dict):
                                    for key, value in audience_specs.items():
                                        if isinstance(value, list):
                                            flattened_description += f"{key}: {', '.join(map(str, value))}. "
                                        else:
                                            flattened_description += f"{key}: {value}. "
                                elif isinstance(audience_specs, list):
                                    flattened_description += ". ".join(map(str, audience_specs)) + ". "
                            
                            # If we couldn't extract specific sections, convert the whole thing to string
                            if not flattened_description.strip():
                                # Recursive function to extract all text from nested structures
                                def extract_all_text(obj):
                                    if isinstance(obj, dict):
                                        text_parts = []
                                        for key, value in obj.items():
                                            if isinstance(value, (str, int, float)):
                                                text_parts.append(f"{key}: {value}")
                                            else:
                                                text_parts.append(f"{key}: {extract_all_text(value)}")
                                        return ". ".join(text_parts)
                                    elif isinstance(obj, list):
                                        return ". ".join(str(item) for item in obj)
                                    else:
                                        return str(obj)
                                
                                flattened_description = extract_all_text(description)
                            
                            # Clean up the string and ensure it's comprehensive
                            flattened_description = flattened_description.strip()
                            if len(flattened_description) < 200:
                                # If too short, add padding with comprehensive details
                                flattened_description += " This content should include detailed specifications, technical requirements, learning objectives, assessment criteria, target audience considerations, accessibility requirements, and implementation guidelines to ensure comprehensive content development."
                            
                            # Update the block with flattened string
                            block["product_description"] = flattened_description
                            logger.info(f"Flattened product description to {len(flattened_description)} characters")
                        elif not isinstance(description, str):
                            # Handle other non-string types
                            logger.warning(f"Product description is not a string: {type(description)}, converting to string")
                            block["product_description"] = str(description)
            
            # Log the recommended products for debugging
            logger.info(f"Payload recommended products: {payload.recommendedProducts}")
            # Extract product names from contentDevelopmentSpecifications for validation
            ai_generated_products = []
            for block in lesson_plan_data.get('contentDevelopmentSpecifications', []):
                if block.get('type') == 'product':
                    ai_generated_products.append(block.get('product_name'))
            logger.info(f"AI generated product types: {ai_generated_products}")
            
            # Create a mapping of common product name variations
            product_name_mapping = {
                'video-lesson': ['video-lesson', 'videoLesson', 'video_lesson', 'video lesson'],
                'presentation': ['presentation', 'presentations'],
                'quiz': ['quiz', 'quizzes'],
                'one-pager': ['one-pager', 'onePager', 'one_pager', 'one pager']
            }
            
            # Create reverse mapping for validation
            normalized_payload_products = set()
            for product in payload.recommendedProducts:
                # Find the canonical name for this product
                canonical_name = None
                for canonical, variations in product_name_mapping.items():
                    if product.lower() in [v.lower() for v in variations]:
                        canonical_name = canonical
                        break
                if canonical_name:
                    normalized_payload_products.add(canonical_name)
                else:
                    normalized_payload_products.add(product.lower())
            
            # Validate product blocks only contain products from the request
            for product_name in ai_generated_products:
                # Normalize the AI-generated product name
                canonical_name = None
                for canonical, variations in product_name_mapping.items():
                    if product_name.lower() in [v.lower() for v in variations]:
                        canonical_name = canonical
                        break
                
                if canonical_name:
                    if canonical_name not in normalized_payload_products:
                        logger.warning(f"AI generated product '{product_name}' (canonical: '{canonical_name}') not in normalized recommended products: {normalized_payload_products}")
                        raise ValueError(f"Product {product_name} not in recommended products list")
                else:
                    if product_name.lower() not in normalized_payload_products:
                        logger.warning(f"AI generated unknown product '{product_name}' not in recommended products: {payload.recommendedProducts}")
                        raise ValueError(f"Product {product_name} not in recommended products list")
                    
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse OpenAI response: {e}")
            logger.error(f"Raw AI response: {ai_response}")
            raise HTTPException(
                status_code=500,
                detail="Failed to generate valid lesson plan structure"
            )
        
        # Override AI materials with our extracted source materials
        logger.info(f"Extracted source materials: {source_materials}")
        lesson_plan_data['materials'] = source_materials
        logger.info(f"Final materials for lesson plan: {lesson_plan_data['materials']}")
        
        # Debug the prompts to see what the AI returned
        logger.info(f"AI returned {len(lesson_plan_data.get('suggestedPrompts', []))} prompts")
        for i, prompt in enumerate(lesson_plan_data.get('suggestedPrompts', [])):
            logger.info(f"Prompt {i+1}: {prompt[:100]}...")  # Log first 100 chars of each prompt
            
        # Ensure prompts are properly formatted with titles if they're not
        if 'suggestedPrompts' in lesson_plan_data:
            formatted_prompts = []
            for i, prompt in enumerate(lesson_plan_data['suggestedPrompts']):
                # Check if prompt already has a title format
                if not prompt.strip().startswith('**'):
                    # Try to infer the product type from the recommended products
                    if i < len(ai_generated_products):
                        product_name = ai_generated_products[i]
                        # Format the product name as a title
                        if 'video' in product_name.lower():
                            title = "Video Lesson Creation Prompt"
                        elif 'presentation' in product_name.lower():
                            title = "Presentation Creation Prompt"
                        elif 'quiz' in product_name.lower():
                            title = "Quiz Creation Prompt"
                        elif 'one-pager' in product_name.lower():
                            title = "One-Pager Creation Prompt"
                        else:
                            title = f"{product_name.replace('-', ' ').title()} Creation Prompt"
                        
                        formatted_prompt = f"**{title}:**\n{prompt}"
                        formatted_prompts.append(formatted_prompt)
                        logger.info(f"Formatted prompt {i+1} with title: {title}")
                    else:
                        formatted_prompts.append(prompt)
                else:
                    formatted_prompts.append(prompt)
            
            lesson_plan_data['suggestedPrompts'] = formatted_prompts
        
        # Debug the prompts to see what the AI returned
        logger.info(f"AI returned prompts: {lesson_plan_data.get('suggestedPrompts', [])}")
        
        # Ensure prompts are properly formatted with titles if they're not
        formatted_prompts = []
        for i, prompt in enumerate(lesson_plan_data.get('suggestedPrompts', [])):
            # Check if prompt already has a title format
            if not prompt.strip().startswith('**'):
                # Try to infer the product type from the recommended products
                if i < len(ai_generated_products):
                    product_name = ai_generated_products[i]
                    # Format the product name as a title
                    if 'video' in product_name.lower():
                        title = "Video Lesson Creation Prompt"
                    elif 'presentation' in product_name.lower():
                        title = "Presentation Creation Prompt"
                    elif 'quiz' in product_name.lower():
                        title = "Quiz Creation Prompt"
                    elif 'one-pager' in product_name.lower():
                        title = "One-Pager Creation Prompt"
                    else:
                        title = f"{product_name.title()} Creation Prompt"
                    
                    formatted_prompt = f"**{title}:**\n{prompt}"
                    formatted_prompts.append(formatted_prompt)
                else:
                    formatted_prompts.append(prompt)
            else:
                formatted_prompts.append(prompt)
        
        lesson_plan_data['suggestedPrompts'] = formatted_prompts
        logger.info(f"Formatted prompts: {formatted_prompts}")
        
        # Create the lesson plan project in database
        project_name = f"{outline_row['project_name']}: {payload.lessonTitle}"
        
        # Get a design template for lesson plans
        async with pool.acquire() as conn:
            template_row = await conn.fetchrow(
                "SELECT id FROM design_templates WHERE component_name = 'LessonPlanDisplay' LIMIT 1"
            )
            
            if not template_row:
                # Create a basic template if none exists
                template_id = await conn.fetchval(
                    """
                    INSERT INTO design_templates 
                    (template_name, component_name, microproduct_type, template_structuring_prompt)
                    VALUES ($1, $2, $3, $4) RETURNING id
                    """,
                    "Lesson Plan Template",
                    "LessonPlanDisplay",
                    "Lesson Plan",
                    "Generate a lesson plan with objectives, materials, and product recommendations."
                )
            else:
                template_id = template_row["id"]
        
        # Create project data
        project_data = ProjectCreateRequest(
            projectName=project_name,
            design_template_id=template_id,
            microProductName="Lesson Plan",
            aiResponse=json.dumps(lesson_plan_data),
            chatSessionId=None,
            outlineId=payload.outlineProjectId,
            folder_id=None,
            theme="default",
            source_context_type=source_context_type,
            source_context_data=source_context_data
        )
        
        # Add the project to database
        created_project = await add_project_to_custom_db(project_data, onyx_user_id, pool)
        
        # Update the project with lesson plan specific data
        async with pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE projects 
                SET product_type = 'lesson-plan',
                   lesson_plan_data = $1,
                   parent_outline_id = $2
                WHERE id = $3
                """,
                json.dumps(lesson_plan_data),
                payload.outlineProjectId,
                created_project.id
            )
        
        logger.info(f"Successfully created lesson plan project {created_project.id}")
        
        return LessonPlanResponse(
            success=True,
            project_id=created_project.id,
            lesson_plan_data=LessonPlanData(**lesson_plan_data),
            message="Lesson plan generated successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in lesson plan generation: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail="An unexpected error occurred during lesson plan generation"
        )

@app.delete("/api/custom/projects/{project_id}", status_code=204)
async def delete_project(
    project_id: int,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Delete a single project by ID.
    """
    try:
        async with pool.acquire() as conn:
            # Check if project exists and belongs to user
            project_row = await conn.fetchrow(
                "SELECT id FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
            if not project_row:
                raise HTTPException(
                    status_code=404,
                    detail="Project not found"
                )
            
            # Delete the project
            await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                project_id, onyx_user_id
            )
            
        logger.info(f"Successfully deleted project {project_id}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error deleting project {project_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred while deleting the project"
        )

@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# --- Restore trashed projects ---

@app.post("/api/custom/projects/restore-multiple", status_code=status.HTTP_200_OK)
async def restore_multiple_projects(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for restore."})

    ids_to_restore: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            # Expand scope to related lessons when requested
            if delete_request.scope == 'all':
                for pid in delete_request.project_ids:
                    row = await conn.fetchrow(
                        "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                        pid, onyx_user_id
                    )
                    if not row:
                        continue
                    pname: str = row["project_name"]
                    if row["microproduct_type"] not in ("Training Plan", "Course Outline"):
                        continue
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_restore.add(lr["id"])

            if not ids_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            # First, fetch all the data we need to restore
            projects_to_restore = await conn.fetch("""
                SELECT 
                    id, onyx_user_id, project_name, product_type, microproduct_type,
                    microproduct_name, microproduct_content, design_template_id, created_at,
                    source_chat_session_id, folder_id, "order", completion_time
                FROM trashed_projects 
                WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2
            """, list(ids_to_restore), onyx_user_id)

            if not projects_to_restore:
                return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": "No projects found to restore."})

            async with conn.transaction():
                # Process each project individually to handle data conversion safely
                for project in projects_to_restore:
                    # Safely convert order and completion_time to strings (never integers)
                    order_value = "0"
                    completion_time_value = "0"
                    
                    # Handle order field - always convert to string
                    if project['order'] is not None:
                        try:
                            if isinstance(project['order'], str):
                                if project['order'].strip() and project['order'].isdigit():
                                    order_value = project['order'].strip()
                                else:
                                    order_value = "0"
                            else:
                                # Convert any non-string value to string
                                order_value = str(project['order']) if project['order'] is not None else "0"
                        except (ValueError, TypeError):
                            order_value = "0"
                    
                    # Handle completion_time field - always convert to string
                    if project['completion_time'] is not None:
                        try:
                            if isinstance(project['completion_time'], str):
                                if project['completion_time'].strip() and project['completion_time'].isdigit():
                                    completion_time_value = project['completion_time'].strip()
                                else:
                                    completion_time_value = "0"
                            else:
                                # Convert any non-string value to string
                                completion_time_value = str(project['completion_time']) if project['completion_time'] is not None else "0"
                        except (ValueError, TypeError):
                            completion_time_value = "0"

                    # Insert into projects with safe values
                    await conn.execute("""
                        INSERT INTO projects (
                            id, onyx_user_id, project_name, product_type, microproduct_type,
                            microproduct_name, microproduct_content, design_template_id, created_at,
                            source_chat_session_id, folder_id, "order", completion_time
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    """,
                        project['id'], project['onyx_user_id'], project['project_name'],
                        project['product_type'], project['microproduct_type'], project['microproduct_name'],
                        project['microproduct_content'], project['design_template_id'], project['created_at'],
                        project['source_chat_session_id'], project['folder_id'], order_value, completion_time_value
                    )

                # Delete from trashed_projects table
                await conn.execute(
                    "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id = $2",
                    list(ids_to_restore), onyx_user_id
                )

        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully restored {len(ids_to_restore)} project(s)."})

    except Exception as e:
        logger.error(f"Error restoring projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while restoring projects." if IS_PRODUCTION else f"DB error while restoring projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


# --- Permanently delete trashed projects ---

@app.post("/api/custom/projects/delete-permanently", status_code=status.HTTP_200_OK)
async def delete_permanently(delete_request: ProjectsDeleteRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    if not delete_request.project_ids:
        return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"detail": "No project IDs provided for permanent deletion."})

    ids_to_delete: set[int] = set(delete_request.project_ids)

    try:
        async with pool.acquire() as conn:
            for pid in delete_request.project_ids:
                row = await conn.fetchrow(
                    "SELECT project_name, microproduct_type FROM trashed_projects WHERE id=$1 AND onyx_user_id=$2",
                    pid, onyx_user_id
                )
                if not row:
                    continue
                pname: str = row["project_name"]
                # If this is an outline, cascade to its lessons
                if row["microproduct_type"] in ("Training Plan", "Course Outline"):
                    pattern = pname + ":%"
                    lesson_rows = await conn.fetch(
                        "SELECT id FROM trashed_projects WHERE onyx_user_id=$1 AND (project_name=$2 OR project_name LIKE $3)",
                        onyx_user_id, pname, pattern
                    )
                    for lr in lesson_rows:
                        ids_to_delete.add(lr["id"])

            # Perform deletion of all collected ids
            result = await conn.execute(
                "DELETE FROM trashed_projects WHERE id = ANY($1::bigint[]) AND onyx_user_id=$2",
                list(ids_to_delete), onyx_user_id
            )

        deleted_count = int(result.split(" ")[1]) if result else 0
        return JSONResponse(status_code=status.HTTP_200_OK, content={"detail": f"Successfully deleted {deleted_count} project(s) permanently."})
    except Exception as e:
        logger.error(f"Error permanently deleting projects: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred during permanent deletion." if IS_PRODUCTION else f"DB error during permanent deletion: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)


@app.get("/api/custom/projects/trash", response_model=List[ProjectApiResponse])
async def get_user_trashed_projects(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Return projects that were moved to trash (soft-deleted)."""
    query = """
        SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
               dt.template_name as design_template_name,
               dt.microproduct_type as design_microproduct_type
        FROM trashed_projects p
        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
        WHERE p.onyx_user_id = $1 ORDER BY p.created_at DESC;
    """
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(query, onyx_user_id)
        resp: List[ProjectApiResponse] = []
        for row in rows:
            row_d = dict(row)
            resp.append(ProjectApiResponse(
                id=row_d["id"],
                projectName=row_d["project_name"],
                projectSlug=create_slug(row_d["project_name"]),
                microproduct_name=row_d.get("microproduct_name"),
                design_template_name=row_d.get("design_template_name"),
                design_microproduct_type=row_d.get("design_microproduct_type"),
                created_at=row_d["created_at"],
                design_template_id=row_d.get("design_template_id")
            ))
        return resp
    except Exception as e:
        logger.error(f"Error fetching trashed projects list: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while fetching trashed projects." if IS_PRODUCTION else f"DB error fetching trashed projects: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

# Add the new model for training plan editing
class TrainingPlanEditRequest(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: Optional[str] = None
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during edit
    # File context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs

@app.post("/api/custom/training-plan/edit")
async def edit_training_plan_with_prompt(payload: TrainingPlanEditRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Edit an existing training plan using AI prompt"""
    logger.info(f"[edit_training_plan_with_prompt] projectId={payload.projectId} prompt='{payload.prompt[:50]}...'")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    # Fast path variables - delay initialization until needed
    existing_content = row["microproduct_content"]

    # Stream the response
    async def streamer():
        # Fast path: request immediate JSON when there is no file context
        if should_use_openai_direct(payload):
            logger.info(f"[SMART_EDIT_STREAM] ✅ USING OPENAI DIRECT FREE TEXT (no file context)")
            try:
                client = get_openai_client()
                model = LLM_DEFAULT_MODEL
                
                # Use free text with forcing instructions like course outline preview
                original_json_str = existing_content
                logger.info(f"[SMART_EDIT_DEBUG] Original JSON length: {len(original_json_str)}")
                
                user_prompt = (
                    f"ORIGINAL JSON:\n{original_json_str}\n\n" +
                    f"EDIT INSTRUCTION (language={payload.language or 'en'}):\n{payload.prompt}\n\n" +
                    f"CRITICAL EDIT RULES - FOLLOW EXACTLY:\n" +
                    f"1. COPY the ORIGINAL JSON exactly as your starting point\n" +
                    f"2. PRESERVE every existing module/section and lesson unchanged\n" +
                    f"3. ONLY make the specific change requested in the instruction\n" +
                    f"4. If adding a module: append it to the existing sections array\n" +
                    f"5. If modifying content: change only that specific part\n" +
                    f"6. KEEP all existing IDs, titles, hours, sources, assessments\n" +
                    f"7. FORBIDDEN: Replacing entire course, removing existing modules\n" +
                    f"8. Result = Original course + minimal requested change\n\n" +
                    f"CONCRETE EXAMPLE:\n" +
                    f"Original: 4 modules about Project Management\n" +
                    f"Request: 'add 5th module about AI tools'\n" +
                    f"Correct result: ALL 4 original modules + 1 new AI module = 5 total\n" +
                    f"WRONG result: Only 1 module about AI (this deletes original content!)\n\n" +
                    f"CRITICAL OUTPUT FORMAT (JSON-ONLY):\n" +
                    f"Return the ORIGINAL JSON above with your minimal edits applied.\n" +
                    f"DO NOT use any example template - use the ORIGINAL JSON as your base.\n" +
                    f"NEVER create a generic 'Example Training Program' - use the actual course above.\n" +                    
                    f"Do NOT include code fences, markdown, or commentary. Return JSON object only."
                )
                
                messages = [
                    {"role": "system", "content": "You are ContentBuilder.ai assistant. You MUST make minimal edits to preserve existing content. NEVER replace entire training plans unless explicitly requested to do so. Follow the edit rules in the user message exactly."},
                    {"role": "user", "content": user_prompt}
                ]
                
                completion = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.1,  # Lower temperature for more deterministic edits
                    max_tokens=6000
                )
                content_text = completion.choices[0].message.content or "{}"
                
                # Debug: Log what the AI returned
                logger.info(f"[SMART_EDIT_DEBUG] AI returned content length: {len(content_text)}")
                logger.info(f"[SMART_EDIT_DEBUG] AI content preview: {content_text[:500]}...")
                
                # Parse JSON from free text response
                try:
                    # Try to extract JSON if wrapped in code fences
                    if "```json" in content_text:
                        start = content_text.find("```json") + 7
                        end = content_text.find("```", start)
                        if end != -1:
                            content_text = content_text[start:end].strip()
                    elif "```" in content_text:
                        start = content_text.find("```") + 3
                        end = content_text.find("```", start)
                        if end != -1:
                            content_text = content_text[start:end].strip()
                    
                    updated_content_dict = json.loads(content_text)
                except json.JSONDecodeError:
                    # Fallback: try to find JSON object boundaries
                    start = content_text.find("{")
                    end = content_text.rfind("}") + 1
                    if start >= 0 and end > start:
                        try:
                            updated_content_dict = json.loads(content_text[start:end])
                        except:
                            raise ValueError("Could not parse JSON from response")
                try:
                    if isinstance(existing_content, dict):
                        original_language = existing_content.get("detectedLanguage", payload.language)
                        original_theme = existing_content.get("theme", payload.theme or "cherry")
                    else:
                        original_language = payload.language or "en"
                        original_theme = payload.theme or "cherry"
                    updated_content_dict.setdefault("detectedLanguage", original_language)
                    updated_content_dict.setdefault("theme", original_theme)
                except Exception:
                    pass
                try:
                    import re
                    for section in updated_content_dict.get("sections", []):
                        # Normalize section IDs
                        sid = section.get("id")
                        if sid and isinstance(sid, str):
                            if sid.isdigit():
                                section["id"] = f"№{sid}"
                            elif sid.startswith("#") and sid[1:].isdigit():
                                section["id"] = f"№{sid[1:]}"
                            elif not sid.startswith("№"):
                                m = re.search(r"\d+", sid)
                                if m:
                                    section["id"] = f"№{m.group()}"
                        
                        # Clean lesson titles - remove prefixes like "Lesson 1.1:", "1.2:", etc.
                        lessons = section.get("lessons", [])
                        for lesson in lessons:
                            title = lesson.get("title", "")
                            if title and isinstance(title, str):
                                # Remove patterns like "Lesson 1.1:", "Lesson 1:", "1.1:", "1:", etc.
                                cleaned_title = re.sub(r'^(Lesson\s+)?\d+(\.\d+)?:\s*', '', title, flags=re.IGNORECASE)
                                if cleaned_title != title:
                                    lesson["title"] = cleaned_title
                                    logger.debug(f"[SMART_EDIT_LESSON_CLEANUP] '{title}' -> '{cleaned_title}'")
                except Exception as e:
                    logger.warning(f"[SMART_EDIT_NORMALIZATION] Normalization warning: {e}")
                done_packet = {"type": "done", "updatedContent": updated_content_dict, "isPreview": True}
                yield (json.dumps(done_packet) + "\n").encode()
                return
            except Exception as e:
                logger.error(f"[SMART_EDIT_JSON_ERROR] {e}")
                # fall through to legacy path if JSON fast path fails
        # Legacy path: initialize chat session and markdown outline for file context cases
        if not should_use_openai_direct(payload):
            # Get or create chat session
            if payload.chatSessionId:
                chat_id = payload.chatSessionId
            else:
                persona_id = await get_contentbuilder_persona_id(cookies)
                chat_id = await create_onyx_chat_session(persona_id, cookies)

            # Convert existing training plan to markdown format for AI processing
            current_outline = ""
            
            if existing_content:
                # Convert existing training plan to markdown format with full details
                content_data = existing_content
                if isinstance(content_data, dict):
                    main_title = content_data.get("mainTitle", "Training Plan")
                    current_outline = f"# {main_title}\n\n"
                    
                    sections = content_data.get("sections", [])
                    for section in sections:
                        section_id = section.get("id", "")
                        section_title = section.get("title", "")
                        total_hours = section.get("totalHours", 0.0)
                        # Get module quality tier information for preservation
                        section_quality_tier = section.get("quality_tier", "")
                        
                        # Convert special characters to safe ASCII for AI processing
                        # We'll convert back after AI response to preserve user-visible format
                        if section_id and section_title:
                            # Replace № with # for AI processing (encoding-safe)
                            safe_section_id = section_id.replace("№", "#")
                            if section_id != safe_section_id:
                                logger.info(f"[SMART_EDIT_ENCODING] Converted '{section_id}' to '{safe_section_id}' for AI processing")
                            # Check if section_id already contains "Module" keyword
                            if "Module" in safe_section_id or "Модуль" in safe_section_id:
                                current_outline += f"## {safe_section_id}: {section_title}\n"
                            else:
                                # For other formats (#1, mod1, etc.), preserve them exactly as they are
                                current_outline += f"## {safe_section_id}: {section_title}\n"
                        else:
                            # Fallback for empty IDs
                            current_outline += f"## {section_title}\n"
                        current_outline += f"**Total Hours:** {total_hours}\n"
                        if section_quality_tier:
                            current_outline += f"**Module Quality Tier:** {section_quality_tier}\n"
                        current_outline += "\n"
                        
                        lessons = section.get("lessons", [])
                        if lessons:
                            current_outline += "### Lessons:\n"
                            for idx, lesson in enumerate(lessons, 1):
                                lesson_title = lesson.get("title", "")
                                lesson_hours = lesson.get("hours", 1.0)
                                lesson_source = lesson.get("source", "Create from scratch")
                                
                                # Get check details
                                check = lesson.get("check", {})
                                check_type = check.get("type", "none")
                                check_text = check.get("text", "No")
                                
                                # Get content availability
                                content_available = lesson.get("contentAvailable", {})
                                content_type = content_available.get("type", "yes")
                                content_text = content_available.get("text", "100%")
                                
                                # Get quality tier information for preservation
                                lesson_quality_tier = lesson.get("quality_tier", "")
                                
                                current_outline += f"{idx}. **{lesson_title}**\n"
                                current_outline += f"   - Hours: {lesson_hours}\n"
                                current_outline += f"   - Source: {lesson_source}\n"
                                current_outline += f"   - Assessment: {check_type} ({check_text})\n"
                                current_outline += f"   - Content Available: {content_type} ({content_text})\n"
                                if lesson_quality_tier:
                                    current_outline += f"   - Quality Tier: {lesson_quality_tier}\n"
                                current_outline += "\n"
                        else:
                            current_outline += "*No lessons defined*\n\n"
                        current_outline += "\n"

            # Prepare wizard payload
            wiz_payload = {
                "product": "Training Plan Edit",
                "prompt": payload.prompt,
                "language": payload.language,
                "originalOutline": current_outline,
                "editMode": True
            }

            wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

            assistant_reply: str = ""
            last_send = asyncio.get_event_loop().time()

            # Use longer timeout for large text processing to prevent AI memory issues
            timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
            logger.info(f"Using timeout duration: {timeout_duration} seconds for AI processing")
            
            # Use Onyx when file context is present
            logger.info(f"[SMART_EDIT_STREAM] ❌ USING ONYX API (file context detected)")
            logger.info(f"[SMART_EDIT_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            
            try:
                async with httpx.AsyncClient(timeout=timeout_duration) as client:
                    # Parse folder and file IDs for Onyx
                    folder_ids_list = []
                    file_ids_list = []
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    send_payload = {
                        "chat_session_id": chat_id,
                        "message": wizard_message,
                        "parent_message_id": None,
                        "file_descriptors": [],
                        "user_file_ids": file_ids_list,
                        "user_folder_ids": folder_ids_list,
                        "prompt_id": None,
                        "search_doc_ids": None,
                        "retrieval_options": {"run_search": "never", "real_time": False},
                        "stream_response": True,
                    }
                    logger.info(f"[PREVIEW_ONYX] Sending request to Onyx /chat/send-message with payload: user_file_ids={file_ids_list}, user_folder_ids={folder_ids_list}")
                    async with client.stream("POST", f"{ONYX_API_SERVER_URL}/chat/send-message", json=send_payload, cookies=cookies) as resp:
                        logger.info(f"[PREVIEW_ONYX] Response status: {resp.status_code}")
                        async for raw_line in resp.aiter_lines():
                            if not raw_line:
                                continue
                            line = raw_line.strip()
                            if line.startswith("data:"):
                                line = line.split("data:", 1)[1].strip()
                            if line == "[DONE]":
                                logger.info("[PREVIEW_ONYX] Received [DONE] from Onyx stream")
                                break
                            try:
                                pkt = json.loads(line)
                                if "answer_piece" in pkt:
                                    delta_text = pkt["answer_piece"].replace("\\n", "\n")
                                    assistant_reply += delta_text
                                    logger.debug(f"[PREVIEW_ONYX] Received chunk: {delta_text[:80]}")
                                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                            except Exception as e:
                                logger.error(f"[PREVIEW_ONYX] Error parsing chunk: {e} | Raw: {line[:100]}")
                                continue

                            # send keep-alive every 8s
                            now = asyncio.get_event_loop().time()
                            if now - last_send > 8:
                                yield b" "
                                last_send = now
            except Exception as e:
                logger.error(f"[PREVIEW_ONYX] Exception in streaming: {e}")
                raise

        # Cache full raw outline for later finalize step
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[PREVIEW_CACHE] Cached preview for chat_id={chat_id}, length={len(assistant_reply)}")

        modules_preview = _parse_outline_markdown(assistant_reply)
        logger.info(f"[PREVIEW_DONE] Parsed modules: {len(modules_preview)}")

        # Convert back from safe ASCII characters to original special characters
        # Replace # back to № to restore original format for user display
        assistant_reply_restored = assistant_reply.replace("## #", "## №")
        if assistant_reply_restored != assistant_reply:
            logger.info(f"[SMART_EDIT_ENCODING] Restored special characters in AI response")
        
        # Update the cached version and the one used for parsing
        if chat_id:
            OUTLINE_PREVIEW_CACHE[chat_id] = assistant_reply_restored
        
        # Use the restored version for all subsequent processing
        assistant_reply = assistant_reply_restored
        
        # NEW: Parse AI response into structured TrainingPlanDetails but DON'T save to database yet
        # This is for preview - user will confirm before saving
        updated_content_dict: Optional[Dict[str, Any]] = None
        try:
            # Use the proper LLM parser to convert AI response to TrainingPlanDetails
            # Use the SAME parsing instructions as normal generation to ensure consistent ID handling
            component_specific_instructions = """
            You are an expert text-to-JSON parsing assistant for 'Training Plan' content.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into structured JSON that represents a multi-module training programme. Capture all information and hierarchical relationships. Preserve the original language for all textual fields.

            **Global Fields:**
            1.  `mainTitle` (string): Title of the whole programme. If the input lacks a clear title, use the project name given by the caller.
            2.  `sections` (array): Ordered list of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `quality_tier` (string, optional): Quality tier for this module. PRESERVE EXACTLY from source if mentioned as "Module Quality Tier: X". If not specified, omit this field entirely.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `quality_tier` (string, optional): Quality tier for this lesson. PRESERVE EXACTLY from source if mentioned as "Quality Tier: X". If not specified, omit this field entirely.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
            
            # Create a default TrainingPlanDetails instance for error handling
            # Preserve theme from existing content or use payload theme
            theme_to_use = "cherry"
            if existing_content and isinstance(existing_content, dict):
                theme_to_use = existing_content.get("theme", "cherry")
            else:
                theme_to_use = payload.theme or "cherry"
                
            default_training_plan = TrainingPlanDetails(
                mainTitle=row["project_name"],
                sections=[],
                detectedLanguage=detect_language(assistant_reply),
                theme=theme_to_use
            )
            
            # Example JSON structure for the LLM parser
            llm_json_example = json.dumps({
                "mainTitle": "Example Training Plan",
                "sections": [
                    {
                        "id": "№1",
                        "title": "Introduction to Topic",
                        "totalHours": 10,
                        "quality_tier": "premium",
                        "lessons": [
                            {
                                "title": "Lesson 1: Basics",
                                "hours": 2,
                                "source": "Create from scratch",
                                "completionTime": "5m",
                                "quality_tier": "interactive",
                                "check": {"type": "test", "text": "Test"},
                                "contentAvailable": {"type": "yes", "text": "100%"}
                            }
                        ],
                        "autoCalculateHours": True
                    }
                ],
                "detectedLanguage": "en",
                "theme": theme_to_use
            })
            
            logger.info(f"[SMART_EDIT_PARSER] Parsing AI response with length: {len(assistant_reply)}")
            logger.info(f"[SMART_EDIT_PARSER] AI response preview: {assistant_reply[:300]}{'...' if len(assistant_reply) > 300 else ''}")
            
            parsed_training_plan = await parse_ai_response_with_llm(
                ai_response=assistant_reply,
                project_name=row["project_name"],
                target_model=TrainingPlanDetails,
                default_error_model_instance=default_training_plan,
                dynamic_instructions=component_specific_instructions,
                target_json_example=llm_json_example
            )
            
            if parsed_training_plan:
                # Preserve the original language and theme
                if existing_content and isinstance(existing_content, dict):
                    # Preserve original language
                    original_language = existing_content.get("detectedLanguage", payload.language)
                    parsed_training_plan.detectedLanguage = original_language
                    logger.info(f"[SMART_EDIT_LANGUAGE] Preserved original language: {original_language}")
                    
                    # Preserve original theme
                    original_theme = existing_content.get("theme", "cherry")
                    parsed_training_plan.theme = original_theme
                    logger.info(f"[SMART_EDIT_THEME] Preserved original theme: {original_theme}")
                else:
                    # Use the language and theme from the request payload if available
                    parsed_training_plan.detectedLanguage = payload.language or "en"
                    parsed_training_plan.theme = payload.theme or "cherry"
                    logger.info(f"[SMART_EDIT_LANGUAGE] Using language from payload: {payload.language}")
                    logger.info(f"[SMART_EDIT_THEME] Using theme from payload: {payload.theme}")
                
                # Post-process module IDs to ensure № character is preserved
                for section in parsed_training_plan.sections:
                    if section.id:
                        # Fix module IDs that lost the № character
                        if section.id.isdigit():
                            # Plain number like "2" -> "№2"
                            section.id = f"№{section.id}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                        elif section.id.startswith("#"):
                            # Hash format like "#2" -> "№2"
                            number = section.id[1:]
                            section.id = f"№{number}"
                            logger.info(f"[SMART_EDIT_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                        elif not section.id.startswith("№"):
                            # Other formats without № - try to extract number and format correctly
                            import re
                            number_match = re.search(r'\d+', section.id)
                            if number_match:
                                number = number_match.group()
                                section.id = f"№{number}"
                                logger.info(f"[SMART_EDIT_ID_FIX] Fixed ID format to '{section.id}'")
                
                updated_content_dict = parsed_training_plan.model_dump(mode='json', exclude_none=True)
                
                logger.info(f"[SMART_EDIT_PREVIEW] Generated preview for training plan projectId={payload.projectId}")
            
        except Exception as e:
            logger.error(f"[SMART_EDIT_ERROR] Error parsing training plan: {e}")
            # Fall back to the preview-only mode if parsing fails
            updated_content_dict = None

        # Send completion packet with updatedContent for frontend preview
        # Note: This is now a PREVIEW - user must confirm to save to database
        if updated_content_dict:
            done_packet = {"type": "done", "updatedContent": updated_content_dict, "isPreview": True}
        else:
            # Fallback to old format if parsing failed
            done_packet = {"type": "done", "modules": modules_preview, "raw": assistant_reply}
        
        yield (json.dumps(done_packet) + "\n").encode()

    return StreamingResponse(streamer(), media_type="application/json")

class SmartEditConfirmRequest(BaseModel):
    projectId: int
    updatedContent: dict
    language: str = "en"
    theme: Optional[str] = "cherry"  # Theme to preserve during confirmation

@app.post("/api/custom/training-plan/confirm-edit")
async def confirm_training_plan_edit(payload: SmartEditConfirmRequest, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Confirm and save smart-edit changes to the database"""
    logger.info(f"[confirm_training_plan_edit] projectId={payload.projectId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Verify the project exists and belongs to the user
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")

    try:
        # Log the content structure for debugging
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content structure: {type(payload.updatedContent)}")
        logger.info(f"[SMART_EDIT_CONFIRM_CONTENT] Content keys: {list(payload.updatedContent.keys()) if isinstance(payload.updatedContent, dict) else 'Not a dict'}")
        
        # Validate & normalize JSON using TrainingPlanDetails
        try:
            validated: TrainingPlanDetails = TrainingPlanDetails.model_validate(payload.updatedContent)
        except Exception as ve:
            logger.error(f"[SMART_EDIT_CONFIRM_VALIDATION_ERROR] {ve}")
            raise HTTPException(status_code=400, detail="Invalid training plan JSON structure")

        if not validated.detectedLanguage:
            validated.detectedLanguage = payload.language or "en"
        if not validated.theme:
            validated.theme = payload.theme or "cherry"

        try:
            import re
            for section in validated.sections:
                # Normalize section IDs
                sid = section.id
                if sid:
                    if sid.isdigit():
                        section.id = f"№{sid}"
                    elif sid.startswith("#") and sid[1:].isdigit():
                        section.id = f"№{sid[1:]}"
                    elif not sid.startswith("№"):
                        m = re.search(r"\d+", sid)
                        if m:
                            section.id = f"№{m.group()}"
                
                # Clean lesson titles - remove prefixes like "Lesson 1.1:", "1.2:", etc.
                for lesson in section.lessons:
                    title = lesson.title
                    if title and isinstance(title, str):
                        # Remove patterns like "Lesson 1.1:", "Lesson 1:", "1.1:", "1:", etc.
                        cleaned_title = re.sub(r'^(Lesson\s+)?\d+(\.\d+)?:\s*', '', title, flags=re.IGNORECASE)
                        if cleaned_title != title:
                            lesson.title = cleaned_title
                            logger.debug(f"[SMART_EDIT_CONFIRM_LESSON_CLEANUP] '{title}' -> '{cleaned_title}'")
        except Exception as e:
            logger.warning(f"[SMART_EDIT_CONFIRM_NORMALIZATION] {e}")

        normalized_dict = validated.model_dump(mode='json', exclude_none=True)
        
        # Save the confirmed changes to the database
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE projects 
                SET microproduct_content = $1
                WHERE id = $2 AND onyx_user_id = $3
            """, normalized_dict, payload.projectId, onyx_user_id)
        
        logger.info(f"[SMART_EDIT_CONFIRMED] Successfully saved changes for training plan projectId={payload.projectId}")
        
        return {"success": True, "message": "Changes confirmed and saved successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SMART_EDIT_CONFIRM_ERROR] Error saving confirmed changes: {e}")
        raise HTTPException(status_code=500, detail="Failed to save changes")

# Add the finalize model for training plan editing
class TrainingPlanEditFinalize(BaseModel):
    prompt: str
    projectId: int
    chatSessionId: str
    editedOutline: Dict[str, Any]
    language: str = "en"

@app.post("/api/custom/training-plan/finalize")
async def finalize_training_plan_edit(payload: TrainingPlanEditFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize and apply the edited training plan"""
    logger.info(f"[finalize_training_plan_edit] projectId={payload.projectId} chatSessionId={payload.chatSessionId}")
    
    # Get current user
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get the cached preview
    cached_preview = OUTLINE_PREVIEW_CACHE.get(payload.chatSessionId)
    if not cached_preview:
        raise HTTPException(status_code=400, detail="No preview found for this session. Please regenerate the preview.")
    
    # Get the existing project data
    async with pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT p.*, dt.component_name 
            FROM projects p 
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id 
            WHERE p.id = $1 AND p.onyx_user_id = $2
        """, payload.projectId, onyx_user_id)
        
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        if row["component_name"] != COMPONENT_NAME_TRAINING_PLAN:
            raise HTTPException(status_code=400, detail="Project is not a training plan")
    
    # Parse the edited outline from the cached preview using LLM-based parsing
    try:
        # Create a default TrainingPlanDetails instance for error handling
        default_training_plan = TrainingPlanDetails(
            mainTitle=row["project_name"],
            sections=[],
            detectedLanguage=detect_language(cached_preview)
        )
        
        # Component-specific instructions for TrainingPlanDetails parsing
        component_specific_instructions = """
            Parse the training plan outline into a structured JSON format. Extract all modules and their lessons with complete details.

            **Main Object:**
            * `mainTitle` (string): The main title of the training plan.
            * `sections` (array): List of module objects.
            3.  `detectedLanguage` (string): 2-letter code such as "en", "ru", "uk", "es".

            **Section Object (`sections` array items):**
            * `id` (string): CRITICAL - Extract the exact module ID from the markdown headers. If you see "## №2: Title", extract "№2". If you see "## #2: Title", convert it to "№2". If you see "## Module 3: Title", convert it to "№3". Always preserve the original numbering but use "№X" format.
            * `title` (string): Module name without the word "Module".
            * `totalHours` (number): Sum of all lesson hours in this module, rounded to one decimal. If not present in the source, set to 0 and rely on `autoCalculateHours`.
            * `lessons` (array): List of lesson objects belonging to the module.
            * `autoCalculateHours` (boolean, default true): Leave as `true` unless the source explicitly provides `totalHours`.

            **Lesson Object (`lessons` array items):**
            * `title` (string): Lesson title WITHOUT leading numeration like "Lesson 1.1".
            * `hours` (number): Duration in hours. If absent, default to 1.
            * `source` (string): Where the learning material comes from (e.g., "Internal Documentation"). "Create from scratch" if unknown.
            * `completionTime` (string): Estimated completion time in minutes, randomly generated between 5-8 minutes. Format as "5m", "6m", "7m", or "8m". This should be randomly assigned for each lesson.
            * `check` (object):
                - `type` (string): One of "test", "quiz", "practice", "none".
                - `text` (string): Description of the assessment. Must be in the original language. If `type` is not "none" and the description is missing, use "No".
            * `contentAvailable` (object):
                - `type` (string): One of "yes", "no", "percentage".
                - `text` (string): Same information expressed as free text in original language. If not specified in the input, default to {"type": "yes", "text": "100%"}.

            **CRITICAL ID EXTRACTION RULES:**
            • When you see "## #2: Technical Setup", extract the ID as "№2" (convert # to №)
            • When you see "## №3: Advanced Topics", extract the ID as "№3" (preserve exactly)  
            • When you see "## Module 5: Data Analysis", extract the ID as "№5" (extract number and convert to № format)
            • NEVER generate sequential IDs like №1, №2, №3 - ALWAYS extract the actual number from the header
            • ALWAYS use the № character (U+2116) in module IDs, never just plain numbers like "2" or "3"
            • If you extract just a number like "2", format it as "№2"
            
            Return ONLY the JSON object.
            """
        
        # Example JSON structure for the LLM parser
        llm_json_example = json.dumps({
            "mainTitle": "Example Training Plan",
            "sections": [
                {
                    "id": "№1",
                    "title": "Introduction to Topic",
                    "totalHours": 10,
                    "lessons": [
                        {
                            "title": "Lesson 1: Basics",
                            "hours": 2,
                            "source": "Create from scratch",
                            "completionTime": "5m",
                            "check": {"type": "test", "text": "Test"},
                            "contentAvailable": {"type": "yes", "text": "100%"}
                        }
                    ],
                    "autoCalculateHours": True
                }
            ],
            "detectedLanguage": "en",
            "theme": "cherry"
        })
        
        # First, parse the outline to get auto-calculated totalHours
        parsed_orig = _parse_outline_markdown(cached_preview)
        logger.info(f"[FINALIZE] Parsed outline with {len(parsed_orig)} modules")
        
        # Create a mapping of module titles to auto-calculated totalHours
        auto_calculated_hours = {}
        for module in parsed_orig:
            title = module.get('title', '')
            total_hours = module.get('totalHours', 0.0)
            auto_calculated_hours[title] = total_hours
            logger.info(f"[FINALIZE] Auto-calculated hours for '{title}': {total_hours}")
        
        training_plan_details = await parse_ai_response_with_llm(
            ai_response=cached_preview,
            project_name=row["project_name"],
            target_model=TrainingPlanDetails,
            default_error_model_instance=default_training_plan,
            dynamic_instructions=component_specific_instructions,
            target_json_example=llm_json_example
        )
        
        if not training_plan_details:
            raise HTTPException(status_code=400, detail="Failed to parse the edited training plan")
        
        # Override LLM-calculated totalHours with auto-calculated values
        for section in training_plan_details.sections:
            section_title = section.title
            if section_title in auto_calculated_hours:
                original_hours = section.totalHours
                section.totalHours = auto_calculated_hours[section_title]
                logger.info(f"[FINALIZE] Overrode totalHours for '{section_title}': {original_hours} -> {section.totalHours}")
            else:
                logger.warning(f"[FINALIZE] No auto-calculated hours found for section: '{section_title}'")
        
        # Post-process module IDs to ensure № character is preserved
        for section in training_plan_details.sections:
            if section.id:
                # Fix module IDs that lost the № character
                if section.id.isdigit():
                    # Plain number like "2" -> "№2"
                    section.id = f"№{section.id}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed plain number ID '{section.id[1:]}' to '{section.id}'")
                elif section.id.startswith("#"):
                    # Hash format like "#2" -> "№2"
                    number = section.id[1:]
                    section.id = f"№{number}"
                    logger.info(f"[FINALIZE_ID_FIX] Fixed hash ID '#{number}' to '{section.id}'")
                elif not section.id.startswith("№"):
                    # Other formats without № - try to extract number and format correctly
                    import re
                    number_match = re.search(r'\d+', section.id)
                    if number_match:
                        number = number_match.group()
                        section.id = f"№{number}"
                        logger.info(f"[FINALIZE_ID_FIX] Fixed ID format to '{section.id}'")
        
        # Update the project with the new content
        await conn.execute("""
            UPDATE projects 
            SET microproduct_content = $1
            WHERE id = $2 AND onyx_user_id = $3
        """, json.dumps(training_plan_details.dict()), payload.projectId, onyx_user_id)
        
        logger.info(f"[FINALIZE_SUCCESS] Updated training plan projectId={payload.projectId}")
        
        return {"success": True, "message": "Training plan updated successfully"}
        
    except Exception as e:
        logger.error(f"[FINALIZE_ERROR] Error finalizing training plan: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to finalize training plan: {str(e)}")

# --- Folders API Models ---
class ProjectFolderCreateRequest(BaseModel):
    name: str
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None  # { presentation, one_pager, quiz, video_lesson }

class ProjectFolderResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

class ProjectFolderListResponse(BaseModel):
    id: int
    name: str
    created_at: datetime
    order: int
    parent_id: Optional[int] = None
    quality_tier: Optional[str] = "medium"  # Default to medium tier
    custom_rate: Optional[int] = 200  # Default to 200 custom rate
    is_advanced: Optional[bool] = False
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None
    project_count: int
    total_lessons: int
    total_hours: int
    total_completion_time: int
    model_config = {"from_attributes": True}

class ProjectFolderRenameRequest(BaseModel):
    name: str

class ProjectFolderMoveRequest(BaseModel):
    parent_id: Optional[int] = None

class ProjectFolderTierRequest(BaseModel):
    quality_tier: str
    custom_rate: int
    is_advanced: Optional[bool] = None
    advanced_rates: Optional[Dict[str, float]] = None
    completion_times: Optional[Dict[str, int]] = None

# --- Folders API Endpoints ---
@app.get("/api/custom/projects/folders", response_model=List[ProjectFolderListResponse])
async def list_folders(onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = """
        SELECT 
            pf.id, 
            pf.name, 
            pf.created_at, 
            pf."order", 
            pf.parent_id,
            COALESCE(pf.quality_tier, 'medium') as quality_tier,
            COALESCE(pf.custom_rate, 200) as custom_rate,
            pf.is_advanced as is_advanced,
            pf.advanced_rates as advanced_rates,
            pf.completion_times as completion_times,
            COUNT(p.id) as project_count,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COUNT(*)::int 
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_lessons,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'hours' IS NOT NULL AND lesson->>'hours' != '' 
                                    THEN (lesson->>'hours')::float
                                    ELSE 0 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_hours,
            COALESCE(
                SUM(
                    CASE 
                        WHEN p.microproduct_content IS NOT NULL 
                        AND p.microproduct_content->>'sections' IS NOT NULL 
                        THEN (
                            SELECT COALESCE(SUM(
                                CASE 
                                    WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                    THEN (
                                        -- Extract numeric part using regex, handling all language units (m, м, хв)
                                        CASE 
                                            WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                            THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                            ELSE 5
                                        END
                                    )
                                    ELSE 5 
                                END
                            ), 0)
                            FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        )
                        ELSE 0 
                    END
                ), 0
            ) as total_completion_time
        FROM project_folders pf
        LEFT JOIN projects p ON pf.id = p.folder_id
        WHERE pf.onyx_user_id = $1
        GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id, pf.is_advanced, pf.advanced_rates
        ORDER BY pf."order" ASC, pf.created_at ASC;
    """
    async with pool.acquire() as conn:
        rows = await conn.fetch(query, onyx_user_id)
    return [ProjectFolderListResponse(**dict(row)) for row in rows]

@app.get("/api/custom/projects/folders/{folder_id}", response_model=ProjectFolderResponse)
async def get_folder(folder_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Get a specific folder by ID"""
    query = """
        SELECT 
            pf.id, 
            pf.name, 
            pf.created_at, 
            pf.parent_id,
            COALESCE(pf.quality_tier, 'medium') as quality_tier,
            COALESCE(pf.custom_rate, 200) as custom_rate,
            pf.is_advanced as is_advanced,
            pf.advanced_rates as advanced_rates,
            pf.completion_times as completion_times
        FROM project_folders pf
        WHERE pf.id = $1 AND pf.onyx_user_id = $2
    """
    async with pool.acquire() as conn:
        row = await conn.fetchrow(query, folder_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Folder not found")
    return ProjectFolderResponse(**dict(row))

@app.post("/api/custom/projects/folders", response_model=ProjectFolderResponse)
async def create_folder(req: ProjectFolderCreateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    async with pool.acquire() as conn:
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
        
        query = "INSERT INTO project_folders (onyx_user_id, name, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates;"
        row = await conn.fetchrow(query, onyx_user_id, req.name, req.parent_id, req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None)
    return ProjectFolderResponse(**dict(row))

@app.patch("/api/custom/projects/folders/{folder_id}", response_model=ProjectFolderResponse)
async def rename_folder(folder_id: int, req: ProjectFolderRenameRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    query = "UPDATE project_folders SET name = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates;"
    async with pool.acquire() as conn:
        row = await conn.fetchrow(query, req.name, folder_id, onyx_user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Folder not found")
    return ProjectFolderResponse(**dict(row))

@app.delete("/api/custom/projects/folders/{folder_id}", status_code=204)
async def delete_folder(folder_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    # Set folder_id to NULL for all projects in this folder (preserve projects)
    async with pool.acquire() as conn:
        await conn.execute("UPDATE projects SET folder_id = NULL WHERE folder_id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
        result = await conn.execute("DELETE FROM project_folders WHERE id = $1 AND onyx_user_id = $2;", folder_id, onyx_user_id)
    if result == "DELETE 0":
        raise HTTPException(status_code=404, detail="Folder not found")
    return JSONResponse(status_code=204, content={})

@app.put("/api/custom/projects/folders/{folder_id}/move", response_model=ProjectFolderResponse)
async def move_folder(folder_id: int, req: ProjectFolderMoveRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Move a folder to a different parent folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # If parent_id is provided, verify it exists and belongs to user
        if req.parent_id is not None:
            parent_folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                req.parent_id, onyx_user_id
            )
            if not parent_folder:
                raise HTTPException(status_code=404, detail="Parent folder not found")
            
            # Prevent circular references - check if the target parent is a descendant of this folder
            if req.parent_id == folder_id:
                raise HTTPException(status_code=400, detail="Cannot move folder into itself")
            
            # Check for circular references by traversing up the tree
            current_parent_id = req.parent_id
            while current_parent_id is not None:
                if current_parent_id == folder_id:
                    raise HTTPException(status_code=400, detail="Cannot move folder into its own descendant")
                parent = await conn.fetchrow(
                    "SELECT parent_id FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                    current_parent_id, onyx_user_id
                )
                if not parent:
                    break
                current_parent_id = parent['parent_id']
        
        # Update the folder's parent_id
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET parent_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING id, name, created_at, parent_id",
            req.parent_id, folder_id, onyx_user_id
        )
        
        return ProjectFolderResponse(**dict(updated_folder))

@app.patch("/api/custom/projects/folders/{folder_id}/tier", response_model=ProjectFolderResponse)
async def update_folder_tier(folder_id: int, req: ProjectFolderTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier of a folder and recalculate creation hours for all projects in the folder"""
    async with pool.acquire() as conn:
        # Verify the folder exists and belongs to user
        folder = await conn.fetchrow(
            "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
            folder_id, onyx_user_id
        )
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the folder's quality_tier/custom_rate and advanced fields
        updated_folder = await conn.fetchrow(
            "UPDATE project_folders SET quality_tier = $1, custom_rate = $2, is_advanced = COALESCE($3, is_advanced), advanced_rates = COALESCE($4, advanced_rates), completion_times = COALESCE($5, completion_times) WHERE id = $6 AND onyx_user_id = $7 RETURNING id, name, created_at, parent_id, quality_tier, custom_rate, is_advanced, advanced_rates, completion_times",
            req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None, json.dumps(req.completion_times) if req.completion_times is not None else None, folder_id, onyx_user_id
        )
        
        # Get all projects in this folder (including subfolders recursively)
        projects_to_update = await conn.fetch("""
            WITH RECURSIVE folder_tree AS (
                -- Base case: the target folder
                SELECT id, parent_id FROM project_folders WHERE id = $1
                UNION ALL
                -- Recursive case: child folders
                SELECT pf.id, pf.parent_id 
                FROM project_folders pf
                INNER JOIN folder_tree ft ON pf.parent_id = ft.id
            )
            SELECT DISTINCT p.id, p.microproduct_content, p.folder_id
            FROM projects p
            INNER JOIN folder_tree ft ON p.folder_id = ft.id
            WHERE p.microproduct_content IS NOT NULL 
            AND p.microproduct_content->>'sections' IS NOT NULL
        """, folder_id)
        
        # Update creation hours for each project based on the new tier and custom rate
        for project in projects_to_update:
            try:
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    total_completion_time = 0
                    
                    # Calculate total completion time and update tier names and hours
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            # Clear any existing module-level tier settings to ensure folder-level tier takes precedence
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            
                            # Update the module's tier name to match the new folder tier
                            section['quality_tier'] = req.quality_tier
                                
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Clear any existing lesson-level tier settings to ensure folder-level tier takes precedence
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    
                                    # Update the tier name to match the new folder tier
                                    lesson['quality_tier'] = req.quality_tier

                                    # Always update recommendations when tier changes to ensure they match the new tier
                                    try:
                                        lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                                lesson.get('title', ''),
                                                req.quality_tier,
                                                {
                                                    'presentation': False,
                                                    'one-pager': False,
                                                    'quiz': False,
                                                    'video-lesson': False,
                                                }
                                            )
                                        # Also record a deterministic completion_breakdown and completionTime
                                        try:
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                            ranges = {
                                                'one-pager': (2,3),
                                                'presentation': (5,10),
                                                'quiz': (5,7),
                                                'video-lesson': (2,5),
                                            }
                                            breakdown = {}
                                            total_m = 0
                                            for p in primary:
                                                r = ranges.get(p)
                                                if r:
                                                    mid = int(round((r[0]+r[1])/2))
                                                    breakdown[p] = mid
                                                    total_m += mid
                                            if total_m > 0:
                                                lesson['completion_breakdown'] = breakdown
                                                lesson['completionTime'] = f"{total_m}m"
                                        except Exception:
                                            pass
                                    except Exception:
                                        pass
                                    
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to total completion time
                                    total_completion_time += completion_time_minutes
                                    
                                    # Recalculate hours considering advanced per-product rates if enabled
                                    try:
                                        primary = []
                                        if isinstance(lesson.get('recommended_content_types'), dict):
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                        is_adv = bool(updated_folder.get('is_advanced'))
                                        adv_rates = updated_folder.get('advanced_rates') if is_adv else None
                                        if is_adv and primary:
                                            breakdown = lesson.get('completion_breakdown') if isinstance(lesson.get('completion_breakdown'), dict) else None
                                            rates = {
                                                'presentation': (adv_rates or {}).get('presentation') or req.custom_rate,
                                                'one_pager': (adv_rates or {}).get('one_pager') or req.custom_rate,
                                                'quiz': (adv_rates or {}).get('quiz') or req.custom_rate,
                                                'video_lesson': (adv_rates or {}).get('video_lesson') or req.custom_rate,
                                            }
                                            total_hours = 0.0
                                            if breakdown:
                                                for p in primary:
                                                    key = 'one_pager' if p == 'one-pager' else ('video_lesson' if p == 'video-lesson' else p)
                                                    minutes = breakdown.get(p, 0)
                                                    total_hours += (minutes / 60.0) * float(rates.get(key, req.custom_rate))
                                            else:
                                                per = max(1, int(round(completion_time_minutes / max(1, len(primary)))))
                                                for p in primary:
                                                    key = 'one_pager' if p == 'one-pager' else ('video_lesson' if p == 'video-lesson' else p)
                                                    total_hours += (per / 60.0) * float(rates.get(key, req.custom_rate))
                                            lesson_creation_hours = int(round(total_hours))
                                        else:
                                            lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    except Exception:
                                        lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project['id']} creation hours: {e}")
                continue
        
        return ProjectFolderResponse(**dict(updated_folder))

# --- Update project queries to support folder_id (backward compatible) ---
# In all project list endpoints, add folder_id to SELECT and response models, and allow filtering by folder_id (optional)
# ... existing code ...

class ProjectFolderUpdateRequest(BaseModel):
    folder_id: Optional[int] = None
    model_config = {"from_attributes": True}

@app.put("/api/custom/projects/update/{project_id}", response_model=ProjectDB)
async def update_project_in_db(project_id: int, project_update_data: ProjectUpdateRequest, request: Request, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    logger.info(f"🔄 [PROJECT UPDATE START] ===========================================")
    logger.info(f"🔄 [PROJECT UPDATE START] Project ID: {project_id}")
    logger.info(f"🔄 [PROJECT UPDATE START] User ID: {onyx_user_id}")
    logger.info(f"🔄 [PROJECT UPDATE START] Timestamp: {datetime.now().isoformat()}")
    logger.info(f"🔄 [PROJECT UPDATE START] Request data type: {type(project_update_data)}")
    logger.info(f"🔄 [PROJECT UPDATE START] MicroProductName: {project_update_data.microProductName}")
    logger.info(f"🔄 [PROJECT UPDATE START] MicroProductContent type: {type(project_update_data.microProductContent)}")
    if project_update_data.microProductContent:
        logger.info(f"🔄 [PROJECT UPDATE START] MicroProductContent data: {project_update_data.microProductContent}")
        if hasattr(project_update_data.microProductContent, '__dict__'):
            logger.info(f"🔄 [PROJECT UPDATE START] MicroProductContent keys: {list(project_update_data.microProductContent.__dict__.keys())}")
    
    try:
        # Get user identifiers for workspace access
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        
        db_microproduct_name_to_store = project_update_data.microProductName
        current_component_name = None
        # Fetch current component_name, project_name and content to detect renames/diffs
        old_project_name: Optional[str] = None
        old_microproduct_content: Optional[dict] = None
        async with pool.acquire() as conn:
            # Check if user owns the project or has workspace access
            project_row = await conn.fetchrow("""
                SELECT p.project_name, p.microproduct_content, p.onyx_user_id, dt.component_name 
                FROM projects p 
                JOIN design_templates dt ON p.design_template_id = dt.id 
                WHERE p.id = $1 AND (
                    p.onyx_user_id = $2 
                    OR EXISTS (
                        SELECT 1 FROM product_access pa
                        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                        WHERE pa.product_id = p.id 
                          AND wm.user_id = $3 
                          AND wm.status = 'active'
                          AND pa.access_type IN ('workspace', 'role', 'individual')
                          AND (
                              pa.access_type = 'workspace' 
                              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                              OR (pa.access_type = 'individual' AND pa.target_id = $3)
                          )
                    )
                )
            """, project_id, user_uuid, user_email)
            if not project_row:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or not owned by user.")
            current_component_name = project_row["component_name"]
            project_owner_id = project_row["onyx_user_id"]
            old_project_name = project_row["project_name"]
            try:
                old_microproduct_content = dict(project_row["microproduct_content"]) if project_row["microproduct_content"] else None
            except Exception:
                old_microproduct_content = project_row["microproduct_content"] if isinstance(project_row["microproduct_content"], dict) else None

        if (not db_microproduct_name_to_store or not db_microproduct_name_to_store.strip()) and project_update_data.design_template_id:
            async with pool.acquire() as conn:
                design_row = await conn.fetchrow("SELECT template_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
                if design_row:
                    db_microproduct_name_to_store = design_row["template_name"]

        content_to_store_for_db = project_update_data.microProductContent if project_update_data.microProductContent else None
        
        # 🎯 CRITICAL INSTRUMENTATION: Table headers in received payload
        if content_to_store_for_db and isinstance(content_to_store_for_db, dict):
            logger.info(f"🎯 [TABLE HEADER BACKEND] ==========================================")
            logger.info(f"🎯 [TABLE HEADER BACKEND] Project {project_id} - Checking for courseOutlineTableHeaders in payload")
            logger.info(f"🎯 [TABLE HEADER BACKEND] Payload keys: {list(content_to_store_for_db.keys())}")
            logger.info(f"🎯 [TABLE HEADER BACKEND] Has courseOutlineTableHeaders: {'courseOutlineTableHeaders' in content_to_store_for_db}")
            
            if 'courseOutlineTableHeaders' in content_to_store_for_db:
                logger.info(f"🎯 [TABLE HEADER BACKEND] ✅ courseOutlineTableHeaders FOUND in payload!")
                logger.info(f"🎯 [TABLE HEADER BACKEND] Full data: {json.dumps(content_to_store_for_db['courseOutlineTableHeaders'], indent=2)}")
                logger.info(f"🎯 [TABLE HEADER BACKEND] - Lessons: '{content_to_store_for_db['courseOutlineTableHeaders'].get('lessons', 'NOT SET')}'")
                logger.info(f"🎯 [TABLE HEADER BACKEND] - Assessment: '{content_to_store_for_db['courseOutlineTableHeaders'].get('assessment', 'NOT SET')}'")
                logger.info(f"🎯 [TABLE HEADER BACKEND] - Duration: '{content_to_store_for_db['courseOutlineTableHeaders'].get('duration', 'NOT SET')}'")
                logger.info(f"🎯 [TABLE HEADER BACKEND] This data WILL BE stored in database")
            else:
                logger.info(f"🎯 [TABLE HEADER BACKEND] ❌ courseOutlineTableHeaders NOT FOUND in payload")
                logger.info(f"🎯 [TABLE HEADER BACKEND] Payload contains: {list(content_to_store_for_db.keys())}")
            logger.info(f"🎯 [TABLE HEADER BACKEND] ==========================================")
        
        # 🚨 CRITICAL: Validate that the data structure matches the component type
        if current_component_name == COMPONENT_NAME_TEXT_PRESENTATION and content_to_store_for_db:
            # Check if this is an AI audit landing page project
            is_ai_audit_project = (old_project_name and "AI-Аудит Landing Page" in old_project_name)
            
            if is_ai_audit_project:
                # For AI audit projects, ensure we're not receiving slide deck/text presentation data
                if isinstance(content_to_store_for_db, dict):
                    has_wrong_structure = ('sections' in content_to_store_for_db and 'theme' in content_to_store_for_db) and \
                                        not ('companyName' in content_to_store_for_db or 'jobPositions' in content_to_store_for_db)
                    
                    if has_wrong_structure:
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Received slide deck/text presentation data for AI audit project!")
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Rejecting save to prevent data corruption")
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Received data: {json.dumps(content_to_store_for_db, indent=2)}")
                        raise HTTPException(
                            status_code=400, 
                            detail=f"Invalid data structure for AI audit project. Expected AI audit data but received slide deck/text presentation data."
                        )
        
        # 🚨 CRITICAL: Validate that the data structure matches the component type
        if current_component_name == COMPONENT_NAME_TEXT_PRESENTATION and content_to_store_for_db:
            # Check if this is an AI audit landing page project
            is_ai_audit_project = (old_project_name and "AI-Аудит Landing Page" in old_project_name)
            
            if is_ai_audit_project:
                # For AI audit projects, ensure we're not receiving slide deck/text presentation data
                if isinstance(content_to_store_for_db, dict):
                    has_wrong_structure = ('sections' in content_to_store_for_db and 'theme' in content_to_store_for_db) and \
                                        not ('companyName' in content_to_store_for_db or 'jobPositions' in content_to_store_for_db)
                    
                    if has_wrong_structure:
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Received slide deck/text presentation data for AI audit project!")
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Rejecting save to prevent data corruption")
                        logger.error(f"❌ [CRITICAL ERROR] Project {project_id} - Received data: {json.dumps(content_to_store_for_db, indent=2)}")
                        raise HTTPException(
                            status_code=400, 
                            detail=f"Invalid data structure for AI audit project. Expected AI audit data but received slide deck/text presentation data."
                        )
        
        # 🔍 BACKEND SAVE LOGGING: What we're about to store in database
        if content_to_store_for_db:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Storing content to DB: {json.dumps(content_to_store_for_db, indent=2)}")
            if 'contentBlocks' in content_to_store_for_db:
                image_blocks = [block for block in content_to_store_for_db['contentBlocks'] if block.get('type') == 'image']
                logger.info(f"💾 [BACKEND SAVE] Project {project_id} - Image blocks to store: {json.dumps(image_blocks, indent=2)}")
        else:
            logger.info(f"💾 [BACKEND SAVE] Project {project_id} - No content to store (content_to_store_for_db is None)")

        derived_product_type = None; derived_microproduct_type = None
        if project_update_data.design_template_id is not None:
            async with pool.acquire() as conn:
                design_template = await conn.fetchrow("SELECT microproduct_type, template_name, component_name FROM design_templates WHERE id = $1", project_update_data.design_template_id)
                if design_template:
                    derived_product_type = design_template["microproduct_type"]
                    derived_microproduct_type = design_template["template_name"]
                    current_component_name = design_template["component_name"]

        update_clauses = []; update_values = []; arg_idx = 1
        
        # Handle project name updates and sync with Training Plan mainTitle
        project_name_updated = False
        if project_update_data.projectName is not None: 
            update_clauses.append(f"project_name = ${arg_idx}")
            update_values.append(project_update_data.projectName)
            arg_idx += 1
            project_name_updated = True
        if db_microproduct_name_to_store is not None: update_clauses.append(f"microproduct_name = ${arg_idx}"); update_values.append(db_microproduct_name_to_store); arg_idx +=1
        if project_update_data.design_template_id is not None:
            update_clauses.append(f"design_template_id = ${arg_idx}"); update_values.append(project_update_data.design_template_id); arg_idx +=1
            if derived_product_type: update_clauses.append(f"product_type = ${arg_idx}"); update_values.append(derived_product_type); arg_idx += 1
            if derived_microproduct_type: update_clauses.append(f"microproduct_type = ${arg_idx}"); update_values.append(derived_microproduct_type); arg_idx += 1
        if project_update_data.microProductContent is not None: 
            update_clauses.append(f"microproduct_content = ${arg_idx}")
            update_values.append(content_to_store_for_db); arg_idx += 1
            
            # Ensure lessons have recommendations when storing training plan
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN and content_to_store_for_db:
                try:
                    for section in (content_to_store_for_db.get('sections') or []):
                        lessons = section.get('lessons') or []
                        for lesson in lessons:
                            if isinstance(lesson, dict) and ('recommended_content_types' not in lesson or not lesson['recommended_content_types']):
                                lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                    lesson.get('title', ''),
                                    lesson.get('quality_tier') or section.get('quality_tier') or content_to_store_for_db.get('quality_tier'),
                                    {'presentation': False, 'one-pager': False, 'quiz': False, 'video-lesson': False}
                                )
                                # Also generate completion_breakdown for advanced mode support
                                try:
                                    primary = lesson['recommended_content_types'].get('primary', [])
                                    ranges = {
                                        'one-pager': (2,3),
                                        'presentation': (5,10),
                                        'quiz': (5,7),
                                        'video-lesson': (2,5),
                                    }
                                    breakdown = {}
                                    total_m = 0
                                    for p in primary:
                                        r = ranges.get(p)
                                        if r:
                                            mid = int(round((r[0]+r[1])/2))
                                            breakdown[p] = mid
                                            total_m += mid
                                    if total_m > 0:
                                        lesson['completion_breakdown'] = breakdown
                                        lesson['completionTime'] = f"{total_m}m"
                                except Exception:
                                    pass
                except Exception:
                    pass
            
            # SYNC TITLES: For Training Plans, keep project_name and mainTitle synchronized
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN and content_to_store_for_db:
                try:
                    # Extract mainTitle from the content
                    main_title = content_to_store_for_db.get('mainTitle')
                    if main_title and isinstance(main_title, str) and main_title.strip():
                        # Update project_name to match mainTitle
                        update_clauses.append(f"project_name = ${arg_idx}")
                        update_values.append(main_title.strip())
                        arg_idx += 1
                        project_name_updated = True
                except Exception as e:
                    logger.warning(f"Could not sync mainTitle to project_name for project {project_id}: {e}")

        # SYNC TITLES: If only project_name was updated (not content), sync it to mainTitle for Training Plans
        if (project_name_updated and project_update_data.microProductContent is None and 
            current_component_name == COMPONENT_NAME_TRAINING_PLAN):
            try:
                # Get current content to update mainTitle
                async with pool.acquire() as conn:
                    current_row = await conn.fetchrow(
                        "SELECT microproduct_content FROM projects WHERE id = $1 AND onyx_user_id = $2", 
                        project_id, onyx_user_id
                    )
                    if current_row and current_row["microproduct_content"]:
                        current_content = dict(current_row["microproduct_content"])
                        current_content["mainTitle"] = project_update_data.projectName
                        update_clauses.append(f"microproduct_content = ${arg_idx}")
                        update_values.append(current_content)
                        arg_idx += 1
            except Exception as e:
                logger.warning(f"Could not sync project_name to mainTitle for project {project_id}: {e}")

        if not update_clauses:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No update data provided.")

        update_values.extend([project_id])
        update_query = f"UPDATE projects SET {', '.join(update_clauses)} WHERE id = ${arg_idx} RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name, microproduct_content, design_template_id, created_at, custom_rate, quality_tier, is_advanced, advanced_rates;"

        logger.info(f"💾 [DATABASE TRANSACTION START] ===========================================")
        logger.info(f"💾 [DATABASE TRANSACTION START] Project ID: {project_id}")
        logger.info(f"💾 [DATABASE TRANSACTION START] User ID: {onyx_user_id}")
        logger.info(f"💾 [DATABASE TRANSACTION START] Query type: UPDATE")
        logger.info(f"💾 [DATABASE TRANSACTION START] Table: projects")
        logger.info(f"💾 [DATABASE TRANSACTION START] Update clauses: {update_clauses}")
        logger.info(f"💾 [DATABASE TRANSACTION START] Update values: {update_values}")
        logger.info(f"💾 [DATABASE TRANSACTION START] Full query: {update_query}")
        logger.info(f"💾 [DATABASE TRANSACTION START] Timestamp: {datetime.now().isoformat()}")

        async with pool.acquire() as conn: 
            row = await conn.fetchrow(update_query, *update_values)
            
            logger.info(f"💾 [DATABASE TRANSACTION RESULT] ===========================================")
            logger.info(f"💾 [DATABASE TRANSACTION RESULT] Query executed successfully")
            logger.info(f"💾 [DATABASE TRANSACTION RESULT] Rows affected: {row is not None}")
            logger.info(f"💾 [DATABASE TRANSACTION RESULT] Returned data: {dict(row) if row else 'None'}")
            logger.info(f"💾 [DATABASE TRANSACTION RESULT] Timestamp: {datetime.now().isoformat()}")
            
            # 🎯 CRITICAL INSTRUMENTATION: Verify table headers were saved to database
            if row and row['microproduct_content']:
                saved_content = dict(row['microproduct_content'])
                logger.info(f"🎯 [TABLE HEADER DB VERIFY] ==========================================")
                logger.info(f"🎯 [TABLE HEADER DB VERIFY] Project {project_id} - Verifying saved content")
                logger.info(f"🎯 [TABLE HEADER DB VERIFY] Saved content keys: {list(saved_content.keys())}")
                logger.info(f"🎯 [TABLE HEADER DB VERIFY] Has courseOutlineTableHeaders: {'courseOutlineTableHeaders' in saved_content}")
                
                if 'courseOutlineTableHeaders' in saved_content:
                    logger.info(f"🎯 [TABLE HEADER DB VERIFY] ✅ courseOutlineTableHeaders CONFIRMED SAVED to database!")
                    logger.info(f"🎯 [TABLE HEADER DB VERIFY] Saved data: {json.dumps(saved_content['courseOutlineTableHeaders'], indent=2)}")
                    logger.info(f"🎯 [TABLE HEADER DB VERIFY] - Lessons: '{saved_content['courseOutlineTableHeaders'].get('lessons', 'NOT SET')}'")
                    logger.info(f"🎯 [TABLE HEADER DB VERIFY] - Assessment: '{saved_content['courseOutlineTableHeaders'].get('assessment', 'NOT SET')}'")
                    logger.info(f"🎯 [TABLE HEADER DB VERIFY] - Duration: '{saved_content['courseOutlineTableHeaders'].get('duration', 'NOT SET')}'")
                else:
                    logger.warning(f"🎯 [TABLE HEADER DB VERIFY] ⚠️ courseOutlineTableHeaders NOT FOUND in saved database content!")
                    logger.warning(f"🎯 [TABLE HEADER DB VERIFY] This means the data was NOT persisted to database")
                    logger.warning(f"🎯 [TABLE HEADER DB VERIFY] Saved content contains: {list(saved_content.keys())}")
                logger.info(f"🎯 [TABLE HEADER DB VERIFY] ==========================================")
            
        if not row:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Project not found or update failed.")

        # --- Propagate outline/lesson renames to connected products (best-effort) ---
        try:
            # Only for Training Plans
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                new_project_name = row["project_name"]
                # 1) If outline name changed, update prefix for all connected products
                if project_name_updated and old_project_name and new_project_name and old_project_name.strip() != new_project_name.strip():
                    old_prefix = f"{old_project_name.strip()}: "
                    new_prefix = f"{new_project_name.strip()}: "
                    async with pool.acquire() as conn:
                        children = await conn.fetch(
                            "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND is_standalone = FALSE AND project_name LIKE $2",
                            project_owner_id, old_prefix + "%"
                        )
                        for child in children:
                            child_id = child["id"]
                            child_pn = child["project_name"] or ""
                            child_mpname = child["microproduct_name"]
                            # Compute new names
                            if ": " in child_pn:
                                suffix = child_pn.split(": ", 1)[1]
                                updated_project_name = new_prefix + suffix
                            else:
                                updated_project_name = child_pn  # unexpected, skip
                            # Update microproduct_name if it matches the old full or is None
                            updated_micro_name = child_mpname
                            if isinstance(child_mpname, str):
                                if child_mpname == child_pn:
                                    updated_micro_name = updated_project_name
                            elif child_mpname is None:
                                updated_micro_name = updated_project_name
                            await conn.execute(
                                "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name) WHERE id = $3 AND onyx_user_id = $4",
                                updated_project_name, updated_micro_name, child_id, project_owner_id
                            )
                
                # 2) If lesson titles changed inside outline content, rename exact-matching children
                # Compute simple diff by position (module, lesson index)
                def extract_titles(content: Optional[dict]) -> list[list[str]]:
                    titles: list[list[str]] = []
                    if not content or not isinstance(content, dict):
                        return titles
                    sections = content.get("sections") or []
                    for sec in sections:
                        sec_titles: list[str] = []
                        lessons = sec.get("lessons") if isinstance(sec, dict) else []
                        for les in lessons:
                            if isinstance(les, dict):
                                title = str(les.get("title") or les.get("name") or "").strip()
                            else:
                                title = str(les).strip()
                            sec_titles.append(title)
                        titles.append(sec_titles)
                    return titles
                old_titles_by_section = extract_titles(old_microproduct_content)
                new_titles_by_section = extract_titles(content_to_store_for_db if project_update_data.microProductContent is not None else old_microproduct_content)
                rename_pairs: list[tuple[str, str]] = []
                if old_titles_by_section and new_titles_by_section and len(old_titles_by_section) == len(new_titles_by_section):
                    for sec_idx in range(len(old_titles_by_section)):
                        old_ls = old_titles_by_section[sec_idx]
                        new_ls = new_titles_by_section[sec_idx]
                        for li in range(min(len(old_ls), len(new_ls))):
                            old_t = (old_ls[li] or "").strip()
                            new_t = (new_ls[li] or "").strip()
                            if old_t and new_t and old_t != new_t:
                                rename_pairs.append((old_t, new_t))
                if rename_pairs:
                    async with pool.acquire() as conn:
                        for (old_title, new_title) in rename_pairs:
                            old_full = f"{(row['project_name'] or new_project_name).strip()}: {old_title}"
                            new_full = f"{(row['project_name'] or new_project_name).strip()}: {new_title}"
                            children = await conn.fetch(
                                "SELECT id, project_name, microproduct_name, microproduct_content FROM projects WHERE onyx_user_id = $1 AND project_name = $2",
                                project_owner_id, old_full
                            )
                            for child in children:
                                child_id = child["id"]
                                child_mpname = child["microproduct_name"]
                                child_content = child["microproduct_content"]
                                # Update microproduct_name smartly: replace exact matches or lesson-only
                                updated_micro_name = child_mpname
                                if isinstance(child_mpname, str):
                                    if child_mpname == old_full:
                                        updated_micro_name = new_full
                                    elif child_mpname == old_title:
                                        updated_micro_name = new_title
                                elif child_mpname is None:
                                    updated_micro_name = new_full
                                # Update content titles if present
                                updated_content = child_content
                                try:
                                    if isinstance(child_content, dict):
                                        # Quiz
                                        if 'quizTitle' in child_content and isinstance(child_content['quizTitle'], str):
                                            if child_content['quizTitle'].strip() in (old_title, old_full):
                                                child_content['quizTitle'] = new_title
                                        # Text Presentation
                                        if 'textTitle' in child_content and isinstance(child_content['textTitle'], str):
                                            if child_content['textTitle'].strip() in (old_title, old_full):
                                                child_content['textTitle'] = new_title
                                        updated_content = child_content
                                except Exception as e:
                                    logger.warning(f"[RENAME_PROPAGATION] Failed to update content titles for child {child_id}: {e}")
                                await conn.execute(
                                    "UPDATE projects SET project_name = $1, microproduct_name = COALESCE($2, microproduct_name), microproduct_content = COALESCE($3, microproduct_content) WHERE id = $4 AND onyx_user_id = $5",
                                    new_full, updated_micro_name, updated_content, child_id, project_owner_id
                                )
        except Exception as e:
            logger.error(f"[RENAME_PROPAGATION] Error during rename propagation for project {project_id}: {e}", exc_info=not IS_PRODUCTION)

        db_content = row["microproduct_content"]
        
        # 🔍 BACKEND RETRIEVE LOGGING: What we got back from database
        logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Retrieved content from DB: {json.dumps(db_content, indent=2) if db_content else 'None'}")
        if db_content and isinstance(db_content, dict) and 'contentBlocks' in db_content:
            image_blocks = [block for block in db_content['contentBlocks'] if block.get('type') == 'image']
            logger.info(f"📥 [BACKEND RETRIEVE] Project {project_id} - Image blocks retrieved: {json.dumps(image_blocks, indent=2)}")
        
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - About to validate with component: {current_component_name}")
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    # Check if this is an AI audit landing page project
                    if (old_project_name and "AI-Аудит Landing Page" in old_project_name) or \
                       (db_content and 'companyName' in db_content and 'jobPositions' in db_content):
                        logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - Validating as AIAuditLandingDetails")
                        final_content_for_model = AIAuditLandingDetails(**db_content)
                        logger.info(f"✅ [BACKEND VALIDATION] Project {project_id} - AIAuditLandingDetails validation successful")
                    else:
                        logger.info(f"🔧 [BACKEND VALIDATION] Project {project_id} - Validating as TextPresentationDetails")
                        final_content_for_model = TextPresentationDetails(**db_content)
                        logger.info(f"✅ [BACKEND VALIDATION] Project {project_id} - TextPresentationDetails validation successful")
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    # Apply slide normalization before parsing
                    if 'slides' in db_content and db_content['slides']:
                        db_content['slides'] = await normalize_slide_props(db_content['slides'], current_component_name)
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                
                # 🔍 BACKEND VALIDATION RESULT LOGGING
                if final_content_for_model and hasattr(final_content_for_model, 'contentBlocks'):
                    result_dict = final_content_for_model.model_dump(mode='json', exclude_none=True)
                    logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final validated content: {json.dumps(result_dict, indent=2)}")
                    if 'contentBlocks' in result_dict:
                        result_image_blocks = [block for block in result_dict['contentBlocks'] if block.get('type') == 'image']
                        logger.info(f"✅ [BACKEND VALIDATION RESULT] Project {project_id} - Final image blocks: {json.dumps(result_image_blocks, indent=2)}")
                
            except Exception as e_parse:
                logger.error(f"❌ [BACKEND VALIDATION ERROR] Project {project_id} - Error parsing updated content from DB: {e_parse}", exc_info=not IS_PRODUCTION)

        response_data = ProjectDB(
            id=row["id"], onyx_user_id=row["onyx_user_id"], project_name=row["project_name"],
            product_type=row["product_type"], microproduct_type=row["microproduct_type"],
            microproduct_name=row["microproduct_name"], microproduct_content=final_content_for_model,
            design_template_id=row["design_template_id"], created_at=row["created_at"],
            custom_rate=row["custom_rate"], quality_tier=row["quality_tier"]
        )
        
        logger.info(f"📤 [API RESPONSE] ===========================================")
        logger.info(f"📤 [API RESPONSE] Project ID: {project_id}")
        logger.info(f"📤 [API RESPONSE] Response status: 200 OK")
        logger.info(f"📤 [API RESPONSE] Response data: {response_data}")
        logger.info(f"📤 [API RESPONSE] Timestamp: {datetime.now().isoformat()}")
        
        return response_data
    except HTTPException as http_e:
        logger.error(f"❌ [API ERROR] HTTP Exception for project {project_id}: {http_e.detail}")
        logger.error(f"❌ [API ERROR] Status code: {http_e.status_code}")
        logger.error(f"❌ [API ERROR] Timestamp: {datetime.now().isoformat()}")
        raise
    except Exception as e:
        logger.error(f"❌ [API ERROR] ===========================================")
        logger.error(f"❌ [API ERROR] Project ID: {project_id}")
        logger.error(f"❌ [API ERROR] Error type: {type(e).__name__}")
        logger.error(f"❌ [API ERROR] Error message: {str(e)}")
        logger.error(f"❌ [API ERROR] Error details: {e}", exc_info=not IS_PRODUCTION)
        logger.error(f"❌ [API ERROR] Timestamp: {datetime.now().isoformat()}")
        detail_msg = "An error occurred while updating project." if IS_PRODUCTION else f"DB error on project update: {str(e)}"
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@app.put("/api/custom/projects/{project_id}/folder", response_model=ProjectDB)
async def update_project_folder(project_id: int, update_data: ProjectFolderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update a project's folder assignment and recalculate creation hours based on the new folder's tier"""
    async with pool.acquire() as conn:
        # Verify project belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # If folder_id is provided, verify it exists and belongs to user
        if update_data.folder_id is not None:
            folder = await conn.fetchrow(
                "SELECT * FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                update_data.folder_id, onyx_user_id
            )
            if not folder:
                raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update the project's folder_id
        updated_project = await conn.fetchrow(
            "UPDATE projects SET folder_id = $1 WHERE id = $2 AND onyx_user_id = $3 RETURNING *",
            update_data.folder_id, project_id, onyx_user_id
        )
        
        # If the project has content and is being moved to a folder, recalculate creation hours
        if update_data.folder_id is not None and project['microproduct_content']:
            try:
                # Get the folder's custom rate
                folder_custom_rate = await get_folder_custom_rate(update_data.folder_id, pool)
                
                content = project['microproduct_content']
                if isinstance(content, dict) and 'sections' in content:
                    content = sanitize_training_plan_for_parse(dict(content))
                    sections = content['sections']
                    
                    # Update the hours in each lesson and recalculate section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    # Parse completion time - treat missing as 5 minutes
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Calculate hours using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with tier-adjusted sum
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Round all hours in the content to ensure they are integers
                    content = round_hours_in_content(content)
                    
                    # Update the project in the database with new hours
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project_id
                    )
                    
                    # Update the returned project data
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        project_id, onyx_user_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours after folder move: {e}")
        
        # Parse the content properly based on component type
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        
        if db_content and isinstance(db_content, dict):
            try:
                # Get the component name to determine the content type
                component_row = await conn.fetchrow(
                    "SELECT dt.component_name FROM projects p JOIN design_templates dt ON p.design_template_id = dt.id WHERE p.id = $1",
                    project_id
                )
                current_component_name = component_row["component_name"] if component_row else COMPONENT_NAME_TRAINING_PLAN
                
                # Round hours to integers before parsing to prevent float validation errors
                if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = round_hours_in_content(db_content)
                
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    db_content = sanitize_training_plan_for_parse(db_content)
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"],
            custom_rate=updated_project.get("custom_rate"),
            quality_tier=updated_project.get("quality_tier"),
            is_advanced=updated_project.get("is_advanced"),
            advanced_rates=updated_project.get("advanced_rates")
        )

@app.patch("/api/custom/projects/{project_id}/tier", response_model=ProjectDB)
async def update_project_tier(project_id: int, req: ProjectTierRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the quality tier, custom rate, and advanced rates of a project and recalculate creation hours"""
    async with pool.acquire() as conn:
        # Verify the project exists and belongs to user
        project = await conn.fetchrow(
            "SELECT * FROM projects WHERE id = $1 AND onyx_user_id = $2",
            project_id, onyx_user_id
        )
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # Update the project's quality_tier, custom_rate, and advanced fields
        updated_project = await conn.fetchrow(
            "UPDATE projects SET quality_tier = $1, custom_rate = $2, is_advanced = COALESCE($3, is_advanced), advanced_rates = COALESCE($4, advanced_rates), completion_times = COALESCE($5, completion_times) WHERE id = $6 AND onyx_user_id = $7 RETURNING *",
            req.quality_tier, req.custom_rate, req.is_advanced, json.dumps(req.advanced_rates) if req.advanced_rates is not None else None, json.dumps(req.completion_times) if req.completion_times is not None else None, project_id, onyx_user_id
        )
        
        # If the project has content, recalculate creation hours
        if project['microproduct_content']:
            try:
                content = dict(project['microproduct_content'])
                if isinstance(content, dict) and 'sections' in content:
                    sections = content['sections']
                    
                    # Update tier names, update recommendations, and sum existing hours for section totals
                    for section in sections:
                        if isinstance(section, dict) and 'lessons' in section:
                            if 'custom_rate' in section:
                                del section['custom_rate']
                            if 'quality_tier' in section:
                                del section['quality_tier']
                            section['quality_tier'] = req.quality_tier
                            
                            section_total_hours = 0
                            for lesson in section['lessons']:
                                if isinstance(lesson, dict):
                                    if 'custom_rate' in lesson:
                                        del lesson['custom_rate']
                                    if 'quality_tier' in lesson:
                                        del lesson['quality_tier']
                                    lesson['quality_tier'] = req.quality_tier

                                    try:
                                        # Always update recommendations when tier changes to ensure they match the new tier
                                        lesson['recommended_content_types'] = analyze_lesson_content_recommendations(
                                            lesson.get('title', ''),
                                            req.quality_tier,
                                            {
                                                'presentation': False,
                                                'one-pager': False,
                                                'quiz': False,
                                                'video-lesson': False,
                                            }
                                        )
                                        # Also generate completion_breakdown for advanced mode support
                                        try:
                                            primary = lesson['recommended_content_types'].get('primary', [])
                                            ranges = {
                                                'one-pager': (2,3),
                                                'presentation': (5,10),
                                                'quiz': (5,7),
                                                'video-lesson': (2,5),
                                            }
                                            breakdown = {}
                                            total_m = 0
                                            for p in primary:
                                                r = ranges.get(p)
                                                if r:
                                                    mid = int(round((r[0]+r[1])/2))
                                                    breakdown[p] = mid
                                                    total_m += mid
                                            if total_m > 0:
                                                lesson['completion_breakdown'] = breakdown
                                                lesson['completionTime'] = f"{total_m}m"
                                        except Exception:
                                            pass
                                    except Exception:
                                        pass
                                    
                                    completion_time_str = lesson.get('completionTime', '')
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            if time_str.endswith('m'):
                                                try:
                                                    completion_time_minutes = int(time_str[:-1])
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.endswith('h'):
                                                try:
                                                    hours = int(time_str[:-1])
                                                    completion_time_minutes = hours * 60
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            elif time_str.isdigit():
                                                try:
                                                    completion_time_minutes = int(time_str)
                                                except ValueError:
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # Fallback to 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Recalculate hours with new project rate using completion time (or 5 minutes default)
                                    lesson_creation_hours = calculate_creation_hours(completion_time_minutes, req.custom_rate)
                                    lesson['hours'] = lesson_creation_hours
                                    section_total_hours += lesson_creation_hours
                            
                            # Update the section's totalHours with sum of existing lesson hours
                            if 'totalHours' in section:
                                section['totalHours'] = round(section_total_hours)
                    
                    # Update the project in the database
                    await conn.execute(
                        "UPDATE projects SET microproduct_content = $1 WHERE id = $2",
                        content, project['id']
                    )
                    
                    # Re-fetch the updated project
                    updated_project = await conn.fetchrow(
                        "SELECT * FROM projects WHERE id = $1",
                        project_id
                    )
                    
            except Exception as e:
                logger.error(f"Error updating project {project_id} creation hours: {e}")
        
        # Get current component name for proper content parsing
        current_component_name = None
        if updated_project["design_template_id"]:
            design_template = await conn.fetchrow(
                "SELECT component_name FROM design_templates WHERE id = $1", 
                updated_project["design_template_id"]
            )
            if design_template:
                current_component_name = design_template["component_name"]
        
        # Parse the content into the appropriate model
        db_content = updated_project["microproduct_content"]
        final_content_for_model: Optional[MicroProductContentType] = None
        if db_content and isinstance(db_content, dict):
            # Round hours to integers before parsing to prevent float validation errors
            if current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                db_content = round_hours_in_content(db_content)
            
            try:
                if current_component_name == COMPONENT_NAME_PDF_LESSON:
                    final_content_for_model = PdfLessonDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TEXT_PRESENTATION:
                    final_content_for_model = TextPresentationDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_TRAINING_PLAN:
                    final_content_for_model = TrainingPlanDetails(**db_content)
                elif current_component_name == COMPONENT_NAME_VIDEO_LESSON:
                    final_content_for_model = VideoLessonData(**db_content)
                elif current_component_name == COMPONENT_NAME_QUIZ:
                    final_content_for_model = QuizData(**db_content)
                elif current_component_name == COMPONENT_NAME_SLIDE_DECK:
                    final_content_for_model = SlideDeckDetails(**db_content)
                else:
                    final_content_for_model = TrainingPlanDetails(**db_content)
            except Exception as e_parse:
                logger.error(f"Error parsing updated content from DB (proj ID {updated_project['id']}): {e_parse}", exc_info=not IS_PRODUCTION)
        
        return ProjectDB(
            id=updated_project["id"], 
            onyx_user_id=updated_project["onyx_user_id"], 
            project_name=updated_project["project_name"],
            product_type=updated_project["product_type"], 
            microproduct_type=updated_project["microproduct_type"],
            microproduct_name=updated_project["microproduct_name"], 
            microproduct_content=final_content_for_model,
            design_template_id=updated_project["design_template_id"], 
            created_at=updated_project["created_at"],
            custom_rate=updated_project["custom_rate"],
            quality_tier=updated_project["quality_tier"],
            is_advanced=updated_project.get("is_advanced"),
            advanced_rates=updated_project.get("advanced_rates")
        )

@app.get("/api/custom/projects/{project_id}/effective-rates")
async def get_effective_rates(
    project_id: int, 
    section_index: Optional[int] = None, 
    lesson_index: Optional[int] = None, 
    onyx_user_id: str = Depends(get_current_onyx_user_id), 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get effective advanced rates for a project/section/lesson following inheritance chain"""
    async with pool.acquire() as conn:
        # Get project and folder data
        project_row = await conn.fetchrow(
            """
            SELECT p.*, pf.is_advanced as folder_is_advanced, pf.advanced_rates as folder_advanced_rates, 
                   pf.custom_rate as folder_custom_rate, pf.completion_times as folder_completion_times
            FROM projects p
            LEFT JOIN project_folders pf ON p.folder_id = pf.id
            WHERE p.id = $1 AND p.onyx_user_id = $2
            """,
            project_id, onyx_user_id
        )
        if not project_row:
            raise HTTPException(status_code=404, detail="Project not found")
        
        project = dict(project_row)
        
        # Extract section and lesson if specified
        section = None
        lesson = None
        if project.get("microproduct_content"):
            content = project["microproduct_content"]
            if isinstance(content, dict) and isinstance(content.get('sections'), list):
                sections = content['sections']
                if section_index is not None and 0 <= section_index < len(sections):
                    section = sections[section_index]
                    if isinstance(section, dict) and isinstance(section.get('lessons'), list):
                        lessons = section['lessons']
                        if lesson_index is not None and 0 <= lesson_index < len(lessons):
                            lesson = lessons[lesson_index]
        
        # Resolve effective advanced config following inheritance: lesson > section > project > folder
        is_advanced = False
        rates = {}
        completion_times = {}
        completion_times = {}
        
        # Start with folder defaults
        if project.get('folder_is_advanced'):
            is_advanced = True
            if project.get('folder_advanced_rates'):
                try:
                    folder_rates = project['folder_advanced_rates']
                    if isinstance(folder_rates, str):
                        folder_rates = json.loads(folder_rates)
                    rates.update(folder_rates)
                except:
                    pass
            if project.get('folder_completion_times'):
                try:
                    folder_completion_times = project['folder_completion_times']
                    if isinstance(folder_completion_times, str):
                        folder_completion_times = json.loads(folder_completion_times)
                    completion_times.update(folder_completion_times)
                except:
                    pass
        folder_single_rate = project.get('folder_custom_rate') or 200
        
        # Override with project level
        if project.get('is_advanced') is not None:
            is_advanced = bool(project['is_advanced'])
        if project.get('advanced_rates'):
            try:
                project_rates = project['advanced_rates']
                if isinstance(project_rates, str):
                    project_rates = json.loads(project_rates)
                rates.update(project_rates)
            except:
                pass
        if project.get('completion_times'):
            try:
                project_completion_times = project['completion_times']
                if isinstance(project_completion_times, str):
                    project_completion_times = json.loads(project_completion_times)
                completion_times.update(project_completion_times)
            except:
                pass
        project_single_rate = project.get('custom_rate') or folder_single_rate
        
        # Override with section level
        if section:
            if section.get('advanced') is not None:
                is_advanced = bool(section['advanced'])
            if section.get('advancedRates'):
                section_rates = section['advancedRates']
                if isinstance(section_rates, dict):
                    # Convert frontend naming to backend naming
                    backend_rates = {}
                    if 'presentation' in section_rates:
                        backend_rates['presentation'] = section_rates['presentation']
                    if 'onePager' in section_rates:
                        backend_rates['one_pager'] = section_rates['onePager']
                    if 'quiz' in section_rates:
                        backend_rates['quiz'] = section_rates['quiz']
                    if 'videoLesson' in section_rates:
                        backend_rates['video_lesson'] = section_rates['videoLesson']
                    rates.update(backend_rates)
            if section.get('completionTimes'):
                section_completion_times = section['completionTimes']
                if isinstance(section_completion_times, dict):
                    # Convert frontend naming to backend naming
                    backend_completion_times = {}
                    if 'presentation' in section_completion_times:
                        backend_completion_times['presentation'] = section_completion_times['presentation']
                    if 'onePager' in section_completion_times:
                        backend_completion_times['one_pager'] = section_completion_times['onePager']
                    if 'quiz' in section_completion_times:
                        backend_completion_times['quiz'] = section_completion_times['quiz']
                    if 'videoLesson' in section_completion_times:
                        backend_completion_times['video_lesson'] = section_completion_times['videoLesson']
                    completion_times.update(backend_completion_times)
            section_single_rate = section.get('custom_rate') or project_single_rate
        else:
            section_single_rate = project_single_rate
        
        # Override with lesson level
        if lesson:
            if lesson.get('advanced') is not None:
                is_advanced = bool(lesson['advanced'])
            if lesson.get('advancedRates'):
                lesson_rates = lesson['advancedRates']
                if isinstance(lesson_rates, dict):
                    # Convert frontend naming to backend naming
                    backend_rates = {}
                    if 'presentation' in lesson_rates:
                        backend_rates['presentation'] = lesson_rates['presentation']
                    if 'onePager' in lesson_rates:
                        backend_rates['one_pager'] = lesson_rates['onePager']
                    if 'quiz' in lesson_rates:
                        backend_rates['quiz'] = lesson_rates['quiz']
                    if 'videoLesson' in lesson_rates:
                        backend_rates['video_lesson'] = lesson_rates['videoLesson']
                    rates.update(backend_rates)
            if lesson.get('completionTimes'):
                lesson_completion_times = lesson['completionTimes']
                if isinstance(lesson_completion_times, dict):
                    # Convert frontend naming to backend naming
                    backend_completion_times = {}
                    if 'presentation' in lesson_completion_times:
                        backend_completion_times['presentation'] = lesson_completion_times['presentation']
                    if 'onePager' in lesson_completion_times:
                        backend_completion_times['one_pager'] = lesson_completion_times['onePager']
                    if 'quiz' in lesson_completion_times:
                        backend_completion_times['quiz'] = lesson_completion_times['quiz']
                    if 'videoLesson' in lesson_completion_times:
                        backend_completion_times['video_lesson'] = lesson_completion_times['videoLesson']
                    completion_times.update(backend_completion_times)
            lesson_single_rate = lesson.get('custom_rate') or section_single_rate
        else:
            lesson_single_rate = section_single_rate
        
        fallback_single_rate = lesson_single_rate
        
        # Fill in missing rates with fallback
        default_rates = {
            'presentation': fallback_single_rate,
            'one_pager': fallback_single_rate,
            'quiz': fallback_single_rate,
            'video_lesson': fallback_single_rate
        }
        for key in default_rates:
            if key not in rates:
                rates[key] = default_rates[key]
        
        return {
            "is_advanced": is_advanced,
            "rates": {
                "presentation": rates.get('presentation', fallback_single_rate),
                "one_pager": rates.get('one_pager', fallback_single_rate),
                "quiz": rates.get('quiz', fallback_single_rate),
                "video_lesson": rates.get('video_lesson', fallback_single_rate),
            },
            "completion_times": {
                "presentation": completion_times.get('presentation', 8),  # Will be replaced with proper inheritance logic
                "one_pager": completion_times.get('one_pager', 3),
                "quiz": completion_times.get('quiz', 6),
                "video_lesson": completion_times.get('video_lesson', 4),
            },
            "fallback_single_rate": fallback_single_rate
        }

class ProjectOrderUpdateRequest(BaseModel):
    orders: List[Dict[str, int]]  # List of {projectId: int, order: int}

@app.put("/api/custom/projects/update-order", status_code=status.HTTP_200_OK)
async def update_project_order(order_data: ProjectOrderUpdateRequest, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Update the order of multiple projects"""
    try:
        async with pool.acquire() as conn:
            # Update each project's order
            for order_item in order_data.orders:
                project_id = order_item.get('projectId')
                order = order_item.get('order')
                
                if project_id is not None and order is not None:
                    # Verify project belongs to user and update order
                    result = await conn.execute(
                        "UPDATE projects SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                        order, project_id, onyx_user_id
                    )
                    
                    if result == "UPDATE 0":
                        logger.warning(f"Project {project_id} not found or not owned by user {onyx_user_id}")
        
        return {"message": "Project order updated successfully"}
    except Exception as e:
        logger.error(f"Error updating project order: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update project order")

@app.put("/api/custom/projects/folders/update-order")
async def update_folder_order(
    orders: List[Dict[str, int]], 
    onyx_user_id: str = Depends(get_current_onyx_user_id), 
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Update the order of folders"""
    async with pool.acquire() as conn:
        for order_data in orders:
            folder_id = order_data.get("folderId")
            order = order_data.get("order")
            if folder_id is not None and order is not None:
                await conn.execute(
                    "UPDATE project_folders SET \"order\" = $1 WHERE id = $2 AND onyx_user_id = $3",
                    order, folder_id, onyx_user_id
                )
    return {"message": "Folder order updated successfully"}

@app.get("/api/custom/projects/{project_id}/lesson-data")
async def get_project_lesson_data(project_id: int, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Get lesson data for a project with tier-adjusted creation hours"""
    try:
        # Get user identifiers for workspace access
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        
        async with pool.acquire() as conn:
            # Get project details including folder_id (with workspace access check)
            project = await conn.fetchrow("""
                SELECT p.microproduct_content, p.folder_id, dt.component_name 
                FROM projects p 
                JOIN design_templates dt ON p.design_template_id = dt.id 
                WHERE p.id = $1 AND (
                    p.onyx_user_id = $2 
                    OR EXISTS (
                        SELECT 1 FROM product_access pa
                        INNER JOIN workspace_members wm ON pa.workspace_id = wm.workspace_id
                        WHERE pa.product_id = p.id 
                          AND wm.user_id = $3 
                          AND wm.status = 'active'
                          AND pa.access_type IN ('workspace', 'role', 'individual')
                          AND (
                              pa.access_type = 'workspace' 
                              OR (pa.access_type = 'role' AND (pa.target_id = CAST(wm.role_id AS TEXT) OR pa.target_id IN (SELECT name FROM workspace_roles WHERE id = wm.role_id)))
                              OR (pa.access_type = 'individual' AND pa.target_id = $3)
                          )
                    )
                )
            """, project_id, user_uuid, user_email)
            
            if not project:
                raise HTTPException(status_code=404, detail="Project not found")
            
            content = project["microproduct_content"]
            component_name = project["component_name"]
            folder_id = project["folder_id"]
            
            # Only Training Plans have lesson data
            if component_name != COMPONENT_NAME_TRAINING_PLAN or not content:
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            
            # Get the folder's custom rate (with inheritance from parent)
            folder_custom_rate = 200  # Default custom rate
            if folder_id:
                folder_custom_rate = await get_folder_custom_rate(folder_id, pool)
            
            # Parse the training plan content
            try:
                if isinstance(content, dict):
                    sections = content.get("sections", [])
                    total_lessons = 0
                    total_hours = 0
                    total_completion_time = 0
                    sections_data = []
                    
                    for section in sections:
                        if isinstance(section, dict):
                            lessons = section.get("lessons", [])
                            section_lessons = len(lessons)
                            section_hours = 0
                            section_completion_time = 0
                            
                            total_lessons += section_lessons
                            
                            # Sum up completion time and use existing lesson hours for this section
                            for lesson in lessons:
                                if isinstance(lesson, dict):
                                    # Parse completion time (handles all language units: m, м, хв) - treat missing as 5 minutes
                                    completion_time_str = lesson.get("completionTime", "")
                                    completion_time_minutes = 5  # Default to 5 minutes
                                    
                                    if completion_time_str:
                                        time_str = str(completion_time_str).strip()
                                        if time_str and time_str != '':
                                            # Extract numeric part using regex to handle all language units
                                            import re
                                            numbers = re.findall(r'\d+', time_str)
                                            if numbers:
                                                try:
                                                    # If it contains 'h' (hour indicator), convert to minutes
                                                    if 'h' in time_str.lower():
                                                        completion_time_minutes = int(numbers[0]) * 60
                                                    else:
                                                        # For minutes (m, м, хв), just use the number
                                                        completion_time_minutes = int(numbers[0])
                                                except (ValueError, IndexError):
                                                    completion_time_minutes = 5  # Fallback to 5 minutes
                                            else:
                                                completion_time_minutes = 5  # No numbers found, use 5 minutes
                                        else:
                                            completion_time_minutes = 5  # Empty string, use 5 minutes
                                    else:
                                        completion_time_minutes = 5  # No completion time, use 5 minutes
                                    
                                    # Add to totals
                                    section_completion_time += completion_time_minutes
                                    total_completion_time += completion_time_minutes
                                    
                                    # Use existing lesson hours if available, otherwise calculate with folder rate
                                    if lesson.get('hours'):
                                        try:
                                            lesson_creation_hours = float(lesson['hours'])
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                        except (ValueError, TypeError):
                                            # If hours parsing fails, calculate with completion time
                                            lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                            section_hours += lesson_creation_hours
                                            total_hours += lesson_creation_hours
                                    else:
                                        # No existing hours, calculate with completion time
                                        lesson_creation_hours = calculate_creation_hours(completion_time_minutes, folder_custom_rate)
                                        section_hours += lesson_creation_hours
                                        total_hours += lesson_creation_hours
                            
                            # Add section data with tier-adjusted totals
                            sections_data.append({
                                "id": section.get("id", ""),
                                "title": section.get("title", ""),
                                "totalHours": round(section_hours),
                                "totalCompletionTime": section_completion_time,
                                "lessonCount": section_lessons
                            })
                    
                    return {
                        "lessonCount": total_lessons, 
                        "totalHours": round(total_hours), 
                        "completionTime": total_completion_time,
                        "sections": sections_data
                    }
                else:
                    return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
            except Exception as e:
                logger.warning(f"Error parsing lesson data for project {project_id}: {e}")
                return {"lessonCount": 0, "totalHours": 0, "completionTime": 0, "sections": []}
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting lesson data for project {project_id}: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to get lesson data")

@app.get("/api/custom/pdf/projects-list", response_class=FileResponse, responses={404: {"model": ErrorDetail}, 500: {"model": ErrorDetail}})
async def download_projects_list_pdf(
    folder_id: Optional[int] = Query(None),
    column_visibility: Optional[str] = Query(None),  # JSON string of column visibility settings
    client_name: Optional[str] = Query(None),  # Client name for PDF header customization
    selected_folders: Optional[str] = Query(None),  # JSON string of selected folder IDs
    selected_projects: Optional[str] = Query(None),  # JSON string of selected project IDs
    column_widths: Optional[str] = Query(None),  # JSON string of column width settings
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Download projects list as PDF with all folders expanded, deduplicated like the products page."""
    try:
        # Parse column visibility settings
        column_visibility_settings = {
            'title': True,
            'created': False,
            'creator': False,
            'numberOfLessons': True,
            'estCreationTime': True,
            'estCompletionTime': True
        }
        if column_visibility:
            try:
                parsed_settings = json.loads(column_visibility)
                column_visibility_settings.update(parsed_settings)
            except json.JSONDecodeError:
                logger.warning("Invalid column_visibility JSON, using defaults")

        # Parse selected projects first to use in the query
        selected_project_ids = set()
        if selected_projects:
            try:
                selected_project_ids = set(json.loads(selected_projects))
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing selected_projects: {e}")

        # Fetch projects and folders data
        async with pool.acquire() as conn:
            # Fetch projects
            projects_query = """
                SELECT p.id, p.project_name, p.microproduct_name, p.created_at, p.design_template_id,
                       dt.template_name as design_template_name,
                       dt.microproduct_type as design_microproduct_type,
                       p.folder_id, p."order", p.microproduct_content, p.quality_tier
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.onyx_user_id = $1
            """
            projects_params = [onyx_user_id]
            param_count = 1
            
            if folder_id is not None:
                projects_query += f" AND p.folder_id = ${param_count + 1}"
                projects_params.append(folder_id)
                param_count += 1
            
            # Add selected projects filter if provided
            if selected_project_ids:
                placeholders = ','.join([f'${i + param_count + 1}' for i in range(len(selected_project_ids))])
                projects_query += f" AND p.id IN ({placeholders})"
                projects_params.extend(selected_project_ids)
            
            projects_query += " ORDER BY p.\"order\" ASC, p.created_at DESC;"
            
            projects_rows = await conn.fetch(projects_query, *projects_params)
            
            # Fetch folders with hierarchical structure (only if not viewing a specific folder)
            folders_data = []
            if folder_id is None:
                folders_query = """
                    SELECT 
                        pf.id, 
                        pf.name, 
                        pf.created_at, 
                        pf."order", 
                        pf.parent_id,
                        pf.quality_tier,
                        pf.custom_rate,
                        COUNT(p.id) as project_count,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COUNT(*)::int 
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_lessons,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM((lesson->>'hours')::float), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_hours,
                        COALESCE(
                            SUM(
                                CASE 
                                    WHEN p.microproduct_content IS NOT NULL 
                                    AND p.microproduct_content->>'sections' IS NOT NULL 
                                    THEN (
                                        SELECT COALESCE(SUM(
                                            CASE 
                                                WHEN lesson->>'completionTime' IS NOT NULL AND lesson->>'completionTime' != '' 
                                                THEN (
                                                    -- Extract numeric part using regex, handling all language units (m, м, хв)
                                                    CASE 
                                                        WHEN lesson->>'completionTime' ~ '^[0-9]+[mмхв]*$'
                                                        THEN CAST(regexp_replace(lesson->>'completionTime', '[^0-9]', '', 'g') AS INTEGER)
                                                        ELSE 5
                                                    END
                                                )
                                                ELSE 5 
                                            END
                                        ), 0)
                                        FROM jsonb_array_elements(p.microproduct_content->'sections') AS section
                                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                                    )
                                    ELSE 0 
                                END
                            ), 0
                        ) as total_completion_time
                    FROM project_folders pf
                    LEFT JOIN projects p ON pf.id = p.folder_id
                    WHERE pf.onyx_user_id = $1
                    GROUP BY pf.id, pf.name, pf.created_at, pf."order", pf.parent_id, pf.quality_tier, pf.custom_rate
                    ORDER BY pf."order" ASC, pf.created_at ASC;
                """
                folders_rows = await conn.fetch(folders_query, onyx_user_id)
                folders_data = [dict(row) for row in folders_rows]

        # Process projects data
        projects_data = []
        for row in projects_rows:
            row_dict = dict(row)
            
            # Calculate individual project times
            total_lessons = 0
            total_hours = 0.0
            total_completion_time = 0
            
            if row_dict.get('microproduct_content') and isinstance(row_dict['microproduct_content'], dict):
                content = row_dict['microproduct_content']
                if content.get('sections') and isinstance(content['sections'], list):
                    for section in content['sections']:
                        if section.get('lessons') and isinstance(section['lessons'], list):
                            for lesson in section['lessons']:
                                total_lessons += 1
                                if lesson.get('hours'):
                                    try:
                                        total_hours += float(lesson['hours'])
                                    except (ValueError, TypeError):
                                        pass
                                
                                # Calculate completion time - treat missing completion time as 5 minutes
                                completion_time_str = lesson.get('completionTime', '')
                                if completion_time_str:
                                    time_str = str(completion_time_str).strip()
                                    if time_str and time_str != '':
                                        if time_str.endswith('m'):
                                            try:
                                                minutes = int(time_str[:-1])
                                                total_completion_time += minutes
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.endswith('h'):
                                            try:
                                                hours = int(time_str[:-1])
                                                total_completion_time += (hours * 60)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        elif time_str.isdigit():
                                            try:
                                                total_completion_time += int(time_str)
                                            except ValueError:
                                                total_completion_time += 5  # Fallback to 5 minutes
                                        else:
                                            total_completion_time += 5  # Fallback to 5 minutes
                                    else:
                                        total_completion_time += 5  # Empty string, use 5 minutes
                                else:
                                    total_completion_time += 5  # No completion time, use 5 minutes
            
            projects_data.append({
                'id': row_dict['id'],
                'title': row_dict.get('project_name') or row_dict.get('microproduct_name') or 'Untitled',
                'created_at': row_dict['created_at'],
                'created_by': 'You',
                'design_microproduct_type': row_dict.get('design_microproduct_type'),
                'folder_id': row_dict.get('folder_id'),
                'order': row_dict.get('order', 0),
                'microproduct_content': row_dict.get('microproduct_content'),
                'total_lessons': total_lessons,
                'total_hours': round(total_hours),
                'total_completion_time': total_completion_time
            })

        # --- Deduplicate projects: only show top-level products and outlines, hide lessons/quizzes that belong to an outline ---
        def deduplicate_projects(projects_arr):
            outline_names = set()
            filtered_projects = []
            grouped = {}
            # First pass: collect all outline names and group by title
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    outline_names.add(proj['title'].strip())
                if proj['title'] not in grouped:
                    grouped[proj['title']] = {'outline': None, 'others': []}
                if is_outline:
                    if not grouped[proj['title']]['outline']:
                        grouped[proj['title']]['outline'] = proj
                else:
                    grouped[proj['title']]['others'].append(proj)
            # Second pass: filter projects
            for proj in projects_arr:
                is_outline = (proj.get('design_microproduct_type') or '').lower() == 'training plan'
                if is_outline:
                    filtered_projects.append(proj)
                else:
                    project_title = proj['title'].strip()
                    belongs_to_outline = False
                    group_for_this_title = grouped[proj['title']]
                    if group_for_this_title and group_for_this_title['outline']:
                        belongs_to_outline = True
                    if not belongs_to_outline and ': ' in project_title:
                        outline_part = project_title.split(': ')[0].strip()
                        if outline_part in outline_names:
                            belongs_to_outline = True
                    if not belongs_to_outline:
                        filtered_projects.append(proj)
            return filtered_projects

        projects_data = deduplicate_projects(projects_data)

        # Build folder tree structure
        def build_folder_tree(folders):
            folder_map = {}
            root_folders = []
            
            # Create folder map
            for folder in folders:
                folder['children'] = []
                folder_map[folder['id']] = folder
            
            # Build tree structure
            for folder in folders:
                if folder['parent_id'] is None:
                    root_folders.append(folder)
                else:
                    parent = folder_map.get(folder['parent_id'])
                    if parent:
                        parent['children'].append(folder)
            
            return root_folders

        # Group projects by folder
        folder_projects = {}
        unassigned_projects = []
        
        for project in projects_data:
            if project['folder_id']:
                if project['folder_id'] not in folder_projects:
                    folder_projects[project['folder_id']] = []
                folder_projects[project['folder_id']].append(project)
            else:
                unassigned_projects.append(project)

        # Build hierarchical folder structure
        folder_tree = build_folder_tree(folders_data) if folders_data else []

        # Calculate recursive totals for folders (including subfolder projects)
        def calculate_recursive_totals(folder):
            # Start with direct project totals
            direct_projects = folder_projects.get(folder['id'], [])
            total_lessons = sum(p['total_lessons'] for p in direct_projects)
            total_hours = sum(p['total_hours'] for p in direct_projects)
            total_completion_time = sum(p['total_completion_time'] for p in direct_projects)
            total_items = len(direct_projects)
            
            # Add subfolder totals recursively
            if folder.get('children'):
                for child in folder['children']:
                    child_totals = calculate_recursive_totals(child)
                    total_lessons += child_totals['total_lessons']
                    total_hours += child_totals['total_hours']
                    total_completion_time += child_totals['total_completion_time']
                    total_items += child_totals['total_items']
            
            # Update folder with recursive totals
            folder['total_lessons'] = total_lessons
            folder['total_hours'] = total_hours
            folder['total_completion_time'] = total_completion_time
            folder['project_count'] = total_items
            
            return {
                'total_lessons': total_lessons,
                'total_hours': total_hours,
                'total_completion_time': total_completion_time,
                'total_items': total_items
            }

        # Calculate recursive totals for all root folders
        for folder in folder_tree:
            calculate_recursive_totals(folder)

        # Helper function to get tier color
        def get_tier_color(tier):
            tier_colors = {
                'basic': '#22c55e',        # green-500
                'interactive': '#f97316',  # orange-500
                'advanced': '#a855f7',     # purple-500
                'immersive': '#3b82f6',    # blue-500
                # Legacy tier support
                'starter': '#22c55e',      # green-500 (mapped to basic)
                'medium': '#f97316',       # orange-500 (mapped to interactive)
                'professional': '#3b82f6'  # blue-500 (mapped to immersive)
            }
            return tier_colors.get(tier, '#f97316')  # default to interactive

        # Helper function to check if folder has course outlines
        def has_course_outlines(folder_id):
            projects = folder_projects.get(folder_id, [])
            return any(p.get('design_microproduct_type', '').lower() == 'training plan' for p in projects)

        # Helper function to check if folder or any subfolder has course outlines
        def has_course_outlines_recursive(folder):
            # Check direct projects
            if has_course_outlines(folder['id']):
                return True
            
            # Check subfolders recursively
            if folder.get('children'):
                for child in folder['children']:
                    if has_course_outlines_recursive(child):
                        return True
            
            return False

        # Add tier information and check for course outlines
        def add_tier_info(folder):
            folder['tier_color'] = get_tier_color(folder.get('quality_tier', 'interactive'))
            folder['has_course_outlines'] = has_course_outlines_recursive(folder)
            
            # Recursively process children
            if folder.get('children'):
                for child in folder['children']:
                    add_tier_info(child)

        # Add tier information to all folders
        for folder in folder_tree:
            add_tier_info(folder)

        # Filter data based on selected folders (projects are already filtered in the query)
        if selected_folders:
            try:
                selected_folder_ids = set()
                
                # Parse selected folders
                if selected_folders:
                    selected_folder_ids = set(json.loads(selected_folders))
                
                # Filter folders - only include selected folders and their children
                def filter_folders_recursive(folders_list):
                    filtered_folders = []
                    for folder in folders_list:
                        # Include folder if it's selected or if any of its children are selected
                        if folder['id'] in selected_folder_ids:
                            filtered_folders.append(folder)
                        else:
                            # Check if any children are selected
                            if folder.get('children'):
                                filtered_children = filter_folders_recursive(folder['children'])
                                if filtered_children:
                                    folder_copy = folder.copy()
                                    folder_copy['children'] = filtered_children
                                    filtered_folders.append(folder_copy)
                    return filtered_folders
                
                filtered_folder_tree = filter_folders_recursive(folder_tree)
                
                # Filter folder projects - only include projects from selected folders
                filtered_folder_projects = {}
                for folder_id, projects in folder_projects.items():
                    if folder_id in selected_folder_ids:
                        filtered_folder_projects[folder_id] = projects
                
                # Use filtered data
                folder_tree = filtered_folder_tree
                folder_projects = filtered_folder_projects
                # Note: unassigned_projects are already filtered by the query when selected_projects is provided
                
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing selected folders: {e}. Using all data.")
                # If parsing fails, use all data (fallback)

        # Parse column widths if provided
        column_widths_settings = {}
        if column_widths:
            try:
                column_widths_settings = json.loads(column_widths)
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"Error parsing column widths: {e}. Using default widths.")
                column_widths_settings = {}

        # Calculate summary statistics for the mini table
        def calculate_summary_stats(folders, folder_projects, unassigned_projects):
            total_projects = 0
            total_lessons = 0
            total_creation_time = 0
            total_completion_time = 0
            
            # Calculate from folders and their projects
            for folder in folders:
                if folder['id'] in folder_projects:
                    for project in folder_projects[folder['id']]:
                        total_projects += 1
                        total_lessons += project.get('total_lessons', 0) or 0
                        total_creation_time += project.get('total_hours', 0) or 0
                        total_completion_time += project.get('total_completion_time', 0) or 0
                
                # Recursively calculate from subfolders
                if folder.get('children'):
                    child_stats = calculate_summary_stats(folder['children'], folder_projects, [])
                    total_projects += child_stats['total_projects']
                    total_lessons += child_stats['total_lessons']
                    total_creation_time += child_stats['total_creation_time']
                    total_completion_time += child_stats['total_completion_time']
            
            # Add unassigned projects
            for project in unassigned_projects:
                total_projects += 1
                total_lessons += project.get('total_lessons', 0) or 0
                total_creation_time += project.get('total_hours', 0) or 0
                total_completion_time += project.get('total_completion_time', 0) or 0
            
            return {
                'total_projects': total_projects,
                'total_lessons': total_lessons,
                'total_creation_time': total_creation_time,
                'total_completion_time': total_completion_time
            }

        # Calculate summary statistics
        summary_stats = calculate_summary_stats(folder_tree, folder_projects, unassigned_projects)

        # Collect all project IDs that are actually included in the filtered PDF data
        included_project_ids = set()
        
        # Add projects from filtered folders
        for folder_id_key, projects in folder_projects.items():
            for project in projects:
                included_project_ids.add(project['id'])
        
        # Add filtered unassigned projects
        for project in unassigned_projects:
            included_project_ids.add(project['id'])
        
        logger.info(f"[PDF_ANALYTICS] Found {len(included_project_ids)} projects included in PDF: {list(included_project_ids)}")

        # Fetch real analytics data for pie charts using only the projects actually included in the PDF
        product_distribution = None
        quality_distribution = None
        
        logger.info(f"[PDF_ANALYTICS] Starting analytics data fetch for user: {onyx_user_id}, included projects: {len(included_project_ids)}")
        
        try:
            # Use a new connection for analytics queries since the original conn might be released
            async with pool.acquire() as analytics_conn:
                # Get product distribution data - since lessons don't have pre-computed recommendations,
                # we'll generate them on-the-fly using Python logic
                if included_project_ids:
                    # Convert project IDs to a format suitable for SQL IN clause
                    project_ids_list = list(included_project_ids)
                    project_ids_placeholder = ','.join(['$' + str(i+2) for i in range(len(project_ids_list))])
                    
                    lessons_query = f"""
                        SELECT 
                            p.id as project_id,
                            lesson->>'title' as lesson_title,
                            lesson->>'quality_tier' as lesson_quality_tier,
                            p.quality_tier as project_quality_tier,
                            pf.quality_tier as folder_quality_tier
                        FROM projects p
                        LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                        LEFT JOIN project_folders pf ON p.folder_id = pf.id
                        CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                        CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                        WHERE p.onyx_user_id = $1
                        AND p.id IN ({project_ids_placeholder})
                        AND p.microproduct_content IS NOT NULL
                        AND p.microproduct_content->>'sections' IS NOT NULL
                        AND dt.component_name = 'TrainingPlanTable'
                    """
                    product_params = [onyx_user_id] + project_ids_list
                else:
                    # No projects included, use empty result set
                    lessons_query = "SELECT NULL as project_id, NULL as lesson_title, NULL as lesson_quality_tier, NULL as project_quality_tier, NULL as folder_quality_tier WHERE FALSE"
                    product_params = []
                
                logger.info(f"[PDF_ANALYTICS] Lessons query: {lessons_query}")
                logger.info(f"[PDF_ANALYTICS] Lessons params: {product_params}")
                
                lessons_rows = await analytics_conn.fetch(lessons_query, *product_params)
                logger.info(f"[PDF_ANALYTICS] Found {len(lessons_rows)} lessons for product analysis")
                
                # Generate recommendations for each lesson using the existing function
                product_counts = {}
                total_products = 0
                
                for row in lessons_rows:
                    lesson_title = row['lesson_title'] or ''
                    # Determine effective quality tier
                    quality_tier = (
                        row['lesson_quality_tier'] or 
                        row['project_quality_tier'] or 
                        row['folder_quality_tier'] or 
                        'interactive'
                    )
                    
                    logger.info(f"[PDF_ANALYTICS] Processing lesson: '{lesson_title}' (quality: {quality_tier})")
                    
                    # Generate recommendations using the existing function
                    recommendations = analyze_lesson_content_recommendations(lesson_title, quality_tier)
                    primary_types = recommendations.get('primary', [])
                    
                    logger.info(f"[PDF_ANALYTICS] Generated recommendations: {primary_types}")
                    
                    # Count each recommended product type
                    for product_type_str in primary_types:
                        total_products += 1
                        
                        # Map to our ProductType enum
                        product_type_mapping = {
                            'one-pager': ProductType.ONE_PAGER,
                            'presentation': ProductType.PRESENTATION,
                            'quiz': ProductType.QUIZ,
                            'video-lesson': ProductType.VIDEO_LESSON
                        }
                        
                        product_type = product_type_mapping.get(product_type_str)
                        if product_type:
                            if product_type not in product_counts:
                                product_counts[product_type] = 0
                            product_counts[product_type] += 1
                            logger.info(f"[PDF_ANALYTICS] Added {product_type_str} -> {product_type}, count now: {product_counts[product_type]}")
                        else:
                            logger.warning(f"[PDF_ANALYTICS] Unknown product type: {product_type_str}")
                
                # We've already computed product_counts and total_products above using Python logic
                
                logger.info(f"[PDF_ANALYTICS] Total products: {total_products}")
                logger.info(f"[PDF_ANALYTICS] Product counts: {product_counts}")
                
                # Create product distribution data for template
                product_distribution = {
                    'total_products': total_products,
                    'one_pager_count': product_counts.get(ProductType.ONE_PAGER, 0),
                    'presentation_count': product_counts.get(ProductType.PRESENTATION, 0),
                    'quiz_count': product_counts.get(ProductType.QUIZ, 0),
                    'video_lesson_count': product_counts.get(ProductType.VIDEO_LESSON, 0)
                }
                
                logger.info(f"[PDF_ANALYTICS] Product distribution before percentages: {product_distribution}")
                
                # Calculate percentages
                if total_products > 0:
                    product_distribution['one_pager_percentage'] = round((product_distribution['one_pager_count'] / total_products * 100), 1)
                    product_distribution['presentation_percentage'] = round((product_distribution['presentation_count'] / total_products * 100), 1)
                    product_distribution['quiz_percentage'] = round((product_distribution['quiz_count'] / total_products * 100), 1)
                    product_distribution['video_lesson_percentage'] = round((product_distribution['video_lesson_count'] / total_products * 100), 1)
                else:
                    product_distribution['one_pager_percentage'] = 0
                    product_distribution['presentation_percentage'] = 0
                    product_distribution['quiz_percentage'] = 0
                    product_distribution['video_lesson_percentage'] = 0
                
                logger.info(f"[PDF_ANALYTICS] Final product distribution: {product_distribution}")
                
                # Get quality distribution data using the same project filtering
                if included_project_ids:
                    # Use the same project IDs that were included in the product analysis
                    quality_query = f"""
                        WITH lesson_quality_tiers AS (
                            SELECT 
                                COALESCE(
                                    lesson->>'quality_tier',
                                    section->>'quality_tier', 
                                    p.quality_tier,
                                    pf.quality_tier,
                                    'interactive'
                                ) as effective_quality_tier
                            FROM projects p
                            LEFT JOIN project_folders pf ON p.folder_id = pf.id
                            CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                            CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                            WHERE p.onyx_user_id = $1
                            AND p.id IN ({project_ids_placeholder})
                            AND p.microproduct_content IS NOT NULL
                            AND p.microproduct_content->>'sections' IS NOT NULL
                        )
                        SELECT 
                            LOWER(effective_quality_tier) as quality_tier,
                            COUNT(*) as count
                        FROM lesson_quality_tiers
                        GROUP BY LOWER(effective_quality_tier)
                        ORDER BY count DESC
                    """
                    quality_params = [onyx_user_id] + project_ids_list
                else:
                    # No projects included, use empty result set
                    quality_query = "SELECT NULL as quality_tier, 0 as count WHERE FALSE"
                    quality_params = []
                
                logger.info(f"[PDF_ANALYTICS] Quality query: {quality_query}")
                logger.info(f"[PDF_ANALYTICS] Quality params: {quality_params}")
                
                quality_rows = await analytics_conn.fetch(quality_query, *quality_params)
                logger.info(f"[PDF_ANALYTICS] Quality query returned {len(quality_rows)} rows")
                
                # Process quality distribution
                tier_counts = {}
                total_lessons = 0
                
                for row in quality_rows:
                    tier_name = row['quality_tier'].lower()
                    count = row['count']
                    total_lessons += count
                    logger.info(f"[PDF_ANALYTICS] Raw quality tier: {tier_name}, count: {count}")
                    
                    # Map tier names to enum values
                    tier_mapping = {
                        'basic': 'basic',
                        'interactive': 'interactive',
                        'advanced': 'advanced',
                        'immersive': 'immersive',
                        'medium': 'interactive',  # Map medium to interactive
                        'premium': 'advanced',    # Map premium to advanced
                    }
                    
                    tier = tier_mapping.get(tier_name, 'interactive')
                    logger.info(f"[PDF_ANALYTICS] Mapped quality tier {tier_name} -> {tier}")
                    if tier not in tier_counts:
                        tier_counts[tier] = 0
                    tier_counts[tier] += count
                    logger.info(f"[PDF_ANALYTICS] Added {count} to {tier}, total now: {tier_counts[tier]}")
                
                logger.info(f"[PDF_ANALYTICS] Total lessons: {total_lessons}")
                logger.info(f"[PDF_ANALYTICS] Tier counts: {tier_counts}")
                
                # Create quality distribution data for template
                quality_distribution = {
                    'total_lessons': total_lessons,
                    'basic_count': tier_counts.get('basic', 0),
                    'interactive_count': tier_counts.get('interactive', 0),
                    'advanced_count': tier_counts.get('advanced', 0),
                    'immersive_count': tier_counts.get('immersive', 0)
                }
                
                logger.info(f"[PDF_ANALYTICS] Quality distribution before percentages: {quality_distribution}")
                
                # Calculate percentages
                if total_lessons > 0:
                    quality_distribution['basic_percentage'] = round((quality_distribution['basic_count'] / total_lessons * 100), 1)
                    quality_distribution['interactive_percentage'] = round((quality_distribution['interactive_count'] / total_lessons * 100), 1)
                    quality_distribution['advanced_percentage'] = round((quality_distribution['advanced_count'] / total_lessons * 100), 1)
                    quality_distribution['immersive_percentage'] = round((quality_distribution['immersive_count'] / total_lessons * 100), 1)
                else:
                    quality_distribution['basic_percentage'] = 0
                    quality_distribution['interactive_percentage'] = 0
                    quality_distribution['advanced_percentage'] = 0
                    quality_distribution['immersive_percentage'] = 0
                
                logger.info(f"[PDF_ANALYTICS] Final quality distribution: {quality_distribution}")
                
        except Exception as e:
            logger.error(f"[PDF_ANALYTICS] Failed to fetch analytics data for PDF: {str(e)}", exc_info=True)
            # Use fallback data if analytics fetch fails
            product_distribution = {
                'total_products': 0,
                'one_pager_count': 0,
                'presentation_count': 0,
                'quiz_count': 0,
                'video_lesson_count': 0,
                'one_pager_percentage': 0,
                'presentation_percentage': 0,
                'quiz_percentage': 0,
                'video_lesson_percentage': 0
            }
            quality_distribution = {
                'total_lessons': 0,
                'basic_count': 0,
                'interactive_count': 0,
                'advanced_count': 0,
                'immersive_count': 0,
                'basic_percentage': 0,
                'interactive_percentage': 0,
                'advanced_percentage': 0,
                'immersive_percentage': 0
            }
            logger.info(f"[PDF_ANALYTICS] Using fallback data due to error")

        # Prepare data for template
        template_data = {
            'folders': folder_tree,  # Use hierarchical structure
            'folder_projects': folder_projects,
            'unassigned_projects': unassigned_projects,
            'column_visibility': column_visibility_settings,
            'column_widths': column_widths_settings,
            'folder_id': folder_id,
            'client_name': client_name,  # Client name for header customization
            'generated_at': datetime.now().isoformat(),
            'summary_stats': summary_stats,  # Add summary statistics to template data
            'product_distribution': product_distribution,  # Add real product distribution data
            'quality_distribution': quality_distribution   # Add real quality distribution data
        }
        
        logger.info(f"[PDF_ANALYTICS] Template data prepared:")
        logger.info(f"[PDF_ANALYTICS] - product_distribution: {product_distribution}")
        logger.info(f"[PDF_ANALYTICS] - quality_distribution: {quality_distribution}")
        logger.info(f"[PDF_ANALYTICS] - summary_stats: {summary_stats}")

        # Generate PDF
        logger.info(f"[PDF_ANALYTICS] About to generate PDF with template data keys: {list(template_data.keys())}")
        logger.info(f"[PDF_ANALYTICS] Template data summary:")
        logger.info(f"[PDF_ANALYTICS] - folders count: {len(template_data.get('folders', []))}")
        logger.info(f"[PDF_ANALYTICS] - folder_projects count: {len(template_data.get('folder_projects', {}))}")
        logger.info(f"[PDF_ANALYTICS] - unassigned_projects count: {len(template_data.get('unassigned_projects', []))}")
        logger.info(f"[PDF_ANALYTICS] - product_distribution: {template_data.get('product_distribution')}")
        logger.info(f"[PDF_ANALYTICS] - quality_distribution: {template_data.get('quality_distribution')}")
        
        unique_output_filename = f"projects_list_{onyx_user_id}_{uuid.uuid4().hex[:12]}.pdf"
        pdf_path = await generate_pdf_from_html_template("projects_list_pdf_template.html", template_data, unique_output_filename)
        
        if not os.path.exists(pdf_path):
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="PDF file not found after generation.")
        
        user_friendly_filename = f"projects_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        return FileResponse(
            path=pdf_path, 
            filename=user_friendly_filename, 
            media_type='application/pdf', 
            headers={"Cache-Control": "no-cache, no-store, must-revalidate", "Pragma": "no-cache", "Expires": "0"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating projects list PDF: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate PDF: {str(e)[:200]}")

# Quiz endpoints
class QuizWizardPreview(BaseModel):
    outlineId: Optional[int] = None  # Parent Training Plan project id
    lesson: Optional[str] = None      # Specific lesson to generate quiz for, optional when prompt-based
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    prompt: Optional[str] = None           # Fallback free-form prompt
    language: str = "en"
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"  # comma-separated question types
    questionCount: int = 10  # Number of questions to generate
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: Knowledge Base context for creation from Knowledge Base search
    fromKnowledgeBase: Optional[bool] = None
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths

class QuizWizardFinalize(BaseModel):
    outlineId: Optional[int] = None
    lesson: str                      # May be explicitly empty string for no-lesson quizzes
    courseName: Optional[str] = None  # Course name (outline name) for proper course context
    aiResponse: str                        # User-edited quiz data
    prompt: str
    chatSessionId: Optional[str] = None
    questionTypes: str = "multiple-choice,multi-select,matching,sorting,open-answer"
    questionCount: int = 10  # Number of questions to generate
    language: str = "en"
    # NEW: file context for creation from documents
    fromFiles: Optional[bool] = None
    folderIds: Optional[str] = None  # comma-separated folder IDs
    fileIds: Optional[str] = None    # comma-separated file IDs
    # NEW: text context for creation from user text
    fromText: Optional[bool] = None
    textMode: Optional[str] = None   # "context" or "base"
    userText: Optional[str] = None   # User's pasted text
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: user edits tracking (like in Course Outline)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False
    # NEW: indices of edited questions for selective regeneration (comma-separated: "0,2,5")
    editedQuestionIndices: Optional[str] = None

class QuizEditRequest(BaseModel):
    currentContent: str
    editPrompt: str
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    questionTypes: Optional[str] = None
    language: str = "en"
    fromFiles: bool = False
    fromText: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    textMode: Optional[str] = None
    questionCount: int = 10
    chatSessionId: Optional[str] = None
    # NEW: indicate if content is clean (questions only, no options/answers)
    isCleanContent: Optional[bool] = False

async def _ensure_quiz_template(pool: asyncpg.Pool) -> int:
    """Ensure quiz design template exists, return template ID"""
    try:
        # Check if quiz template exists
        template_query = """
            SELECT id FROM design_templates 
            WHERE microproduct_type = 'Quiz' 
            LIMIT 1
        """
        template_result = await pool.fetchval(template_query)
        
        if template_result:
            return template_result
        
        # Create quiz template if it doesn't exist
        insert_query = """
            INSERT INTO design_templates 
            (template_name, template_structuring_prompt, microproduct_type, component_name, design_image_path)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
        """
        template_id = await pool.fetchval(
            insert_query,
            "Quiz Template",
            "Create an interactive quiz with various question types including multiple choice, multi-select, matching, sorting, and open answer questions.",
            "Quiz",
            COMPONENT_NAME_QUIZ,
            "/quiz.png"
        )
        return template_id
        
    except Exception as e:
        logger.error(f"Error ensuring quiz template: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to ensure quiz template")

@app.post("/api/custom/quiz/generate")
async def quiz_generate(payload: QuizWizardPreview, request: Request):
    """Generate quiz content with streaming response"""
    logger.info(f"[QUIZ_PREVIEW_START] Quiz preview initiated")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] questionTypes={payload.questionTypes} lang={payload.language}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[QUIZ_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_PREVIEW_CHAT] Creating new chat session")
        try:
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz",
        "prompt": payload.prompt or "Create a quiz",
        "language": payload.language,
        "questionTypes": payload.questionTypes,
        "questionCount": payload.questionCount,
    }

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - send directly in wizard request (no file conversion)
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        # Check if we're using hybrid approach (files present) or direct approach (text-only)
        if should_use_hybrid_approach(payload):
            # Hybrid approach: create virtual files for text (existing behavior for file-based scenarios)
            if text_length > LARGE_TEXT_THRESHOLD:
                logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system for hybrid approach")
                try:
                    virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                    wiz_payload["virtualFileId"] = virtual_file_id
                    wiz_payload["textCompressed"] = False
                    logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
                except Exception as e:
                    logger.error(f"Failed to create virtual file for large text: {e}")
                    # Fallback to compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Use compression for hybrid approach with medium/small text
                if text_length > TEXT_SIZE_THRESHOLD:
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Using compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
                else:
                    wiz_payload["userText"] = payload.userText
                    wiz_payload["textCompressed"] = False
        else:
            # Direct approach: send text directly in wizard request (no file conversion)
            logger.info(f"✅ Using DIRECT approach: sending text directly in wizard request ({text_length} chars)")
            
            # For very large texts, use compression to reduce payload size
            if text_length > TEXT_SIZE_THRESHOLD:
                compressed_text = compress_text(payload.userText)
                wiz_payload["userText"] = compressed_text
                wiz_payload["textCompressed"] = True
                logger.info(f"Compressed text for direct wizard request ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Send text directly without compression
                wiz_payload["userText"] = payload.userText
                wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wiz_payload["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for quiz generation")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    # Enforce diverse question types by default unless explicitly told otherwise
    diversity_note = ""
    try:
        chosen_types_csv = (payload.questionTypes or "").strip()
        if not chosen_types_csv:
            default_types = ["multiple-choice", "multi-select", "matching", "sorting", "open-answer"]
            wiz_payload["questionTypes"] = ",".join(default_types)
            chosen_types = default_types
            enforce_diverse = True
        else:
            chosen_types = [t.strip() for t in chosen_types_csv.split(",") if t.strip()]
            enforce_diverse = len(chosen_types) > 1
        if enforce_diverse:
            total = payload.questionCount or 10
            base = total // len(chosen_types)
            remainder = total % len(chosen_types)
            # Prioritize non-multiple-choice types for remainders to reduce MC dominance
            remainder_order = [t for t in chosen_types if t != "multiple-choice"] + [t for t in chosen_types if t == "multiple-choice"]
            counts = {t: base for t in chosen_types}
            for i in range(remainder):
                counts[remainder_order[i % len(remainder_order)]] += 1
            plan_str = ", ".join([f"{t}: {counts[t]}" for t in chosen_types])
            diversity_note = (
                f"CRITICAL QUIZ DIVERSITY INSTRUCTION: Generate a balanced mix of question types. "
                f"Unless the user's prompt explicitly requests a different mix, distribute the {total} questions across types exactly as follows: {plan_str}. "
                f"Do not exceed a difference of +1 between any two types. Avoid making 'multiple-choice' more than any other type unless required by the user."
            )
    except Exception as e:
        logger.warning(f"[QUIZ_DIVERSITY_NOTE] Failed to build diversity instruction: {e}")
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations - For quizzes: questions, answers, explanations ALL must be in {payload.language}" + (("\n" + diversity_note) if diversity_note else "")  

    # Force JSON-ONLY preview output for Quiz to enable immediate parsed preview (like Presentations/Outline)
    try:
        json_preview_instructions_quiz = f"""

CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Quiz preview, strictly following this example structure:
{DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.

CRITICAL SCHEMA AND CONTENT RULES (MUST MATCH FINAL FORMAT):
- Include exact fields: quizTitle, questions[], detectedLanguage.
- Each question MUST include: question_type, question_text, and appropriate fields based on type
  (options[] + correct_option_id | options[] + correct_option_ids[] | prompts[] + options[] + correct_matches{{}} | items_to_sort[] + correct_order[] | acceptable_answers[]), and explanation.
- Use exact field names and value shapes as in the example. Preserve original language across all text.
"""
        wizard_message = wizard_message + json_preview_instructions_quiz
        logger.info("[QUIZ_PREVIEW] Added JSON-only preview instructions")
    except Exception as e:
        logger.warning(f"[QUIZ_PREVIEW_JSON_INSTR] Failed to append JSON-only preview instructions: {e}")

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[QUIZ_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[QUIZ_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[QUIZ_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    # For connector-based filtering, extract context from specific connectors
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                    file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromConnectors and payload.selectedFiles:
                    # SmartDrive files only (no connectors)
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from SmartDrive files only: {payload.selectedFiles}")
                    
                    # Map SmartDrive paths to Onyx file IDs
                    raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                    
                    # Normalize paths to handle URL encoding and character variations
                    smartdrive_file_paths = []
                    for path in raw_paths:
                        # Try multiple variations to match database records
                        from urllib.parse import unquote, quote
                        import re
                        
                        candidates = []
                        # Base variants
                        candidates.append(path)
                        try:
                            decoded_path = unquote(path)
                            candidates.append(decoded_path)
                        except:
                            decoded_path = path
                        try:
                            encoded_path = quote(path, safe='/')
                            candidates.append(encoded_path)
                        except:
                            pass
                        
                        # Character normalization variants
                        for candidate in list(candidates):
                            # Handle spaces encoded as + or %20
                            if '+' in candidate:
                                candidates.append(candidate.replace('+', ' '))
                                candidates.append(candidate.replace('+', '%20'))
                            if '%20' in candidate:
                                candidates.append(candidate.replace('%20', ' '))
                                candidates.append(candidate.replace('%20', '+'))
                            if ' ' in candidate:
                                candidates.append(candidate.replace(' ', '+'))
                                candidates.append(candidate.replace(' ', '%20'))
                        
                        # Remove duplicates while preserving order
                        seen = set()
                        for candidate in candidates:
                            if candidate not in seen:
                                smartdrive_file_paths.append(candidate)
                                seen.add(candidate)
                    
                    onyx_user_id = await get_current_onyx_user_id(request)
                    
                    # DEBUG: Log the mapping attempt
                    logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                    logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                    logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                    
                    file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                    
                    if file_ids:
                        logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                        # Extract file context from SmartDrive files
                        file_context = await extract_file_context_from_onyx(file_ids, [], cookies)
                    else:
                        logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths")
                        file_context = ""
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Quiz"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[QUIZ_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[QUIZ_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[QUIZ_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[QUIZ_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[QUIZ_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[QUIZ_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[QUIZ_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/quiz/edit")
async def quiz_edit(payload: QuizEditRequest, request: Request):
    """Edit quiz content with streaming response"""
    logger.info(f"[QUIZ_EDIT_START] Quiz edit initiated")
    logger.info(f"[QUIZ_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[QUIZ_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[QUIZ_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[QUIZ_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[QUIZ_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Quiz Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,
        "originalContent": payload.currentContent,
        "editMode": True,
        "isCleanContent": payload.isCleanContent
    }

    # Add context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName
    if payload.questionTypes:
        wiz_payload["questionTypes"] = payload.questionTypes
    if payload.questionCount:
        wiz_payload["questionCount"] = payload.questionCount

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided
    if payload.fromText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload)

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[QUIZ_EDIT_STREAM] Starting streamer")
        logger.info(f"[QUIZ_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for quiz editing
        logger.info(f"[QUIZ_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for quiz editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[QUIZ_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[QUIZ_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[QUIZ_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[QUIZ_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[QUIZ_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[QUIZ_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        
        # NEW: Cache the quiz content for later finalization
        if chat_id:
            QUIZ_PREVIEW_CACHE[chat_id] = assistant_reply
            logger.info(f"[QUIZ_PREVIEW_CACHE] Cached quiz content for chat_id={chat_id}, length={len(assistant_reply)}")
        
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/quiz/finalize")
async def quiz_finalize(payload: QuizWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize quiz creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    
    # Get user ID and deduct credits for quiz creation
    try:
        credits_needed = calculate_product_credits("quiz")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "Quiz creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for quiz creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for quiz creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this quiz finalization to prevent duplicates
    quiz_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this quiz is already being processed
    if quiz_key in ACTIVE_QUIZ_FINALIZE_KEYS:
        logger.warning(f"[QUIZ_FINALIZE_DUPLICATE] Quiz finalization already in progress for key: {quiz_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Quiz finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(quiz_key)
    QUIZ_FINALIZE_TIMESTAMPS[quiz_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Cleaned up stale quiz key: {stale_key}")
    
    try:
        # NEW: Check for user edits and decide strategy (like in Course Outline)
        use_direct_parser = False
        use_ai_parser = True
        
        if payload.hasUserEdits and payload.originalContent:
            # User has made edits - check if they're significant
            any_changes = _any_quiz_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No quiz changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Quiz changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # Ensure quiz template exists
        template_id = await _ensure_quiz_template(pool)
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = None
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip()}"
                        logger.info(f"[QUIZ_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[QUIZ_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[QUIZ_FINALIZE_NAMING] Failed to fetch outline name for quiz naming: {e}")
                # Continue with plain title if outline fetch fails
                project_name = payload.lesson.strip() if payload.lesson.strip() else "Untitled Quiz"
        
        logger.info(f"[QUIZ_FINALIZE_START] Starting quiz finalization for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] quiz_key: {quiz_key}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] isCleanContent: {payload.isCleanContent}")
        logger.info(f"[QUIZ_FINALIZE_PARAMS] use_direct_parser: {use_direct_parser}")
        
        # Fast-path: check if aiResponse is already JSON (like presentations)
        try:
            candidate = json.loads(payload.aiResponse)
            # Basic schema checks for QuizData
            if isinstance(candidate, dict) and 'quizTitle' in candidate and 'questions' in candidate:
                logger.info("[QUIZ_FINALIZE_FASTPATH] aiResponse is valid JSON, using directly without AI parsing")
                parsed_quiz = QuizData(**candidate)  # type: ignore
                use_direct_parser = False
                use_ai_parser = False
                # Proceed to save with parsed_quiz below
            else:
                logger.info("[QUIZ_FINALIZE_FASTPATH] aiResponse is JSON but missing required fields; falling back to AI parser")
        except Exception as e:
            logger.info(f"[QUIZ_FINALIZE_FASTPATH] aiResponse is not JSON ({type(e).__name__}), will use AI parser")

        # NEW: Choose parsing strategy based on user edits
        if use_direct_parser and 'parsed_quiz' not in locals():
            # DIRECT PARSER PATH: Use cached content directly since no changes were made
            logger.info("Using direct parser path for quiz finalization")
            
            # Use the original content for parsing since no changes were made
            content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
            
            parsed_quiz: QuizData = await parse_ai_response_with_llm(
                ai_response=content_to_parse,
                project_name=project_name,
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
            logger.info("Direct parser path completed successfully")
        elif use_ai_parser and 'parsed_quiz' not in locals():
            # AI PARSER PATH: Use AI for parsing (original behavior)
            logger.info("Using AI parser path for quiz finalization")
            
            # NEW: Handle clean content (questions only) differently
            if payload.isCleanContent:
                # Parse edited question indices if provided
                edited_indices = set()
                if payload.editedQuestionIndices:
                    try:
                        edited_indices = set(int(idx.strip()) for idx in payload.editedQuestionIndices.split(',') if idx.strip())
                        logger.info(f"✅ [QUIZ_SELECTIVE_REGEN] Selective regeneration enabled for questions at indices: {sorted(edited_indices)}")
                    except Exception as e:
                        logger.warning(f"[QUIZ_SELECTIVE_REGEN] Failed to parse editedQuestionIndices: {e}, will regenerate all")
                
                if edited_indices:
                    logger.info(f"✅ [QUIZ_SELECTIVE_REGEN] Processing {len(edited_indices)} edited questions - will preserve unchanged ones")
                else:
                    logger.info("✅ [QUIZ_CLEAN_CONTENT] Processing clean content (questions only) - will generate NEW options AND answers for ALL")
                
                logger.info(f"[QUIZ_CLEAN_CONTENT] Input content preview: {payload.aiResponse[:200]}...")
                logger.info(f"[QUIZ_CLEAN_CONTENT] Language: {payload.language}, Question Types: {payload.questionTypes}")
                # For clean content, we need to generate complete quiz with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                IMPORTANT: The AI response contains ONLY quiz questions WITHOUT any options, answers, or explanations. You MUST generate ALL of these components from scratch:
                1. NEW Multiple choice options (A, B, C, D) for EVERY question - create completely new options that fit the question
                2. NEW Correct answers - determine which option should be correct
                3. NEW Explanations - write detailed explanations for each answer

                REQUIREMENTS:
                1. Extract the quiz title from the content or use the lesson name
                2. For EACH question, you MUST generate COMPLETELY NEW:
                   - "question_type": Determine appropriate type based on question (default to "multiple-choice")
                   - "question_text": Use the exact question text from the input
                   - "options": Create a BRAND NEW array with 4 options that are:
                     * Relevant to the specific question being asked
                     * Plausible but with only one correct answer
                     * Formatted as {{"id": "A", "text": "option text"}}, {{"id": "B", "text": "option text"}}, etc.
                   - "correct_option_id": Choose which option letter (A, B, C, or D) is the correct answer
                   - "explanation": Write a NEW detailed explanation explaining why the correct answer is right and why other options are wrong

                CRITICAL RULES FOR OPTION GENERATION:
                - DO NOT preserve any existing options - generate completely new ones
                - Each option must be unique and relevant to the question
                - Only one option should be correct
                - Options should be challenging but fair
                - Make distractors (wrong answers) plausible but clearly incorrect
                - Ensure options are in the correct language: {payload.language}
                - Question types allowed: {payload.questionTypes}
                
                EXAMPLE - If question is "What is the capital of France?":
                - Generate NEW options like: A) Paris, B) London, C) Berlin, D) Madrid
                - Set correct_option_id: "A"
                - Write explanation: "Paris is the capital of France. London is the capital of England, Berlin is the capital of Germany, and Madrid is the capital of Spain."
                """
            else:
                # Regular content with options and answers
                dynamic_instructions = f"""
                CRITICAL: You must output ONLY valid JSON in the exact format shown in the example. Do not include any natural language, explanations, or markdown formatting.

                The AI response contains quiz questions in natural language format. You need to convert this into a structured QuizData JSON format.

                REQUIREMENTS:
                1. Extract the quiz title from the content:
                   - Look for patterns like "**Course Name** : **Quiz** : **Quiz Title**" or "**Quiz** : **Quiz Title**"
                   - Extract ONLY the quiz title part (the last part after the last "**")
                   - For example: "**Code Optimization Course** : **Quiz** : **Common Optimization Techniques**" → extract "Common Optimization Techniques"
                   - For example: "**Quiz** : **JavaScript Basics**" → extract "JavaScript Basics"
                   - Do NOT include the course name or "Quiz" label in the title
                   - If no clear pattern is found, use the first meaningful title or heading
                
                2. For each question in the content, create a structured question object with:
                   - "question_type": MUST be one of: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
                   - "question_text": The actual question text
                   - For multiple-choice: "options" array with {{"id": "A", "text": "option text"}}, "correct_option_id": "A"
                   - For multi-select: "options" array, "correct_option_ids": ["A", "B"] (array)
                   - For matching: "prompts" array, "options" array, "correct_matches": {{"A": "1", "B": "2"}}
                   - For sorting: "items_to_sort" array, "correct_order": ["step1", "step2"]
                   - For open-answer: "acceptable_answers": ["answer1", "answer2"]
                   - "explanation": Explanation for the answer

                CRITICAL RULES:
                - Output ONLY the JSON object, no other text
                - Every question MUST have "question_type" field
                - Use exact field names as shown in the example
                - All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
                - If content is unclear, infer question types based on structure
                - Language: {payload.language}
                - TITLE EXTRACTION: Focus on extracting the specific quiz title, not the course name or generic labels
                """
            
            # Parse the quiz data using LLM - only call once with consistent project name
            parsed_quiz: QuizData = await parse_ai_response_with_llm(
                ai_response=payload.aiResponse,
                project_name=project_name,  # Use consistent project name
                target_model=QuizData,
                default_error_model_instance=QuizData(
                    quizTitle=project_name,
                    questions=[],
                    detectedLanguage=payload.language
                ),
                dynamic_instructions=dynamic_instructions,
                target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
            )
        
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Parsed quiz title: {parsed_quiz.quizTitle}")
        logger.info(f"[QUIZ_FINALIZE_PARSE] Number of questions: {len(parsed_quiz.questions)}")
        
        # NEW: Selective regeneration - merge original unchanged questions with newly generated edited ones
        if payload.isCleanContent and payload.editedQuestionIndices and payload.originalContent:
            try:
                edited_indices = set(int(idx.strip()) for idx in payload.editedQuestionIndices.split(',') if idx.strip())
                if edited_indices and len(edited_indices) < len(parsed_quiz.questions):
                    logger.info(f"[QUIZ_SELECTIVE_MERGE] Starting selective merge for {len(edited_indices)} edited questions")
                    
                    # Parse original content to extract original questions
                    original_quiz = None
                    try:
                        # Try parsing as JSON first
                        original_parsed = json.loads(payload.originalContent)
                        if isinstance(original_parsed, dict) and 'quizTitle' in original_parsed and 'questions' in original_parsed:
                            original_quiz = QuizData(**original_parsed)
                            logger.info(f"[QUIZ_SELECTIVE_MERGE] Parsed original content as JSON: {len(original_quiz.questions)} questions")
                    except:
                        # If not JSON, try parsing with LLM
                        logger.info(f"[QUIZ_SELECTIVE_MERGE] Original content not JSON, parsing with LLM")
                        original_quiz = await parse_ai_response_with_llm(
                            ai_response=payload.originalContent,
                            project_name=project_name,
                            target_model=QuizData,
                            default_error_model_instance=QuizData(quizTitle=project_name, questions=[], detectedLanguage=payload.language),
                            dynamic_instructions=f"Parse this quiz content into structured JSON. Preserve all options, answers, and explanations exactly as they are.",
                            target_json_example=DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM
                        )
                    
                    if original_quiz and len(original_quiz.questions) == len(parsed_quiz.questions):
                        logger.info(f"[QUIZ_SELECTIVE_MERGE] Original quiz has {len(original_quiz.questions)} questions, merging...")
                        
                        # Merge: use original options/explanations for unchanged questions
                        for i in range(len(parsed_quiz.questions)):
                            if i not in edited_indices:
                                # This question wasn't edited - restore original options/explanations
                                if hasattr(original_quiz.questions[i], 'options'):
                                    parsed_quiz.questions[i].options = original_quiz.questions[i].options
                                if hasattr(original_quiz.questions[i], 'correct_option_id'):
                                    parsed_quiz.questions[i].correct_option_id = original_quiz.questions[i].correct_option_id
                                if hasattr(original_quiz.questions[i], 'correct_option_ids'):
                                    parsed_quiz.questions[i].correct_option_ids = original_quiz.questions[i].correct_option_ids
                                if hasattr(original_quiz.questions[i], 'explanation'):
                                    parsed_quiz.questions[i].explanation = original_quiz.questions[i].explanation
                                if hasattr(original_quiz.questions[i], 'correct_matches'):
                                    parsed_quiz.questions[i].correct_matches = original_quiz.questions[i].correct_matches
                                if hasattr(original_quiz.questions[i], 'correct_order'):
                                    parsed_quiz.questions[i].correct_order = original_quiz.questions[i].correct_order
                                if hasattr(original_quiz.questions[i], 'acceptable_answers'):
                                    parsed_quiz.questions[i].acceptable_answers = original_quiz.questions[i].acceptable_answers
                                if hasattr(original_quiz.questions[i], 'prompts'):
                                    parsed_quiz.questions[i].prompts = original_quiz.questions[i].prompts
                                if hasattr(original_quiz.questions[i], 'items_to_sort'):
                                    parsed_quiz.questions[i].items_to_sort = original_quiz.questions[i].items_to_sort
                                
                                logger.info(f"[QUIZ_SELECTIVE_MERGE] ✅ Q{i+1}: Preserved original options/explanations (unchanged)")
                            else:
                                logger.info(f"[QUIZ_SELECTIVE_MERGE] ✅ Q{i+1}: Using newly generated options/explanations (edited)")
                        
                        logger.info(f"[QUIZ_SELECTIVE_MERGE] ✅ Merge complete: {len(edited_indices)} regenerated, {len(parsed_quiz.questions) - len(edited_indices)} preserved")
                    else:
                        logger.warning(f"[QUIZ_SELECTIVE_MERGE] Cannot merge: original has {len(original_quiz.questions) if original_quiz else 0} questions vs current {len(parsed_quiz.questions)}")
            except Exception as e:
                logger.error(f"[QUIZ_SELECTIVE_MERGE] Error during selective merge: {e}", exc_info=True)
                logger.warning(f"[QUIZ_SELECTIVE_MERGE] Falling back to using all newly generated content")
        
        # Log details about generated options if this was clean content
        if payload.isCleanContent and len(parsed_quiz.questions) > 0:
            logger.info(f"[QUIZ_CLEAN_CONTENT_RESULT] ✅ Generated complete quiz from clean questions")
            for i, q in enumerate(parsed_quiz.questions[:3]):  # Log first 3 questions
                has_options = hasattr(q, 'options') and q.options
                has_explanation = hasattr(q, 'explanation') and q.explanation
                logger.info(f"[QUIZ_CLEAN_CONTENT_RESULT] Q{i+1}: {q.question_text[:50]}... | Has options: {has_options} ({len(q.options) if has_options else 0}) | Has explanation: {has_explanation}")
        
        # Detect language if not provided
        if not parsed_quiz.detectedLanguage:
            parsed_quiz.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no questions, create a basic quiz structure
        if not parsed_quiz.questions:
            logger.warning(f"[QUIZ_FINALIZE_FALLBACK] LLM parsing failed for quiz, creating fallback structure")
            # Create a simple quiz with the AI response as content
            parsed_quiz.quizTitle = project_name
            parsed_quiz.questions = [
                {
                    "question_type": "open-answer",
                    "question_text": "Please review the quiz content and answer the questions.",
                    "acceptable_answers": ["See quiz content for answers"],
                    "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                }
            ]
        else:
            # Validate that all questions have the required question_type field
            valid_questions = []
            for i, question in enumerate(parsed_quiz.questions):
                if hasattr(question, 'question_type') and question.question_type:
                    valid_questions.append(question)
                else:
                    logger.warning(f"[QUIZ_FINALIZE_VALIDATION] Question {i} missing question_type, converting to open-answer")
                    # Convert to open-answer if question_type is missing
                    if hasattr(question, 'question_text'):
                        valid_questions.append({
                            "question_type": "open-answer",
                            "question_text": question.question_text,
                            "acceptable_answers": ["See original content for answer"],
                            "explanation": "This question was converted from the original format."
                        })
            
            if not valid_questions:
                logger.warning(f"[QUIZ_FINALIZE_VALIDATION] No valid questions found, creating fallback structure")
                parsed_quiz.questions = [
                    {
                        "question_type": "open-answer",
                        "question_text": "Please review the quiz content and answer the questions.",
                        "acceptable_answers": ["See quiz content for answers"],
                        "explanation": "This is a fallback quiz structure. The original content is preserved in the AI response."
                    }
                ]
            else:
                parsed_quiz.questions = valid_questions
        
        # Always use the consistent project name for database storage
        if not payload.outlineId:
            # Standalone quiz - use lesson title or default
            final_project_name = parsed_quiz.quizTitle or project_name
        else:
            final_project_name = project_name
        
        logger.info(f"[QUIZ_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # Determine if this is a standalone quiz or part of an outline
        is_standalone_quiz = payload.outlineId is None
        
        # For quiz components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW(), $11)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Quiz",  # product_type
                COMPONENT_NAME_QUIZ,  # microproduct_type - use the correct component name
                final_project_name,  # microproduct_name
                parsed_quiz.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_quiz,  # is_standalone
                payload.outlineId if payload.outlineId else None,  # course_id
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create quiz project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        logger.info(f"[QUIZ_FINALIZE_SUCCESS] Quiz finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_quiz}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[QUIZ_FINALIZE_ERROR] Error in quiz finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(quiz_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(quiz_key, None)
        logger.info(f"[QUIZ_FINALIZE_CLEANUP] Removed quiz_key from active set: {quiz_key}")

@app.delete("/api/custom/lessons/{lesson_id}", status_code=204)
async def delete_lesson(lesson_id: int, onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """Delete a lesson project permanently"""
    try:
        async with pool.acquire() as conn:
            # First, verify the lesson exists and belongs to the user
            lesson = await conn.fetchrow(
                "SELECT id, project_name, microproduct_type FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if not lesson:
                raise HTTPException(status_code=404, detail="Lesson not found or not owned by user")
            
            # Check if this is actually a lesson (not a training plan/course outline)
            if lesson['microproduct_type'] in ('Training Plan', 'Course Outline'):
                raise HTTPException(status_code=400, detail="Cannot delete training plans or course outlines. Please delete individual lessons instead.")
            
            # Delete the lesson
            result = await conn.execute(
                "DELETE FROM projects WHERE id = $1 AND onyx_user_id = $2",
                lesson_id, onyx_user_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(status_code=404, detail="Lesson not found")
            
            logger.info(f"User {onyx_user_id} deleted lesson {lesson_id} ({lesson['project_name']})")
            return JSONResponse(status_code=204, content={})
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting lesson {lesson_id} for user {onyx_user_id}: {e}", exc_info=not IS_PRODUCTION)
        detail_msg = "An error occurred while deleting the lesson." if IS_PRODUCTION else f"Database error during lesson deletion: {str(e)}"
        raise HTTPException(status_code=500, detail=detail_msg)


# Default quiz JSON example for LLM parsing
DEFAULT_QUIZ_JSON_EXAMPLE_FOR_LLM = """
{
  "quizTitle": "Example Quiz with All Question Types",
  "questions": [
    {
      "question_type": "multiple-choice",
      "question_text": "What is the capital of France?",
      "options": [
        {"id": "A", "text": "London"},
        {"id": "B", "text": "Paris"},
        {"id": "C", "text": "Berlin"},
        {"id": "D", "text": "Madrid"}
      ],
      "correct_option_id": "B",
      "explanation": "Paris is the capital and largest city of France."
    },
    {
      "question_type": "multi-select",
      "question_text": "Which of the following are programming languages?",
      "options": [
        {"id": "A", "text": "Python"},
        {"id": "B", "text": "HTML"},
        {"id": "C", "text": "JavaScript"},
        {"id": "D", "text": "CSS"}
      ],
      "correct_option_ids": ["A", "C"],
      "explanation": "Python and JavaScript are programming languages, while HTML and CSS are markup/styling languages."
    },
    {
      "question_type": "matching",
      "question_text": "Match the countries with their capitals:",
      "prompts": [
        {"id": "A", "text": "Germany"},
        {"id": "B", "text": "Italy"},
        {"id": "C", "text": "Spain"}
      ],
      "options": [
        {"id": "1", "text": "Berlin"},
        {"id": "2", "text": "Rome"},
        {"id": "3", "text": "Madrid"}
      ],
      "correct_matches": {"A": "1", "B": "2", "C": "3"},
      "explanation": "Germany-Berlin, Italy-Rome, Spain-Madrid are the correct country-capital pairs."
    },
    {
      "question_type": "sorting",
      "question_text": "Arrange the following steps in the correct order for a sales process:",
      "items_to_sort": [
        {"id": "step1", "text": "Identify customer needs"},
        {"id": "step2", "text": "Present solution"},
        {"id": "step3", "text": "Handle objections"},
        {"id": "step4", "text": "Close the sale"}
      ],
      "correct_order": ["step1", "step2", "step3", "step4"],
      "explanation": "The sales process follows a logical sequence: first understand needs, then present solutions, address concerns, and finally close."
    },
    {
      "question_type": "open-answer",
      "question_text": "What are the three key elements of an effective elevator pitch?",
      "acceptable_answers": [
        "Problem, Solution, Call to Action",
        "Problem statement, Your solution, What you want them to do next",
        "The issue, How you solve it, What action to take"
      ],
      "explanation": "An effective elevator pitch should clearly state the problem, present your solution, and include a clear call to action."
    }
  ],
  "detectedLanguage": "en"
}

CRITICAL REQUIREMENTS:
- Output ONLY the JSON object, no other text or formatting
- Every question MUST have "question_type" field with exact values: "multiple-choice", "multi-select", "matching", "sorting", "open-answer"
- Use exact field names as shown above
- All IDs must be strings: "A", "B", "C", "D" or "1", "2", "3"
- The "question_type" field is MANDATORY for every question
"""

# Default text presentation JSON example for LLM parsing
DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM = """
{
  "textTitle": "Organizing Neighbor Support During Crisis Situations",
  "contentBlocks": [
    { "type": "headline", "level": 2, "text": "Introduction" },
    { "type": "paragraph", "text": "In times of crisis, community support can be a vital lifeline that makes the difference between struggle and survival. Organizing neighbor support not only helps those in immediate need but also strengthens community bonds and creates lasting relationships that extend far beyond the emergency situation. This presentation will outline effective strategies for mobilizing neighbors during emergencies, ensuring everyone is prepared, supported, and connected to resources they may need in challenging times." },
    { "type": "headline", "level": 2, "text": "📋 Importance of Community Support", "iconName": "info" },
    {
      "type": "bullet_list",
      "items": [
        "**Strengthens Resilience**: Communities that actively support each other can recover significantly faster from crises and emerge stronger than before. When neighbors work together, they pool resources, share knowledge, and create support networks that help everyone bounce back more quickly from difficult situations.",
        "**Enhances Safety**: Neighbors looking out for one another can dramatically reduce risks and improve overall safety for everyone in the community. Regular check-ins, shared awareness of potential hazards, and coordinated emergency responses create a protective network that benefits all residents, especially vulnerable individuals who may need extra assistance.",
        "**Builds Trust**: Collaborative efforts during difficult times foster deep trust and meaningful cooperation among residents that can last for years. When people work together to overcome challenges, they develop stronger relationships, better communication patterns, and a shared sense of purpose that transforms neighborhoods into genuine communities."
      ]
    },
    { "type": "headline", "level": 2, "text": "🚀 Steps to Organize Support", "iconName": "info" },
    {
      "type": "numbered_list",
      "items": [
        {
          "type": "bullet_list",
          "items": [
            { "type": "headline", "level": 3, "text": "Identify Key Resources" },
            { "type": "paragraph", "text": "Begin by thoroughly assessing what resources are currently available within your community, including essential supplies like food, water, medical equipment, generators, and other emergency items that neighbors may have on hand. Create a detailed inventory of these resources so you know exactly what's available when crisis strikes." },
            { "type": "paragraph", "text": "Create a comprehensive list of skills and services that neighbors can offer to the community, such as medical assistance, transportation services, childcare, pet care, technical support, language translation, or specialized knowledge that could prove valuable during emergencies. This skills inventory becomes an invaluable resource when coordinating community responses to various challenges." }
          ],
          "iconName": "none"
        },
        {
          "type": "bullet_list",
          "items": [
            { "type": "headline", "level": 3, "text": "Establish Communication Channels" },
            { "type": "paragraph", "text": "Set up a dedicated group chat or social media group specifically for quick emergency updates and ongoing communication between neighbors. Choose platforms that most community members already use and ensure everyone knows how to access and use these channels effectively, even during power outages or internet disruptions when possible." },
            { "type": "paragraph", "text": "Use multiple communication methods including community bulletin boards, flyers posted in common areas, door-to-door notifications, and email lists to share critical information with all residents. This multi-channel approach ensures that even those without smartphones or internet access can stay informed and connected to community support networks." }
          ],
          "iconName": "none"
        },
        {
          "type": "bullet_list",
          "items": [
            { "type": "headline", "level": 3, "text": "Create a Support Network" },
            { "type": "paragraph", "text": "Organize a dedicated group of reliable volunteers who are committed to coordinating support efforts and serving as points of contact during emergencies. Ensure this core team represents diverse areas of your neighborhood so coverage is comprehensive and volunteers can respond quickly to nearby residents who need assistance." },
            { "type": "paragraph", "text": "Assign specific roles and responsibilities to volunteers based on their unique skills, availability, and comfort levels with different tasks. For example, designate coordinators to manage overall efforts, communicators to disseminate information, logistics specialists to manage resources, and field responders to provide direct assistance to neighbors in need." }
          ],
          "iconName": "none"
        },
        {
          "type": "bullet_list",
          "items": [
            { "type": "headline", "level": 3, "text": "Plan Regular Meetings" },
            { "type": "paragraph", "text": "Schedule regular check-in meetings, whether in-person or virtual, to discuss evolving community needs, share updates about available resources, address concerns, and maintain strong connections between neighbors. These meetings should occur both during calm periods (for planning and relationship-building) and during crises (for coordination and rapid response)." },
            { "type": "paragraph", "text": "Use these meetings strategically to build deeper relationships, strengthen the support network, share success stories, and ensure everyone feels valued and heard within the community. Regular gatherings also help identify emerging leaders, uncover hidden resources or skills, and keep the momentum of community engagement strong even when there's no immediate crisis." }
          ],
          "iconName": "none"
        },
        {
          "type": "bullet_list",
          "items": [
            { "type": "headline", "level": 3, "text": "Develop Emergency Plans" },
            { "type": "paragraph", "text": "Create a detailed, written community emergency plan that clearly outlines specific roles, responsibilities, communication protocols, and action steps for various crisis scenarios your neighborhood might face. Include contact information for all key volunteers, locations of emergency supplies, evacuation routes, and designated meeting points so everyone knows exactly what to do when disaster strikes." },
            { "type": "paragraph", "text": "Ensure that every household has easy access to the emergency plan and clearly understands how to access critical resources, request assistance, and contribute their own skills and resources to support others. Regularly review and update the plan as the community evolves, new members join the neighborhood, or lessons are learned from past experiences." }
          ],
          "iconName": "none"
        }
      ]
    },
    { "type": "headline", "level": 2, "text": "🔑 Key Considerations", "iconName": "info" },
    {
      "type": "bullet_list",
      "items": [
        "**Inclusivity**: Ensure that all community members, regardless of age, ability, language spoken, or socioeconomic status, feel genuinely included, valued, and welcomed in the planning process. Actively seek input from diverse voices and make sure your support systems can accommodate the unique needs of every resident, including those who may face barriers to participation.",
        "**Cultural Sensitivity**: Be deeply aware of and respectful toward cultural differences, religious practices, dietary restrictions, communication preferences, and other cultural factors within your community. Take time to learn about the diverse backgrounds of your neighbors and ensure that support efforts honor and accommodate these differences rather than imposing a one-size-fits-all approach.",
        "**Flexibility**: Be fully prepared to adapt your plans, strategies, and approaches as situations evolve and new information becomes available. Crisis situations are inherently unpredictable, so maintaining flexibility and being willing to adjust your response based on real-time needs and feedback is essential for effective community support and successful outcomes."
      ]
    },
    { "type": "headline", "level": 2, "text": "💡 Recommendations for Success", "iconName": "info" },
    {
      "type": "bullet_list",
      "items": [
        "**Engage Local Organizations**: Actively partner with local nonprofits, faith-based organizations, government agencies, schools, and businesses to access additional support, resources, funding, expertise, and volunteer networks. These partnerships can dramatically expand your community's capacity to respond to crises and provide professional guidance on best practices for emergency preparedness and response coordination.",
        "**Promote Preparedness**: Consistently encourage and help neighbors prepare their own individual emergency kits, family communication plans, evacuation strategies, and disaster supplies so they can be self-sufficient for at least 72 hours. Offer workshops, share checklists, and provide guidance on essential supplies, document preparation, and household emergency planning to increase overall community resilience.",
        "**Celebrate Successes**: Regularly acknowledge, appreciate, and celebrate the efforts, contributions, and dedication of volunteers and community members who step up to help others. Public recognition, thank-you events, success story sharing, and appreciation ceremonies foster a positive, encouraging environment that motivates continued engagement and attracts new volunteers to join your community support network."
      ]
    },
    { "type": "headline", "level": 2, "text": "📌 Conclusion", "iconName": "info" },
    { "type": "paragraph", "text": "Organizing neighbor support during crisis situations is absolutely essential for building resilient, connected, and caring communities that can weather any storm together. By following these comprehensive steps and fostering a genuine spirit of collaboration, mutual respect, and shared responsibility, we can ensure that everyone is prepared, supported, and protected in times of need. Together, through coordinated action and compassionate support, we can make a truly significant and lasting difference in our neighborhoods and the lives of all our neighbors." },
    { "type": "alert", "alertType": "info", "title": "Recommendation", "text": "To effectively implement these strategies and build genuine community connections, consider hosting an engaging community event, neighborhood meeting, or workshop to discuss, plan, and organize neighbor support initiatives in a welcoming, collaborative atmosphere. This can be an excellent way to engage residents of all ages, build a stronger support network, identify volunteer leaders, and create the foundation for lasting community resilience." }
  ],
  "detectedLanguage": "en"
}
"""

# Text Presentation Pydantic models
class TextPresentationWizardPreview(BaseModel):
    outlineId: Optional[int] = None
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    prompt: Optional[str] = None
    language: str = "en"
    length: str = "medium"
    styles: Optional[str] = None
    fromFiles: bool = False
    folderIds: Optional[str] = None
    fileIds: Optional[str] = None
    fromText: bool = False
    textMode: Optional[str] = None
    userText: Optional[str] = None
    fromKnowledgeBase: bool = False
    # NEW: connector context for creation from selected connectors
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None  # comma-separated connector IDs
    connectorSources: Optional[str] = None  # comma-separated connector sources
    # NEW: SmartDrive file paths for combined connector + file context
    selectedFiles: Optional[str] = None  # comma-separated SmartDrive file paths
    chatSessionId: Optional[str] = None

class TextPresentationWizardFinalize(BaseModel):
    aiResponse: str
    prompt: str
    outlineId: Optional[int] = None  # Add outlineId for consistent naming
    lesson: Optional[str] = None
    courseName: Optional[str] = None
    language: str = "en"
    chatSessionId: Optional[str] = None
    # NEW: folder context for creation from inside a folder
    folderId: Optional[str] = None  # single folder ID when coming from inside a folder
    # NEW: User edits tracking (like in Quiz)
    hasUserEdits: Optional[bool] = False
    originalContent: Optional[str] = None
    isCleanContent: Optional[bool] = False
    # NEW: indices of edited sections for selective regeneration (comma-separated: "0,2,5")
    editedSectionIndices: Optional[str] = None
    # Connector context fields
    fromConnectors: Optional[bool] = None
    connectorIds: Optional[str] = None
    connectorSources: Optional[str] = None

class TextPresentationEditRequest(BaseModel):
    content: str
    editPrompt: str
    language: Optional[str] = "en"  # Add language field with default fallback
    chatSessionId: Optional[str] = None
    isCleanContent: Optional[bool] = False

@app.post("/api/custom/text-presentation/generate")
async def text_presentation_generate(payload: TextPresentationWizardPreview, request: Request):
    """Generate text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_START] Text presentation preview initiated")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] outlineId={payload.outlineId} lesson='{payload.lesson}' prompt='{payload.prompt[:50] if payload.prompt else None}...'")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] lang={payload.language}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] fromFiles={payload.fromFiles} fromText={payload.fromText} textMode={payload.textMode}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] userText length={len(payload.userText) if payload.userText else 0}")
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_PARAMS] folderIds={payload.folderIds} fileIds={payload.fileIds}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_PREVIEW_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Creating new chat session")
        try:
            # Check if this is a Knowledge Base search request
            use_search_persona = hasattr(payload, 'fromKnowledgeBase') and payload.fromKnowledgeBase
            persona_id = await get_contentbuilder_persona_id(cookies, use_search_persona=use_search_persona)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Got persona ID: {persona_id} (Knowledge Base search: {use_search_persona})")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_PREVIEW_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_PREVIEW_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation",
        "prompt": payload.prompt or "Create a comprehensive text presentation",
        "language": payload.language,
        "length": payload.length,
    }

    # Add styles if provided
    if payload.styles:
        wiz_payload["styles"] = payload.styles

    # Add outline context if provided
    if payload.outlineId:
        wiz_payload["outlineId"] = payload.outlineId
    if payload.lesson:
        wiz_payload["lesson"] = payload.lesson
    if payload.courseName:
        wiz_payload["courseName"] = payload.courseName

    # Add file context if provided
    if payload.fromFiles:
        wiz_payload["fromFiles"] = True
        if payload.folderIds:
            wiz_payload["folderIds"] = payload.folderIds
        if payload.fileIds:
            wiz_payload["fileIds"] = payload.fileIds

    # Add text context if provided - send directly in wizard request (no file conversion)
    if payload.fromText and payload.userText:
        wiz_payload["fromText"] = True
        wiz_payload["textMode"] = payload.textMode
        
        text_length = len(payload.userText)
        logger.info(f"Processing text input: mode={payload.textMode}, length={text_length} chars")
        
        # Check if we're using hybrid approach (files present) or direct approach (text-only)
        if should_use_hybrid_approach(payload):
            # Hybrid approach: create virtual files for text (existing behavior for file-based scenarios)
            if text_length > LARGE_TEXT_THRESHOLD:
                logger.info(f"Text exceeds large threshold ({LARGE_TEXT_THRESHOLD}), using virtual file system for hybrid approach")
                try:
                    virtual_file_id = await create_virtual_text_file(payload.userText, cookies)
                    wiz_payload["virtualFileId"] = virtual_file_id
                    wiz_payload["textCompressed"] = False
                    logger.info(f"Successfully created virtual file for large text ({text_length} chars) -> file ID: {virtual_file_id}")
                except Exception as e:
                    logger.error(f"Failed to create virtual file for large text: {e}")
                    # Fallback to compression
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Fallback to compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Use compression for hybrid approach with medium/small text
                if text_length > TEXT_SIZE_THRESHOLD:
                    compressed_text = compress_text(payload.userText)
                    wiz_payload["userText"] = compressed_text
                    wiz_payload["textCompressed"] = True
                    logger.info(f"Using compressed text for hybrid approach ({text_length} -> {len(compressed_text)} chars)")
                else:
                    wiz_payload["userText"] = payload.userText
                    wiz_payload["textCompressed"] = False
        else:
            # Direct approach: send text directly in wizard request (no file conversion)
            logger.info(f"✅ Using DIRECT approach: sending text directly in wizard request ({text_length} chars)")
            
            # For very large texts, use compression to reduce payload size
            if text_length > TEXT_SIZE_THRESHOLD:
                compressed_text = compress_text(payload.userText)
                wiz_payload["userText"] = compressed_text
                wiz_payload["textCompressed"] = True
                logger.info(f"Compressed text for direct wizard request ({text_length} -> {len(compressed_text)} chars)")
            else:
                # Send text directly without compression
                wiz_payload["userText"] = payload.userText
                wiz_payload["textCompressed"] = False
    elif payload.fromText and not payload.userText:
        # Log this problematic case to help with debugging
        logger.warning(f"Received fromText=True but userText is empty or None. This may cause infinite loading. textMode={payload.textMode}")
        # Don't process fromText if userText is empty to avoid confusing the AI
    elif payload.fromText:
        logger.warning(f"Received fromText=True but userText evaluation failed. userText type: {type(payload.userText)}, value: {repr(payload.userText)[:100] if payload.userText else 'None'}")

    # Add Knowledge Base context if provided
    if payload.fromKnowledgeBase:
        wiz_payload["fromKnowledgeBase"] = True
        logger.info(f"Added Knowledge Base context for text presentation generation")

    # Decompress text if it was compressed
    if wiz_payload.get("textCompressed") and wiz_payload.get("userText"):
        try:
            decompressed_text = decompress_text(wiz_payload["userText"])
            wiz_payload["userText"] = decompressed_text
            wiz_payload["textCompressed"] = False  # Mark as decompressed
            logger.info(f"Decompressed text for assistant ({len(decompressed_text)} chars)")
        except Exception as e:
            logger.error(f"Failed to decompress text: {e}")
            # Continue with original text if decompression fails
    
    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # Force JSON-ONLY preview output for Text Presentation to enable immediate parsed preview (like Course Outline)
    try:
        json_preview_instructions_text = f"""

CRITICAL PREVIEW OUTPUT FORMAT (JSON-ONLY):
You MUST output ONLY a single JSON object for the Text Presentation preview, strictly following this example structure:
{DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM}
Do NOT include code fences, markdown or extra commentary. Return JSON object only.

CRITICAL SCHEMA AND CONTENT RULES (MUST MATCH FINAL FORMAT):
- Include exact fields: textTitle, contentBlocks[], detectedLanguage.
- contentBlocks is an ordered array. Each block MUST include type and associated fields per spec (headline|paragraph|bullet_list|numbered_list|table, etc.).
- Preserve original language across all text.
"""
        wizard_message = wizard_message + json_preview_instructions_text
        logger.info("[TEXT_PRESENTATION_PREVIEW] Added JSON-only preview instructions")
    except Exception as e:
        logger.warning(f"[TEXT_PRESENTATION_PREVIEW_JSON_INSTR] Failed to append JSON-only preview instructions: {e}")

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0
        total_bytes_received = 0
        done_received = False

        # Use longer timeout for large text processing to prevent AI memory issues
        timeout_duration = 300.0 if wiz_payload.get("virtualFileId") else None  # 5 minutes for large texts
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Starting streamer with timeout: {timeout_duration} seconds")
        logger.info(f"[TEXT_PRESENTATION_PREVIEW_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Check if we should use hybrid approach (Onyx for context + OpenAI for generation)
        if should_use_hybrid_approach(payload):
            logger.info(f"[TEXT_PRESENTATION_STREAM] 🔄 USING HYBRID APPROACH (Onyx context extraction + OpenAI generation)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}, fromKnowledgeBase={getattr(payload, 'fromKnowledgeBase', None)}, fromConnectors={getattr(payload, 'fromConnectors', None)}, connectorSources={getattr(payload, 'connectorSources', None)}")
            
            try:
                # Step 1: Extract context from Onyx
                if payload.fromConnectors and payload.connectorSources:
                    # For connector-based filtering, extract context from specific connectors
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from connectors: {payload.connectorSources}")
                    file_context = await extract_connector_context_from_onyx(payload.connectorSources, payload.prompt, cookies)
                elif payload.fromConnectors and payload.selectedFiles:
                    # SmartDrive files only (no connectors)
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from SmartDrive files only: {payload.selectedFiles}")
                    
                    # Map SmartDrive paths to Onyx file IDs
                    raw_paths = [path.strip() for path in payload.selectedFiles.split(',') if path.strip()]
                    
                    # Normalize paths to handle URL encoding and character variations
                    smartdrive_file_paths = []
                    for path in raw_paths:
                        # Try multiple variations to match database records
                        from urllib.parse import unquote, quote
                        import re
                        
                        candidates = []
                        # Base variants
                        candidates.append(path)
                        try:
                            decoded_path = unquote(path)
                            candidates.append(decoded_path)
                        except:
                            decoded_path = path
                        try:
                            encoded_path = quote(path, safe='/')
                            candidates.append(encoded_path)
                        except:
                            pass
                        
                        # Character normalization variants
                        for candidate in list(candidates):
                            # Handle spaces encoded as + or %20
                            if '+' in candidate:
                                candidates.append(candidate.replace('+', ' '))
                                candidates.append(candidate.replace('+', '%20'))
                            if '%20' in candidate:
                                candidates.append(candidate.replace('%20', ' '))
                                candidates.append(candidate.replace('%20', '+'))
                            if ' ' in candidate:
                                candidates.append(candidate.replace(' ', '+'))
                                candidates.append(candidate.replace(' ', '%20'))
                        
                        # Remove duplicates while preserving order
                        seen = set()
                        for candidate in candidates:
                            if candidate not in seen:
                                smartdrive_file_paths.append(candidate)
                                seen.add(candidate)
                    
                    onyx_user_id = await get_current_onyx_user_id(request)
                    
                    # DEBUG: Log the mapping attempt
                    logger.info(f"[SMARTDRIVE_DEBUG] Attempting to map paths for user {onyx_user_id}:")
                    logger.info(f"[SMARTDRIVE_DEBUG] Raw paths: {raw_paths}")
                    logger.info(f"[SMARTDRIVE_DEBUG] Normalized paths: {smartdrive_file_paths}")
                    
                    file_ids = await map_smartdrive_paths_to_onyx_files(smartdrive_file_paths, onyx_user_id)
                    
                    if file_ids:
                        logger.info(f"[HYBRID_CONTEXT] Mapped {len(file_ids)} SmartDrive files to Onyx file IDs")
                        # Extract file context from SmartDrive files
                        file_context = await extract_file_context_from_onyx(file_ids, [], cookies)
                    else:
                        logger.warning(f"[HYBRID_CONTEXT] No Onyx file IDs found for SmartDrive paths")
                        file_context = ""
                elif payload.fromKnowledgeBase:
                    # For Knowledge Base searches, extract context from the entire Knowledge Base
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from entire Knowledge Base for topic: {payload.prompt}")
                    file_context = await extract_knowledge_base_context(payload.prompt, cookies)
                else:
                    # For file-based searches, extract context from specific files/folders
                    folder_ids_list = []
                    file_ids_list = []
                    
                    if payload.fromFiles and payload.folderIds:
                        folder_ids_list = parse_id_list(payload.folderIds, "folder")
                        logger.info(f"[HYBRID_CONTEXT] Parsed folder IDs: {folder_ids_list}")
                    
                    if payload.fromFiles and payload.fileIds:
                        file_ids_list = parse_id_list(payload.fileIds, "file")
                        logger.info(f"[HYBRID_CONTEXT] Parsed file IDs: {file_ids_list}")
                    
                    # Add virtual file ID if created for large text
                    if wiz_payload.get("virtualFileId"):
                        file_ids_list.append(wiz_payload["virtualFileId"])
                        logger.info(f"[HYBRID_CONTEXT] Added virtual file ID {wiz_payload['virtualFileId']} to file_ids_list")
                    
                    # Extract context from Onyx
                    logger.info(f"[HYBRID_CONTEXT] Extracting context from {len(file_ids_list)} files and {len(folder_ids_list)} folders")
                    file_context = await extract_file_context_from_onyx(file_ids_list, folder_ids_list, cookies)
                
                # Step 2: Use OpenAI with enhanced context
                logger.info(f"[HYBRID_STREAM] Starting OpenAI generation with enhanced context")
                async for chunk_data in stream_hybrid_response(wizard_message, file_context, "Text Presentation"):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[HYBRID_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[HYBRID_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[HYBRID_STREAM] Sent keep-alive")
                
                logger.info(f"[HYBRID_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                
            except Exception as e:
                logger.error(f"[HYBRID_STREAM_ERROR] Error in hybrid streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return
        
        # FALLBACK: Use OpenAI directly when no file context
        else:
            logger.info(f"[TEXT_PRESENTATION_STREAM] ✅ USING OPENAI DIRECT STREAMING (no file context)")
            logger.info(f"[TEXT_PRESENTATION_STREAM] Payload check: fromFiles={getattr(payload, 'fromFiles', None)}, fileIds={getattr(payload, 'fileIds', None)}, folderIds={getattr(payload, 'folderIds', None)}")
            try:
                async for chunk_data in stream_openai_response(wizard_message):
                    if chunk_data["type"] == "delta":
                        delta_text = chunk_data["text"]
                        assistant_reply += delta_text
                        chunks_received += 1
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                        yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                    elif chunk_data["type"] == "error":
                        logger.error(f"[TEXT_PRESENTATION_OPENAI_ERROR] {chunk_data['text']}")
                        yield (json.dumps(chunk_data) + "\n").encode()
                        return
                    
                    # Send keep-alive every 8s
                    now = asyncio.get_event_loop().time()
                    if now - last_send > 8:
                        yield b" "
                        last_send = now
                        logger.debug(f"[TEXT_PRESENTATION_OPENAI_STREAM] Sent keep-alive")
                
                logger.info(f"[TEXT_PRESENTATION_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
                yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()
                return
                    
            except Exception as e:
                logger.error(f"[TEXT_PRESENTATION_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
                yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
                return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/custom/text-presentation/edit")
async def text_presentation_edit(payload: TextPresentationEditRequest, request: Request):
    """Edit text presentation content with streaming response"""
    logger.info(f"[TEXT_PRESENTATION_EDIT_START] Text presentation edit initiated")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] editPrompt='{payload.editPrompt[:50]}...'")
    logger.info(f"[TEXT_PRESENTATION_EDIT_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
    
    cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
    logger.info(f"[TEXT_PRESENTATION_EDIT_AUTH] Cookie present: {bool(cookies[ONYX_SESSION_COOKIE_NAME])}")

    if payload.chatSessionId:
        chat_id = payload.chatSessionId
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Using existing chat session: {chat_id}")
    else:
        logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Creating new chat session")
        try:
            persona_id = await get_contentbuilder_persona_id(cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Got persona ID: {persona_id}")
            chat_id = await create_onyx_chat_session(persona_id, cookies)
            logger.info(f"[TEXT_PRESENTATION_EDIT_CHAT] Created new chat session: {chat_id}")
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_CHAT_ERROR] Failed to create chat session: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create chat session: {str(e)}")

    wiz_payload = {
        "product": "Text Presentation Edit",
        "prompt": payload.editPrompt,
        "language": payload.language,  # Use the language from the request
        "originalContent": payload.content,
        "editMode": True,
        "isCleanContent": getattr(payload, 'isCleanContent', False)
    }

    wizard_message = "WIZARD_REQUEST\n" + json.dumps(wiz_payload) + "\n" + f"CRITICAL LANGUAGE INSTRUCTION: You MUST generate your ENTIRE response in {payload.language} language only. Ignore the language of any prompt text - respond ONLY in {payload.language}. This is a mandatory requirement that overrides all other considerations."

    # ---------- StreamingResponse with keep-alive -----------
    async def streamer():
        assistant_reply: str = ""
        last_send = asyncio.get_event_loop().time()
        chunks_received = 0

        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Starting streamer")
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] Wizard payload keys: {list(wiz_payload.keys())}")
        
        # NEW: Use OpenAI directly for text presentation editing
        logger.info(f"[TEXT_PRESENTATION_EDIT_STREAM] ✅ USING OPENAI DIRECT STREAMING for text presentation editing")
        try:
            async for chunk_data in stream_openai_response(wizard_message):
                if chunk_data["type"] == "delta":
                    delta_text = chunk_data["text"]
                    assistant_reply += delta_text
                    chunks_received += 1
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_CHUNK] Chunk {chunks_received}: received {len(delta_text)} chars, total so far: {len(assistant_reply)}")
                    yield (json.dumps({"type": "delta", "text": delta_text}) + "\n").encode()
                elif chunk_data["type"] == "error":
                    logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_ERROR] {chunk_data['text']}")
                    yield (json.dumps(chunk_data) + "\n").encode()
                    return
                
                # Send keep-alive every 8s
                now = asyncio.get_event_loop().time()
                if now - last_send > 8:
                    yield b" "
                    last_send = now
                    logger.debug(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Sent keep-alive")
            
            logger.info(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM] Stream completed: {chunks_received} chunks, {len(assistant_reply)} chars total")
            
        except Exception as e:
            logger.error(f"[TEXT_PRESENTATION_EDIT_OPENAI_STREAM_ERROR] Error in OpenAI streaming: {e}", exc_info=True)
            yield (json.dumps({"type": "error", "text": str(e)}) + "\n").encode()
            return

        logger.info(f"[TEXT_PRESENTATION_EDIT_COMPLETE] Final assistant reply length: {len(assistant_reply)}")
        yield (json.dumps({"type": "done", "content": assistant_reply}) + "\n").encode()

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

@app.post("/api/custom/text-presentation/finalize")
async def text_presentation_finalize(payload: TextPresentationWizardFinalize, request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Finalize text presentation creation by parsing AI response and saving to database"""
    onyx_user_id = await get_current_onyx_user_id(request)
    styles_param = getattr(payload, 'styles', None)
    logger.info(f"[TEXT_PRESENTATION_FINALIZE] styles param: {styles_param}")
    
    # Get user ID and deduct credits for one-pager creation
    try:
        credits_needed = calculate_product_credits("one_pager")
        
        # Check and deduct credits
        user_credits = await get_or_create_user_credits(onyx_user_id, "User", pool)
        if user_credits.credits_balance < credits_needed:
            raise HTTPException(
                status_code=402, 
                detail=f"Insufficient credits. Need {credits_needed} credits, have {user_credits.credits_balance}"
            )
        
        # Deduct credits
        await deduct_credits(onyx_user_id, credits_needed, pool, "One-pager creation")
        logger.info(f"Deducted {credits_needed} credits from user {onyx_user_id} for one-pager creation")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing credits for one-pager creation: {e}")
        raise HTTPException(status_code=500, detail="Failed to process credits")
    
    # Create a unique key for this text presentation finalization to prevent duplicates
    text_presentation_key = f"{onyx_user_id}:{payload.lesson}:{hash(payload.aiResponse) % 1000000}"
    
    # Check if this text presentation is already being processed
    if text_presentation_key in ACTIVE_QUIZ_FINALIZE_KEYS:  # Reuse the same set for simplicity
        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_DUPLICATE] Text presentation finalization already in progress for key: {text_presentation_key}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Text presentation finalization already in progress")
    
    # Add to active set and track timestamp
    ACTIVE_QUIZ_FINALIZE_KEYS.add(text_presentation_key)
    QUIZ_FINALIZE_TIMESTAMPS[text_presentation_key] = time.time()
    
    # Clean up stale entries (older than 5 minutes)
    current_time = time.time()
    stale_keys = [key for key, timestamp in QUIZ_FINALIZE_TIMESTAMPS.items() if current_time - timestamp > 300]
    for stale_key in stale_keys:
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(stale_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(stale_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Cleaned up stale text presentation key: {stale_key}")
    
    try:
        # Ensure text presentation template exists
        template_id = await _ensure_text_presentation_template(pool)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_TEMPLATE] Template ID: {template_id}")
        
        # CONSISTENT NAMING: Use the same pattern as lesson presentations
        # Determine the project name - if connected to outline, use correct naming convention
        project_name = None
        if payload.outlineId:
            try:
                # Fetch outline name from database
                async with pool.acquire() as conn:
                    outline_row = await conn.fetchrow(
                        "SELECT project_name FROM projects WHERE id = $1 AND onyx_user_id = $2",
                        payload.outlineId, onyx_user_id
                    )
                    if outline_row:
                        outline_name = outline_row["project_name"]
                        project_name = f"{outline_name}: {payload.lesson.strip() if payload.lesson else 'Standalone Presentation'}"
                        logger.info(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Using outline-based naming: {project_name}")
                    else:
                        logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Outline not found for ID {payload.outlineId}, using lesson title only")
            except Exception as e:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_NAMING] Failed to fetch outline name for text presentation naming: {e}")
                # Continue with plain lesson title if outline fetch fails
                project_name = payload.lesson.strip() if payload.lesson else "Standalone Presentation"
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_START] Starting text presentation finalization for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] aiResponse length: {len(payload.aiResponse)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] lesson: {payload.lesson}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] outlineId: {payload.outlineId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] chatSessionId: {payload.chatSessionId}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] language: {payload.language}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] text_presentation_key: {text_presentation_key}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] hasUserEdits: {getattr(payload, 'hasUserEdits', False)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] isCleanContent: {getattr(payload, 'isCleanContent', False)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] editedSectionIndices: {getattr(payload, 'editedSectionIndices', None)}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARAMS] aiResponse preview: {payload.aiResponse[:200]}")
        
        # NEW: Check for user edits and decide strategy (like in Quiz)
        use_direct_parser = False
        use_ai_parser = True
        
        if getattr(payload, 'hasUserEdits', False) and getattr(payload, 'originalContent', None):
            # User has made edits - check if they're significant
            any_changes = _any_text_presentation_changes_made(payload.originalContent, payload.aiResponse)
            
            if not any_changes:
                # NO CHANGES: Use direct parser path (fastest)
                use_direct_parser = True
                use_ai_parser = False
                logger.info("No text presentation changes detected - using direct parser path")
            else:
                # CHANGES DETECTED: Use AI parser
                use_direct_parser = False
                use_ai_parser = True
                logger.info("Text presentation changes detected - using AI parser path")
        else:
            # No edit information available - use AI parser
            use_direct_parser = False
            use_ai_parser = True
            logger.info("No edit information available - using AI parser path")
        
        # Fast-path: check if aiResponse is already JSON (like presentations)
        # CRITICAL: Skip fast-path if isCleanContent is true (should be clean titles, not JSON)
        parsed_text_presentation_from_fastpath = None
        
        if getattr(payload, 'isCleanContent', False):
            logger.info("[TEXT_PRESENTATION_FINALIZE_FASTPATH] Skipping fast-path because isCleanContent=True (expecting clean titles)")
        else:
            try:
                candidate = json.loads(payload.aiResponse)
                # Basic schema checks for TextPresentationDetails
                if isinstance(candidate, dict) and 'textTitle' in candidate and 'contentBlocks' in candidate:
                    logger.info("[TEXT_PRESENTATION_FINALIZE_FASTPATH] aiResponse is valid JSON, using directly without AI parsing")
                    parsed_text_presentation_from_fastpath = TextPresentationDetails(**candidate)  # type: ignore
                    use_direct_parser = False
                    use_ai_parser = False
                    # Proceed to save with parsed_text_presentation below
                else:
                    logger.info("[TEXT_PRESENTATION_FINALIZE_FASTPATH] aiResponse is JSON but missing required fields; falling back to AI parser")
            except Exception as e:
                logger.info(f"[TEXT_PRESENTATION_FINALIZE_FASTPATH] aiResponse is not JSON ({type(e).__name__}), will use AI parser")

        # NEW: Check if we should skip all parsing and go straight to selective merge
        doing_selective_merge = (getattr(payload, 'isCleanContent', False) and 
                                getattr(payload, 'editedSectionIndices', None) and 
                                payload.originalContent)
        
        if doing_selective_merge:
            logger.info("[TEXT_PRESENTATION_FINALIZE_SELECTIVE] Skipping full AI parsing, will do selective merge with targeted generation")
            # Create a dummy parsed presentation - will be replaced by selective merge below
            parsed_text_presentation = TextPresentationDetails(
                textTitle=project_name or "Untitled",
                contentBlocks=[],
                detectedLanguage=payload.language
            )
        elif parsed_text_presentation_from_fastpath:
            # Fast-path succeeded, use it directly
            logger.info("[TEXT_PRESENTATION_FINALIZE_FASTPATH] Using parsed JSON from fast-path")
            parsed_text_presentation = parsed_text_presentation_from_fastpath
        else:
            # Need to parse with AI
            if use_direct_parser:
                # DIRECT PARSER PATH: Use cached content directly since no changes were made
                logger.info("Using direct parser path for text presentation finalization")
                
                # Use the original content for parsing since no changes were made
                content_to_parse = payload.originalContent if payload.originalContent else payload.aiResponse
            elif use_ai_parser:
                # AI PARSER PATH: Use the provided content (which may be clean titles only)
                logger.info("Using AI parser path for text presentation finalization")
                
                # NEW: Check if we have clean content (only titles without descriptions)
                if getattr(payload, 'isCleanContent', False):
                    logger.info("Detected clean content - titles only, will generate descriptions for empty sections")
                    
                    # Parse the clean content to identify sections that need content generation
                    content_to_parse = await _generate_content_for_clean_titles(
                        clean_content=payload.aiResponse,
                        original_content=payload.originalContent,
                        language=payload.language
                    )
                else:
                    content_to_parse = payload.aiResponse
            else:
                # Fallback - shouldn't happen but just in case
                logger.warning("No parsing path selected, using aiResponse as fallback")
                content_to_parse = payload.aiResponse
            
            # Parse the text presentation data using LLM - only call once with consistent project name
            parsed_text_presentation: TextPresentationDetails = await parse_ai_response_with_llm(
            ai_response=content_to_parse,
            project_name=project_name,  # Use consistent project name
            target_model=TextPresentationDetails,
            default_error_model_instance=TextPresentationDetails(
                textTitle=project_name,
                contentBlocks=[],
                detectedLanguage=payload.language
            ),
            dynamic_instructions=f"""
            You are an expert text-to-JSON parsing assistant for 'Text Presentation' content.
            This product is for general text like introductions, goal descriptions, etc.
            Your output MUST be a single, valid JSON object. Strictly follow the JSON structure provided in the example.

            **Overall Goal:** Convert the *entirety* of the "Raw text to parse" into a structured JSON. Capture all information and hierarchical relationships. Maintain original language.

            **Global Fields:**
            1.  `textTitle` (string): Main title for the document. This should be derived from a Level 1 headline (`#`) or from the document header.
               - Look for patterns like "**Course Name** : **Text Presentation** : **Title**" or "**Text Presentation** : **Title**"
               - Extract ONLY the title part (the last part after the last "**")
               - For example: "**Code Optimization Course** : **Text Presentation** : **Introduction to Optimization**" → extract "Introduction to Optimization"
               - For example: "**Text Presentation** : **JavaScript Basics**" → extract "JavaScript Basics"
               - Do NOT include the course name or "Text Presentation" label in the title
               - If no clear pattern is found, use the first meaningful title or heading
            2.  `contentBlocks` (array): Ordered array of content block objects that form the body of the lesson.
            3.  `detectedLanguage` (string): e.g., "en", "ru".

            **Content Block Instructions (`contentBlocks` array items):** Each object has a `type`.

            1.  **`type: "headline"`**
                * `level` (integer):
                    * `1`: Reserved for the main title of a document, usually handled by `textTitle`. If the input text contains a clear main title that is also part of the body, use level 1.
                    * `2`: Major Section Header (e.g., "Understanding X", "Typical Mistakes"). These should use `iconName: "info"`.
                    * `3`: Sub-section Header or Mini-Title. When used as a mini-title inside a numbered list item (see `numbered_list` instruction below), it should not have an icon.
                    * `4`: Special Call-outs (e.g., "Module Goal", "Important Note"). Typically use `iconName: "target"` for goals, or lesson objectives.
                * `text` (string): Headline text.
                * `iconName` (string, optional): Based on level and context as described above.
                * `isImportant` (boolean, optional): Set to `true` for Level 3 and 4 headlines like "Lesson Goal" or "Lesson Target". If `true`, this headline AND its *immediately following single block* will be grouped into a visually distinct highlighted box. Do NOT set this to 'true' for sections like 'Conclusion', 'Key Takeaways' or any other section that comes in the very end of the lesson. Do not use this as 'true' for more than 1 section.

            2.  **`type: "paragraph"`**
                * `text` (string): Full paragraph text.
                * `isRecommendation` (boolean, optional): If this paragraph is a 'recommendation' within a numbered list item, set this to `true`. Or set this to true if it is a concluding thought in the very end of the lesson (this case applies only to one VERY last thought). Cannot be 'true' for ALL the elements in one list. HAS to be 'true' if the paragraph starts with the keyword for recommendation — e.g., 'Recommendation', 'Рекомендація', 'Рекомендация' — or their localized equivalents, and isn't a part of the bullet list.

            3.  **`type: "bullet_list"`**
                * `items` (array of `ListItem`): Can be strings or other nested content blocks.
                * `iconName` (string, optional): Default to `chevronRight`. If this bullet list is acting as a structural container for a numbered list item's content (mini-title + description), set `iconName: "none"`.

            4.  **`type: "numbered_list"`**
                * `items` (array of `ListItem`):
                    * Can be simple strings for basic numbered points.
                    * For complex items that should appear as a single visual "box" with a mini-title, description, and optional recommendation:
                        * Each such item in the `numbered_list`'s `items` array should itself be a `bullet_list` block with `iconName: "none"`.
                        * The `items` of this *inner* `bullet_list` should then be:
                            1. A `headline` block (e.g., `level: 3`, `text: "Mini-Title Text"`, no icon).
                            2. A `paragraph` block (for the main descriptive text).
                            3. Optionally, another `paragraph` block with `isRecommendation: true`.
                    * Only use round numbers in this list, no a1, a2 or 1.1, 1.2.

            5.  **`type: "table"`**
                * `headers` (array of strings): The column headers for the table.
                * `rows` (array of arrays of strings): Each inner array is a row, with each string representing a cell value. The number of cells in each row should match the number of headers.
                * `caption` (string, optional): A short description or title for the table, if present in the source text.
                * Use a table block whenever the source text contains tabular data, a grid, or a Markdown table (with | separators). Do not attempt to represent tables as lists or paragraphs.


            6.  **`type: "alert"`**
                *   `alertType` (string): One of `info`, `success`, `warning`, `danger`.
                *   `title` (string, optional): The title of the alert.
                *   `text` (string): The body text of the alert.
                *   **Parsing Rule:** An alert is identified in the raw text by a blockquote. The first line of the blockquote MUST be `> [!TYPE] Optional Title`. The `TYPE` is extracted for `alertType`. The text after the tag is the `title`. All subsequent lines within the blockquote form the `text`.

            7.  **`type: "section_break"`**
                * `style` (string, optional): e.g., "solid", "dashed", "none". Parse from `---` in the raw text.

            **General Parsing Rules & Icon Names:**
            * Ensure correct `level` for headlines. Section headers are `level: 2`. Mini-titles in lists are `level: 3`.
            * Icons: `info` for H2. `target` or `award` for H4 `isImportant`. `chevronRight` for general bullet lists. No icons for H3 mini-titles.
            * Permissible Icon Names: `info`, `target`, `award`, `chevronRight`, `bullet-circle`, `compass`.
            * Make sure to not have any tags in '<>' brackets (e.g. '<u>') in the list elements, UNLESS it is logically a part of the lesson.
            * DO NOT remove the '**' from the text, treat it as an equal part of the text. Moreover, ADD '**' around short parts of the text if you are sure that they should be bold.
            * Make sure to analyze the numbered lists in depth to not break their logically intended structure.

            Important Localization Rule: All auxiliary headings or keywords such as "Recommendation", "Conclusion", "Create from scratch", "Goal", etc. MUST be translated into the same language as the surrounding content. Examples:
              • Ukrainian → "Рекомендація", "Висновок", "Створити з нуля"
              • Russian   → "Рекомендация", "Заключение", "Создать с нуля"
              • Spanish   → "Recomendación", "Conclusión", "Crear desde cero"

            Return ONLY the JSON object.
            """,
            target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
        )
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsing completed successfully for project: {project_name}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Parsed text title: {parsed_text_presentation.textTitle}")
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_PARSE] Number of content blocks: {len(parsed_text_presentation.contentBlocks)}")

        logger.info(parsed_text_presentation.contentBlocks)
        
        # NEW: SELECTIVE SECTION REGENERATION - Generate JSON blocks directly and merge manually
        # If user edited only specific sections, we preserve unchanged sections
        if payload.isCleanContent and payload.editedSectionIndices and payload.originalContent:
            try:
                # Parse edited section indices
                edited_indices = set(int(idx.strip()) for idx in payload.editedSectionIndices.split(',') if idx.strip())
                logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Edited section indices: {edited_indices}")
                
                # Parse clean content to get section titles
                clean_sections = []
                lines = payload.aiResponse.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
                    if header_match:
                        clean_sections.append(header_match.group(2).strip())
                
                logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Found {len(clean_sections)} sections in clean content")
                
                # Only proceed if we have specific edited sections
                if edited_indices and len(edited_indices) < len(clean_sections):
                    logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Starting selective merge for {len(edited_indices)} edited sections")
                    
                    # Parse original content to extract original JSON (fast-path first, AI fallback)
                    original_json = None
                    try:
                        # First try to parse originalContent as JSON
                        original_json = json.loads(payload.originalContent)
                        if isinstance(original_json, dict) and 'contentBlocks' in original_json:
                            logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] ✅ Parsed original as JSON directly (no AI needed)")
                        else:
                            original_json = None
                    except:
                        pass
                    
                    # If not JSON, parse with AI (only if needed)
                    if not original_json:
                        logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Original content is not JSON, parsing with AI")
                        original_presentation = await parse_ai_response_with_llm(
                            ai_response=payload.originalContent,
                            project_name=project_name,
                            target_model=TextPresentationDetails,
                            default_error_model_instance=TextPresentationDetails(
                                textTitle=project_name,
                                contentBlocks=[],
                                detectedLanguage=payload.language
                            ),
                            dynamic_instructions=f"Parse this text presentation content into structured JSON. Preserve all sections and content exactly as they are.",
                            target_json_example=DEFAULT_TEXT_PRESENTATION_JSON_EXAMPLE_FOR_LLM
                        )
                        original_json = json.loads(original_presentation.json())
                    
                    # Group original content blocks by section
                    original_sections = []
                    current_section_blocks = []
                    
                    for block in original_json['contentBlocks']:
                        if block.get('type') == 'headline' and block.get('level') == 2:
                            if current_section_blocks:
                                original_sections.append(current_section_blocks)
                            current_section_blocks = [block]
                        else:
                            current_section_blocks.append(block)
                    
                    if current_section_blocks:
                        original_sections.append(current_section_blocks)
                    
                    logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Found {len(original_sections)} sections in original")
                    logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Need to regenerate sections: {edited_indices}")
                    
                    # Generate new JSON blocks for edited sections only (no AI parsing needed!)
                    new_section_blocks = {}
                    for idx in edited_indices:
                        if idx < len(clean_sections):
                            section_title = clean_sections[idx]
                            logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] 🔄 Generating JSON blocks for section {idx}: {section_title}")
                            
                            # Generate JSON blocks directly (1 AI call per edited section)
                            blocks = await _generate_content_blocks_for_section(
                                section_title=section_title,
                                all_section_titles=clean_sections,
                                language=payload.language
                            )
                            new_section_blocks[idx] = blocks
                            logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] ✅ Generated {len(blocks)} blocks for section {idx}")
                    
                    # Manual merge: no AI needed, just array manipulation
                    if len(original_sections) == len(clean_sections):
                        merged_blocks = []
                        
                        for i in range(len(clean_sections)):
                            if i not in edited_indices:
                                # Use original section blocks
                                merged_blocks.extend(original_sections[i])
                                logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] ✅ Section {i}: Preserved original ({len(original_sections[i])} blocks)")
                            else:
                                # Use newly generated blocks
                                if i in new_section_blocks:
                                    merged_blocks.extend(new_section_blocks[i])
                                    logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] ✅ Section {i}: Using newly generated ({len(new_section_blocks[i])} blocks)")
                        
                        # Convert merged blocks to proper objects
                        # First, ensure all blocks are dicts for uniform processing
                        unified_blocks = []
                        for b in merged_blocks:
                            if isinstance(b, dict):
                                unified_blocks.append(b)
                            elif hasattr(b, 'dict'):
                                unified_blocks.append(b.dict())
                            else:
                                logger.warning(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Unknown block type: {type(b)}")
                                unified_blocks.append(b)
                        
                        # Parse unified blocks into proper Pydantic models
                        temp_presentation = TextPresentationDetails.parse_obj({
                            'textTitle': original_json.get('textTitle', project_name or 'Untitled'),
                            'contentBlocks': unified_blocks,
                            'detectedLanguage': payload.language
                        })
                        parsed_text_presentation.contentBlocks = temp_presentation.contentBlocks
                        parsed_text_presentation.textTitle = temp_presentation.textTitle
                        
                        logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] ✅ Merge complete: {len(edited_indices)} regenerated, {len(clean_sections) - len(edited_indices)} preserved")
                    else:
                        logger.warning(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Cannot merge: section count mismatch (original={len(original_sections)}, clean={len(clean_sections)})")
                        logger.warning(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Falling back to full regeneration")
                else:
                    logger.info(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] All sections edited or none specified, using all new content")
            except Exception as e:
                logger.error(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Error during selective merge: {e}", exc_info=True)
                logger.warning(f"[TEXT_PRESENTATION_SELECTIVE_MERGE] Falling back to using all newly generated content")
        
        # Detect language if not provided
        if not parsed_text_presentation.detectedLanguage:
            parsed_text_presentation.detectedLanguage = detect_language(payload.aiResponse)
        
        # If parsing failed and we have no content blocks, create a basic structure
        if not parsed_text_presentation.contentBlocks:
            logger.warning(f"[TEXT_PRESENTATION_FINALIZE_FALLBACK] LLM parsing failed for text presentation, creating fallback structure")
            # Create a simple text presentation with the AI response as content
            parsed_text_presentation.textTitle = project_name
            parsed_text_presentation.contentBlocks = [
                {
                    "type": "paragraph",
                    "text": payload.aiResponse
                }
            ]
        else:
            # Validate that all content blocks have the required type field
            valid_content_blocks = []
            for i, block in enumerate(parsed_text_presentation.contentBlocks):
                if hasattr(block, 'type') and block.type:
                    valid_content_blocks.append(block)
                else:
                    logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] Content block {i} missing type, converting to paragraph")
                    # Convert to paragraph if type is missing
                    if hasattr(block, 'text'):
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": block.text
                        })
                    elif hasattr(block, 'items'):
                        valid_content_blocks.append({
                            "type": "bullet_list",
                            "items": block.items
                        })
                    else:
                        # Fallback to paragraph with string representation
                        valid_content_blocks.append({
                            "type": "paragraph",
                            "text": str(block)
                        })
            
            if not valid_content_blocks:
                logger.warning(f"[TEXT_PRESENTATION_FINALIZE_VALIDATION] No valid content blocks found, creating fallback structure")
                parsed_text_presentation.contentBlocks = [
                    {
                        "type": "paragraph",
                        "text": payload.aiResponse
                    }
                ]
            else:
                parsed_text_presentation.contentBlocks = valid_content_blocks
        
        # Always use the consistent project name for database storage
        if not payload.outlineId:
            # Standalone presentation - use lesson title or default
            final_project_name = parsed_text_presentation.textTitle or project_name
        else:
            final_project_name = project_name

        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CREATE] Creating project with name: {final_project_name}")
        
        # CONSISTENT STANDALONE FLAG: Set based on whether connected to outline
        is_standalone_text_presentation = payload.outlineId is None
        
        # For text presentation components, we need to insert directly to avoid double parsing
        # since add_project_to_custom_db would call parse_ai_response_with_llm again
        insert_query = """
        INSERT INTO projects (
            onyx_user_id, project_name, product_type, microproduct_type,
            microproduct_name, microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW(), $11)
        RETURNING id, onyx_user_id, project_name, product_type, microproduct_type, microproduct_name,
                  microproduct_content, design_template_id, source_chat_session_id, is_standalone, course_id, created_at, folder_id;
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                insert_query,
                onyx_user_id,
                final_project_name,  # Use final_project_name for project_name to match the expected pattern
                "Text Presentation",  # product_type
                COMPONENT_NAME_TEXT_PRESENTATION,  # microproduct_type - use the correct component name
                final_project_name,  # microproduct_name
                parsed_text_presentation.model_dump(mode='json', exclude_none=True),  # microproduct_content
                template_id,  # design_template_id
                payload.chatSessionId,  # source_chat_session_id
                is_standalone_text_presentation,  # is_standalone - consistent with outline connection
                payload.outlineId if payload.outlineId else None,  # course_id
                int(payload.folderId) if hasattr(payload, 'folderId') and payload.folderId else None  # folder_id
            )
        
        if not row:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create text presentation project entry.")
        
        created_project = ProjectDB(**dict(row))
        
        # Log full saved JSON for inspection
        try:
            async with pool.acquire() as conn:
                content_row = await conn.fetchrow("SELECT microproduct_content FROM projects WHERE id=$1", created_project.id)
                if content_row:
                    logger.info(f"[TEXT_PRESENTATION_FINALIZE_SAVED_JSON] Project {created_project.id} content: {json.dumps(content_row['microproduct_content'], ensure_ascii=False)[:10000]}")
        except Exception as log_e:
            logger.warning(f"Failed to log saved text presentation JSON for project {created_project.id}: {log_e}")
        
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_SUCCESS] Text presentation finalization successful: project_id={created_project.id}, project_name={final_project_name}, is_standalone={is_standalone_text_presentation}")
        return {"id": created_project.id, "name": final_project_name}
        
    except Exception as e:
        logger.error(f"[TEXT_PRESENTATION_FINALIZE_ERROR] Error in text presentation finalization: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    finally:
        # Always remove from active set and timestamps
        ACTIVE_QUIZ_FINALIZE_KEYS.discard(text_presentation_key)
        QUIZ_FINALIZE_TIMESTAMPS.pop(text_presentation_key, None)
        logger.info(f"[TEXT_PRESENTATION_FINALIZE_CLEANUP] Removed text_presentation_key from active set: {text_presentation_key}")

@app.get("/api/custom/projects/latest-by-chat")
async def get_latest_project_by_chat(chatId: str = Query(..., alias="chatId"), onyx_user_id: str = Depends(get_current_onyx_user_id), pool: asyncpg.Pool = Depends(get_db_pool)):
    """
    Return the most recently created project for the given source_chat_session_id
    for the current user. This is used by finalize fallbacks to navigate reliably
    even when the original finalize request times out.
    """
    try:
        chat_uuid = uuid.UUID(chatId)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid chatId format. Must be UUID")

    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id, project_name, design_template_id, product_type, microproduct_type
                FROM projects
                WHERE onyx_user_id = $1 AND source_chat_session_id = $2
                ORDER BY created_at DESC
                LIMIT 1
                """,
                onyx_user_id, chat_uuid
            )
        if not row:
            return JSONResponse(status_code=404, content={"detail": "No project found for chat session"})
        return {
            "id": row["id"],
            "projectName": row["project_name"],
            "productType": row.get("product_type"),
            "microproductType": row.get("microproduct_type"),
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching latest project by chat: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to fetch latest project by chat session")

# ============================
# CREDITS MANAGEMENT ENDPOINTS
# ============================

@app.get("/api/custom/credits/me", response_model=UserCredits)
async def get_my_credits(
    request: Request,
    background_tasks: BackgroundTasks,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get current user's credit balance (auto-creates if new user)"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # This will auto-create the user if they don't exist yet
        # SmartDrive provisioning happens in background to return credits faster
        credits = await get_or_create_user_credits(onyx_user_id, "User", pool, background_tasks)
        return credits
    except Exception as e:
        logger.error(f"Error getting user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve credits")

# ============================
# BILLING (Stripe) ENDPOINTS
# ============================

@app.get("/api/custom/billing/me")
async def get_billing_info(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Return current subscription info for the logged-in user.
    Reads from our user_billing table and, if needed, fetches live status from Stripe.
    """
    try:
        onyx_user_id = await get_current_onyx_user_id(request)

        async with pool.acquire() as conn:
            record = await conn.fetchrow(
                "SELECT * FROM user_billing WHERE onyx_user_id = $1",
                onyx_user_id,
            )

        if not record:
            return {
                "plan": "starter",
                "status": "inactive",
                "interval": None,
                "priceId": None,
                "subscriptionId": None,
            }

        # Start with DB values
        plan = record.get("current_plan") or "starter"
        status = record.get("subscription_status") or "inactive"
        interval = record.get("current_interval")
        price_id = record.get("current_price_id")
        subscription_id = record.get("subscription_id")

        # If explicitly requested (refresh=1), optionally refresh live status from Stripe
        refresh = request.query_params.get("refresh")
        should_refresh = str(refresh).lower() in ("1", "true", "yes")
        if should_refresh and STRIPE_SECRET_KEY and subscription_id:
            try:
                import stripe  # type: ignore
                stripe.api_key = STRIPE_SECRET_KEY
                sub = stripe.Subscription.retrieve(subscription_id, expand=["items.data.price.product"])
                status = sub.get("status") or getattr(sub, "status", None)
                items = sub.get("items") or sub["items"] if isinstance(sub, dict) else sub["items"]
                data_list = items.get("data") if isinstance(items, dict) else getattr(items, "data", None)
                if data_list and len(data_list) > 0:
                    price = data_list[0]["price"]
                    price_id = price.get("id") or getattr(price, "id", None)
                    interval = (price.get("recurring", {}) or getattr(price, "recurring", None) or {}).get("interval")
                    # Infer plan from product name if available
                    product = price.get("product") if isinstance(price, dict) else getattr(price, "product", None)
                    product_name = (product.get("name") if isinstance(product, dict) else getattr(product, "name", "")) if product else ""
                    lowered = (product_name or "").lower()
                    if "business" in lowered:
                        plan = "business"
                    elif "pro" in lowered:
                        plan = "pro"
                    # Yearly/Monthly kept via interval
            except Exception as e:
                logger.warning(f"Could not refresh subscription from Stripe: {e}")

        return {
            "plan": plan,
            "status": status,
            "interval": interval,
            "priceId": price_id,
            "subscriptionId": subscription_id,
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting billing info: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to retrieve billing info")


# ============================
# ENTITLEMENTS ENDPOINTS
# ============================

async def _fetch_effective_entitlements(onyx_user_id: str, pool: asyncpg.Pool) -> dict:
    """Compute effective entitlements from base + overrides. Fallback to plan defaults."""
    # Defaults for Starter
    result = {"connectors_limit": 0, "storage_gb": 1, "slides_max": 20, "plan": "starter", "slides_options": [20]}
    async with pool.acquire() as conn:
        # Plan from billing
        billing = await conn.fetchrow("SELECT current_plan FROM user_billing WHERE onyx_user_id = $1", onyx_user_id)
        plan = (billing and (billing.get("current_plan") or "starter")) or "starter"
        result["plan"] = plan
        # Base (Stripe-derived)
        base = await conn.fetchrow(
            "SELECT connectors_limit, storage_gb, slides_max FROM user_entitlement_base WHERE onyx_user_id = $1",
            onyx_user_id,
        )
        if base:
            result.update({
                "connectors_limit": int(base["connectors_limit"] or 0),
                "storage_gb": int(base["storage_gb"] or 1),
                "slides_max": int(base["slides_max"] or 20),
            })
        else:
            # Fallback to plan defaults if no base
            if plan == "pro":
                result.update({"connectors_limit": 2, "storage_gb": 5, "slides_max": 40})
            elif plan == "business":
                result.update({"connectors_limit": 5, "storage_gb": 10, "slides_max": 40})
        # Overrides
        overrides = await conn.fetchrow(
            "SELECT connectors_limit, storage_gb, slides_max FROM user_entitlement_overrides WHERE onyx_user_id = $1",
            onyx_user_id,
        )
        if overrides:
            if overrides["connectors_limit"] is not None:
                result["connectors_limit"] = int(overrides["connectors_limit"])  # type: ignore
            if overrides["storage_gb"] is not None:
                result["storage_gb"] = int(overrides["storage_gb"])  # type: ignore
            if overrides["slides_max"] is not None:
                result["slides_max"] = int(overrides["slides_max"])  # type: ignore

        # Add-ons (connectors/storage) aggregation from user_billing_addons
        try:
            rows = await conn.fetch(
                """
                SELECT addon_type, COALESCE(quantity,1) AS qty, stripe_price_id, status
                FROM user_billing_addons
                WHERE onyx_user_id = $1 AND status IN ('active','trialing')
                """,
                onyx_user_id
            )
            logger.info(f"[ENTITLEMENTS] Found {len(rows)} active addon(s) for user {onyx_user_id}")
            add_connectors = 0
            add_storage = 0
            for r in rows:
                addon = PRICE_TO_ADDON.get(r['stripe_price_id'] or '', None)
                units = int(addon.get('units', 0)) if addon else 0
                qty = int(r['qty'] or 1)
                logger.info(f"[ENTITLEMENTS] Addon: type={r['addon_type']}, units={units}, qty={qty}, status={r['status']}, price_id={r['stripe_price_id']}")
                if r['addon_type'] == 'connectors':
                    add_connectors += units * qty
                elif r['addon_type'] == 'storage':
                    add_storage += units * qty
            logger.info(f"[ENTITLEMENTS] Base limits: connectors={result['connectors_limit']}, storage={result['storage_gb']}GB")
            result['connectors_limit'] = int(result['connectors_limit']) + add_connectors
            result['storage_gb'] = int(result['storage_gb']) + add_storage
            logger.info(f"[ENTITLEMENTS] Final limits (with addons): connectors={result['connectors_limit']}, storage={result['storage_gb']}GB")
        except Exception as e:
            logger.error(f"[ENTITLEMENTS] Failed to aggregate addons: {e}", exc_info=True)

        # Slides options for UI (Pro/Business can choose 25-40)
        if result["slides_max"] > 20 or plan in ("pro", "business"):
            result["slides_options"] = [25, 30, 35, 40]
        else:
            result["slides_options"] = [20]
    return result


@app.get("/api/custom/entitlements/me")
async def get_my_entitlements(request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        ent = await _fetch_effective_entitlements(onyx_user_id, pool)
        
        # Add usage information
        async with pool.acquire() as conn:
            # Connector count - Call Onyx API to get real connector count
            conn_count = 0
            try:
                # Get session cookie from request to authenticate with Onyx API
                session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
                if session_cookie:
                    connector_status_url = f"{ONYX_API_SERVER_URL}/manage/admin/connector/status"
                    cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie}
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        response = await client.get(connector_status_url, cookies=cookies_to_forward)
                        if response.status_code == 200:
                            connectors_data = response.json()
                            # Count only private connectors (same as frontend logic)
                            conn_count = len([c for c in connectors_data if c.get('access_type') == 'private'])
                            logger.info(f"[ENTITLEMENTS] Fetched {conn_count} private connectors from Onyx API for user {onyx_user_id}")
                else:
                            logger.warning(f"[ENTITLEMENTS] Failed to fetch connectors from Onyx API: {response.status_code}")
            except Exception as e:
                logger.warning(f"[ENTITLEMENTS] Error fetching connectors from Onyx API: {e}")
            
            # Storage usage
            storage_row = await conn.fetchval(
                "SELECT used_bytes FROM user_storage_usage WHERE onyx_user_id = $1",
                onyx_user_id
            )
            ent["connectors_used"] = int(conn_count)
            ent["storage_used_bytes"] = int(storage_row or 0)
            ent["storage_used_gb"] = round((storage_row or 0) / (1024 * 1024 * 1024), 2)
            
            logger.info(f"[ENTITLEMENTS] User {onyx_user_id}: connectors={conn_count}, storage_bytes={storage_row}, entitlements={ent}")
        
        return ent
    except Exception as e:
        logger.error(f"Error getting entitlements: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve entitlements")


class EntitlementOverrideUpdate(BaseModel):
    connectors_limit: Optional[int] = None
    storage_gb: Optional[int] = None
    slides_max: Optional[int] = None


class BatchEntitlementUpdate(BaseModel):
    user_ids: List[str]
    connectors_limit: Optional[int] = None
    storage_gb: Optional[int] = None
    slides_max: Optional[int] = None


@app.get("/api/custom/admin/entitlements")
async def admin_list_entitlements(request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    await verify_admin_user(request)
    try:
        # Fetch user emails from Onyx API (robust mapping)
        user_emails_map = {}
        try:
            session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
            if session_cookie:
                users_url = f"{ONYX_API_SERVER_URL}/manage/users"
                cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie}
                async with httpx.AsyncClient(timeout=15.0) as client:
                    response = await client.get(users_url, cookies=cookies_to_forward)
                    if response.status_code == 200:
                        users_data = response.json()
                        # Handle both array and object shapes
                        if isinstance(users_data, dict) and 'users' in users_data:
                            users_iterable = users_data.get('users', [])
                        else:
                            users_iterable = users_data if isinstance(users_data, list) else []

                        # Map user IDs to emails with multiple key fallbacks
                        for user in users_iterable:
                            try:
                                # Try various id/email key names
                                user_id_val = (
                                    user.get('userId')
                                    or user.get('id')
                                    or user.get('uuid')
                                    or user.get('user_id')
                                )
                                email_val = (
                                    user.get('email')
                                    or user.get('userEmail')
                                    or user.get('primary_email')
                                )
                                if user_id_val and email_val:
                                    user_emails_map[str(user_id_val)] = str(email_val)
                            except Exception:
                                continue
                        logger.info(f"[ENTITLEMENTS] Fetched {len(user_emails_map)} user emails from Onyx API")
                    else:
                        logger.warning(f"[ENTITLEMENTS] Failed to fetch users from Onyx API: {response.status_code}")
        except Exception as e:
            logger.warning(f"[ENTITLEMENTS] Error fetching user emails from Onyx API: {e}")
        
        async with pool.acquire() as conn:
            # Persist any newly fetched emails into cache for future requests
            try:
                if user_emails_map:
                    for _uid, _email in user_emails_map.items():
                        await conn.execute(
                            """
                            INSERT INTO user_email_cache (onyx_user_id, email, updated_at)
                            VALUES ($1, $2, now())
                            ON CONFLICT (onyx_user_id)
                            DO UPDATE SET email = EXCLUDED.email, updated_at = now()
                            """,
                            _uid,
                            _email,
                        )
            except Exception as e:
                logger.warning(f"[ENTITLEMENTS] Failed to upsert user_email_cache: {e}")

            rows = await conn.fetch(
                """
                SELECT uc.onyx_user_id,
                       uc.name AS user_name,
                       uec.email AS cached_email,
                       COALESCE(ub.current_plan, 'starter') AS plan,
                       eb.connectors_limit AS base_connectors,
                       eb.storage_gb AS base_storage_gb,
                       eb.slides_max AS base_slides_max,
                       eo.connectors_limit AS override_connectors,
                       eo.storage_gb AS override_storage_gb,
                       eo.slides_max AS override_slides_max
                FROM user_credits uc
                LEFT JOIN user_billing ub ON ub.onyx_user_id = uc.onyx_user_id
                LEFT JOIN user_entitlement_base eb ON eb.onyx_user_id = uc.onyx_user_id
                LEFT JOIN user_entitlement_overrides eo ON eo.onyx_user_id = uc.onyx_user_id
                LEFT JOIN user_email_cache uec ON uec.onyx_user_id = uc.onyx_user_id
                ORDER BY uc.updated_at DESC
                """
            )
            out = []
            for r in rows:
                eff = await _fetch_effective_entitlements(r["onyx_user_id"], pool)
                # Get email from map, fallback to cached email, then onyx_user_id
                cached_email = dict(r).get("cached_email")
                user_email = user_emails_map.get(r["onyx_user_id"], cached_email or r["onyx_user_id"]) 
                out.append({
                    "onyx_user_id": r["onyx_user_id"],
                    "user_name": r["user_name"] or "Unknown",
                    "user_email": user_email,
                    "email": user_email,
                    "plan": r["plan"],
                    "base": {
                        "connectors_limit": int(r["base_connectors"] or 0),
                        "storage_gb": int(r["base_storage_gb"] or 1),
                        "slides_max": int(r["base_slides_max"] or 20),
                    },
                    "overrides": {
                        "connectors_limit": r["override_connectors"],
                        "storage_gb": r["override_storage_gb"],
                        "slides_max": r["override_slides_max"],
                    },
                    "effective": eff,
                })
            return out
    except Exception as e:
        logger.error(f"Error listing entitlements: {e}")
        raise HTTPException(status_code=500, detail="Failed to list entitlements")


@app.post("/api/custom/admin/entitlements/{target_user_id}")
async def admin_update_entitlements(
    target_user_id: str,
    payload: EntitlementOverrideUpdate,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool),
):
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            # Upsert overrides, keeping unspecified fields unchanged
            existing = await conn.fetchrow(
                "SELECT connectors_limit, storage_gb, slides_max FROM user_entitlement_overrides WHERE onyx_user_id = $1",
                target_user_id,
            )
            new_vals = {
                "connectors_limit": payload.connectors_limit if payload.connectors_limit is not None else (existing and existing["connectors_limit"]),
                "storage_gb": payload.storage_gb if payload.storage_gb is not None else (existing and existing["storage_gb"]),
                "slides_max": payload.slides_max if payload.slides_max is not None else (existing and existing["slides_max"]),
            }
            await conn.execute(
                """
                INSERT INTO user_entitlement_overrides (onyx_user_id, connectors_limit, storage_gb, slides_max, updated_at)
                VALUES ($1, $2, $3, $4, now())
                ON CONFLICT (onyx_user_id)
                DO UPDATE SET connectors_limit = EXCLUDED.connectors_limit,
                              storage_gb = EXCLUDED.storage_gb,
                              slides_max = EXCLUDED.slides_max,
                              updated_at = now()
                """,
                target_user_id,
                new_vals["connectors_limit"],
                new_vals["storage_gb"],
                new_vals["slides_max"],
            )
        eff = await _fetch_effective_entitlements(target_user_id, pool)
        return {"updated": True, "effective": eff}
    except Exception as e:
        logger.error(f"Error updating entitlements: {e}")
        raise HTTPException(status_code=500, detail="Failed to update entitlements")


@app.post("/api/custom/admin/entitlements-batch")
async def admin_batch_update_entitlements(
    payload: BatchEntitlementUpdate,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool),
):
    """Update entitlements for multiple users in batch."""
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            # Get all existing overrides for the batch of users in one query
            existing_overrides = await conn.fetch(
                """
                SELECT onyx_user_id, connectors_limit, storage_gb, slides_max 
                FROM user_entitlement_overrides 
                WHERE onyx_user_id = ANY($1)
                """,
                payload.user_ids
            )
            
            # Create a lookup map for existing overrides
            existing_map = {
                row['onyx_user_id']: {
                    'connectors_limit': row['connectors_limit'],
                    'storage_gb': row['storage_gb'],
                    'slides_max': row['slides_max']
                }
                for row in existing_overrides
            }
            
            # Prepare batch data for upsert
            batch_data = []
            for user_id in payload.user_ids:
                existing = existing_map.get(user_id, {})
                
                # Prepare new values, keeping unspecified fields unchanged
                new_vals = {
                    "connectors_limit": payload.connectors_limit if payload.connectors_limit is not None else existing.get("connectors_limit"),
                    "storage_gb": payload.storage_gb if payload.storage_gb is not None else existing.get("storage_gb"),
                    "slides_max": payload.slides_max if payload.slides_max is not None else existing.get("slides_max"),
                }
                
                batch_data.append((
                    user_id,
                    new_vals["connectors_limit"],
                    new_vals["storage_gb"],
                    new_vals["slides_max"]
                ))
            
            # Execute batch upsert using executemany
            await conn.executemany(
                """
                INSERT INTO user_entitlement_overrides (onyx_user_id, connectors_limit, storage_gb, slides_max, updated_at)
                VALUES ($1, $2, $3, $4, now())
                ON CONFLICT (onyx_user_id)
                DO UPDATE SET connectors_limit = EXCLUDED.connectors_limit,
                              storage_gb = EXCLUDED.storage_gb,
                              slides_max = EXCLUDED.slides_max,
                              updated_at = now()
                """,
                batch_data
            )
            
            return {
                "updated": True,
                "updated_count": len(payload.user_ids),
                "total_requested": len(payload.user_ids)
            }
    except Exception as e:
        logger.error(f"Error in batch updating entitlements: {e}")
        raise HTTPException(status_code=500, detail="Failed to batch update entitlements")


@app.post("/api/custom/admin/entitlements/refresh-all")
async def admin_refresh_all_entitlements(request: Request, pool: asyncpg.Pool = Depends(get_db_pool)):
    """Refresh base entitlements for all users based on their current plan."""
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            # Get all users with billing records
            users = await conn.fetch("SELECT onyx_user_id, current_plan FROM user_billing WHERE current_plan IS NOT NULL")
            
            updated_count = 0
            for user in users:
                onyx_user_id = user['onyx_user_id']
                plan = user['current_plan'] or 'starter'
                
                # Set base entitlements by plan
                if plan == 'pro':
                    base_connectors, base_storage, base_slides = 2, 5, 40
                elif plan == 'business':
                    base_connectors, base_storage, base_slides = 5, 10, 40
                else:
                    base_connectors, base_storage, base_slides = 0, 1, 20
                
                await conn.execute(
                    """
                    INSERT INTO user_entitlement_base (onyx_user_id, connectors_limit, storage_gb, slides_max, updated_at)
                    VALUES ($1, $2, $3, $4, now())
                    ON CONFLICT (onyx_user_id)
                    DO UPDATE SET connectors_limit=EXCLUDED.connectors_limit, storage_gb=EXCLUDED.storage_gb, slides_max=EXCLUDED.slides_max, updated_at=now()
                    """,
                    onyx_user_id, base_connectors, base_storage, base_slides
                )
                updated_count += 1
            
            logger.info(f"Refreshed entitlements for {updated_count} users")
            return {"success": True, "updated_count": updated_count}
    except Exception as e:
        logger.error(f"Error refreshing entitlements: {e}")
        raise HTTPException(status_code=500, detail="Failed to refresh entitlements")


@app.post("/api/custom/billing/portal")
async def create_billing_portal_session(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a Stripe Billing Portal session for the current user.
    Requires STRIPE_SECRET_KEY and STRIPE_BILLING_RETURN_URL env vars.
    """
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        onyx_user_id, user_email = await get_current_onyx_user_with_email(request)

        async with pool.acquire() as conn:
            record = await conn.fetchrow(
                "SELECT stripe_customer_id FROM user_billing WHERE onyx_user_id = $1",
                onyx_user_id,
            )

        # Lazy import to avoid hard dependency if not configured
        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY

        # Auto-create a Stripe customer if missing
        stripe_customer_id = record.get("stripe_customer_id") if record else None
        if not stripe_customer_id:
            customer = stripe.Customer.create(
                email=user_email or None,
                metadata={"onyx_user_id": onyx_user_id},
            )
            stripe_customer_id = customer.id
            async with pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO user_billing (onyx_user_id, stripe_customer_id, subscription_status, current_plan, updated_at)
                    VALUES ($1, $2, 'inactive', 'starter', now())
                    ON CONFLICT (onyx_user_id)
                    DO UPDATE SET stripe_customer_id = EXCLUDED.stripe_customer_id, updated_at = now()
                    """,
                    onyx_user_id,
                    stripe_customer_id,
                )

        # Determine return URL: prefer WEB_DOMAIN, then CUSTOM_FRONTEND_URL, else default to docker hostname
        preferred_domain = os.environ.get('WEB_DOMAIN') or (settings.CUSTOM_FRONTEND_URL if hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get('CUSTOM_FRONTEND_URL'))
        default_return = f"{(preferred_domain or 'http://custom_frontend:3001').rstrip('/')}/custom-projects-ui/payments"
        return_url = STRIPE_BILLING_RETURN_URL or default_return

        session = stripe.billing_portal.Session.create(
            customer=stripe_customer_id,
            return_url=return_url,
        )
        return {"url": session.url}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating billing portal session: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to create billing portal session")


class CreateCheckoutRequest(BaseModel):
    priceId: str
    planName: Optional[str] = None
    upgradeFromSubscriptionId: Optional[str] = None


@app.post("/api/custom/billing/checkout")
async def create_checkout_session(
    request: Request,
    payload: CreateCheckoutRequest,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a Stripe Checkout session for purchasing a new subscription."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        onyx_user_id, user_email = await get_current_onyx_user_with_email(request)

        async with pool.acquire() as conn:
            record = await conn.fetchrow(
                "SELECT stripe_customer_id FROM user_billing WHERE onyx_user_id = $1",
                onyx_user_id,
            )

        # Lazy import to avoid hard dependency if not configured
        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY

        # Auto-create a Stripe customer if missing
        stripe_customer_id = record.get("stripe_customer_id") if record else None
        if not stripe_customer_id:
            customer = stripe.Customer.create(
                email=user_email or None,
                metadata={"onyx_user_id": onyx_user_id},
            )
            stripe_customer_id = customer.id
            async with pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO user_billing (onyx_user_id, stripe_customer_id, subscription_status, current_plan, updated_at)
                    VALUES ($1, $2, 'inactive', 'starter', now())
                    ON CONFLICT (onyx_user_id)
                    DO UPDATE SET stripe_customer_id = EXCLUDED.stripe_customer_id, updated_at = now()
                    """,
                    onyx_user_id,
                    stripe_customer_id,
                )

        # Determine return URLs
        preferred_domain = os.environ.get('WEB_DOMAIN') or (settings.CUSTOM_FRONTEND_URL if hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get('CUSTOM_FRONTEND_URL'))
        base_url = (preferred_domain or 'http://custom_frontend:3001').rstrip('/')
        success_url = f"{base_url}/custom-projects-ui/payments?session_id={{CHECKOUT_SESSION_ID}}"
        cancel_url = f"{base_url}/custom-projects-ui/payments"

        # Create checkout session
        checkout_session = stripe.checkout.Session.create(
            customer=stripe_customer_id,
            payment_method_types=['card'],
            line_items=[{
                'price': payload.priceId,
                'quantity': 1,
            }],
            mode='subscription',
            success_url=success_url,
            cancel_url=cancel_url,
            metadata={
                'onyx_user_id': onyx_user_id,
                'plan_name': payload.planName or 'Unknown Plan',
                'upgrade_from_subscription_id': payload.upgradeFromSubscriptionId or ''
            }
        )
        
        return {"url": checkout_session.url}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating checkout session: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to create checkout session")


# ============================
# ADD-ONS CHECKOUT/LIST/CANCEL/ONE-TIME CREDITS
# ============================

class AddonItem(BaseModel):
    priceId: Optional[str] = None
    sku: Optional[str] = None  # e.g., connectors_1, storage_5gb, credits_300
    quantity: int = 1

class AddonsCheckoutRequest(BaseModel):
    items: List[AddonItem]

def _sku_to_price_id(sku: Optional[str]) -> Optional[str]:
    if not sku:
        return None
    key = sku.lower()
    # Direct mapping to provided price IDs
    sku_map = {
        'credits_100': 'price_1SGHlMH2U2KQUmUhkXKhj4g3',
        'credits_300': 'price_1SGHm0H2U2KQUmUhG5utzGFf',
        'credits_1000': 'price_1SGHmYH2U2KQUmUh89PNgGAx',
        'storage_1gb': 'price_1SGHjIH2U2KQUmUhpWRcRxxH',
        'storage_5gb': 'price_1SGHk9H2U2KQUmUhLrwnk2tQ',
        'storage_10gb': 'price_1SGHkgH2U2KQUmUh0hI2Mp07',
        'connectors_1': 'price_1SGHegH2U2KQUmUh4guOuoV7',
        'connectors_5': 'price_1SGHgFH2U2KQUmUhS0Blys9w',
        'connectors_10': 'price_1SGHgZH2U2KQUmUhSuFJ6SOi',
    }
    # Fallback to env if present
    env_overrides = {
        'connectors_1': os.getenv('STRIPE_PRICE_CONNECTORS_1'),
        'connectors_5': os.getenv('STRIPE_PRICE_CONNECTORS_5'),
        'connectors_10': os.getenv('STRIPE_PRICE_CONNECTORS_10'),
        'storage_1gb': os.getenv('STRIPE_PRICE_STORAGE_1GB'),
        'storage_5gb': os.getenv('STRIPE_PRICE_STORAGE_5GB'),
        'storage_10gb': os.getenv('STRIPE_PRICE_STORAGE_10GB'),
        'credits_100': os.getenv('STRIPE_PRICE_CREDITS_100'),
        'credits_300': os.getenv('STRIPE_PRICE_CREDITS_300'),
        'credits_1000': os.getenv('STRIPE_PRICE_CREDITS_1000'),
    }
    return env_overrides.get(key) or sku_map.get(key)

@app.post("/api/custom/billing/addons/checkout")
async def addons_checkout(
    request: Request,
    payload: AddonsCheckoutRequest,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a Stripe Checkout session for recurring add-ons (subscription mode)."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        onyx_user_id, user_email = await get_current_onyx_user_with_email(request)
        logger.info(f"[BILLING] Addon checkout request from user {onyx_user_id}, items: {len(payload.items)}")

        async with pool.acquire() as conn:
            record = await conn.fetchrow(
                "SELECT stripe_customer_id, subscription_id, current_plan FROM user_billing WHERE onyx_user_id = $1",
                onyx_user_id,
            )

        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY

        stripe_customer_id = record.get("stripe_customer_id") if record else None
        existing_subscription_id = record.get("subscription_id") if record else None
        current_plan = record.get("current_plan") if record else None
        
        logger.info(f"[BILLING] User {onyx_user_id} current plan: {current_plan}, has subscription: {bool(existing_subscription_id)}")
        
        if not stripe_customer_id:
            customer = stripe.Customer.create(email=user_email or None, metadata={"onyx_user_id": onyx_user_id})
            stripe_customer_id = customer.id
            async with pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO user_billing (onyx_user_id, stripe_customer_id, subscription_status, current_plan, updated_at)
                    VALUES ($1, $2, 'inactive', 'starter', now())
                    ON CONFLICT (onyx_user_id)
                    DO UPDATE SET stripe_customer_id = EXCLUDED.stripe_customer_id, updated_at = now()
                    """,
                    onyx_user_id,
                    stripe_customer_id,
                )

        preferred_domain = os.environ.get('WEB_DOMAIN') or (settings.CUSTOM_FRONTEND_URL if hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get('CUSTOM_FRONTEND_URL'))
        base_url = (preferred_domain or 'http://custom_frontend:3001').rstrip('/')
        success_url = f"{base_url}/custom-projects-ui/payments?session_id={{CHECKOUT_SESSION_ID}}"
        cancel_url = f"{base_url}/custom-projects-ui/payments"

        line_items = []
        for it in payload.items:
            pid = it.priceId or _sku_to_price_id(it.sku)
            if not pid:
                raise HTTPException(status_code=400, detail="Missing or invalid priceId/sku")
            line_items.append({'price': pid, 'quantity': max(1, it.quantity)})

        # If user has an existing active subscription, add items to it instead of creating new subscription
        if existing_subscription_id:
            try:
                logger.info(f"[BILLING] Attempting to add addons to existing subscription {existing_subscription_id}")
                existing_sub = stripe.Subscription.retrieve(existing_subscription_id)
                logger.info(f"[BILLING] Existing subscription status: {existing_sub.status}, items count: {len(existing_sub.get('items', {}).get('data', []))}")
                
                if existing_sub.status in ['active', 'trialing']:
                    # Get current_period_end safely from subscription object
                    current_period_end = (existing_sub.get('current_period_end') if isinstance(existing_sub, dict) 
                                        else getattr(existing_sub, 'current_period_end', None))
                    
                    # Debug: log the subscription object structure
                    if not current_period_end:
                        logger.warning(f"[BILLING] current_period_end not found! Subscription type: {type(existing_sub)}, has attr: {hasattr(existing_sub, 'current_period_end')}")
                        # Try accessing as dict
                        if hasattr(existing_sub, '__dict__'):
                            logger.warning(f"[BILLING] Subscription __dict__ keys: {list(existing_sub.__dict__.keys())}")
                        # Fallback to 0 for now
                        current_period_end = 0
                    
                    logger.info(f"[BILLING] Subscription current_period_end: {current_period_end}")
                    
                    # Build map of existing subscription items by price_id
                    existing_items = existing_sub.get('items', {}).get('data', []) if isinstance(existing_sub, dict) else getattr(existing_sub, 'items', {}).get('data', [])
                    existing_items_map = {}
                    for sub_item in existing_items:
                        price = sub_item.get('price') if isinstance(sub_item, dict) else getattr(sub_item, 'price', None)
                        price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                        item_id = sub_item.get('id') if isinstance(sub_item, dict) else getattr(sub_item, 'id', None)
                        if price_id and item_id:
                            existing_items_map[price_id] = {
                                'id': item_id,
                                'quantity': sub_item.get('quantity') if isinstance(sub_item, dict) else getattr(sub_item, 'quantity', 1)
                            }
                    
                    logger.info(f"[BILLING] Existing subscription items: {list(existing_items_map.keys())}")
                    
                    # Add or update items in existing subscription and sync to DB immediately
                    addons_updated = []
                    addons_added = []
                    async with pool.acquire() as conn:
                        for idx, item in enumerate(line_items):
                            logger.info(f"[BILLING] Processing addon item {idx+1}/{len(line_items)}: price_id={item['price']}, quantity={item['quantity']}")
                            
                            # Check if item already exists
                            if item['price'] in existing_items_map:
                                # Update existing item quantity
                                existing_item = existing_items_map[item['price']]
                                new_quantity = int(existing_item['quantity']) + int(item['quantity'])
                                logger.info(f"[BILLING] Addon already exists (item_id={existing_item['id']}), updating quantity from {existing_item['quantity']} to {new_quantity}")
                                logger.info(f"[BILLING] NOTE: Stripe will charge prorated amount immediately for quantity increase")
                                
                                sub_item = stripe.SubscriptionItem.modify(
                                    existing_item['id'],
                                    quantity=new_quantity,
                                )
                                sub_item_id = existing_item['id']
                                addons_updated.append(item['price'])
                            else:
                                # Create new item
                                logger.info(f"[BILLING] Adding new addon item: price_id={item['price']}, quantity={item['quantity']}")
                                logger.info(f"[BILLING] NOTE: Stripe will charge prorated amount immediately for new addon")
                                sub_item = stripe.SubscriptionItem.create(
                                    subscription=existing_subscription_id,
                                    price=item['price'],
                                    quantity=item['quantity'],
                                )
                                sub_item_id = sub_item.id
                                addons_added.append(item['price'])
                            
                            # Immediately sync to user_billing_addons
                            addon = PRICE_TO_ADDON.get(item['price'], None)
                            if addon and addon.get('type') in ('connectors', 'storage'):
                                final_quantity = sub_item.get('quantity') if isinstance(sub_item, dict) else getattr(sub_item, 'quantity', item['quantity'])
                                logger.info(f"[BILLING] Syncing addon to DB: type={addon.get('type')}, quantity={final_quantity}, period_end={current_period_end}")
                                await conn.execute(
                                    """
                                    INSERT INTO user_billing_addons (id, onyx_user_id, stripe_customer_id, stripe_subscription_id, stripe_subscription_item_id,
                                        stripe_price_id, addon_type, quantity, status, current_period_end, created_at, updated_at)
                                    VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9, to_timestamp($10), now(), now())
                                    ON CONFLICT (id) DO UPDATE SET quantity=EXCLUDED.quantity, status=EXCLUDED.status, current_period_end=EXCLUDED.current_period_end, updated_at=now()
                                    """,
                                    sub_item_id, onyx_user_id, stripe_customer_id, existing_subscription_id, sub_item_id, item['price'],
                                    addon['type'], final_quantity, existing_sub.status, int(current_period_end)
                                )
                    
                    action = "updated" if addons_updated else "added"
                    logger.info(f"[BILLING] Successfully processed {len(line_items)} add-on items: {len(addons_added)} added, {len(addons_updated)} updated")
                    return {
                        "url": f"{base_url}/custom-projects-ui/payments?addon_{action}=true",
                        "message": f"Addon(s) {action}. You will be charged prorated amount immediately.",
                        "immediate_charge": True
                    }
                else:
                    logger.warning(f"[BILLING] Existing subscription status is {existing_sub.status}, not active/trialing. Will create new checkout.")
            except Exception as e:
                logger.error(f"[BILLING] Failed to add to existing subscription: {e}, creating new checkout", exc_info=True)

        # Fallback: create new subscription checkout (for users without active subscription)
        logger.warning(f"[BILLING] Creating NEW subscription checkout for addons (user has no active subscription). Current plan: {current_plan}")
        logger.warning(f"[BILLING] WARNING: This may create an addon-only subscription! Items: {[item['price'] for item in line_items]}")
        checkout_session = stripe.checkout.Session.create(
            customer=stripe_customer_id,
            payment_method_types=['card'],
            line_items=line_items,
            mode='subscription',
            success_url=success_url,
            cancel_url=cancel_url,
            metadata={'onyx_user_id': onyx_user_id, 'purpose': 'addons'}
        )
        logger.info(f"[BILLING] Created checkout session: {checkout_session.id}")
        return {"url": checkout_session.url}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[BILLING] Error creating addons checkout: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to create addons checkout session")

class CreditsCheckoutRequest(BaseModel):
    priceId: Optional[str] = None
    sku: Optional[str] = None
    quantity: int = 1

@app.post("/api/custom/billing/credits/checkout")
async def credits_checkout(
    request: Request,
    payload: CreditsCheckoutRequest,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create Checkout session in payment mode for one-time credits packs."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        onyx_user_id, user_email = await get_current_onyx_user_with_email(request)
        async with pool.acquire() as conn:
            record = await conn.fetchrow("SELECT stripe_customer_id FROM user_billing WHERE onyx_user_id = $1", onyx_user_id)
        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY

        stripe_customer_id = record.get("stripe_customer_id") if record else None
        if not stripe_customer_id:
            customer = stripe.Customer.create(email=user_email or None, metadata={"onyx_user_id": onyx_user_id})
            stripe_customer_id = customer.id
            async with pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO user_billing (onyx_user_id, stripe_customer_id, subscription_status, current_plan, updated_at)
                    VALUES ($1, $2, 'inactive', 'starter', now())
                    ON CONFLICT (onyx_user_id) DO UPDATE SET stripe_customer_id = EXCLUDED.stripe_customer_id, updated_at = now()
                    """,
                    onyx_user_id, stripe_customer_id
                )

        preferred_domain = os.environ.get('WEB_DOMAIN') or (settings.CUSTOM_FRONTEND_URL if hasattr(settings, 'CUSTOM_FRONTEND_URL') else os.environ.get('CUSTOM_FRONTEND_URL'))
        base_url = (preferred_domain or 'http://custom_frontend:3001').rstrip('/')
        success_url = f"{base_url}/custom-projects-ui/payments?session_id={{CHECKOUT_SESSION_ID}}"
        cancel_url = f"{base_url}/custom-projects-ui/payments"

        price_id = payload.priceId or _sku_to_price_id(payload.sku)
        if not price_id:
            raise HTTPException(status_code=400, detail="Missing or invalid priceId/sku")

        checkout_session = stripe.checkout.Session.create(
            customer=stripe_customer_id,
            payment_method_types=['card'],
            line_items=[{'price': price_id, 'quantity': max(1, payload.quantity)}],
            mode='payment',
            success_url=success_url,
            cancel_url=cancel_url,
            metadata={'onyx_user_id': onyx_user_id, 'purpose': 'credits_pack'}
        )
        return {"url": checkout_session.url}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating credits checkout: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to create credits checkout session")

@app.get("/api/custom/billing/addons")
async def list_my_addons(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT id, addon_type, quantity, status, current_period_end, stripe_subscription_id, stripe_subscription_item_id, stripe_price_id
                FROM user_billing_addons
                WHERE onyx_user_id = $1
                ORDER BY updated_at DESC
                """,
                onyx_user_id
            )
        out = []
        for r in rows:
            out.append({
                'id': r['id'],
                'type': r['addon_type'],
                'quantity': int(r['quantity'] or 1),
                'status': r['status'],
                'next_billing_at': (r['current_period_end'].isoformat() if r['current_period_end'] else None),
                'stripe_subscription_id': r['stripe_subscription_id'],
                'stripe_subscription_item_id': r['stripe_subscription_item_id'],
                'stripe_price_id': r['stripe_price_id'],
            })
        return out
    except Exception as e:
        logger.error(f"Error listing add-ons: {e}")
        raise HTTPException(status_code=500, detail="Failed to list add-ons")

class CancelAddonRequest(BaseModel):
    subscriptionId: Optional[str] = None
    subscriptionItemId: Optional[str] = None
    immediate: Optional[bool] = True

@app.post("/api/custom/billing/addons/cancel")
async def cancel_addon(
    request: Request,
    payload: CancelAddonRequest,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")
        onyx_user_id = await get_current_onyx_user_id(request)
        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY
        if payload.subscriptionItemId:
            item = stripe.SubscriptionItem.delete(payload.subscriptionItemId)
            async with pool.acquire() as conn:
                await conn.execute(
                    "UPDATE user_billing_addons SET status='canceled', updated_at=now() WHERE id=$1 AND onyx_user_id=$2",
                    payload.subscriptionItemId, onyx_user_id
                )
            return {'status': item.get('deleted') and 'canceled' or 'updated'}
        elif payload.subscriptionId:
            if payload.immediate:
                sub = stripe.Subscription.delete(payload.subscriptionId)
            else:
                sub = stripe.Subscription.modify(payload.subscriptionId, cancel_at_period_end=True)
            async with pool.acquire() as conn:
                await conn.execute(
                    "UPDATE user_billing_addons SET status='canceled', updated_at=now() WHERE stripe_subscription_id=$1 AND onyx_user_id=$2",
                    payload.subscriptionId, onyx_user_id
                )
            return {'status': sub['status']}
        else:
            raise HTTPException(status_code=400, detail="Provide subscriptionItemId or subscriptionId")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error canceling add-on: {e}")
        raise HTTPException(status_code=500, detail="Failed to cancel add-on")

@app.get("/api/custom/billing/credits/history")
async def credits_history(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT amount, stripe_invoice_id, created_at
                FROM credit_grant_events
                WHERE onyx_user_id = $1 AND source = 'one_time_pack'
                ORDER BY created_at DESC
                """,
                onyx_user_id
            )
        return [{ 'amount': int(r['amount'] or 0), 'invoice_id': r['stripe_invoice_id'], 'created_at': r['created_at'].isoformat() } for r in rows]
    except Exception as e:
        logger.error(f"Error fetching credits history: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch credits history")

@app.get("/api/custom/billing/catalog")
async def get_billing_catalog():
    """Return Stripe price info for our SKUs to show correct prices in UI."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")
        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY
        # List of SKUs we support
        skus = [
            'credits_100','credits_300','credits_1000',
            'storage_1gb','storage_5gb','storage_10gb',
            'connectors_1','connectors_5','connectors_10',
        ]
        out = []
        for sku in skus:
            pid = _sku_to_price_id(sku)
            if not pid:
                continue
            try:
                price = stripe.Price.retrieve(pid)
                unit_amount = getattr(price, 'unit_amount', None)
                currency = getattr(price, 'currency', 'usd')
                recurring = getattr(price, 'recurring', None)
                interval = (recurring and recurring.get('interval')) or None
                out.append({
                    'sku': sku,
                    'price_id': pid,
                    'unit_amount': unit_amount,
                    'currency': currency,
                    'interval': interval,
                })
            except Exception as e:
                logger.warning(f"Failed to retrieve price {pid} for sku {sku}: {e}")
        return out
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error building billing catalog: {e}")
        raise HTTPException(status_code=500, detail="Failed to load billing catalog")

@app.post("/api/custom/billing/webhook")
async def stripe_webhook(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Handle Stripe webhooks to update subscription status."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        # Get the raw body and signature
        body = await request.body()
        signature = request.headers.get('stripe-signature')
        
        if not signature:
            raise HTTPException(status_code=400, detail="Missing Stripe signature")

        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY
        
        # You'll need to set STRIPE_WEBHOOK_SECRET in your environment
        webhook_secret = os.getenv("STRIPE_WEBHOOK_SECRET")
        if not webhook_secret:
            logger.warning("STRIPE_WEBHOOK_SECRET not set, skipping signature verification")
            event = stripe.Event.construct_from(json.loads(body), stripe.api_key)
        else:
            try:
                event = stripe.Webhook.construct_event(body, signature, webhook_secret)
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid payload")
            except stripe.SignatureVerificationError:
                raise HTTPException(status_code=400, detail="Invalid signature")

        # Helper: derive base entitlements from subscription items
        def _derive_entitlements_from_subscription(sub_obj) -> dict:
            base = {"connectors": 0, "storage_gb": 1, "slides_max": 20}
            try:
                items = sub_obj.get('items') if isinstance(sub_obj, dict) else getattr(sub_obj, 'items', None)
                data_list = items.get('data') if isinstance(items, dict) else getattr(items, 'data', None)
                if not data_list:
                    return base
                # Walk items to find features
                for it in data_list:
                    price = it.get('price') if isinstance(it, dict) else getattr(it, 'price', None)
                    product = price.get('product') if isinstance(price, dict) else getattr(price, 'product', None)
                    # Expand already requested above in retrieve where needed
                    name = (product.get('name') if isinstance(product, dict) else getattr(product, 'name', '')) if product else ''
                    metadata = (product.get('metadata') if isinstance(product, dict) else getattr(product, 'metadata', {})) or {}
                    lname = (name or '').lower()
                    # Connectors
                    if 'connectors_2' in lname:
                        base['connectors'] = max(base['connectors'], int(metadata.get('amount', 2) or 2))
                    if 'connectors_5' in lname:
                        base['connectors'] = max(base['connectors'], int(metadata.get('amount', 5) or 5))
                    # Storage
                    if 'storage_5gb' in lname:
                        base['storage_gb'] = max(base['storage_gb'], int(metadata.get('amount', 5) or 5))
                    if 'storage_10gb' in lname:
                        base['storage_gb'] = max(base['storage_gb'], int(metadata.get('amount', 10) or 10))
                    # Slides feature (treat unlimited_slides as higher caps for Pro/Business handled via plan below)
                    if 'unlimited_slides' in lname:
                        # We'll not set unlimited; caps handled per plan later
                        pass
                return base
            except Exception:
                return base

        # Idempotency: skip already processed events
        async with pool.acquire() as conn:
            if await conn.fetchval("SELECT 1 FROM processed_stripe_events WHERE event_id=$1", event['id']):
                return {"status": "ignored"}

        # Handle the event
        if event['type'] == 'checkout.session.completed':
            session = event['data']['object']
            onyx_user_id = session.get('metadata', {}).get('onyx_user_id')
            
            logger.info(f"[BILLING] checkout.session.completed for user {onyx_user_id}, mode={session.get('mode')}, session_id={session.get('id')}")
            
            if onyx_user_id and session.get('mode') == 'subscription':
                subscription_id = session.get('subscription')
                customer_id = session.get('customer')
                
                # Get subscription details
                subscription = stripe.Subscription.retrieve(subscription_id, expand=['items.data.price.product'])
                
                # Get existing user plan before making any changes
                existing_plan = None
                async with pool.acquire() as conn:
                    existing_record = await conn.fetchrow(
                        "SELECT current_plan FROM user_billing WHERE onyx_user_id = $1",
                        onyx_user_id
                    )
                    if existing_record:
                        existing_plan = existing_record['current_plan']
                
                logger.info(f"[BILLING] User {onyx_user_id} existing plan: {existing_plan}, subscription items count: {len(subscription.get('items', {}).get('data', []))}")
                
                # Extract plan info - LOOP through ALL items to find tier price (not just first!)
                plan = None
                interval = None
                price_id = None
                
                items = subscription.get('items') if isinstance(subscription, dict) else getattr(subscription, 'items', None)
                data_list = items.get('data') if isinstance(items, dict) else getattr(items, 'data', None)
                
                # Check metadata to see if this is an addon-only purchase
                is_addon_purchase = session.get('metadata', {}).get('purpose') == 'addons'
                
                if data_list:
                    # Log all items for debugging
                    for idx, item_data in enumerate(data_list):
                        item_price = item_data.get('price') if isinstance(item_data, dict) else getattr(item_data, 'price', None)
                        item_price_id = item_price.get('id') if isinstance(item_price, dict) else getattr(item_price, 'id', None)
                        logger.info(f"[BILLING] Item {idx}: price_id={item_price_id}")
                    
                    # Look for base tier price among all items
                    for item_data in data_list:
                        price = item_data.get('price') if isinstance(item_data, dict) else getattr(item_data, 'price', None)
                        item_price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                        
                        # Check if this is a base tier price
                        tier_key = PRICE_TO_TIER.get(item_price_id, '')
                        if tier_key:
                            price_id = item_price_id
                            plan = tier_key.replace('_monthly', '').replace('_yearly', '')
                            recurring = price.get('recurring') if isinstance(price, dict) else getattr(price, 'recurring', None)
                            interval = (recurring or {}).get('interval') if isinstance(recurring, dict) else getattr(recurring, 'interval', None)
                            logger.info(f"[BILLING] Found tier price: {item_price_id} -> plan={plan}, interval={interval}")
                            break
                    
                    # If no tier price found, try product name fallback on first item
                    if not plan and len(data_list) > 0:
                        price = data_list[0].get('price') if isinstance(data_list[0], dict) else getattr(data_list[0], 'price', None)
                        item_price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                        recurring = price.get('recurring') if isinstance(price, dict) else getattr(price, 'recurring', None)
                        interval = (recurring or {}).get('interval') if isinstance(recurring, dict) else getattr(recurring, 'interval', None)
                        
                        product = price.get('product') if isinstance(price, dict) else getattr(price, 'product', None)
                        product_name = (product.get('name') if isinstance(product, dict) else getattr(product, 'name', '')) if product else ''
                        lowered = product_name.lower()
                        logger.info(f"[BILLING] No tier price found, checking product name: {product_name}")
                        if 'business' in lowered:
                            plan = 'business'
                            price_id = item_price_id
                        elif 'pro' in lowered:
                            plan = 'pro'
                            price_id = item_price_id
                
                # CRITICAL FIX: If no plan found (addon-only subscription) and user has existing plan, preserve it
                if not plan and is_addon_purchase and existing_plan:
                    logger.warning(f"[BILLING] Addon-only subscription detected for user {onyx_user_id}. Preserving existing plan: {existing_plan}")
                    plan = existing_plan
                elif not plan:
                    logger.warning(f"[BILLING] No tier price found for user {onyx_user_id}, defaulting to starter")
                    plan = "starter"
                
                # Update user billing - preserve plan if this was addon-only purchase
                async with pool.acquire() as conn:
                    if is_addon_purchase and not price_id:
                        # Addon-only purchase: only update subscription_id and status, preserve plan
                        logger.info(f"[BILLING] Updating billing for addon-only purchase, preserving plan {plan}")
                        await conn.execute(
                            """
                            INSERT INTO user_billing (
                                onyx_user_id, stripe_customer_id, subscription_status, 
                                subscription_id, current_plan, updated_at
                            )
                            VALUES ($1, $2, $3, $4, $5, now())
                            ON CONFLICT (onyx_user_id)
                            DO UPDATE SET 
                                subscription_status = EXCLUDED.subscription_status,
                                subscription_id = EXCLUDED.subscription_id,
                                updated_at = now()
                            """,
                            onyx_user_id, customer_id, subscription.status,
                            subscription_id, plan
                        )
                        
                        # Sync addon items to user_billing_addons for addon-only subscriptions
                        try:
                            logger.info(f"[BILLING] Syncing addon items from addon-only subscription to database")
                            current_period_end = (subscription.get('current_period_end') if isinstance(subscription, dict) 
                                                else getattr(subscription, 'current_period_end', 0)) or 0
                            
                            if data_list:
                                for item_data in data_list:
                                    price = item_data.get('price') if isinstance(item_data, dict) else getattr(item_data, 'price', None)
                                    item_price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                                    item_id = item_data.get('id') if isinstance(item_data, dict) else getattr(item_data, 'id', None)
                                    item_quantity = item_data.get('quantity') if isinstance(item_data, dict) else getattr(item_data, 'quantity', 1)
                                    
                                    addon = PRICE_TO_ADDON.get(item_price_id, None)
                                    if addon and addon.get('type') in ('connectors', 'storage'):
                                        logger.info(f"[BILLING] Syncing addon to DB: type={addon.get('type')}, quantity={item_quantity}, price_id={item_price_id}")
                                        await conn.execute(
                                            """
                                            INSERT INTO user_billing_addons (id, onyx_user_id, stripe_customer_id, stripe_subscription_id, stripe_subscription_item_id,
                                                stripe_price_id, addon_type, quantity, status, current_period_end, created_at, updated_at)
                                            VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9, to_timestamp($10), now(), now())
                                            ON CONFLICT (id) DO UPDATE SET quantity=EXCLUDED.quantity, status=EXCLUDED.status, current_period_end=EXCLUDED.current_period_end, updated_at=now()
                                            """,
                                            item_id, onyx_user_id, customer_id, subscription_id, item_id, item_price_id,
                                            addon['type'], item_quantity, subscription.status, int(current_period_end)
                                        )
                                logger.info(f"[BILLING] Successfully synced {len(data_list)} addon items to database")
                        except Exception as addon_sync_err:
                            logger.error(f"[BILLING] Failed to sync addon items from addon-only subscription: {addon_sync_err}", exc_info=True)
                    else:
                        # Normal plan purchase: update everything including plan
                        logger.info(f"[BILLING] Updating billing for plan purchase: plan={plan}, price_id={price_id}")
                    await conn.execute(
                        """
                        INSERT INTO user_billing (
                            onyx_user_id, stripe_customer_id, subscription_status, 
                            subscription_id, current_price_id, current_plan, 
                            current_interval, updated_at
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, now())
                        ON CONFLICT (onyx_user_id)
                        DO UPDATE SET 
                            subscription_status = EXCLUDED.subscription_status,
                            subscription_id = EXCLUDED.subscription_id,
                            current_price_id = EXCLUDED.current_price_id,
                            current_plan = EXCLUDED.current_plan,
                            current_interval = EXCLUDED.current_interval,
                            updated_at = now()
                        """,
                        onyx_user_id, customer_id, subscription.status,
                        subscription_id, price_id, plan, interval
                    )

                # If this was an upgrade, cancel the previous subscription
                try:
                    prev_sub_id = session.get('metadata', {}).get('upgrade_from_subscription_id')
                    if prev_sub_id:
                        stripe.Subscription.delete(prev_sub_id)
                except Exception as cancel_err:
                    logger.warning(f"Upgrade cancel previous subscription failed: {cancel_err}")
                
                # Compute and persist entitlements based on plan tier
                # Only update base entitlements if this is NOT an addon-only purchase
                if not is_addon_purchase or price_id:
                    try:
                        # Set base entitlements by plan
                        if plan == 'pro':
                            base_connectors, base_storage, base_slides = 2, 5, 40
                        elif plan == 'business':
                            base_connectors, base_storage, base_slides = 5, 10, 40
                        else:
                            base_connectors, base_storage, base_slides = 0, 1, 20
                        
                            logger.info(f"[BILLING] Setting base entitlements for plan {plan}: connectors={base_connectors}, storage={base_storage}GB, slides={base_slides}")
                        async with pool.acquire() as conn:
                            await conn.execute(
                                """
                                INSERT INTO user_entitlement_base (onyx_user_id, connectors_limit, storage_gb, slides_max, updated_at)
                                VALUES ($1, $2, $3, $4, now())
                                ON CONFLICT (onyx_user_id)
                                DO UPDATE SET connectors_limit=EXCLUDED.connectors_limit, storage_gb=EXCLUDED.storage_gb, slides_max=EXCLUDED.slides_max, updated_at=now()
                                """,
                                onyx_user_id, base_connectors, base_storage, base_slides
                            )
                    except Exception as e:
                            logger.error(f"[BILLING] Failed to persist base entitlements: {e}")
                else:
                    logger.info(f"[BILLING] Skipping base entitlements update for addon-only purchase")
                
                logger.info(f"[BILLING] Updated billing for user {onyx_user_id}: plan={plan}, interval={interval}, is_addon_purchase={is_addon_purchase}")

            # One-time credits purchase via Checkout Session (mode=payment)
            if onyx_user_id and session.get('mode') == 'payment':
                try:
                    line_items = stripe.checkout.Session.list_line_items(session['id'])
                    total_credits = 0
                    for li in line_items.data:
                        price_id = getattr(getattr(li, 'price', None), 'id', None)
                        qty = int(getattr(li, 'quantity', 1) or 1)
                        addon = PRICE_TO_ADDON.get(price_id or '', None)
                        if addon and addon.get('type') == 'credits':
                            total_credits += int(addon.get('units', 0)) * qty
                    if total_credits > 0:
                        async with pool.acquire() as conn:
                            await conn.execute(
                                """
                                UPDATE user_credits
                                SET credits_balance = credits_balance + $2,
                                    credits_purchased = credits_purchased + $2,
                                    updated_at = now()
                                WHERE onyx_user_id = $1
                                """,
                                onyx_user_id, total_credits
                            )
                            await conn.execute(
                                """
                                INSERT INTO credit_grant_events (id, onyx_user_id, source, amount, stripe_invoice_id, created_at)
                                VALUES ($1, $2, 'one_time_pack', $3, NULL, now())
                                """,
                                str(uuid.uuid4()), onyx_user_id, total_credits
                            )
                except Exception as ce:
                    logger.error(f"Failed to grant one-time credits: {ce}")

        elif event['type'] == 'customer.subscription.updated':
            subscription_obj = event['data']['object']
            subscription_id = subscription_obj.get('id') if isinstance(subscription_obj, dict) else getattr(subscription_obj, 'id', None)
            customer_id = subscription_obj.get('customer') if isinstance(subscription_obj, dict) else getattr(subscription_obj, 'customer', None)
            
            logger.info(f"[BILLING] customer.subscription.updated event: subscription_id={subscription_id}, customer_id={customer_id}")
            
            # Re-fetch subscription with expanded product data to ensure we have all info
            subscription = stripe.Subscription.retrieve(subscription_id, expand=['items.data.price.product'])
            logger.info(f"[BILLING] Subscription status: {subscription.get('status')}, items count: {len(subscription.get('items', {}).get('data', []))}")
            
            # Find user by customer ID
            async with pool.acquire() as conn:
                user_record = await conn.fetchrow(
                    "SELECT onyx_user_id, current_plan FROM user_billing WHERE stripe_customer_id = $1",
                    customer_id
                )
            
            if user_record:
                onyx_user_id = user_record['onyx_user_id']
                existing_plan = user_record['current_plan']
                logger.info(f"[BILLING] Found user {onyx_user_id} with existing plan: {existing_plan}")
                try:
                    # Cache email if present in customer object
                    cust = stripe.Customer.retrieve(customer_id)
                    user_email = (cust.get('email') if isinstance(cust, dict) else getattr(cust, 'email', '')) or ''
                    if user_email:
                        async with pool.acquire() as conn:
                            await conn.execute(
                                """
                                INSERT INTO user_email_cache (onyx_user_id, email, updated_at)
                                VALUES ($1, $2, now())
                                ON CONFLICT (onyx_user_id) DO UPDATE SET email = EXCLUDED.email, updated_at = now()
                                """,
                                onyx_user_id,
                                user_email,
                            )
                except Exception as e:
                    logger.warning(f"Failed to cache user email: {e}")
                
                # Extract updated plan info - find base tier price (not add-ons)
                plan = None
                interval = None
                price_id = None
                
                items = subscription.get('items') if isinstance(subscription, dict) else getattr(subscription, 'items', None)
                data_list = items.get('data') if isinstance(items, dict) else getattr(items, 'data', None)
                if data_list:
                    # Log all items for debugging
                    logger.info(f"[BILLING] Subscription items:")
                    for idx, item_data in enumerate(data_list):
                        item_price = item_data.get('price') if isinstance(item_data, dict) else getattr(item_data, 'price', None)
                        item_price_id = item_price.get('id') if isinstance(item_price, dict) else getattr(item_price, 'id', None)
                        is_tier = item_price_id in PRICE_TO_TIER
                        is_addon = item_price_id in PRICE_TO_ADDON
                        logger.info(f"[BILLING]   Item {idx}: price_id={item_price_id}, is_tier={is_tier}, is_addon={is_addon}")
                    
                    # Look for base tier price among all items
                    for item_data in data_list:
                        price = item_data.get('price') if isinstance(item_data, dict) else getattr(item_data, 'price', None)
                        item_price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                        
                        # Check if this is a base tier price
                        tier_key = PRICE_TO_TIER.get(item_price_id, '')
                        if tier_key:
                            price_id = item_price_id
                            plan = tier_key.replace('_monthly', '').replace('_yearly', '')
                            recurring = price.get('recurring') if isinstance(price, dict) else getattr(price, 'recurring', None)
                            interval = (recurring or {}).get('interval') if isinstance(recurring, dict) else getattr(recurring, 'interval', None)
                            logger.info(f"[BILLING] Found tier price: {item_price_id} -> plan={plan}, interval={interval}")
                            break
                    
                    # If no tier price found, try product name fallback on first item
                    if not plan and len(data_list) > 0:
                        logger.info(f"[BILLING] No tier price found among items, trying product name fallback")
                        price = data_list[0].get('price') if isinstance(data_list[0], dict) else getattr(data_list[0], 'price', None)
                        price_id = price.get('id') if isinstance(price, dict) else getattr(price, 'id', None)
                        recurring = price.get('recurring') if isinstance(price, dict) else getattr(price, 'recurring', None)
                        interval = (recurring or {}).get('interval') if isinstance(recurring, dict) else getattr(recurring, 'interval', None)
                        
                        product = price.get('product') if isinstance(price, dict) else getattr(price, 'product', None)
                        product_name = (product.get('name') if isinstance(product, dict) else getattr(product, 'name', '')) if product else ''
                        lowered = product_name.lower()
                        logger.info(f"[BILLING] Product name: '{product_name}'")
                        if 'business' in lowered:
                            plan = 'business'
                        elif 'pro' in lowered:
                            plan = 'pro'
                        
                        if plan:
                            logger.info(f"[BILLING] Inferred plan from product name: {plan}")
                        else:
                            logger.warning(f"[BILLING] Could not infer plan from product name")
                
                # Only update user_billing if we found a base tier plan (don't overwrite with add-ons)
                if plan:
                    logger.info(f"[BILLING] Found base tier plan '{plan}', updating user_billing")
                    async with pool.acquire() as conn:
                        await conn.execute(
                            """
                            UPDATE user_billing 
                            SET subscription_status = $2, current_price_id = $3, 
                                current_plan = $4, current_interval = $5, updated_at = now()
                            WHERE onyx_user_id = $1
                            """,
                            onyx_user_id, subscription['status'], price_id, plan, interval
                        )
                    logger.info(f"[BILLING] Updated subscription for user {onyx_user_id}: plan={plan}, status={subscription['status']}")
                else:
                    # No base tier found - this is likely just add-ons being added
                    # Only update status, preserve existing plan
                    logger.info(f"[BILLING] No base tier found in subscription update for user {onyx_user_id}, preserving existing plan '{existing_plan}'")
                    async with pool.acquire() as conn:
                        await conn.execute(
                            """
                            UPDATE user_billing 
                            SET subscription_status = $2, updated_at = now()
                            WHERE onyx_user_id = $1
                            """,
                            onyx_user_id, subscription['status']
                        )
                    plan = existing_plan  # Use existing plan for entitlements update below
                    logger.info(f"[BILLING] Using existing plan '{plan}' for entitlements calculation")
                
                # Refresh base entitlements on subscription update (for both cases)
                if plan:
                    try:
                        # Set base entitlements by plan
                        if plan == 'pro':
                            base_connectors, base_storage, base_slides = 2, 5, 40
                        elif plan == 'business':
                            base_connectors, base_storage, base_slides = 5, 10, 40
                        else:
                            base_connectors, base_storage, base_slides = 0, 1, 20
                        
                        logger.info(f"[BILLING] Updating base entitlements for plan '{plan}': connectors={base_connectors}, storage={base_storage}GB, slides={base_slides}")
                        async with pool.acquire() as conn:
                            await conn.execute(
                                """
                                INSERT INTO user_entitlement_base (onyx_user_id, connectors_limit, storage_gb, slides_max, updated_at)
                                VALUES ($1, $2, $3, $4, now())
                                ON CONFLICT (onyx_user_id)
                                DO UPDATE SET connectors_limit=EXCLUDED.connectors_limit, storage_gb=EXCLUDED.storage_gb, slides_max=EXCLUDED.slides_max, updated_at=now()
                                """,
                                onyx_user_id, base_connectors, base_storage, base_slides
                            )
                        logger.info(f"[BILLING] Successfully updated base entitlements for user {onyx_user_id}")
                    except Exception as e:
                        logger.error(f"[BILLING] Failed to persist base entitlements on update: {e}", exc_info=True)

        elif event['type'] == 'invoice.paid':
            invoice = event['data']['object']
            customer_id = invoice.get('customer')
            async with pool.acquire() as conn:
                rec = await conn.fetchrow("SELECT onyx_user_id FROM user_billing WHERE stripe_customer_id = $1", customer_id)
            if rec:
                onyx_user_id = rec['onyx_user_id']
                # Grant credits for base tier renewal
                try:
                    total_tier_credits = 0
                    for li in invoice.get('lines', {}).get('data', []):
                        price = (li.get('price') or {})
                        pid = price.get('id')
                        tier_key = PRICE_TO_TIER.get(pid or '', '')
                        if tier_key:
                            total_tier_credits += TIER_TO_CREDITS.get(tier_key, 0)
                    if total_tier_credits > 0:
                        async with pool.acquire() as conn:
                            await conn.execute(
                                """
                                UPDATE user_credits
                                SET credits_balance = credits_balance + $2,
                                    credits_purchased = credits_purchased + $2,
                                    updated_at = now()
                                WHERE onyx_user_id = $1
                                """,
                                onyx_user_id, total_tier_credits
                            )
                            await conn.execute(
                                """
                                INSERT INTO credit_grant_events (id, onyx_user_id, source, amount, stripe_invoice_id, created_at)
                                VALUES ($1, $2, 'tier_renewal', $3, $4, now())
                                """,
                                str(uuid.uuid4()), onyx_user_id, total_tier_credits, invoice.get('id')
                            )
                except Exception as te:
                    logger.error(f"Failed to grant tier renewal credits: {te}")

                # Sync recurring add-ons status from subscription
                try:
                    sub_id = invoice.get('subscription')
                    if sub_id:
                        sub = stripe.Subscription.retrieve(sub_id, expand=['items.data.price'])
                        async with pool.acquire() as conn:
                            for it in sub['items']['data']:
                                price_id = it['price']['id']
                                addon = PRICE_TO_ADDON.get(price_id, None)
                                if addon and addon.get('type') in ('connectors','storage'):
                                    await conn.execute(
                                        """
                                        INSERT INTO user_billing_addons (id, onyx_user_id, stripe_customer_id, stripe_subscription_id, stripe_subscription_item_id,
                                            stripe_price_id, addon_type, quantity, status, current_period_end, created_at, updated_at)
                                        VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9, to_timestamp($10), now(), now())
                                        ON CONFLICT (id) DO UPDATE SET quantity=EXCLUDED.quantity, status=EXCLUDED.status, current_period_end=EXCLUDED.current_period_end, updated_at=now()
                                        """,
                                        it['id'], onyx_user_id, customer_id, sub_id, it['id'], price_id,
                                        addon['type'], int(it.get('quantity') or 1), sub['status'], int(sub.get('current_period_end') or 0)
                                    )
                except Exception as ae:
                    logger.error(f"Failed to sync add-ons: {ae}")

        elif event['type'] == 'customer.subscription.deleted':
            subscription = event['data']['object']
            customer_id = subscription.get('customer')
            
            # Find user by customer ID and mark as cancelled
            async with pool.acquire() as conn:
                await conn.execute(
                    """
                    UPDATE user_billing 
                    SET subscription_status = 'canceled', current_plan = 'starter',
                        current_price_id = NULL, current_interval = NULL, updated_at = now()
                    WHERE stripe_customer_id = $1
                    """,
                    customer_id
                )
            
            logger.info(f"Cancelled subscription for customer {customer_id}")

        # Mark processed
        try:
            async with pool.acquire() as conn:
                await conn.execute("INSERT INTO processed_stripe_events (event_id, created_at) VALUES ($1, now()) ON CONFLICT DO NOTHING", event['id'])
        except Exception:
            pass
        return {"status": "success"}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing webhook: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Webhook processing failed")


class CancelSubscriptionRequest(BaseModel):
    subscriptionId: Optional[str] = None


@app.post("/api/custom/billing/cancel")
async def cancel_subscription(
    request: Request,
    payload: CancelSubscriptionRequest,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Cancel user's active subscription now in Stripe and update our user_billing table."""
    try:
        if not STRIPE_SECRET_KEY:
            raise HTTPException(status_code=500, detail="Stripe is not configured")

        onyx_user_id = await get_current_onyx_user_id(request)

        async with pool.acquire() as conn:
            record = await conn.fetchrow(
                "SELECT subscription_id FROM user_billing WHERE onyx_user_id = $1",
                onyx_user_id,
            )

        subscription_id = payload.subscriptionId or (record and record.get("subscription_id"))
        if not subscription_id:
            raise HTTPException(status_code=404, detail="No active subscription found")

        import stripe  # type: ignore
        stripe.api_key = STRIPE_SECRET_KEY

        # Cancel immediately (or set cancel_at_period_end=True for end-of-term)
        canceled = stripe.Subscription.delete(subscription_id)

        async with pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE user_billing
                SET subscription_status = $2,
                    updated_at = now()
                WHERE onyx_user_id = $1
                """,
                onyx_user_id,
                canceled.status,
            )

        return {"status": canceled.status}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error canceling subscription: {e}", exc_info=not IS_PRODUCTION)
        raise HTTPException(status_code=500, detail="Failed to cancel subscription")

@app.get("/api/custom/admin/credits/users", response_model=List[AdminUserCredits])
async def list_all_user_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to list all user credits"""
    await verify_admin_user(request)
    
    try:
        # Build a map of Onyx user IDs to emails from Onyx API; fallback to local cache
        user_emails_map: dict[str, str] = {}
        try:
            session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
            if session_cookie:
                users_url = f"{ONYX_API_SERVER_URL}/manage/users"
                cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie}
                async with httpx.AsyncClient(timeout=15.0) as client:
                    response = await client.get(users_url, cookies=cookies_to_forward)
                    if response.status_code == 200:
                        users_data = response.json()
                        # Log top-level keys and shape for troubleshooting
                        try:
                            if isinstance(users_data, dict):
                                logger.info(f"[CREDITS] Onyx /manage/users keys: {list(users_data.keys())}")
                            else:
                                logger.info(f"[CREDITS] Onyx /manage/users returned list of len={len(users_data)}")
                        except Exception:
                            pass

                        # Normalize to a flat list of user snapshots
                        users_iterable: list[dict] = []
                        if isinstance(users_data, dict):
                            if 'users' in users_data and isinstance(users_data['users'], list):
                                users_iterable = users_data['users']
                            else:
                                accepted = users_data.get('accepted', []) or []
                                slack_users = users_data.get('slack_users', []) or []
                                invited = users_data.get('invited', []) or []
                                # We only map accepted + slack user emails
                                users_iterable = []
                                if isinstance(accepted, list):
                                    users_iterable += accepted
                                if isinstance(slack_users, list):
                                    users_iterable += slack_users
                                # invited is email strings; skip
                        elif isinstance(users_data, list):
                            users_iterable = users_data

                        # Build id->email map (handles different field names)
                        for user in users_iterable:
                            try:
                                user_id_val = (
                                    user.get('userId')
                                    or user.get('id')
                                    or user.get('uuid')
                                    or user.get('user_id')
                                )
                                email_val = (
                                    user.get('email')
                                    or user.get('userEmail')
                                    or user.get('primary_email')
                                )
                                if user_id_val and email_val:
                                    user_emails_map[str(user_id_val)] = str(email_val)
                            except Exception:
                                continue

                        # Log a small sample of the mapping for debugging
                        try:
                            sample_items = list(user_emails_map.items())[:5]
                            logger.info(f"[CREDITS] Onyx users mapped: count={len(user_emails_map)}, sample={sample_items}")
                        except Exception:
                            pass
        except Exception:
            # Non-fatal; we'll fallback to cache
            pass

        async with pool.acquire() as conn:
            # Attempt 1: join to Onyx users table named "user" (FastAPI Users default)
            try:
                rows = await conn.fetch(
                    """
                    SELECT 
                        uc.id,
                        uc.onyx_user_id,
                        uc.name,
                        u1.email AS db_email,
                        uec.email AS cached_email,
                        uc.credits_balance,
                        uc.total_credits_used,
                        uc.credits_purchased,
                        uc.last_purchase_date,
                        -- prefer real plan from billing; default to 'starter' when missing
                        COALESCE(ub.current_plan, 'starter') ||
                        CASE WHEN ub.current_interval IS NOT NULL THEN
                            ' (' || CASE WHEN ub.current_interval = 'year' THEN 'Yearly' WHEN ub.current_interval = 'month' THEN 'Monthly' ELSE ub.current_interval END || ')'
                        ELSE '' END AS subscription_tier,
                        uc.created_at,
                        uc.updated_at
                    FROM user_credits uc
                    LEFT JOIN user_billing ub ON ub.onyx_user_id = uc.onyx_user_id
                    LEFT JOIN user_email_cache uec ON uec.onyx_user_id = uc.onyx_user_id
                    LEFT JOIN "user" u1 ON (u1.id::text = uc.onyx_user_id OR u1.email = uc.onyx_user_id)
                    ORDER BY uc.updated_at DESC
                    """
                )
                db_join_table = '"user"'
            except Exception as e_user_table:
                logger.warning(f"[CREDITS] Join to table 'user' failed, trying 'users': {e_user_table}")
                # Attempt 2: join to Onyx users table named users
                try:
                    rows = await conn.fetch(
                        """
                        SELECT 
                            uc.id,
                            uc.onyx_user_id,
                            uc.name,
                            u2.email AS db_email,
                            uec.email AS cached_email,
                            uc.credits_balance,
                            uc.total_credits_used,
                            uc.credits_purchased,
                            uc.last_purchase_date,
                            COALESCE(ub.current_plan, 'starter') ||
                            CASE WHEN ub.current_interval IS NOT NULL THEN
                                ' (' || CASE WHEN ub.current_interval = 'year' THEN 'Yearly' WHEN ub.current_interval = 'month' THEN 'Monthly' ELSE ub.current_interval END || ')'
                            ELSE '' END AS subscription_tier,
                            uc.created_at,
                            uc.updated_at
                        FROM user_credits uc
                        LEFT JOIN user_billing ub ON ub.onyx_user_id = uc.onyx_user_id
                        LEFT JOIN user_email_cache uec ON uec.onyx_user_id = uc.onyx_user_id
                        LEFT JOIN users u2 ON (u2.id::text = uc.onyx_user_id OR u2.email = uc.onyx_user_id)
                        ORDER BY uc.updated_at DESC
                        """
                    )
                    db_join_table = 'users'
                except Exception as e_users_table:
                    logger.warning(f"[CREDITS] Join to table 'users' also failed; falling back to API/cache only: {e_users_table}")
                    # Fallback: query without joining any users table
                    rows = await conn.fetch(
                        """
                        SELECT 
                            uc.id,
                            uc.onyx_user_id,
                            uc.name,
                            NULL::text AS db_email,
                            uec.email AS cached_email,
                            uc.credits_balance,
                            uc.total_credits_used,
                            uc.credits_purchased,
                            uc.last_purchase_date,
                            COALESCE(ub.current_plan, 'starter') ||
                            CASE WHEN ub.current_interval IS NOT NULL THEN
                                ' (' || CASE WHEN ub.current_interval = 'year' THEN 'Yearly' WHEN ub.current_interval = 'month' THEN 'Monthly' ELSE ub.current_interval END || ')'
                            ELSE '' END AS subscription_tier,
                            uc.created_at,
                            uc.updated_at
                        FROM user_credits uc
                        LEFT JOIN user_billing ub ON ub.onyx_user_id = uc.onyx_user_id
                        LEFT JOIN user_email_cache uec ON uec.onyx_user_id = uc.onyx_user_id
                        ORDER BY uc.updated_at DESC
                        """
                    )
                    db_join_table = None

            enriched: list[AdminUserCredits] = []
            stats_total = 0
            stats_from_db = 0
            stats_from_api = 0
            stats_from_cache = 0
            unresolved: list[str] = []
            for row in rows:
                d = dict(row)
                # Determine email: Onyx API map, then cached, else None
                resolved_email = d.get("db_email") or user_emails_map.get(d["onyx_user_id"], d.get("cached_email"))
                # Compute display identity: email → meaningful name → onyx_user_id
                name_val = (d.get("name") or "").strip()
                display_identity = (
                    resolved_email
                    or (name_val if name_val and name_val.lower() != "user" else None)
                    or d["onyx_user_id"]
                )
                d["email"] = resolved_email
                d["display_identity"] = display_identity
                # Remove helper
                d.pop("cached_email", None)
                d.pop("db_email", None)
                stats_total += 1
                if resolved_email:
                    if resolved_email == user_emails_map.get(d["onyx_user_id"], None):
                        stats_from_api += 1
                    elif resolved_email:
                        # came from DB join or cache; prefer to count DB explicitly
                        stats_from_db += 1 if display_identity == resolved_email else stats_from_cache
                else:
                    unresolved.append(d["onyx_user_id"])
                enriched.append(AdminUserCredits(**d))

            try:
                logger.info(
                    f"[CREDITS] Users listed: total={stats_total}, from_db_join={stats_from_db}, from_api={stats_from_api}, from_cache={stats_from_cache}, unresolved={len(unresolved)}, db_table={db_join_table or 'none'}"
                )
                if unresolved:
                    logger.debug(f"[CREDITS] Unresolved user identifiers (sample): {unresolved[:10]}")
            except Exception:
                pass

            return enriched
    except Exception as e:
        logger.error(f"Error listing user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: Usage analytics across all users
@app.get("/api/custom/admin/credits/usage-analytics", response_model=CreditUsageAnalyticsResponse)
async def get_usage_analytics(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT COALESCE(product_type, 'Unknown') as product_type,
                       COALESCE(SUM(credits), 0) AS credits_used
                FROM credit_transactions
                WHERE type = 'product_generation'
                GROUP BY COALESCE(product_type, 'Unknown')
                ORDER BY credits_used DESC
                """
            )
            usage_by_product = [ProductUsage(product_type=row["product_type"], credits_used=int(row["credits_used"] or 0)) for row in rows]
            total_credits = sum(u.credits_used for u in usage_by_product)
            return CreditUsageAnalyticsResponse(usage_by_product=usage_by_product, total_credits_used=total_credits)
    except Exception as e:
        logger.error(f"Error fetching usage analytics: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch usage analytics")

# get Questionnaire answers for each user
@app.get("/api/custom/admin/questionnaire/all", response_model=List[UserQuestionnaire])
async def list_all_user_questionnaires(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to list all users' initial questionnaire answers"""
    await verify_admin_user(request)
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT onyx_user_id, data
                FROM initial_questionnaire
            """)
            result = []
            for row in rows:
                answers = []
                if isinstance(row["data"], list):
                    for item in row["data"]:
                        q = item.get("question")
                        a = item.get("answer")
                        if isinstance(q, str) and isinstance(a, str):
                            answers.append(QuestionnaireAnswer(question=q, answer=a))
                result.append(UserQuestionnaire(onyx_user_id=row["onyx_user_id"], answers=answers))
            return result
    except Exception as e:
        logger.error(f"Error listing user questionnaires: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user questionnaires")

@app.post("/api/custom/questionnaires/add")
async def add_user_questionnaire(
    questionnaire_request: UserQuestionnaireInsertRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Admin endpoint to insert a user's initial questionnaire answers.
    If user already has answers, this will overwrite them.
    """
    try:
        async with pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO initial_questionnaire (onyx_user_id, data)
                VALUES ($1, $2)
                ON CONFLICT (onyx_user_id) DO UPDATE SET data = EXCLUDED.data
            """, questionnaire_request.onyx_user_id, [answer.dict() for answer in questionnaire_request.answers])
        return {"success": True}
    except Exception as e:
        logger.error(f"Error inserting user questionnaire: {e}")
        raise HTTPException(status_code=500, detail="Failed to insert user questionnaire")

@app.get("/api/custom/questionnaires/{user_id}/completion")
async def check_user_questionnaire_completion(
    user_id: str,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Returns True if the user with the given id has completed the questionnaire, otherwise False.
    """
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT 1 FROM initial_questionnaire WHERE onyx_user_id = $1",
                user_id
            )
            return {"completed": bool(row)}
    except Exception as e:
        logger.error(f"Error checking questionnaire completion for user {user_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to check questionnaire completion")

# Slide analytics across all users
@app.get("/api/custom/admin/analytics/slides", response_model=SlidesAnalyticsResponse)
async def get_slides_analytics(
    request: Request,
    date_from: str,
    date_to: str,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    try:
        start_date = date.fromisoformat(date_from)
        end_date = date.fromisoformat(date_to)

        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                WITH last_usages AS (
                    SELECT
                        slide->>'templateId' AS lu_template_id,
                        MAX(projects.created_at) AS last_usage
                    FROM
                        projects
                    CROSS JOIN LATERAL
                        jsonb_array_elements(microproduct_content->'slides') AS slide
                    WHERE
                        microproduct_content ? 'slides'
                        AND projects.created_at >= $1 AND projects.created_at <= $2
                    GROUP BY
                        lu_template_id
                )
                SELECT
                    slide->>'templateId' AS template_id,
                    COUNT(*) AS total_generated,
                    COUNT(DISTINCT projects.onyx_user_id) AS client_count,
                    COALESCE(error_counts.error_count, 0) AS error_count,
                    COALESCE(last_usages.last_usage, NULL) AS last_usage
                FROM
                    projects
                CROSS JOIN LATERAL
                    jsonb_array_elements(microproduct_content->'slides') AS slide
                LEFT JOIN (
                    SELECT sce.template_id AS ec_template_id, COUNT(*) AS error_count
                    FROM slide_creation_errors sce
                    WHERE sce.created_at >= $1 AND sce.created_at <= $2
                    GROUP BY ec_template_id
                ) AS error_counts
                ON slide->>'templateId' = error_counts.ec_template_id
                LEFT JOIN last_usages
                ON slide->>'templateId' = last_usages.lu_template_id
                WHERE
                    microproduct_content ? 'slides'
                    AND projects.created_at >= $1 AND projects.created_at <= $2
                GROUP BY
                    slide->>'templateId', error_counts.error_count, last_usages.last_usage
                ORDER BY
                    total_generated DESC
                """
            , start_date, end_date)
            template_stats = [
                TemplateTypeUsage(
                    template_id=row['template_id'],
                    total_generated=row['total_generated'],
                    client_count=row['client_count'],
                    error_count=row['error_count'],
                    #error_count=1,
                    last_usage=row['last_usage'].isoformat() if row['last_usage'] else "",
                ) for row in rows
            ]
            return SlidesAnalyticsResponse(usage_by_template=template_stats)
    except Exception as e:
        logger.error(f"Error fetching slides analytics: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch slides analytics")

@app.get("/api/custom/admin/analytics/slides-errors", response_model=SlidesErrorsAnalyticsResponse)
async def get_slides_errors_analytics(
    request: Request,
    date_from: str,
    date_to: str,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    try:
        start_date = date.fromisoformat(date_from)
        end_date = date.fromisoformat(date_to)

        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT 
                    sce.id,
                    sce.user_id,
                    sce.template_id,
                    sce.props,
                    sce.error_message,
                    sce.created_at
                FROM slide_creation_errors sce
                WHERE sce.created_at >= $1 AND sce.created_at <= $2
                ORDER BY sce.created_at DESC
                """,
                start_date, end_date
            )
            errors = [
                SlideGenerationError(
                    id=row["id"],
                    user_id=row["user_id"],
                    template_id=row["template_id"],
                    props=row["props"],
                    error_message=row["error_message"],
                    created_at=row["created_at"]
                )
                for row in rows
            ]
            return SlidesErrorsAnalyticsResponse(errors=errors)
    except Exception as e:
        logger.error(f"Error fetching slides errors analytics: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch slides errors analytics")

@app.post("/api/custom/admin/credits/migrate-users")
async def migrate_onyx_users_to_credits(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to manually trigger migration of Onyx users to credits table"""
    await verify_admin_user(request)
    
    try:
        # Use the same migration function as startup
        migrated_count = await migrate_onyx_users_to_credits_table()
        
        return {
            "success": True,
            "message": f"Successfully migrated {migrated_count} new users with 100 credits each and SmartDrive accounts",
            "users_migrated": migrated_count
        }
    except Exception as e:
        logger.error(f"Error migrating users: {e}")
        raise HTTPException(status_code=500, detail="Failed to migrate users")

@app.post("/api/custom/admin/smartdrive/create-missing-accounts")
async def create_missing_smartdrive_accounts(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to create SmartDrive accounts for users who don't have them yet"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            # Find users with credits but no SmartDrive account
            users_without_smartdrive = await conn.fetch("""
                SELECT uc.onyx_user_id, uc.name
                FROM user_credits uc
                LEFT JOIN smartdrive_accounts sa ON uc.onyx_user_id = sa.onyx_user_id
                WHERE sa.onyx_user_id IS NULL
            """)
            
            if not users_without_smartdrive:
                return {
                    "success": True,
                    "message": "All users already have SmartDrive accounts",
                    "accounts_created": 0
                }
            
            created_count = 0
            for user in users_without_smartdrive:
                try:
                    # Create SmartDrive account placeholder
                    await conn.execute("""
                        INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (onyx_user_id) DO NOTHING
                    """, user['onyx_user_id'], '{}', datetime.now(timezone.utc), datetime.now(timezone.utc))
                    
                    created_count += 1
                    logger.info(f"Created SmartDrive account for existing user: {user['onyx_user_id']} ({user['name']})")
                    
                except Exception as e:
                    logger.warning(f"Failed to create SmartDrive account for user {user['onyx_user_id']}: {e}")
            
            return {
                "success": True,
                "message": f"Successfully created SmartDrive accounts for {created_count} users",
                "accounts_created": created_count
            }
            
    except Exception as e:
        logger.error(f"Error creating missing SmartDrive accounts: {e}")
        raise HTTPException(status_code=500, detail="Failed to create SmartDrive accounts")

@app.post("/api/custom/admin/credits/modify", response_model=CreditTransactionResponse)
async def modify_user_credits(
    transaction: CreditTransactionRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to add or remove credits for a user by email"""
    await verify_admin_user(request)
    
    try:
        if transaction.amount <= 0:
            raise HTTPException(status_code=400, detail="Amount must be positive")
        
        updated_credits = await modify_user_credits_by_email(
            transaction.user_email,
            transaction.amount,
            transaction.action,
            pool,
            transaction.reason
        )
        
        action_msg = "added to" if transaction.action == "add" else "removed from"
        message = f"Successfully {action_msg} {transaction.user_email}: {transaction.amount} credits"
        
        return CreditTransactionResponse(
            success=True,
            message=message,
            new_balance=updated_credits.credits_balance,
            user_credits=updated_credits
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error modifying user credits: {e}")
        raise HTTPException(status_code=500, detail="Failed to modify credits")

@app.get("/api/custom/admin/credits/user/{user_email}", response_model=UserCredits)
async def get_user_credits_by_email(
    user_email: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Admin endpoint to get specific user's credits by email"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM user_credits WHERE onyx_user_id = $1",
                user_email
            )
            
            if not row:
                # Create entry for user if doesn't exist
                row = await conn.fetchrow("""
                    INSERT INTO user_credits (onyx_user_id, name, credits_balance)
                    VALUES ($1, $2, $3)
                    RETURNING *
                """, user_email, user_email.split('@')[0], 0)
            
            return UserCredits(**dict(row))
            
    except Exception as e:
        logger.error(f"Error getting user credits by email: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve user credits")

# NEW: User transaction history (purchases + product generations)
@app.get("/api/custom/admin/credits/user/{user_id}/transactions", response_model=UserTransactionHistoryResponse)
async def get_user_transactions(
    user_id: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    await verify_admin_user(request)
    async with pool.acquire() as conn:
        # Resolve user by numeric id or email
        if user_id.isdigit():
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE id = $1", int(user_id))
        else:
            user_row = await conn.fetchrow("SELECT * FROM user_credits WHERE onyx_user_id = $1", user_id)

        if not user_row:
            raise HTTPException(status_code=404, detail="User not found")

        tx_rows = await conn.fetch(
            """
            SELECT id, type, title, credits, created_at, product_type
            FROM credit_transactions
            WHERE onyx_user_id = $1
            ORDER BY created_at DESC
            LIMIT 200
            """,
            user_row["onyx_user_id"]
        )

        activities = [
            TimelineActivity(
                id=str(r["id"]),
                type=r["type"],
                title=r["title"] or (r["type"].replace('_',' ').title()),
                credits=int(r["credits"] or 0),
                timestamp=r["created_at"],
                product_type=r["product_type"]
            )
            for r in tx_rows
        ]

        return UserTransactionHistoryResponse(
            user_id=int(user_row["id"]),
            user_email=user_row["onyx_user_id"],
            user_name=user_row["name"],
            transactions=activities
        )

# --- Feature Management Endpoints ---

@app.get("/api/custom/admin/features/definitions")
async def get_feature_definitions(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get all feature definitions"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM feature_definitions 
                WHERE is_active = true 
                ORDER BY category, display_name
            """)
            
            return [dict(row) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching feature definitions: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch feature definitions")

@app.get("/api/custom/admin/features/users")
async def get_users_with_features(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get all users and their feature permissions"""
    await verify_admin_user(request)
    
    try:
        # Fetch user emails from Onyx API (robust mapping)
        user_emails_map = {}
        try:
            session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
            if session_cookie:
                users_url = f"{ONYX_API_SERVER_URL}/manage/users"
                cookies_to_forward = {ONYX_SESSION_COOKIE_NAME: session_cookie}
                async with httpx.AsyncClient(timeout=15.0) as client:
                    response = await client.get(users_url, cookies=cookies_to_forward)
                    if response.status_code == 200:
                        users_data = response.json()
                        # Handle both array and object shapes
                        if isinstance(users_data, dict) and 'users' in users_data:
                            users_iterable = users_data.get('users', [])
                        else:
                            users_iterable = users_data if isinstance(users_data, list) else []

                        # Map user IDs to emails with multiple key fallbacks
                        for user in users_iterable:
                            try:
                                user_id_val = (
                                    user.get('userId')
                                    or user.get('id')
                                    or user.get('uuid')
                                    or user.get('user_id')
                                )
                                email_val = (
                                    user.get('email')
                                    or user.get('userEmail')
                                    or user.get('primary_email')
                                )
                                if user_id_val and email_val:
                                    user_emails_map[str(user_id_val)] = str(email_val)
                            except Exception:
                                continue
                        logger.info(f"[FEATURES] Fetched {len(user_emails_map)} user emails from Onyx API")
                    else:
                        logger.warning(f"[FEATURES] Failed to fetch users from Onyx API: {response.status_code}")
        except Exception as e:
            logger.warning(f"[FEATURES] Error fetching user emails from Onyx API: {e}")
        
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT 
                    uc.onyx_user_id AS user_id,
                    uc.name AS user_name,
                    uec.email AS cached_email,
                    uf.feature_name,
                    uf.is_enabled,
                    uf.created_at,
                    uf.updated_at,
                    fd.display_name,
                    fd.description,
                    fd.category
                FROM user_credits uc
                LEFT JOIN user_features uf ON uc.onyx_user_id = uf.user_id
                LEFT JOIN feature_definitions fd 
                    ON uf.feature_name = fd.feature_name AND fd.is_active = true
                LEFT JOIN user_email_cache uec ON uec.onyx_user_id = uc.onyx_user_id
                ORDER BY uc.onyx_user_id, fd.category, fd.display_name
            """)
            
            users_features = {}
            for row in rows:
                user_id = row['user_id']
                if user_id not in users_features:
                    # Get email from map, fallback to cache, then user_id
                    cached_email = dict(row).get('cached_email')
                    user_email = user_emails_map.get(user_id, cached_email or user_id)
                    users_features[user_id] = {
                        'user_id': user_id,
                        'user_email': user_email,
                        'user_name': row['user_name'] or 'Unknown User',
                        'features': []
                    }
                
                if row['feature_name']:
                    users_features[user_id]['features'].append({
                        'feature_name': row['feature_name'],
                        'display_name': row['display_name'],
                        'description': row['description'],
                        'category': row['category'],
                        'is_enabled': row['is_enabled'],
                        'created_at': row['created_at'],
                        'updated_at': row['updated_at']
                    })
            
            return list(users_features.values())
    except Exception as e:
        logger.error(f"Error fetching users with features: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch users with features")

@app.post("/api/custom/admin/features/toggle")
async def toggle_user_feature(
    feature_request: FeatureToggleRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Enable or disable a feature for a single user"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            # Verify feature exists
            feature_row = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_request.feature_name
            )
            
            if not feature_row:
                raise HTTPException(status_code=404, detail="Feature not found")
            
            # Insert or update user feature
            await conn.execute("""
                INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                VALUES ($1, $2, $3, NOW())
                ON CONFLICT (user_id, feature_name) 
                DO UPDATE SET 
                    is_enabled = $3,
                    updated_at = NOW()
            """, feature_request.user_id, feature_request.feature_name, feature_request.is_enabled)
            
            action = "enabled" if feature_request.is_enabled else "disabled"
            return {
                "success": True,
                "message": f"Feature '{feature_row['display_name']}' {action} for user {feature_request.user_id}"
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error toggling user feature: {e}")
        raise HTTPException(status_code=500, detail="Failed to toggle feature")

@app.post("/api/custom/admin/features/bulk-toggle")
async def bulk_toggle_user_features(
    bulk_request: BulkFeatureToggleRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Enable or disable a feature for multiple users"""
    await verify_admin_user(request)
    
    try:
        async with pool.acquire() as conn:
            # Verify feature exists
            feature_row = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                bulk_request.feature_name
            )
            
            if not feature_row:
                raise HTTPException(status_code=404, detail="Feature not found")
            
            # Bulk insert/update user features
            updated_count = 0
            for user_id in bulk_request.user_ids:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                    VALUES ($1, $2, $3, NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = $3,
                        updated_at = NOW()
                """, user_id, bulk_request.feature_name, bulk_request.is_enabled)
                updated_count += 1
            
            action = "enabled" if bulk_request.is_enabled else "disabled"
            return {
                "success": True,
                "message": f"Feature '{feature_row['display_name']}' {action} for {updated_count} users",
                "users_updated": updated_count
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error bulk toggling user features: {e}")
        raise HTTPException(status_code=500, detail="Failed to bulk toggle features")

@app.get("/api/custom/features/check/{feature_name}")
async def check_user_feature(
    feature_name: str,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Check if a feature is enabled for the current user"""
    try:
        user_id = await get_current_onyx_user_id(request)
        if not user_id:
            return {"is_enabled": False}
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT uf.is_enabled 
                FROM user_features uf
                JOIN feature_definitions fd ON uf.feature_name = fd.feature_name
                WHERE uf.user_id = $1 AND uf.feature_name = $2 AND fd.is_active = true
            """, user_id, feature_name)
            
            return {"is_enabled": bool(row['is_enabled']) if row else False}
    except Exception as e:
        logger.error(f"Error checking user feature: {e}")
        return {"is_enabled": False}

@app.get("/api/custom/admin/features/user-types")
async def get_user_types(request: Request):
    """Get available user types and their features"""
    await verify_admin_user(request)
    return USER_TYPES

@app.post("/api/custom/admin/features/assign-user-type")
async def assign_user_type(
    assignment_request: UserTypeAssignmentRequest,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Assign a user type to multiple users, enabling features for that type and disabling others"""
    await verify_admin_user(request)
    
    if assignment_request.user_type not in USER_TYPES:
        raise HTTPException(status_code=400, detail="Invalid user type")
    
    try:
        user_type_info = USER_TYPES[assignment_request.user_type]
        features_to_enable = set(user_type_info["features"])
        
        async with pool.acquire() as conn:
            # Get all active features
            all_features_rows = await conn.fetch(
                "SELECT feature_name FROM feature_definitions WHERE is_active = true"
            )
            all_features = {row['feature_name'] for row in all_features_rows}
            
            # Features to disable = all features - features to enable
            features_to_disable = all_features - features_to_enable
            
            users_updated = 0
            features_enabled = 0
            features_disabled = 0
            
            for user_id in assignment_request.user_ids:
                # Enable features for this user type
                for feature_name in features_to_enable:
                    if feature_name in all_features:  # Verify feature exists
                        await conn.execute("""
                            INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                            VALUES ($1, $2, true, NOW())
                            ON CONFLICT (user_id, feature_name) 
                            DO UPDATE SET 
                                is_enabled = true,
                                updated_at = NOW()
                        """, user_id, feature_name)
                        features_enabled += 1
                
                # Disable features NOT in this user type
                for feature_name in features_to_disable:
                    if feature_name in all_features:  # Verify feature exists
                        await conn.execute("""
                            INSERT INTO user_features (user_id, feature_name, is_enabled, updated_at)
                            VALUES ($1, $2, false, NOW())
                            ON CONFLICT (user_id, feature_name) 
                            DO UPDATE SET 
                                is_enabled = false,
                                updated_at = NOW()
                        """, user_id, feature_name)
                        features_disabled += 1
                
                users_updated += 1
            
            return {
                "success": True,
                "message": f"Assigned '{user_type_info['display_name']}' type to {users_updated} users ({features_enabled} features enabled, {features_disabled} features disabled)",
                "users_updated": users_updated,
                "features_enabled": features_enabled,
                "features_disabled": features_disabled,
                "user_type": user_type_info["display_name"]
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error assigning user type: {e}")
        raise HTTPException(status_code=500, detail="Failed to assign user type")

async def assign_default_user_type(user_id: str, conn: asyncpg.Connection):
    """Assign default 'Normal (HR)' user type to a new user"""
    try:
        default_user_type = "normal_hr"
        if default_user_type not in USER_TYPES:
            logger.warning(f"Default user type {default_user_type} not found in USER_TYPES")
            return
        
        user_type_info = USER_TYPES[default_user_type]
        features_to_enable = user_type_info["features"]
        
        # Enable features for the default user type
        features_assigned = 0
        for feature_name in features_to_enable:
            # Check if feature exists before trying to assign it
            feature_exists = await conn.fetchrow(
                "SELECT * FROM feature_definitions WHERE feature_name = $1 AND is_active = true",
                feature_name
            )
            
            if feature_exists:
                await conn.execute("""
                    INSERT INTO user_features (user_id, feature_name, is_enabled, created_at, updated_at)
                    VALUES ($1, $2, true, NOW(), NOW())
                    ON CONFLICT (user_id, feature_name) 
                    DO UPDATE SET 
                        is_enabled = true,
                        updated_at = NOW()
                """, user_id, feature_name)
                features_assigned += 1
            else:
                logger.warning(f"Feature {feature_name} not found or inactive for new user {user_id}")
        
        logger.info(f"Assigned default user type '{user_type_info['display_name']}' to new user {user_id} ({features_assigned} features enabled)")
        
    except Exception as e:
        logger.error(f"Error assigning default user type to new user {user_id}: {e}")
        # Don't raise exception to avoid blocking user creation

@app.post("/api/custom/projects/duplicate/{project_id}", response_model=ProjectDuplicationResponse)
async def duplicate_project(project_id: int, request: Request, user_id: str = Depends(get_current_onyx_user_id)):
    """
    Duplicate a project. If it's a Training Plan, also duplicate all connected products (lessons, quizzes, etc.).
    Enhanced with proper transaction management and complete field mapping.
    """
    async with DB_POOL.acquire() as conn:
        # Start transaction for atomic operations
        async with conn.transaction():
            try:
                # Fetch original project with all fields
                orig = await conn.fetchrow("SELECT * FROM projects WHERE id = $1", project_id)
                if not orig:
                    raise HTTPException(status_code=404, detail="Project not found")
                
                # Verify user ownership
                if orig['onyx_user_id'] != user_id:
                    raise HTTPException(status_code=403, detail="Access denied")
                
                new_name = f"Copy of {orig['project_name']}"
                now = datetime.now(timezone.utc)
                
                logger.info(f"Starting duplication of project {project_id} (type: {orig['microproduct_type']}) for user {user_id}")
                
                if orig['microproduct_type'] == "Training Plan":
                    # Training Plan duplication - handle connected products
                    new_session_id = str(uuid4())
                    
                    # Duplicate the main Training Plan with all fields
                    new_outline_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                        RETURNING id
                        """,
                        user_id,
                        new_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],  # JSONB will be handled automatically by asyncpg
                        orig['design_template_id'],
                        now,
                        new_session_id,
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Created new Training Plan with ID {new_outline_id}")
                    
                    # Find all connected products using the same naming patterns as frontend
                    # Get all user's projects to search through
                    all_projects = await conn.fetch(
                        "SELECT * FROM projects WHERE onyx_user_id = $1 ORDER BY created_at",
                        user_id
                    )
                    
                    # Find connected products using frontend naming patterns
                    connected = []
                    original_outline_name = orig['project_name'].strip()
                    
                    for project in all_projects:
                        if project['id'] == orig['id']:
                            continue  # Skip the original training plan.
                        
                        project_name = project['project_name'].strip()
                        micro_name = project['microproduct_name']
                        
                        # Skip other Training Plans
                        if project['microproduct_type'] == "Training Plan":
                            continue
                        
                        is_connected = False
                        
                        # Method 1: Legacy matching - project name matches outline and microProductName matches lesson
                        if project_name == original_outline_name and micro_name:
                            is_connected = True
                            logger.info(f"Found connected product via legacy matching: {project_name} (micro: {micro_name})")
                        
                        # Method 2: New naming convention - project name follows "Outline Name: Lesson Title" pattern
                        elif ': ' in project_name:
                            outline_part = project_name.split(': ')[0].strip()
                            if outline_part == original_outline_name:
                                is_connected = True
                                logger.info(f"Found connected product via new pattern: {project_name}")
                        
                        # Method 3: Legacy patterns for backward compatibility
                        # Legacy Quiz pattern - "Quiz - Outline Name: Lesson Title"
                        elif project_name.startswith('Quiz - ') and ': ' in project_name:
                            quiz_part = project_name.replace('Quiz - ', '', 1)
                            if ': ' in quiz_part:
                                outline_part = quiz_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy quiz pattern: {project_name}")
                        
                        # Legacy Text Presentation pattern - "Text Presentation - Outline Name: Lesson Title"
                        elif project_name.startswith('Text Presentation - ') and ': ' in project_name:
                            text_part = project_name.replace('Text Presentation - ', '', 1)
                            if ': ' in text_part:
                                outline_part = text_part.split(': ')[0].strip()
                                if outline_part == original_outline_name:
                                    is_connected = True
                                    logger.info(f"Found connected product via legacy text presentation pattern: {project_name}")
                        
                        # Method 4: Alternative pattern - project name matches lesson title directly
                        # This is for cases where the lesson title became the project name
                        elif orig['microproduct_content']:
                            # Check if this project name matches any lesson title in the training plan
                            try:
                                content = orig['microproduct_content']
                                if isinstance(content, dict) and 'sections' in content:
                                    for section in content['sections']:
                                        if 'lessons' in section:
                                            for lesson in section['lessons']:
                                                lesson_title = lesson.get('title', '').strip()
                                                if lesson_title and lesson_title == project_name:
                                                    is_connected = True
                                                    logger.info(f"Found connected product via lesson title matching: {project_name}")
                                                    break
                                        if is_connected:
                                            break
                                    if is_connected:
                                        break
                            except Exception as e:
                                logger.warning(f"Error checking lesson title matching for {project_name}: {e}")
                        
                        if is_connected:
                            connected.append(project)
                    
                    logger.info(f"Found {len(connected)} connected products to duplicate")
                    
                    # Duplicate each connected product
                    duplicated_products = []
                    for i, prod in enumerate(connected):
                        try:
                            # Smart name replacement - handle various naming patterns
                            prod_name = prod['project_name']
                            if prod_name.startswith(orig['project_name']):
                                prod_name = prod_name.replace(orig['project_name'], new_name, 1)
                            else:
                                # If name doesn't start with parent name, just add "Copy of" prefix
                                prod_name = f"Copy of {prod_name}"
                            
                            # Update microproduct name if it references the parent
                            micro_name = prod['microproduct_name']
                            if micro_name and micro_name.startswith(orig['project_name']):
                                micro_name = micro_name.replace(orig['project_name'], new_name, 1)
                            
                            # Insert the duplicated product with all fields
                            new_prod_id = await conn.fetchval(
                                """
                                INSERT INTO projects (
                                    onyx_user_id, project_name, product_type, microproduct_type, 
                                    microproduct_name, microproduct_content, design_template_id, 
                                    created_at, source_chat_session_id, folder_id, "order", 
                                    is_standalone, completion_time, custom_rate, quality_tier
                                )
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                                RETURNING id
                                """,
                                user_id,
                                prod_name,
                                prod['product_type'],
                                prod['microproduct_type'],
                                micro_name,
                                prod['microproduct_content'],  # JSONB content preserved
                                prod['design_template_id'],
                                now,
                                new_session_id,  # Link to new Training Plan
                                prod['folder_id'],
                                prod['order'],
                                prod['is_standalone'],
                                prod['completion_time'],
                                prod['custom_rate'],
                                prod['quality_tier']
                            )
                            
                            duplicated_products.append({
                                'original_id': prod['id'],
                                'new_id': new_prod_id,
                                'type': prod['microproduct_type'],
                                'name': prod_name
                            })
                            
                            logger.info(f"Duplicated {prod['microproduct_type']} '{prod['project_name']}' -> '{prod_name}' (ID: {new_prod_id})")
                            
                        except Exception as e:
                            logger.error(f"Failed to duplicate connected product {prod['id']} ({prod['microproduct_type']}): {str(e)}")
                            # Re-raise to trigger transaction rollback
                            raise HTTPException(
                                status_code=500, 
                                detail=f"Failed to duplicate {prod['microproduct_type']} '{prod['project_name']}': {str(e)}"
                            )
                    
                    logger.info(f"Successfully duplicated Training Plan and {len(duplicated_products)} connected products")
                    
                    return {
                        "id": new_outline_id,
                        "name": new_name,
                        "type": "Training Plan",
                        "connected_products": duplicated_products,
                        "total_products_duplicated": len(duplicated_products) + 1
                    }
                    
                else:
                    # Regular product duplication (non-Training Plan)
                    new_prod_name = f"Copy of {orig['project_name']}"
                    
                    new_id = await conn.fetchval(
                        """
                        INSERT INTO projects (
                            onyx_user_id, project_name, product_type, microproduct_type, 
                            microproduct_name, microproduct_content, design_template_id, 
                            created_at, source_chat_session_id, folder_id, "order", 
                            is_standalone, course_id, completion_time, custom_rate, quality_tier
                        )
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
                        RETURNING id
                        """,
                        user_id,
                        new_prod_name,
                        orig['product_type'],
                        orig['microproduct_type'],
                        orig['microproduct_name'],
                        orig['microproduct_content'],
                        orig['design_template_id'],
                        now,
                        str(uuid4()),  # New session ID for standalone product
                        orig['folder_id'],
                        orig['order'],
                        orig['is_standalone'],
                        orig['course_id'],
                        orig['completion_time'],
                        orig['custom_rate'],
                        orig['quality_tier']
                    )
                    
                    logger.info(f"Successfully duplicated {orig['microproduct_type']} '{orig['project_name']}' -> '{new_prod_name}' (ID: {new_id})")
                    
                    return {
                        "id": new_id,
                        "name": new_prod_name,
                        "type": orig['microproduct_type'],
                        "total_products_duplicated": 1
                    }
                    
            except HTTPException:
                # Re-raise HTTP exceptions (these are expected errors)
                raise
            except Exception as e:
                logger.error(f"Unexpected error during project duplication: {str(e)}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to duplicate project: {str(e)}"
                )

# --- Video Generation API Endpoints ---

# Import video generation service safely
video_generation_service = None
presentation_service = None
try:
    from app.services.video_generation_service import video_generation_service
    logger.info("Video generation service imported successfully")
except Exception as e:
    logger.warning(f"Video generation service not available: {e}")
    video_generation_service = None

# Import presentation service safely
try:
    from app.services.presentation_service import presentation_service, PresentationRequest
    logger.info("Presentation service imported successfully")
except Exception as e:
    logger.warning(f"Presentation service not available: {e}")
    presentation_service = None

@app.get("/api/custom/video/avatars")
async def get_avatars():
    """Get available avatars from Elai API."""
    try:
        if not video_generation_service:
            return {
                "success": False, 
                "error": "Video generation service not available. Please check backend configuration.",
                "avatars": []
            }
        
        result = await video_generation_service.get_avatars()
        
        if result["success"]:
            return {"success": True, "avatars": result["avatars"]}
        else:
            return {"success": False, "error": result["error"], "avatars": []}
            
    except Exception as e:
        logger.error(f"Error fetching avatars: {str(e)}")
        return {
            "success": False, 
            "error": f"Failed to fetch avatars: {str(e)}",
            "avatars": []
        }

@app.post("/api/custom/video/generate")
async def generate_video(request: Request):
    """Generate video from slides and avatar data."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        avatar_data = body.get("avatar", {})
        
        # Validate request data
        if not slides_data:
            return {"success": False, "error": "No slides data provided"}
        
        if not avatar_data:
            return {"success": False, "error": "No avatar data provided"}
        
        # Generate video
        result = await video_generation_service.generate_video(slides_data, avatar_data)
        
        if result["success"]:
            return {
                "success": True,
                "video_id": result["video_id"],
                "download_url": result["download_url"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.get("/api/custom/video/status/{video_id}")
async def get_video_status(video_id: str):
    """Get the status of a video generation."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        status_data = await video_generation_service.check_video_status(video_id)
        
        if status_data:
            return {"success": True, "status": status_data}
        else:
            return {"success": False, "error": "Video not found"}
            
    except Exception as e:
        logger.error(f"Error checking video status: {str(e)}")
        return {"success": False, "error": f"Failed to check video status: {str(e)}"}

@app.post("/api/custom/video/create")
async def create_video(request: Request):
    """Create a new video with Elai API."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        project_name = body.get("projectName", "Generated Video")
        voiceover_texts = body.get("voiceoverTexts", [])
        avatar_code = body.get("avatarCode")  # None will trigger auto-selection
        
        # Validate request data
        if not voiceover_texts:
            return {"success": False, "error": "No voiceover texts provided"}
        
        # Create video
        logger.info(f"Creating video with project name: {project_name}")
        logger.info(f"Voiceover texts count: {len(voiceover_texts)}")
        logger.info(f"Avatar code: {avatar_code}")
        
        result = await video_generation_service.create_video_from_texts(project_name, voiceover_texts, avatar_code)
        
        logger.info(f"Video creation result: {result}")
        
        if result["success"]:
            return {
                "success": True,
                "videoId": result["videoId"],
                "message": "Video created successfully"
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error creating video: {str(e)}")
        return {"success": False, "error": f"Failed to create video: {str(e)}"}

@app.post("/api/custom/video/render/{video_id}")
async def render_video(video_id: str):
    """Start rendering a video."""
    try:
        if not video_generation_service:
            return {
                "success": False,
                "error": "Video generation service not available. Please check backend configuration."
            }
        
        # Start rendering
        result = await video_generation_service.render_video(video_id)
        
        if result["success"]:
            return {
                "success": True,
                "message": "Video rendering started successfully"
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error starting video render: {str(e)}")
        return {"success": False, "error": f"Failed to start video render: {str(e)}"}

# ============================================================================
# Clean Video Generation API Endpoints (HTML → PNG → Video Pipeline)
# ============================================================================

@app.post("/api/custom/clean-video/avatar-slide")
async def generate_clean_avatar_slide_video(request: Request):
    """Generate video for a single avatar slide using clean HTML → PNG → Video pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slide_props = body.get("slideProps", {})
        theme = body.get("theme", "dark-purple")
        slide_duration = body.get("slideDuration", 5.0)
        quality = body.get("quality", "high")
        
        # Validate slide props
        validation = await clean_video_generation_service.validate_slide_props(slide_props)
        if not validation["valid"]:
            return {
                "success": False,
                "error": f"Invalid slide props: {validation['error']}"
            }
        
        # Generate video
        result = await clean_video_generation_service.generate_avatar_slide_video(
            slide_props=slide_props,
            theme=theme,
            slide_duration=slide_duration,
            quality=quality
        )
        
        if result["success"]:
            return {
                "success": True,
                "video_url": result["video_url"],
                "video_path": result["video_path"],
                "file_size": result["file_size"],
                "duration": result["duration"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating clean avatar slide video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.post("/api/custom/clean-video/presentation")
async def generate_clean_presentation_video(request: Request):
    """Generate video for multiple avatar slides using clean HTML → PNG → Video pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slides_props = body.get("slidesProps", [])
        theme = body.get("theme", "dark-purple")
        slide_duration = body.get("slideDuration", 5.0)
        quality = body.get("quality", "high")
        
        # Validate request
        if not slides_props:
            return {
                "success": False,
                "error": "No slides provided"
            }
        
        # Validate each slide
        for i, slide_props in enumerate(slides_props):
            validation = await clean_video_generation_service.validate_slide_props(slide_props)
            if not validation["valid"]:
                return {
                    "success": False,
                    "error": f"Invalid slide {i+1} props: {validation['error']}"
                }
        
        # Generate video
        result = await clean_video_generation_service.generate_presentation_video(
            slides_props=slides_props,
            theme=theme,
            slide_duration=slide_duration,
            quality=quality
        )
        
        if result["success"]:
            return {
                "success": True,
                "video_url": result["video_url"],
                "video_path": result["video_path"],
                "file_size": result["file_size"],
                "duration": result["duration"],
                "slides_count": result["slides_count"]
            }
        else:
            return {"success": False, "error": result["error"]}
            
    except Exception as e:
        logger.error(f"Error generating clean presentation video: {str(e)}")
        return {"success": False, "error": f"Failed to generate video: {str(e)}"}

@app.get("/api/custom/clean-video/test")
async def test_clean_video_pipeline():
    """Test the clean video generation pipeline."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        # Run pipeline test
        result = await clean_video_generation_service.test_pipeline()
        
        return result
        
    except Exception as e:
        logger.error(f"Error testing clean video pipeline: {str(e)}")
        return {"success": False, "error": f"Pipeline test failed: {str(e)}"}

@app.get("/api/custom/clean-video/templates")
async def get_supported_avatar_templates():
    """Get list of supported avatar template IDs."""
    try:
        # Import the clean video generation service
        from app.services.clean_video_generation_service import clean_video_generation_service
        
        templates = await clean_video_generation_service.get_supported_templates()
        
        return {
            "success": True,
            "templates": templates
        }
        
    except Exception as e:
        logger.error(f"Error getting supported templates: {str(e)}")
        return {"success": False, "error": f"Failed to get templates: {str(e)}"}

@app.get("/api/custom/video-system/status")
async def get_video_system_status():
    """Get video generation system status."""
    try:
        # Check HTML to Image service status
        from app.services.html_to_image_service import html_to_image_service
        image_service_status = html_to_image_service.get_status()
        
        return {
            "success": True,
            "system": "Clean Video Generation Pipeline",
            "screenshot_services": "DISABLED",
            "chromium_browser": "NOT REQUIRED",
            "clean_pipeline": "ACTIVE",
            "avatar_selection": "DYNAMIC",
            "image_conversion": image_service_status,
            "supported_formats": ["avatar-checklist", "avatar-crm", "avatar-service", "avatar-buttons", "avatar-steps"],
            "output_resolution": "1920x1080",
            "pipeline": "Props → HTML → PNG → Video"
        }
        
    except Exception as e:
        logger.error(f"Error getting video system status: {str(e)}")
        return {"success": False, "error": f"Failed to get status: {str(e)}"}

# ============================================================================
# Professional Presentation API Endpoints
# ============================================================================

@app.post("/api/custom/presentations")
async def create_presentation(request: Request):
    """Create a new professional video presentation."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        # Parse request body
        body = await request.json()
        
        # Extract parameters
        slide_url = body.get("slideUrl")
        voiceover_texts = body.get("voiceoverTexts", [])
        # NEW: Accept actual slide data
        slides_data = body.get("slidesData")  # Optional - actual slide content with text, props, etc.
        theme = body.get("theme", "dark-purple")  # Theme for slide generation
        avatar_code = body.get("avatarCode")  # None will trigger auto-selection
        use_avatar_mask = body.get("useAvatarMask", True)  # NEW: Use avatar mask service by default
        duration = body.get("duration", 30.0)
        layout = body.get("layout", "picture_in_picture")
        quality = body.get("quality", "high")
        resolution = body.get("resolution", [1920, 1080])
        project_name = body.get("projectName", "Generated Presentation")
        
        # NEW: Extract voice parameters
        voice_id = body.get("voiceId")
        voice_provider = body.get("voiceProvider")
        
        # NEW: Extract transitions for multi-slide presentations
        transitions = body.get("transitions", [])  # Optional - transitions between slides
        
        # Add detailed logging for debugging
        logger.info("🎬 [MAIN_ENDPOINT] ========== PRESENTATION REQUEST RECEIVED ==========")
        logger.info("🎬 [MAIN_ENDPOINT] Received presentation request parameters:")
        logger.info(f"  - slide_url: {slide_url}")
        logger.info(f"  - voiceover_texts_count: {len(voiceover_texts) if voiceover_texts else 0}")
        logger.info(f"  - slides_data_count: {len(slides_data) if slides_data else 0}")
        logger.info(f"  - transitions_count: {len(transitions) if transitions else 0}")
        logger.info(f"  - theme: {theme}")
        logger.info(f"  - avatar_code: {avatar_code}")
        logger.info(f"  - use_avatar_mask: {use_avatar_mask}")
        logger.info(f"  - duration: {duration}")
        logger.info(f"  - layout: {layout}")
        logger.info(f"  - quality: {quality}")
        logger.info(f"  - resolution: {resolution}")
        logger.info(f"  - project_name: {project_name}")
        
        # NEW: Log voice parameters
        logger.info("🎤 [MAIN_ENDPOINT] Voice parameters received:")
        logger.info(f"  - voice_id: {voice_id}")
        logger.info(f"  - voice_provider: {voice_provider}")
        logger.info("🎤 [MAIN_ENDPOINT] ========== VOICE PARAMETERS LOGGED ==========")
        
        # NEW: Log transitions
        if transitions and len(transitions) > 0:
            logger.info("🎞️ [MAIN_ENDPOINT] Transitions received:")
            for i, trans in enumerate(transitions):
                logger.info(f"  - Transition {i}: type={trans.get('type')}, duration={trans.get('duration')}s")
        
        # Validate required parameters  
        # slideUrl is required only if no slidesData provided
        if not slide_url and not slides_data:
            return {"success": False, "error": "Either slideUrl or slidesData is required"}
        
        if not voiceover_texts or len(voiceover_texts) == 0:
            return {"success": False, "error": "voiceoverTexts is required"}
        
        # Enforce slides-per-presentation limit via entitlements
        try:
            onyx_user_id = await get_current_onyx_user_id(request)
            ent = await _fetch_effective_entitlements(onyx_user_id, DB_POOL)
            max_slides = int(ent.get("slides_max", 20))
            slides_count = len(slides_data or []) if isinstance(slides_data, list) else 0
            if slides_count == 0 and slide_url:
                # Single slide fallback counts as 1
                slides_count = 1
            if slides_count > max_slides:
                return {"success": False, "error": f"Slide limit exceeded: {slides_count} > {max_slides}"}
        except Exception as _e:
            logger.warning(f"Entitlements check failed, proceeding with defaults: {_e}")
        
        # Validate layout
        allowed_layouts = ["side_by_side", "picture_in_picture", "split_screen"]
        if layout not in allowed_layouts:
            return {"success": False, "error": f"layout must be one of {allowed_layouts}"}
        
        # Create presentation request
        logger.info("🎬 [MAIN_ENDPOINT] Creating PresentationRequest object...")
        presentation_request = PresentationRequest(
            slide_url=slide_url or "",  # Provide empty string if None
            voiceover_texts=voiceover_texts,
            slides_data=slides_data,  # NEW: Pass actual slide data
            theme=theme,  # NEW: Pass theme
            avatar_code=avatar_code,
            use_avatar_mask=use_avatar_mask,  # NEW: Pass avatar mask flag
            duration=duration,
            layout=layout,
            quality=quality,
            resolution=tuple(resolution),
            project_name=project_name,
            voice_id=voice_id,  # NEW: Pass voice ID
            voice_provider=voice_provider,  # NEW: Pass voice provider
            transitions=transitions  # NEW: Pass transitions array
        )
        logger.info(f"🎬 [MAIN_ENDPOINT] PresentationRequest created with use_avatar_mask: {presentation_request.use_avatar_mask}")
        logger.info(f"🎤 [MAIN_ENDPOINT] PresentationRequest created with voice_id: {presentation_request.voice_id}, voice_provider: {presentation_request.voice_provider}")
        logger.info(f"🎞️ [MAIN_ENDPOINT] PresentationRequest created with {len(transitions) if transitions else 0} transitions")
        
        # Create presentation
        job_id = await presentation_service.create_presentation(presentation_request)
        
        # Immediate response to prevent timeout
        response = {
            "success": True,
            "jobId": job_id,
            "status": "processing",
            "progress": 0,
            "message": "Presentation generation started - check status with job ID",
            "estimatedTime": "60-90 seconds"
        }
        
        logger.info(f"Returning immediate response for job {job_id}")
        return response
        
    except Exception as e:
        logger.error(f"Error creating presentation: {str(e)}")
        return {"success": False, "error": f"Failed to create presentation: {str(e)}"}

@app.get("/api/custom/presentations/test/quick")
async def test_quick_response():
    """Quick test endpoint to verify no timeout issues."""
    from datetime import datetime
    return {
        "success": True,
        "message": "Quick response test successful",
        "timestamp": datetime.now().isoformat(),
        "backend_status": "active"
    }

@app.get("/api/custom/presentations/{job_id}")
async def get_presentation_status(job_id: str):
    """Get presentation processing status."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        job = await presentation_service.get_job_status(job_id)
        
        if not job:
            return {"success": False, "error": "Job not found"}
        
        return {
            "success": True,
            "jobId": job.job_id,
            "status": job.status,
            "progress": job.progress,
            "error": job.error,
            "videoUrl": job.video_url,
            "thumbnailUrl": job.thumbnail_url,
            "slideImageUrl": f"/api/custom/presentations/{job.job_id}/slide-image" if job.slide_image_path else None,
            "createdAt": job.created_at.isoformat() if job.created_at else None,
            "completedAt": job.completed_at.isoformat() if job.completed_at else None,
            "lastHeartbeat": job.last_heartbeat.isoformat() if job.last_heartbeat else None
        }
        
    except Exception as e:
        logger.error(f"Error getting presentation status: {str(e)}")
        return {"success": False, "error": f"Failed to get presentation status: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/video")
async def download_presentation_video(job_id: str):
    """Download the completed presentation video."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        video_path = await presentation_service.get_presentation_video(job_id)
        
        if not video_path:
            return {"success": False, "error": "Video not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=video_path,
            media_type="video/mp4",
            filename=f"presentation_{job_id}.mp4"
        )
        
    except Exception as e:
        logger.error(f"Error downloading presentation video: {str(e)}")
        return {"success": False, "error": f"Failed to download video: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/thumbnail")
async def get_presentation_thumbnail(job_id: str):
    """Get the presentation thumbnail."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        thumbnail_path = await presentation_service.get_presentation_thumbnail(job_id)
        
        if not thumbnail_path:
            return {"success": False, "error": "Thumbnail not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=thumbnail_path,
            media_type="image/jpeg",
            filename=f"thumbnail_{job_id}.jpg"
        )
        
    except Exception as e:
        logger.error(f"Error getting presentation thumbnail: {str(e)}")
        return {"success": False, "error": f"Failed to get thumbnail: {str(e)}"}

@app.get("/api/custom/presentations/{job_id}/slide-image")
async def download_presentation_slide_image(job_id: str):
    """Download the generated slide image for debugging."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        slide_image_path = await presentation_service.get_presentation_slide_image(job_id)
        
        if not slide_image_path:
            return {"success": False, "error": "Slide image not found or not completed"}
        
        # Return file response
        return FileResponse(
            path=slide_image_path,
            media_type="image/png",
            filename=f"slide_image_{job_id}.png"
        )
        
    except Exception as e:
        logger.error(f"Error downloading presentation slide image: {str(e)}")
        return {"success": False, "error": f"Failed to download slide image: {str(e)}"}

@app.post("/api/custom/slide-image/generate")
async def generate_slide_image(request: Request):
    """Generate slide image from current slide data (standalone, no video generation)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Generating slide image")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Import the HTML to image service
        from app.services.html_to_image_service import html_to_image_service
        
        # Generate a unique ID for this image generation
        import uuid
        image_id = str(uuid.uuid4())
        
        # Create output directory
        from pathlib import Path
        output_dir = Path("output/slide_images")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate image for the first slide (or all slides if needed)
        slide_props = slides_data[0]  # Use first slide
        template_id = slide_props.get("templateId")
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Template ID: {template_id}")
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Slide props keys: {list(slide_props.keys())}")
        
        if not template_id:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Actual props keys: {list(actual_props.keys())}")
        
        # Log some key props for debugging
        for key, value in actual_props.items():
            if isinstance(value, str):
                logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] {key}: '{value[:100]}...'")
            else:
                logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] {key}: {value}")
        
        # Generate output filename
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"slide_image_{template_id}_{timestamp}_{image_id[:8]}.png"
        output_path = str(output_dir / output_filename)
        
        logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Output path: {output_path}")
        
        # Convert slide to PNG
        success = await html_to_image_service.convert_slide_to_png(
            template_id=template_id,
            props=actual_props,
            theme=theme,
            output_path=output_path
        )
        
        if success:
            # Verify file was created
            if not os.path.exists(output_path):
                logger.error(f"📷 [STANDALONE_SLIDE_IMAGE] File not found after generation: {output_path}")
                return {"success": False, "error": "Generated file not found"}
            
            file_size = os.path.getsize(output_path)
            logger.info(f"📷 [STANDALONE_SLIDE_IMAGE] Successfully generated slide image: {output_path} ({file_size} bytes)")
            
            # Return the image directly
            return FileResponse(
                path=output_path,
                media_type="image/png",
                filename=output_filename
            )
        else:
            logger.error("📷 [STANDALONE_SLIDE_IMAGE] Failed to generate slide image")
            return {"success": False, "error": "Failed to generate slide image"}
        
    except Exception as e:
        logger.error(f"📷 [STANDALONE_SLIDE_IMAGE] Error generating slide image: {str(e)}")
        return {"success": False, "error": f"Failed to generate slide image: {str(e)}"}

@app.post("/api/custom/slide-html/preview")
async def preview_slide_html(request: Request):
    """Preview the static HTML for a slide (debugging feature)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"🔍 [HTML_PREVIEW] Generating HTML preview")
        logger.info(f"🔍 [HTML_PREVIEW] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"🔍 [HTML_PREVIEW] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("🔍 [HTML_PREVIEW] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Get the first slide
        slide_props = slides_data[0]
        template_id = slide_props.get("templateId")
        slide_id = slide_props.get("slideId")
        metadata = slide_props.get("metadata", {})
        
        logger.info(f"🔍 [HTML_PREVIEW] Template ID: {template_id}")
        logger.info(f"🔍 [HTML_PREVIEW] Slide ID: {slide_id}")
        logger.info(f"🔍 [HTML_PREVIEW] Metadata: {metadata}")
        logger.info(f"🔍 [HTML_PREVIEW] Slide props keys: {list(slide_props.keys())}")
        
        if not template_id:
            logger.error("🔍 [HTML_PREVIEW] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        logger.info(f"🔍 [HTML_PREVIEW] Actual props keys: {list(actual_props.keys())}")
        
        # CRITICAL: Log text element positioning data at endpoint level
        logger.info(f"🔍 [ENDPOINT_POSITIONING_DEBUG] === ENDPOINT LEVEL POSITIONING ANALYSIS ===")
        logger.info(f"🔍 [ENDPOINT_POSITIONING_DEBUG] Raw slide data received:")
        logger.info(f"  - Slide ID: {slide_id}")
        logger.info(f"  - Metadata: {metadata}")
        logger.info(f"  - Metadata type: {type(metadata)}")
        
        if metadata and isinstance(metadata, dict):
            element_positions = metadata.get('elementPositions', {})
            logger.info(f"🔍 [ENDPOINT_POSITIONING_DEBUG] Element positions in metadata:")
            logger.info(f"  - Element positions: {element_positions}")
            logger.info(f"  - Element positions keys: {list(element_positions.keys()) if element_positions else 'None'}")
            
            # Log each text element position at endpoint level
            if element_positions:
                for element_id, position in element_positions.items():
                    if 'draggable' in element_id:  # Text elements use draggable IDs
                        logger.info(f"🔍 [ENDPOINT_POSITIONING_DEBUG] Text Element at Endpoint:")
                        logger.info(f"    - Element ID: {element_id}")
                        logger.info(f"    - Position: {position}")
                        logger.info(f"    - X coordinate: {position.get('x', 'MISSING')}")
                        logger.info(f"    - Y coordinate: {position.get('y', 'MISSING')}")
            else:
                logger.warning(f"🔍 [ENDPOINT_POSITIONING_DEBUG] ⚠️ NO ELEMENT POSITIONS FOUND IN SLIDE METADATA")
        else:
            logger.warning(f"🔍 [ENDPOINT_POSITIONING_DEBUG] ⚠️ NO METADATA IN SLIDE DATA")
        
        # Log some key props for debugging
        for key, value in actual_props.items():
            if isinstance(value, str):
                logger.info(f"🔍 [HTML_PREVIEW] {key}: '{value[:100]}...'")
            else:
                logger.info(f"🔍 [HTML_PREVIEW] {key}: {value}")
        
        # Import the HTML template service
        from app.services.html_template_service import html_template_service
        
        # Generate clean HTML with slideId and metadata
        logger.info(f"🔍 [HTML_PREVIEW] Generating HTML content...")
        html_content = html_template_service.generate_clean_html_for_video(
            template_id, actual_props, theme, metadata=metadata, slide_id=slide_id
        )
        
        logger.info(f"🔍 [HTML_PREVIEW] HTML content generated")
        logger.info(f"🔍 [HTML_PREVIEW] HTML content length: {len(html_content)} characters")
        
        # Return the HTML content
        return {
            "success": True,
            "html": html_content,
            "template_id": template_id,
            "theme": theme,
            "props": actual_props
        }
        
    except Exception as e:
        logger.error(f"🔍 [HTML_PREVIEW] Error generating HTML preview: {str(e)}")
        return {"success": False, "error": f"Failed to generate HTML preview: {str(e)}"}

@app.post("/api/custom/slide-video/generate")
async def generate_slide_video(request: Request):
    """Generate video from slide image only (no AI avatar)."""
    try:
        # Parse request body
        body = await request.json()
        slides_data = body.get("slides", [])
        theme = body.get("theme", "dark-purple")
        
        logger.info(f"🎬 [SLIDE_VIDEO] Generating slide-only video")
        logger.info(f"🎬 [SLIDE_VIDEO] Slides count: {len(slides_data) if slides_data else 0}")
        logger.info(f"🎬 [SLIDE_VIDEO] Theme: {theme}")
        
        if not slides_data or len(slides_data) == 0:
            logger.error("🎬 [SLIDE_VIDEO] No slides data provided")
            return {"success": False, "error": "No slides data provided"}
        
        # Import the presentation service
        from app.services.presentation_service import presentation_service
        
        # Create a presentation request with slide-only flag
        from app.services.presentation_service import PresentationRequest
        
        # Get the first slide
        slide_props = slides_data[0]
        template_id = slide_props.get("templateId")
        
        if not template_id:
            logger.error("🎬 [SLIDE_VIDEO] Missing templateId in slide data")
            return {"success": False, "error": "Missing templateId in slide data"}
        
        # Extract actual props
        actual_props = slide_props.get("props", slide_props)
        
        # Create presentation request with required arguments
        presentation_request = PresentationRequest(
            slide_url="",  # Empty for slide-only mode
            voiceover_texts=[],  # Empty for slide-only mode
            slides_data=slides_data,
            theme=theme,
            slide_only=True,  # Flag to indicate slide-only video
            use_avatar_mask=False  # Disable avatar mask for slide-only videos
        )
        
        logger.info(f"🎬 [SLIDE_VIDEO] Creating slide-only presentation...")
        
        # Start the presentation generation
        job_id = await presentation_service.create_presentation(presentation_request)
        
        logger.info(f"🎬 [SLIDE_VIDEO] Slide-only video generation started with job ID: {job_id}")
        
        return {
            "success": True,
            "jobId": job_id,
            "message": "Slide-only video generation started"
        }
        
    except Exception as e:
        logger.error(f"🎬 [SLIDE_VIDEO] Error generating slide-only video: {str(e)}")
        return {"success": False, "error": f"Failed to generate slide-only video: {str(e)}"}

@app.get("/api/custom/presentations")
async def list_presentations(limit: int = 50):
    """List recent presentation jobs."""
    try:
        if not presentation_service:
            return {
                "success": False,
                "error": "Presentation service not available. Please check backend configuration."
            }
        
        jobs = await presentation_service.list_jobs(limit)
        
        return {
            "success": True,
            "jobs": [
                {
                    "jobId": job.job_id,
                    "status": job.status,
                    "progress": job.progress,
                    "error": job.error,
                    "videoUrl": job.video_url,
                    "thumbnailUrl": job.thumbnail_url,
                    "createdAt": job.created_at.isoformat() if job.created_at else None,
                    "completedAt": job.completed_at.isoformat() if job.completed_at else None
                }
                for job in jobs
            ]
        }
        
    except Exception as e:
        logger.error(f"Error listing presentations: {str(e)}")
        return {"success": False, "error": f"Failed to list presentations: {str(e)}"}

def _any_quiz_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited quiz content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Quiz content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No quiz changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during quiz change detection (assuming changes made): {e}")
        return True

def _any_text_presentation_changes_made(original_content: str, edited_content: str) -> bool:
    """Compare original and edited text presentation content to detect changes"""
    try:
        # Normalize content for comparison
        original_normalized = original_content.strip()
        edited_normalized = edited_content.strip()
        
        # Simple text comparison
        if original_normalized != edited_normalized:
            logger.info(f"Text presentation content change detected: content length changed from {len(original_normalized)} to {len(edited_normalized)}")
            return True
        
        logger.info("No text presentation changes detected - content is identical")
        return False
    except Exception as e:
        # On any parsing issue assume changes were made so we use AI
        logger.warning(f"Error during text presentation change detection (assuming changes made): {e}")
        return True

async def _generate_content_blocks_for_section(section_title: str, all_section_titles: list, language: str) -> list:
    """
    Generate content blocks (in JSON format) for a single section.
    Returns a list of content block dictionaries ready to merge.
    """
    try:
        logger.info(f"[GENERATE_SECTION_BLOCKS] Generating JSON blocks for section: {section_title}")
        
        # Build context of all section titles
        context_info = "\n".join([f"- {title}" for title in all_section_titles])
        
        # Create prompt to generate JSON content blocks directly
        prompt = f"""You are generating content blocks for a One-Pager document section. Generate the content as a JSON array of content blocks.

**ALL SECTIONS IN THIS DOCUMENT:**
{context_info}

**YOUR TASK: Generate content blocks for this section:**
**Section Title:** {section_title}

**CRITICAL REQUIREMENTS:**
1. Output ONLY a valid JSON array of content blocks
2. Start with a headline block (level 2) with the section title
3. Follow with content blocks: paragraphs, bullet_list, numbered_list, etc.
4. The content MUST be:
   - Comprehensive and detailed (aim for 200-300 words total)
   - Educational and informative
   - Well-structured with multiple blocks
   - Written in {language} language
5. Keep in mind the other sections to avoid repetition

**OUTPUT FORMAT (JSON array):**
```json
[
  {{"type": "headline", "level": 2, "text": "Section Title Here"}},
  {{"type": "paragraph", "text": "Comprehensive paragraph with detailed information..."}},
  {{"type": "bullet_list", "items": ["Point 1", "Point 2", "Point 3"]}},
  {{"type": "paragraph", "text": "Another paragraph..."}}
]
```

**Available block types:**
- headline: {{"type": "headline", "level": 2, "text": "..."}}
- paragraph: {{"type": "paragraph", "text": "...", "isRecommendation": false}}
- bullet_list: {{"type": "bullet_list", "items": ["...", "..."]}}
- numbered_list: {{"type": "numbered_list", "items": ["...", "..."]}}

Generate ONLY the JSON array for "{section_title}" section:"""
        
        # Get response from OpenAI
        response = await stream_openai_response_direct(prompt)
        logger.info(f"[GENERATE_SECTION_BLOCKS] Generated {len(response)} characters")
        logger.info(f"[GENERATE_SECTION_BLOCKS] Response preview: {response[:200]}")
        
        # Parse the JSON response
        try:
            # Clean up response - remove markdown code blocks if present
            cleaned = response.strip()
            if cleaned.startswith('```'):
                # Remove ```json or ``` from start
                lines = cleaned.split('\n')
                cleaned = '\n'.join(lines[1:-1] if lines[-1].strip() == '```' else lines[1:])
            
            blocks = json.loads(cleaned)
            
            if not isinstance(blocks, list):
                logger.error(f"[GENERATE_SECTION_BLOCKS] Response is not a list: {type(blocks)}")
                raise ValueError("Expected JSON array")
            
            logger.info(f"[GENERATE_SECTION_BLOCKS] ✅ Successfully parsed {len(blocks)} content blocks")
            return blocks
            
        except json.JSONDecodeError as e:
            logger.error(f"[GENERATE_SECTION_BLOCKS] Failed to parse JSON: {e}")
            logger.error(f"[GENERATE_SECTION_BLOCKS] Raw response: {response[:500]}")
            
            # Fallback: create basic blocks manually
            return [
                {{"type": "headline", "level": 2, "text": section_title}},
                {{"type": "paragraph", "text": f"Content for {section_title}. Please refer to the original for detailed information.", "isRecommendation": False}}
            ]
            
    except Exception as e:
        logger.error(f"[GENERATE_SECTION_BLOCKS] Error generating blocks: {e}", exc_info=True)
        # Fallback
        return [
            {{"type": "headline", "level": 2, "text": section_title}},
            {{"type": "paragraph", "text": f"Content for {section_title}.", "isRecommendation": False}}
        ]

async def _generate_content_for_clean_titles(clean_content: str, original_content: str, language: str) -> str:
    """Generate content for clean titles (titles without descriptions) - DEPRECATED, kept for compatibility"""
    try:
        logger.info("Starting content generation for clean titles (DEPRECATED PATH)")
        
        # Parse the clean content to identify sections
        sections = []
        lines = clean_content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if this is a header (## Title)
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if header_match:
                # Save previous section if exists
                if current_section:
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    'title': header_match.group(2).strip(),
                    'content': '',
                    'needs_content': True
                }
            elif current_section:
                # This is content for the current section
                current_section['content'] += line + '\n'
                current_section['needs_content'] = False
        
        # Add the last section
        if current_section:
            sections.append(current_section)
        
        logger.info(f"Found {len(sections)} sections, {sum(1 for s in sections if s['needs_content'])} need content generation")
        
        # Generate content for sections that need it
        # Build context of all section titles for consistency
        all_section_titles = [s['title'] for s in sections]
        context_info = "\n".join([f"- {title}" for title in all_section_titles])
        
        for section in sections:
            if section['needs_content']:
                logger.info(f"Generating content for section: {section['title']}")
                
                # Create prompt for content generation with full context
                prompt = f"""You are generating content for a One-Pager document. This document has multiple sections, and you must generate content for one specific section while being aware of the other sections to ensure consistency and avoid repetition.

**ALL SECTIONS IN THIS DOCUMENT:**
{context_info}

**YOUR TASK: Generate content ONLY for this section:**
**Section Title:** {section['title']}

**CRITICAL REQUIREMENTS:**
1. Generate COMPLETELY NEW content from scratch for this section
2. The content MUST be:
   - Comprehensive and detailed (aim for at least 200-300 words)
   - Educational and informative
   - Well-structured with multiple paragraphs
   - Include relevant examples, explanations, or specific details
   - Written in {language} language
3. Keep in mind the other sections to:
   - Avoid repeating content that belongs in other sections
   - Ensure this section complements the overall document flow
   - Match the overall tone and depth of the document
4. The content should match the tone and style of a professional, educational one-pager document

**Format:** Provide ONLY the text content (paragraphs), without including the section title or any markdown headers.

Generate the comprehensive content for "{section['title']}" section:"""
                
                try:
                    # Use OpenAI to generate content
                    response = await stream_openai_response_direct(prompt)
                    section['content'] = response
                    logger.info(f"Generated {len(response)} characters for section: {section['title']}")
                except Exception as e:
                    logger.error(f"Failed to generate content for section {section['title']}: {e}")
                    # Fallback to a simple description
                    section['content'] = f"This section covers {section['title']}. Please refer to the original content for detailed information."
        
        # Reconstruct the content with generated descriptions
        result_content = ""
        for section in sections:
            result_content += f"## {section['title']}\n\n{section['content']}\n\n"
        
        logger.info(f"Content generation completed. Total length: {len(result_content)} characters")
        return result_content.strip()
        
    except Exception as e:
        logger.error(f"Error in content generation for clean titles: {e}")
        # Fallback to original content
        return clean_content

async def stream_openai_response_direct(prompt: str, model: str = None) -> str:
    """
    Get a complete response directly from OpenAI API (non-streaming).
    Returns the full response as a string.
    """
    try:
        client = get_openai_client()
        model = model or LLM_DEFAULT_MODEL
        
        logger.info(f"[OPENAI_DIRECT] Starting direct OpenAI request with model {model}")
        logger.info(f"[OPENAI_DIRECT] Prompt length: {len(prompt)} chars")
        
        # Read the full ContentBuilder.ai assistant instructions
        assistant_instructions_path = "custom_assistants/content_builder_ai.txt"
        try:
            with open(assistant_instructions_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning(f"[OPENAI_DIRECT] Assistant instructions file not found: {assistant_instructions_path}")
            system_prompt = "You are ContentBuilder.ai assistant. Follow the instructions in the user message exactly."
        
        # Create the chat completion (non-streaming)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.2
        )
        
        if response.choices and len(response.choices) > 0:
            content = response.choices[0].message.content
            logger.info(f"[OPENAI_DIRECT] Response received: {len(content)} characters")
            return content
        else:
            logger.error(f"[OPENAI_DIRECT] No content in response")
            return ""
            
    except Exception as e:
        logger.error(f"[OPENAI_DIRECT] Error in OpenAI direct request: {e}", exc_info=True)
        return f"Error generating content: {str(e)}"


@app.post("/api/custom/poster-image/generate")
async def generate_poster_image(request: Request):
    """
    Generate poster image using server-side HTML-to-image conversion
    Following the same architecture as video lesson slide image generation
    """
    import time
    start_time = time.time()
    request_timestamp = datetime.now().isoformat()
    
    try:
        logger.info(f"📷 [POSTER_IMAGE] ========== BACKEND POSTER GENERATION STARTED ==========")
        logger.info(f"📷 [POSTER_IMAGE] Timestamp: {request_timestamp}")
        logger.info(f"📷 [POSTER_IMAGE] Request method: {request.method}")
        logger.info(f"📷 [POSTER_IMAGE] Request URL: {request.url}")
        logger.info(f"📷 [POSTER_IMAGE] Client host: {request.client.host if request.client else 'unknown'}")
        logger.info(f"📷 [POSTER_IMAGE] User agent: {request.headers.get('user-agent', 'unknown')}")
        
        # Parse request data
        logger.info("📷 [POSTER_IMAGE] === PARSING REQUEST DATA ===")
        try:
            poster_data = await request.json()
            logger.info("📷 [POSTER_IMAGE] ✅ Successfully parsed JSON request body")
        except Exception as parse_error:
            logger.error(f"📷 [POSTER_IMAGE] ❌ Failed to parse JSON: {parse_error}")
            return JSONResponse(
                status_code=400,
                content={"error": f"Invalid JSON in request body: {str(parse_error)}"}
            )
        
        # Log detailed request data
        logger.info("📷 [POSTER_IMAGE] === REQUEST DATA ANALYSIS ===")
        logger.info(f"📷 [POSTER_IMAGE] Total fields in request: {len(poster_data)}")
        
        # Log each field with type and length information
        for key, value in poster_data.items():
            if key == 'speakerImageSrc' and value and len(str(value)) > 100:
                logger.info(f"📷 [POSTER_IMAGE]   - {key}: [base64 data, length: {len(str(value))} chars]")
            else:
                logger.info(f"📷 [POSTER_IMAGE]   - {key}: \"{str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\" (type: {type(value).__name__}, length: {len(str(value))})")
        
        # Extract session ID if provided by frontend
        session_id = poster_data.get('sessionId', f"backend-{int(time.time())}-{uuid.uuid4().hex[:8]}")
        logger.info(f"📷 [POSTER_IMAGE] Session ID: {session_id}")
        
        # Validate required fields
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === FIELD VALIDATION ===")
        required_fields = ['eventName', 'mainSpeaker', 'date', 'topic']
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Required fields: {required_fields}")
        
        validation_results = {}
        for field in required_fields:
            value = poster_data.get(field)
            is_valid = bool(value and str(value).strip())
            validation_results[field] = {'value': value, 'valid': is_valid}
            status = "✅ VALID" if is_valid else "❌ INVALID"
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - {field}: {status} (value: \"{str(value)[:50]}{'...' if len(str(value)) > 50 else ''}\")") 
        
        missing_fields = [field for field, result in validation_results.items() if not result['valid']]
        
        if missing_fields:
            error_msg = f"Missing required fields: {', '.join(missing_fields)}"
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ VALIDATION FAILED: {error_msg}")
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] Validation details: {validation_results}")
            return JSONResponse(
                status_code=400, 
                content={"error": error_msg, "validation_details": validation_results}
            )
        
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ All required fields validated successfully")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === KEY POSTER DATA ===")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Event: {poster_data.get('eventName')[:100]}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Speaker: {poster_data.get('mainSpeaker')[:100]}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Date: {poster_data.get('date')}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Topic: {poster_data.get('topic')[:100]}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Format: {poster_data.get('format', 'not specified')}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Dimensions: {poster_data.get('dimensions', 'not specified')}")
        
        # Generate unique filename
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === FILE SYSTEM SETUP ===")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        poster_id = str(uuid.uuid4())[:8]
        output_filename = f"poster_image_{timestamp}_{poster_id}.png"
        
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Filename components:")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - timestamp: {timestamp}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - poster_id: {poster_id}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - output_filename: {output_filename}")
        
        # Create output directory
        from pathlib import Path
        output_dir = Path("output/poster_images")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Creating output directory: {output_dir.absolute()}")
        
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ Output directory created/verified")
        except Exception as dir_error:
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Failed to create output directory: {dir_error}")
            raise
        
        output_path = str(output_dir / output_filename)
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Full output path: {output_path}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Directory exists: {output_dir.exists()}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Directory is writable: {os.access(output_dir, os.W_OK)}")
        
        # Use the proper template service (same pattern as slides)
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === HTML GENERATION ===")
        html_generation_start = time.time()
        
        try:
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Attempting to import template service...")
            from app.services.poster_template_service import poster_template_service
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ Template service imported successfully")
            
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Calling template service to generate HTML...")
            html_content = poster_template_service.generate_poster_html(poster_data)
            
            html_generation_end = time.time()
            html_generation_duration = (html_generation_end - html_generation_start) * 1000
            
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ HTML generated via template service")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] HTML generation duration: {html_generation_duration:.2f}ms")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] HTML content length: {len(html_content)} characters")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] HTML preview (first 200 chars): {html_content[:200]}...")
            
            # Convert HTML to PNG using the same service as slides
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === IMAGE CONVERSION ===")
            conversion_start = time.time()
            
            try:
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Importing HTML-to-image service...")
                from app.services.html_to_image_service import html_to_image_service
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ HTML-to-image service imported")
                
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Starting HTML-to-PNG conversion...")
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Conversion parameters:")
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - html_content length: {len(html_content)} chars")
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - output_path: {output_path}")
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - template_id: poster")
                
                success = await html_to_image_service.convert_html_to_png(
                    html_content=html_content,
                    output_path=output_path,
                    template_id="poster"
                )
                
                conversion_end = time.time()
                conversion_duration = (conversion_end - conversion_start) * 1000
                
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Image conversion completed in {conversion_duration:.2f}ms")
                logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Conversion success: {success}")
                
            except Exception as conversion_error:
                conversion_end = time.time()
                conversion_duration = (conversion_end - conversion_start) * 1000
                logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Conversion failed after {conversion_duration:.2f}ms: {conversion_error}")
                success = False
                
        except Exception as template_error:
            html_generation_end = time.time()
            html_generation_duration = (html_generation_end - html_generation_start) * 1000
            
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Template service error after {html_generation_duration:.2f}ms: {template_error}")
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] Template error type: {type(template_error).__name__}")
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] Template error details: {str(template_error)}")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] 🔄 Falling back to inline HTML generation")
            
            # Fallback to original method if template service fails
            fallback_start = time.time()
            html_content = generate_poster_html_template(poster_data)
            fallback_end = time.time()
            fallback_duration = (fallback_end - fallback_start) * 1000
            
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ Fallback HTML generated in {fallback_duration:.2f}ms")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Fallback HTML length: {len(html_content)} characters")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Fallback HTML preview: {html_content[:200]}...")
            
            # Now try the conversion with fallback HTML
            from app.services.html_to_image_service import html_to_image_service
            success = await html_to_image_service.convert_html_to_png(
                html_content=html_content,
                output_path=output_path,
                template_id="poster"
            )
        
        # File validation and analysis
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === FILE VALIDATION ===")
        
        if not success:
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Image conversion reported failure")
            return JSONResponse(
                status_code=500, 
                content={"error": "Image conversion failed", "session_id": session_id}
            )
            
        if not os.path.exists(output_path):
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Output file does not exist: {output_path}")
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] Directory contents: {list(output_dir.iterdir()) if output_dir.exists() else 'directory does not exist'}")
            return JSONResponse(
                status_code=500, 
                content={"error": "Generated file not found", "expected_path": output_path, "session_id": session_id}
            )
            
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ Output file exists: {output_path}")
        
        # Comprehensive file analysis
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === FILE ANALYSIS ===")
        
        try:
            file_stats = os.stat(output_path)
            file_size = file_stats.st_size
            file_mtime = datetime.fromtimestamp(file_stats.st_mtime).isoformat()
            
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}] File statistics:")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - size: {file_size} bytes")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - modified: {file_mtime}")
            logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - size category: {'VERY_SMALL' if file_size < 1000 else 'SMALL' if file_size < 10000 else 'MEDIUM' if file_size < 100000 else 'LARGE'}")
            
            # Try to read file header to verify it's a valid image
            try:
                with open(output_path, 'rb') as f:
                    header = f.read(16)
                    header_hex = header.hex()
                    logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - file header (hex): {header_hex}")
                    
                    # Check PNG signature
                    png_signature = b'\x89PNG\r\n\x1a\n'
                    is_png = header.startswith(png_signature)
                    logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - PNG signature valid: {is_png}")
                    
            except Exception as header_error:
                logger.warning(f"📷 [POSTER_IMAGE] [{session_id}] Could not read file header: {header_error}")
            
        except Exception as stat_error:
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] Could not get file stats: {stat_error}")
            file_size = 0
        
        if file_size < 100:  # Less than 100 bytes indicates likely failure
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] ❌ Generated file too small: {file_size} bytes")
            logger.error(f"📷 [POSTER_IMAGE] [{session_id}] This likely indicates a conversion failure")
            return JSONResponse(
                status_code=500, 
                content={"error": "Generated image file is too small", "file_size": file_size, "session_id": session_id}
            )
            
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] ✅ File size validation passed: {file_size} bytes")
        
        # Return image file (same pattern as slide system)
        end_time = time.time()
        total_duration = (end_time - start_time) * 1000
        
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === RESPONSE PREPARATION ===")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Preparing FileResponse...")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Response parameters:")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - path: {output_path}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - media_type: image/png")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - filename: {output_filename}")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}]   - file_size: {file_size} bytes")
        
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] === PROCESS COMPLETED SUCCESSFULLY ===")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Total processing time: {total_duration:.2f}ms")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Final file: {output_filename} ({file_size} bytes)")
        logger.info(f"📷 [POSTER_IMAGE] [{session_id}] Success: Returning FileResponse")
        
        return FileResponse(
            path=output_path,
            media_type="image/png",
            filename=output_filename
        )
        
    except Exception as e:
        end_time = time.time()
        total_duration = (end_time - start_time) * 1000
        
        logger.error(f"📷 [POSTER_IMAGE] ========== BACKEND PROCESS FAILED ==========")
        logger.error(f"📷 [POSTER_IMAGE] Error time: {datetime.now().isoformat()}")
        logger.error(f"📷 [POSTER_IMAGE] Duration before error: {total_duration:.2f}ms")
        logger.error(f"📷 [POSTER_IMAGE] Error type: {type(e).__name__}")
        logger.error(f"📷 [POSTER_IMAGE] Error message: {str(e)}")
        logger.error(f"📷 [POSTER_IMAGE] Error details:", exc_info=True)
        
        # Try to get session_id from local scope
        session_id = locals().get('session_id', 'unknown')
        
        return JSONResponse(
            status_code=500, 
            content={
                "error": "Internal server error during image generation",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "session_id": session_id,
                "duration_ms": total_duration
            }
        )


def generate_poster_html_template(poster_data: dict) -> str:
    """
    Generate HTML template for poster - mirrors slide template generation logic
    Converts EventPoster React component styles to standard HTML/CSS
    """
    logger.info("📷 [POSTER_HTML_TEMPLATE] === INLINE HTML GENERATION (FALLBACK) ===")
    logger.info(f"📷 [POSTER_HTML_TEMPLATE] Input data keys: {list(poster_data.keys())}")
    
    template_start = time.time()
    # Extract all poster fields
    event_name = poster_data.get('eventName', '')
    main_speaker = poster_data.get('mainSpeaker', '')
    speaker_description = poster_data.get('speakerDescription', '')
    date = poster_data.get('date', '')
    topic = poster_data.get('topic', '')
    additional_speakers = poster_data.get('additionalSpeakers', '')
    ticket_price = poster_data.get('ticketPrice', '')
    ticket_type = poster_data.get('ticketType', '')
    free_access = poster_data.get('freeAccessConditions', '')
    speaker_image_src = poster_data.get('speakerImageSrc', '')
    
    # Parse date to separate day/month and year (same logic as React component)
    date_parts = date.split('.')
    day_month = '.'.join(date_parts[:2]) if len(date_parts) >= 2 else date
    year = '.'.join(date_parts[2:]) if len(date_parts) > 2 else ''
    
    # Handle speaker image (base64 or default)
    speaker_image_html = f'''<div class="speaker-photo" style="background-image: url('{speaker_image_src}');"></div>'''
    
    # Generate complete HTML template with exact styles from EventPoster component
    html_template = f'''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Event Poster</title>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
        <style>
            * {{
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }}
            
            body {{
                font-family: 'Montserrat', sans-serif;
                background: #ffffff;
                margin: 0;
                padding: 0;
            }}
            
            .poster-container {{
                position: relative;
                width: 1000px;
                height: 1000px;
                background: rgba(255,255,255,1);
                font-family: 'Montserrat', sans-serif;
                box-sizing: border-box;
                overflow: hidden;
                margin: 0 auto;
            }}
            
            /* Main background - exact copy from React component */
            .main-background {{
                position: absolute;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background: url("/custom-projects-ui/create/event-poster/figma-to-html/images/v1_5.png");
                background-repeat: no-repeat;
                background-position: center center;
                background-size: cover;
            }}
            
            /* Speaker Photo - exact copy from React component */
            .speaker-photo {{
                position: absolute;
                width: 519px;
                height: 690px; /* align with frontend so it ends above pill */
                background-repeat: no-repeat;
                background-position: center center;
                background-size: cover;
                top: 329px;
                left: 525px;
            }}
            
            /* Bottom gradient - exact copy from React component */
            .bottom-gradient {{
                position: absolute;
                width: 1000px;
                height: 455px;
                background: linear-gradient(to bottom, transparent 0%, rgba(0,0,0,0.3) 50%, rgba(0,0,0,1) 100%);
                top: 552px;
                left: 1px;
            }}
            
            /* Main content grid layout - exact copy from React component */
            .main-content {{
                position: relative;
                z-index: 10;
                height: 100%;
                display: grid;
                grid-template-columns: 1fr auto;
                grid-template-rows: auto 1fr auto;
                padding: 53px;
            }}
            
            /* Header section - exact copy from React component */
            .header-section {{
                grid-column: span 2;
                display: flex;
                flex-direction: column;
                gap: 40px;
                margin-bottom: 40px;
            }}
            
            /* First row: Event name and Logo */
            .first-row {{
                display: flex;
                justify-content: space-between;
            }}
            
            .event-name-wrapper {
                display: inline-flex;
                align-items: flex-end;
                gap: 6px;
                margin-top: 10px; /* move PNG version a little lower */
                max-width: calc(100% - 180px);
            }

            .event-name {
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 400;
                font-size: 33px;
                text-align: left;
                line-height: 1.2;
                text-transform: uppercase;
            }

            .event-name-colon {
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 400;
                font-size: 33px;
                line-height: 1.2;
            }
            
            .logo {
                width: 141px;
                height: 78px;
                background: url("/custom-projects-ui/create/event-poster/figma-to-html/images/v1_6.png");
                background-repeat: no-repeat;
                background-position: center center;
                background-size: cover;
            }

            .logo-wrapper {
                width: 220px; /* match date section width */
                display: flex;
                justify-content: center; /* center logo horizontally */
                align-items: flex-start; /* align top edge with event title top */
                margin-top: 10px; /* mirror header lowered spacing */
            }
            
            /* Second row: Speaker info and Date */
            .second-row {{
                display: flex;
                justify-content: space-between;
                align-items: flex-start;
            }}
            
            .speaker-info {{
                display: flex;
                flex-direction: column;
                gap: 20px;
                max-width: 600px;
            }}
            
            .main-speaker {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 600;
                font-size: 41px;
                text-align: left;
                line-height: 1.2;
            }}
            
            .speaker-description {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 400;
                font-size: 22px;
                text-align: left;
                line-height: 1.2;
            }}
            
            .date-wrapper { width: 220px; display: flex; justify-content: center; }
            .date-section {
                border: 4px solid #5416af; /* thicker to match frontend */
                padding: 4px 6px; /* tiny side spacing */
                display: inline-block;
                width: auto;
                box-sizing: border-box;
            }
            
            .day-month {{
                color: #ffffff;
                font-family: 'Montserrat';
                font-weight: 600;
                font-size: 58px;
                text-align: center;
                line-height: 1;
                letter-spacing: 1px;
            }}
            
            .year {{
                color: #ffffff;
                font-family: 'Montserrat';
                font-weight: 300;
                font-size: 52px;
                text-align: center;
                line-height: 1;
                letter-spacing: 1px;
                margin-top: 5px;
            }}
            
            /* Left content area */
            .left-content {{
                display: flex;
                flex-direction: column;
                gap: 40px;
                padding-right: 50px;
                justify-content: center;
                height: 100%;
            }}
            
            .topic {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 600;
                font-size: 48px;
                text-align: left;
                line-height: 1.2;
                max-width: 480px;
            }}
            
            .additional-speakers {{
                color: #ebebeb;
                font-family: 'Montserrat';
                font-weight: 400;
                font-size: 24px;
                text-align: left;
                line-height: 1.2;
                max-width: 460px;
                margin-top: 6px;
            }}
            
            /* Bottom section */
            .bottom-section {{
                grid-column: span 2;
                display: flex;
                align-items: center;
                justify-content: space-between;
                margin-top: auto;
                padding-top: 50px;
            }}
            
            .ticket-section {{
                border: 2px solid #5416af;
                border-radius: 30px;
                padding: 10px 14px;
                min-width: 160px;
                line-height: 1;
                display: flex;
                flex-direction: column;
                align-items: center;
                justify-content: center;
                height: 113px;
            }}
            
            .ticket-label {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 600;
                font-size: 24px;
                text-align: center;
            }}
            
            .ticket-type {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 600;
                font-size: 26px;
                text-align: center;
                margin-top: 4px;
            }}
            
            .ticket-price {{
                color: #E5E5E5;
                font-family: 'Montserrat';
                font-weight: 900;
                font-size: 39px;
                text-align: center;
                margin-top: 2px;
            }}
            
            .free-access {
                color: #E5E5E5;
                font-weight: 600;
                font-size: 37px;
                text-align: center;
                line-height: 1.3;
                background-color: #5416af;
                border-radius: 30px;
                padding: 0 22px; /* +10% horizontal padding */
                box-shadow: 0 0 30px rgba(84,22,175,1), 0 0 60px rgba(84,22,175,0.5);
                backdrop-filter: blur(5px);
                text-transform: uppercase;
                display: flex;
                align-items: center;
                justify-content: center;
                height: 113px;
                width: 620px;
                margin-left: 10px;
            }
        </style>
    </head>
    <body>
        <div class="poster-container">
            <div class="main-background"></div>
            
            {speaker_image_html}
            
            <div class="bottom-gradient"></div>
            
            <div class="main-content">
                <div class="header-section">
                    <div class="first-row">
                        <div class="event-name-wrapper">
                        <div class="event-name">{event_name}</div>
                            <div class="event-name-colon">:</div>
                        </div>
                        <div class="logo-wrapper"><div class="logo"></div></div>
                    </div>
                    
                    <div class="second-row">
                        <div class="speaker-info">
                            <div class="main-speaker">{main_speaker}</div>
                            <div class="speaker-description">{speaker_description}</div>
                        </div>
                        
                        <div class="date-wrapper"><div class="date-section">
                            <div class="day-month">{day_month}</div>
                            <div class="year">{year}</div>
                        </div></div>
                    </div>
                </div>
                
                <div class="left-content">
                    <div class="topic">{topic}</div>
                    <div class="additional-speakers">{additional_speakers}</div>
                </div>
                
                <div></div>
                
                <div class="bottom-section">
                    <div class="ticket-section">
                        <div class="ticket-label">Квиток</div>
                        <div class="ticket-type">{ticket_type}</div>
                        <div class="ticket-price">{ticket_price}</div>
                    </div>
                    
                    <div class="free-access">{free_access}</div>
                </div>
            </div>
        </div>
    </body>
    </html>
    '''
    
    template_end = time.time()
    template_duration = (template_end - template_start) * 1000
    
    logger.info(f"📷 [POSTER_HTML_TEMPLATE] HTML template generated in {template_duration:.2f}ms")
    logger.info(f"📷 [POSTER_HTML_TEMPLATE] Template length: {len(html_template)} characters")
    logger.info(f"📷 [POSTER_HTML_TEMPLATE] Template preview: {html_template[:200]}...")
    
    return html_template
    


# --- Analytics Response Models ---
from enum import Enum

class ProductType(str, Enum):
    ONE_PAGER = "one_pager"
    PRESENTATION = "presentation"
    QUIZ = "quiz"
    VIDEO_LESSON = "video_lesson"

class QualityTier(str, Enum):
    BASIC = "basic"
    INTERACTIVE = "interactive"
    ADVANCED = "advanced"
    IMMERSIVE = "immersive"

class ProductTypeDistribution(BaseModel):
    type: ProductType
    count: int
    percentage: float
    color: str

class ProductsDistributionResponse(BaseModel):
    total_products: int
    distribution: List[ProductTypeDistribution]

class QualityTierDistribution(BaseModel):
    tier: QualityTier
    count: int
    percentage: float
    color: str

class QualityTiersDistributionResponse(BaseModel):
    total_lessons: int
    distribution: List[QualityTierDistribution]

# Color mappings for consistency
PRODUCT_TYPE_COLORS = {
    ProductType.ONE_PAGER: "#9333ea",     # Purple
    ProductType.PRESENTATION: "#2563eb",   # Blue  
    ProductType.QUIZ: "#16a34a",          # Green
    ProductType.VIDEO_LESSON: "#ea580c"   # Orange
}

QUALITY_TIER_COLORS = {
    QualityTier.BASIC: "#059669",        # Green
    QualityTier.INTERACTIVE: "#ea580c",   # Orange  
    QualityTier.ADVANCED: "#7c3aed",      # Purple
    QualityTier.IMMERSIVE: "#2563eb"      # Blue
}

# Component to Product Type mapping
COMPONENT_TO_PRODUCT_TYPE = {
    COMPONENT_NAME_TEXT_PRESENTATION: ProductType.ONE_PAGER,
    COMPONENT_NAME_SLIDE_DECK: ProductType.PRESENTATION,
    COMPONENT_NAME_QUIZ: ProductType.QUIZ,
    COMPONENT_NAME_VIDEO_LESSON: ProductType.VIDEO_LESSON,
    COMPONENT_NAME_VIDEO_LESSON_PRESENTATION: ProductType.VIDEO_LESSON,
    COMPONENT_NAME_PDF_LESSON: ProductType.ONE_PAGER,  # PDF lessons are considered one-pagers
    # Note: TrainingPlanTable components contain lessons, we need to count the lesson products, not the outline itself
}

@app.get("/api/custom/projects/analytics/product-distribution", response_model=ProductsDistributionResponse)
async def get_product_distribution(
    folder_id: Optional[int] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get product type distribution for analytics pie chart."""
    try:
        async with pool.acquire() as conn:
            # Build query to get all projects with their component types
            query = """
                SELECT dt.component_name, COUNT(*) as count
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.onyx_user_id = $1
            """
            params = [onyx_user_id]
            
            if folder_id is not None:
                query += " AND p.folder_id = $2"
                params.append(folder_id)
            
            query += " GROUP BY dt.component_name ORDER BY count DESC"
            
            rows = await conn.fetch(query, *params)
            
            # Process results
            product_counts = {}
            total_products = 0
            
            for row in rows:
                component_name = row['component_name']
                count = row['count']
                total_products += count
                
                # Map component to product type
                product_type = COMPONENT_TO_PRODUCT_TYPE.get(component_name)
                if product_type:
                    if product_type not in product_counts:
                        product_counts[product_type] = 0
                    product_counts[product_type] += count
            
            # Create distribution list
            distribution = []
            for product_type in ProductType:
                count = product_counts.get(product_type, 0)
                percentage = (count / total_products * 100) if total_products > 0 else 0
                
                distribution.append(ProductTypeDistribution(
                    type=product_type,
                    count=count,
                    percentage=round(percentage, 1),
                    color=PRODUCT_TYPE_COLORS[product_type]
                ))
            
            return ProductsDistributionResponse(
                total_products=total_products,
                distribution=distribution
            )
            
    except Exception as e:
        logger.error(f"Error getting product distribution: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get product distribution: {str(e)}")

@app.get("/api/custom/projects/analytics/quality-distribution", response_model=QualityTiersDistributionResponse)
async def get_quality_distribution(
    folder_id: Optional[int] = Query(None),
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get quality tiers distribution for analytics pie chart."""
    try:
        async with pool.acquire() as conn:
            # Build query to get all lessons with their quality tiers
            query = """
                WITH lesson_quality_tiers AS (
                    SELECT 
                        COALESCE(
                            lesson->>'quality_tier',
                            section->>'quality_tier', 
                            p.quality_tier,
                            pf.quality_tier,
                            'interactive'
                        ) as effective_quality_tier
                    FROM projects p
                    LEFT JOIN project_folders pf ON p.folder_id = pf.id
                    CROSS JOIN LATERAL jsonb_array_elements(p.microproduct_content->'sections') AS section
                    CROSS JOIN LATERAL jsonb_array_elements(section->'lessons') AS lesson
                    WHERE p.onyx_user_id = $1
                    AND p.microproduct_content IS NOT NULL
                    AND p.microproduct_content->>'sections' IS NOT NULL
            """
            params = [onyx_user_id]
            
            if folder_id is not None:
                query += " AND p.folder_id = $2"
                params.append(folder_id)
            
            query += """
                )
                SELECT 
                    LOWER(effective_quality_tier) as quality_tier,
                    COUNT(*) as count
                FROM lesson_quality_tiers
                GROUP BY LOWER(effective_quality_tier)
                ORDER BY count DESC
            """
            
            rows = await conn.fetch(query, *params)
            
            # Process results
            tier_counts = {}
            total_lessons = 0
            
            for row in rows:
                tier_name = row['quality_tier'].lower()
                count = row['count']
                total_lessons += count
                
                # Map tier names to enum values
                tier_mapping = {
                    'basic': QualityTier.BASIC,
                    'interactive': QualityTier.INTERACTIVE,
                    'advanced': QualityTier.ADVANCED,
                    'immersive': QualityTier.IMMERSIVE,
                    'medium': QualityTier.INTERACTIVE,  # Map medium to interactive
                    'premium': QualityTier.ADVANCED,    # Map premium to advanced
                }
                
                tier = tier_mapping.get(tier_name, QualityTier.INTERACTIVE)
                if tier not in tier_counts:
                    tier_counts[tier] = 0
                tier_counts[tier] += count
            
            # Create distribution list
            distribution = []
            for tier in QualityTier:
                count = tier_counts.get(tier, 0)
                percentage = (count / total_lessons * 100) if total_lessons > 0 else 0
                
                distribution.append(QualityTierDistribution(
                    tier=tier,
                    count=count,
                    percentage=round(percentage, 1),
                    color=QUALITY_TIER_COLORS[tier]
                ))
            
            return QualityTiersDistributionResponse(
                total_lessons=total_lessons,
                distribution=distribution
            )
            
    except Exception as e:
        logger.error(f"Error getting quality distribution: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get quality distribution: {str(e)}")

# ============================
# SMART DRIVE API ENDPOINTS
# ============================

@app.post("/api/custom/smartdrive/login-token")
async def create_smartdrive_login_token(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a short-lived, one-time token to allow Nextcloud autologin script to fetch credentials.
    The token is bound to the current user and expires quickly to reduce risk.
    """
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        import secrets
        from datetime import datetime, timedelta, timezone

        token = secrets.token_urlsafe(32)
        now = datetime.now(timezone.utc)
        expires_at = now + timedelta(minutes=2)

        async with pool.acquire() as conn:
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS smartdrive_login_tokens (
                    token TEXT PRIMARY KEY,
                    onyx_user_id VARCHAR(255) NOT NULL,
                    created_at TIMESTAMPTZ NOT NULL,
                    expires_at TIMESTAMPTZ NOT NULL,
                    used BOOLEAN NOT NULL DEFAULT FALSE
                )
                """
            )
            await conn.execute(
                """
                INSERT INTO smartdrive_login_tokens (token, onyx_user_id, created_at, expires_at, used)
                VALUES ($1, $2, $3, $4, FALSE)
                """,
                token, str(onyx_user_id), now, expires_at
            )

        return {"success": True, "token": token, "expires_in_seconds": 120}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating SmartDrive login token: {e}")
        raise HTTPException(status_code=500, detail="Failed to create login token")


@app.get("/api/custom/smartdrive/login-credentials")
async def get_smartdrive_login_credentials(
    request: Request,
    token: str = Query(..., description="One-time autologin token"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Redeem a short-lived token for Nextcloud credentials for the bound user.
    Marks the token as used. Returns username/password/base_url for the user's account.
    """
    try:
        from datetime import datetime, timezone

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT token, onyx_user_id, created_at, expires_at, used
                FROM smartdrive_login_tokens
                WHERE token = $1
                """,
                token
            )
            if not row:
                raise HTTPException(status_code=404, detail="Invalid token")
            if row["used"]:
                raise HTTPException(status_code=400, detail="Token already used")
            if row["expires_at"] < datetime.now(timezone.utc):
                raise HTTPException(status_code=400, detail="Token expired")

            # Ensure credits exist before any SmartDrive provisioning
            try:
                await get_or_create_user_credits(row["onyx_user_id"], "User", DB_POOL)
            except Exception as _e:
                logger.warning(f"[SmartDrive] Failed to ensure credits before provisioning: {_e}")

            account = await conn.fetchrow(
                """
                SELECT nextcloud_username, nextcloud_password_encrypted, nextcloud_base_url
                FROM smartdrive_accounts
                WHERE onyx_user_id = $1
                """,
                row["onyx_user_id"]
            )
            if not account or not account.get("nextcloud_username") or not account.get("nextcloud_password_encrypted"):
                # Auto-provision a Nextcloud account for this user
                try:
                    import secrets, re
                    base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
                    nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
                    nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
                    if not (nc_admin_user and nc_admin_pass):
                        raise HTTPException(status_code=400, detail="Credentials not configured for user and auto-provisioning is not configured")

                    raw_id = str(row["onyx_user_id"])  # uuid or string
                    sanitized = re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
                    userid = f"sd_{sanitized[:24]}"
                    new_password = secrets.token_urlsafe(16)

                    # Normalize base to HTTPS to avoid 301 on POST and preserve method
                    from urllib.parse import urlparse
                    parsed = urlparse(base_url)
                    if parsed.scheme == "http":
                        base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
                    else:
                        base_url = (base_url or "").rstrip("/")
                    ocs_base = base_url

                    headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}

                    async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                        create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                        create_resp = await client.post(
                            create_url,
                            data={"userid": userid, "password": new_password},
                            headers=headers,
                            auth=(nc_admin_user, nc_admin_pass)
                        )

                        # Try to parse OCS response if possible
                        ocs_ok = False
                        reset_needed = False
                        try:
                            j = create_resp.json()
                            sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                            # 100 = OK, 102 = Already exists
                            if sc == 100:
                                ocs_ok = True
                            elif sc == 102:
                                reset_needed = True
                        except Exception:
                            pass

                        if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                            # User likely exists; attempt password reset
                            update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                            update_resp = await client.put(
                                update_url,
                                data={"key": "password", "value": new_password},
                                headers=headers,
                                auth=(nc_admin_user, nc_admin_pass)
                            )
                            if update_resp.status_code not in (200, 201, 204):
                                logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                        elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                            logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                            raise HTTPException(status_code=500, detail="Failed to auto-provision Nextcloud user")

                    encrypted = encrypt_password(new_password)
                    await conn.execute(
                        """
                        UPDATE smartdrive_accounts 
                        SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                        WHERE onyx_user_id = $1
                        """,
                        row["onyx_user_id"], userid, encrypted, base_url, datetime.now(timezone.utc)
                    )

                    # Clean default skeleton files with a single comprehensive pass
                    try:
                        deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                        logger.info(f"[SmartDrive] Cleanup: removed {deleted_count} default files for new user {userid}")
                    except Exception as cleanup_err:
                        logger.warning(f"[SmartDrive] Default file cleanup failed (non-fatal): {cleanup_err}")

                    account = {"nextcloud_username": userid, "nextcloud_password_encrypted": encrypted, "nextcloud_base_url": base_url}
                except HTTPException:
                    raise
                except Exception as provision_err:
                    logger.error(f"Auto-provision failed: {provision_err}")
                    raise HTTPException(status_code=500, detail="Failed to auto-provision Nextcloud account")

            try:
                password_plain = decrypt_password(account["nextcloud_password_encrypted"])  # type: ignore
            except Exception:
                raise HTTPException(status_code=500, detail="Failed to decrypt credentials")

            await conn.execute(
                "UPDATE smartdrive_login_tokens SET used = TRUE WHERE token = $1",
                token
            )

        headers = {
            "Cache-Control": "no-store, no-cache, must-revalidate, max-age=0",
            "Pragma": "no-cache",
        }
        return JSONResponse(
            content={
                "success": True,
                "username": account["nextcloud_username"],
                "password": password_plain,
                "base_url": account.get("nextcloud_base_url") or "http://nc1.contentbuilder.ai:8080",
            },
            headers=headers
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error redeeming SmartDrive login token: {e}")
        raise HTTPException(status_code=500, detail="Failed to redeem login token")

@app.post("/api/custom/smartdrive/session")
async def bootstrap_smartdrive_session(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Bootstrap SmartDrive access for the current user"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Bootstrapping SmartDrive session for user: {onyx_user_id}")

        async with pool.acquire() as conn:
            # Ensure credits exist before any SmartDrive account actions
            try:
                await get_or_create_user_credits(onyx_user_id, "User", DB_POOL)
            except Exception as _e:
                logger.warning(f"[SmartDrive] Failed to ensure credits before session bootstrap: {_e}")
            # Check if user already has SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                # This should rarely happen now since accounts are created during user registration
                # But we'll create it as a fallback for existing users who haven't visited yet
                await conn.execute(
                    """
                    INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                    VALUES ($1, $2, $3, $4)
                    """,
                    onyx_user_id,
                    '{}',  # Empty JSON cursor
                    datetime.now(timezone.utc),
                    datetime.now(timezone.utc)
                )
                logger.info(f"Created SmartDrive account placeholder for existing user: {onyx_user_id}")
                has_credentials = False
            else:
                logger.info(f"SmartDrive account already exists for user: {onyx_user_id}")
                has_credentials = bool(account.get('nextcloud_username') and account.get('nextcloud_password_encrypted'))

        return {
            "success": True, 
            "message": "SmartDrive session initialized",
            "has_credentials": has_credentials,
            "setup_required": not has_credentials
        }
        
    except Exception as e:
        logger.error(f"Error bootstrapping SmartDrive session: {e}")
        raise HTTPException(status_code=500, detail="Failed to initialize SmartDrive session")


@app.post("/api/custom/smartdrive/credentials")
async def set_smartdrive_credentials(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Set or update user's Nextcloud credentials"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        
        # Get request data
        data = await request.json()
        nextcloud_username = data.get('nextcloud_username', '').strip()
        nextcloud_password = data.get('nextcloud_password', '').strip()
        nextcloud_base_url = data.get('nextcloud_base_url', 'http://nc1.contentbuilder.ai:8080').strip()
        
        if not nextcloud_username or not nextcloud_password:
            raise HTTPException(status_code=400, detail="Username and password are required")
        
        # Encrypt password
        encrypted_password = encrypt_password(nextcloud_password)
        
        async with pool.acquire() as conn:
            # Update or insert credentials
            await conn.execute(
                """
                UPDATE smartdrive_accounts 
                SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                WHERE onyx_user_id = $1
                """,
                onyx_user_id,
                nextcloud_username,
                encrypted_password,
                nextcloud_base_url,
                datetime.now(timezone.utc)
            )
            
        logger.info(f"Updated Nextcloud credentials for user: {onyx_user_id}")
        return {"success": True, "message": "Nextcloud credentials saved successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error setting SmartDrive credentials: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/custom/smartdrive/list")
async def list_smartdrive_files(
    request: Request,
    path: str = Query("/", description="Path to list files from"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """List files/folders in the user's SmartDrive"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Listing SmartDrive files for user: {onyx_user_id}, path: {path}")

        async with pool.acquire() as conn:
            # Get user's SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account or not account.get("nextcloud_username") or not account.get("nextcloud_password_encrypted"):
                # Attempt auto-provision of Nextcloud user for this onyx user
                try:
                    import secrets
                    import re as _re
                    base_url = os.environ.get("NEXTCLOUD_BASE_URL") or "http://nc1.contentbuilder.ai:8080"
                    nc_admin_user = os.environ.get("NEXTCLOUD_ADMIN_USERNAME")
                    nc_admin_pass = os.environ.get("NEXTCLOUD_ADMIN_PASSWORD")
                    if not (nc_admin_user and nc_admin_pass):
                        raise HTTPException(status_code=401, detail="SmartDrive account not connected")

                    # Ensure placeholder row exists
                    if not account:
                        await conn.execute(
                            """
                            INSERT INTO smartdrive_accounts (onyx_user_id, sync_cursor, created_at, updated_at)
                            VALUES ($1, $2, $3, $4)
                            """,
                            onyx_user_id,
                            '{}',
                            datetime.now(timezone.utc),
                            datetime.now(timezone.utc)
                        )

                    raw_id = str(onyx_user_id)
                    sanitized = _re.sub(r"[^a-zA-Z0-9_\-]", "", raw_id.replace("-", ""))
                    userid = f"sd_{sanitized[:24]}"
                    new_password = secrets.token_urlsafe(16)

                    # Normalize base to https if needed
                    from urllib.parse import urlparse
                    parsed = urlparse(base_url)
                    if parsed.scheme == "http":
                        base_url = f"https://{parsed.netloc}{parsed.path}".rstrip("/")
                    else:
                        base_url = (base_url or "").rstrip("/")
                    ocs_base = base_url

                    headers = {"OCS-APIRequest": "true", "Accept": "application/json", "Content-Type": "application/x-www-form-urlencoded"}
                    async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                        create_url = f"{ocs_base}/ocs/v2.php/cloud/users"
                        create_resp = await client.post(
                            create_url,
                            data={"userid": userid, "password": new_password},
                            headers=headers,
                            auth=(nc_admin_user, nc_admin_pass)
                        )
                        # Parse OCS response
                        ocs_ok = False
                        reset_needed = False
                        try:
                            j = create_resp.json()
                            sc = j.get("ocs", {}).get("meta", {}).get("statuscode")
                            if sc == 100:
                                ocs_ok = True
                            elif sc == 102:
                                reset_needed = True
                        except Exception:
                            pass
                        if create_resp.status_code == 409 or reset_needed or (not ocs_ok and create_resp.status_code in (200, 201)):
                            update_url = f"{ocs_base}/ocs/v2.php/cloud/users/{userid}"
                            update_resp = await client.put(
                                update_url,
                                data={"key": "password", "value": new_password},
                                headers=headers,
                                auth=(nc_admin_user, nc_admin_pass)
                            )
                            if update_resp.status_code not in (200, 201, 204):
                                logger.warning(f"Failed to reset Nextcloud password for existing user {userid}: {update_resp.status_code} {update_resp.text[:200]}")
                        elif (create_resp.status_code not in (200, 201)) and not ocs_ok:
                            logger.error(f"Failed to create Nextcloud user: {create_resp.status_code} {create_resp.text[:200]}")
                            raise HTTPException(status_code=500, detail="Failed to auto-provision Nextcloud user")

                    encrypted = encrypt_password(new_password)
                    await conn.execute(
                        """
                        UPDATE smartdrive_accounts
                        SET nextcloud_username = $2, nextcloud_password_encrypted = $3, nextcloud_base_url = $4, updated_at = $5
                        WHERE onyx_user_id = $1
                        """,
                        onyx_user_id, userid, encrypted, base_url, datetime.now(timezone.utc)
                    )

                    # Clean default skeleton files using comprehensive cleanup function
                    try:
                        deleted_count = await cleanup_nextcloud_default_files(base_url, userid, new_password)
                        logger.info(f"[SmartDrive] Initial cleanup: removed {deleted_count} default files for new user {userid}")
                        
                        # Additional aggressive cleanup to catch any missed or recreated files
                        webdav_base = f"{base_url}/remote.php/dav/files/{userid}"
                        async with httpx.AsyncClient(timeout=15.0) as final_client:
                            # Final check for any remaining files
                            final_prop = await final_client.request(
                                "PROPFIND", f"{webdav_base}/", auth=(userid, new_password),
                                headers={"Depth": "1", "Content-Type": "application/xml"},
                                content='<?xml version="1.0"?><d:propfind xmlns:d="DAV:"><d:prop><d:displayname/></d:prop></d:propfind>'
                            )
                            
                            if final_prop.status_code == 207:
                                import xml.etree.ElementTree as ET_final
                                root_final = ET_final.fromstring(final_prop.text)
                                remaining_files = []
                                for resp in root_final.findall('.//{DAV:}response'):
                                    href = resp.find('.//{DAV:}href')
                                    if href and href.text and href.text.rstrip('/') != f"/remote.php/dav/files/{userid}":
                                        remaining_files.append(href.text)
                                
                                if remaining_files:
                                    logger.warning(f"[SmartDrive] 🔥 AGGRESSIVE CLEANUP: Found {len(remaining_files)} remaining files")
                                    additional_deleted = 0
                                    for file_href in remaining_files:
                                        try:
                                            del_resp = await final_client.delete(f"{base_url}{file_href}", auth=(userid, new_password))
                                            if del_resp.status_code in (204, 404):
                                                additional_deleted += 1
                                                logger.info(f"[SmartDrive] 🔥 Deleted remaining: {file_href.split('/')[-1]}")
                                        except Exception:
                                            pass
                                    logger.info(f"[SmartDrive] Aggressive cleanup: removed {additional_deleted} additional files")
                                else:
                                    logger.info(f"[SmartDrive] ✅ Account completely clean after initial cleanup")
                        
                    except Exception as cleanup_err:
                        logger.warning(f"[SmartDrive] Default file cleanup failed (non-fatal): {cleanup_err}")

                    # Reload account row
                    account = await conn.fetchrow(
                        "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                        onyx_user_id
                    )
                except HTTPException:
                    raise
                except Exception as e:
                    logger.error(f"Auto-provisioning SmartDrive on list failed: {e}")
                    raise HTTPException(status_code=500, detail="Failed to auto-provision SmartDrive account")

            # Use Nextcloud WebDAV API to list files with the user's individual credentials
            username, password, base_url, _ = await _get_nextcloud_credentials(conn, onyx_user_id)
            # Encode DAV path segments to handle special characters like '#'
            webdav_url = f"{base_url}/remote.php/dav/files/{username}{_encode_dav_path(path)}"
            auth = (username, password)
            
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.request(
                        "PROPFIND", 
                        webdav_url,
                        auth=auth,
                        headers={
                            "Depth": "1",
                            "Content-Type": "application/xml"
                        },
                        content="""<?xml version="1.0"?>
                        <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                            <d:prop>
                                <d:resourcetype/>
                                <d:getcontentlength/>
                                <d:getlastmodified/>
                                <d:getcontenttype/>
                            </d:prop>
                        </d:propfind>"""
                    )
                    
                    if response.status_code == 207:  # Multi-Status response
                        # Parse WebDAV XML response
                        files = await parse_webdav_response(response.text, path)
                        return {
                            "files": files,
                            "path": path,
                            "total_count": len(files)
                        }
                    else:
                        logger.error(f"Nextcloud WebDAV error: {response.status_code} - {response.text}")
                        # Fallback to mock data if Nextcloud is unavailable
                        return await get_mock_files_response(path)
                        
            except Exception as nextcloud_error:
                logger.error(f"Failed to connect to Nextcloud: {nextcloud_error}")
                # Fallback to mock data if Nextcloud is unavailable
                return await get_mock_files_response(path)
        
    except Exception as e:
        logger.error(f"Error listing SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to list SmartDrive files")

async def parse_webdav_response(xml_content: str, base_path: str) -> List[Dict]:
    """Parse WebDAV PROPFIND XML response into file list"""
    # Simple XML parsing for WebDAV response
    import xml.etree.ElementTree as ET
    
    files = []
    try:
        root = ET.fromstring(xml_content)
        
        for response in root.findall('.//{DAV:}response'):
            href = response.find('.//{DAV:}href')
            if href is None:
                continue
                
            file_path = href.text
            if file_path.endswith('/remote.php/dav/files/'):
                continue  # Skip the root directory entry
                
            # Extract relative path by removing the WebDAV prefix
            # file_path looks like: /smartdrive/remote.php/dav/files/username/Documents/file.txt
            # We want just: /Documents/file.txt
            if '/remote.php/dav/files/' in file_path:
                # Find the username part and extract everything after it
                parts = file_path.split('/remote.php/dav/files/')
                if len(parts) > 1:
                    # parts[1] is like "username/Documents/file.txt"
                    username_and_path = parts[1]
                    # Split by first "/" to separate username from the actual path
                    path_parts = username_and_path.split('/', 1)
                    if len(path_parts) > 1:
                        # This is the actual relative path we want
                        relative_path = '/' + path_parts[1]
                    else:
                        # Just the username, so root path
                        relative_path = '/'
                else:
                    relative_path = '/'
            else:
                relative_path = file_path
                
            # Decode relative path for human-readable output
            try:
                from urllib.parse import unquote as _unquote
                relative_path = _unquote(relative_path)
            except Exception:
                pass
            # Extract file name
            name = relative_path.split('/')[-1] if not relative_path.endswith('/') else relative_path.split('/')[-2]
            # Ensure name is decoded
            try:
                from urllib.parse import unquote as _unquote
                name = _unquote(name)
            except Exception:
                pass
            if not name:
                continue
                
            # Check if it's a directory
            resourcetype = response.find('.//{DAV:}resourcetype')
            is_directory = resourcetype is not None and resourcetype.find('.//{DAV:}collection') is not None
            
            # Get file size
            size_elem = response.find('.//{DAV:}getcontentlength')
            size = int(size_elem.text) if size_elem is not None and size_elem.text else None
            
            # Get last modified
            modified_elem = response.find('.//{DAV:}getlastmodified')
            modified = modified_elem.text if modified_elem is not None else None
            
            # Get content type
            content_type_elem = response.find('.//{DAV:}getcontenttype')
            mime_type = content_type_elem.text if content_type_elem is not None else None
            
            # Get ETag
            etag_elem = response.find('.//{DAV:}getetag')
            etag = etag_elem.text if etag_elem is not None else None
            # Remove quotes from ETag if present
            if etag and etag.startswith('"') and etag.endswith('"'):
                etag = etag[1:-1]
            
            files.append({
                "name": name,
                "path": relative_path,
                "type": "directory" if is_directory else "file",
                "size": size,
                "modified": modified,
                "mime_type": mime_type,
                "etag": etag
            })
            
    except Exception as e:
        logger.error(f"Error parsing WebDAV response: {e}")
        
    return files

# --- Date Parsing Functions ---
def parse_http_date(date_string: str) -> datetime:
    """Parse HTTP date string (RFC 2822 format) to datetime object"""
    try:
        # Parse HTTP date format like "Wed, 13 Aug 2025 23:31:13 GMT"
        from email.utils import parsedate_to_datetime
        return parsedate_to_datetime(date_string)
    except Exception as e:
        logger.warning(f"Failed to parse date '{date_string}': {e}")
        return datetime.now(timezone.utc)

# --- Encryption Functions ---
def get_or_create_encryption_key():
    """Get or create a Fernet encryption key for the system"""
    key = os.getenv("SMARTDRIVE_ENCRYPTION_KEY")
    if not key:
        # Generate a new key and save it to environment (in production, store securely)
        from cryptography.fernet import Fernet
        key = Fernet.generate_key().decode()
        logger.warning(f"Generated new encryption key. Please set SMARTDRIVE_ENCRYPTION_KEY={key} in your environment for production!")
    return key.encode()

def encrypt_password(password: str) -> str:
    """Encrypt a password for storage"""
    try:
        from cryptography.fernet import Fernet
        f = Fernet(get_or_create_encryption_key())
        return f.encrypt(password.encode()).decode()
    except Exception as e:
        logger.error(f"Failed to encrypt password: {e}")
        raise HTTPException(status_code=500, detail="Encryption failed")

def decrypt_password(encrypted_password: str) -> str:
    """Decrypt a password from storage"""
    try:
        from cryptography.fernet import Fernet
        f = Fernet(get_or_create_encryption_key())
        return f.decrypt(encrypted_password.encode()).decode()
    except Exception as e:
        logger.error(f"Failed to decrypt password: {e}")
        raise HTTPException(status_code=500, detail="Decryption failed")

async def cleanup_nextcloud_default_files(base_url: str, userid: str, password: str) -> int:
    """
    Comprehensive cleanup of Nextcloud skeleton/default files for a new user account.
    Returns the number of items deleted.
    """
    logger.info(f"[SmartDrive] Starting comprehensive cleanup of default files for user: {userid}")
    deleted_count = 0
    
    try:
        webdav_base = f"{base_url}/remote.php/dav/files/{userid}"
        async with httpx.AsyncClient(timeout=30.0) as cleanup_client:
            # First, get all files and folders in user's root directory with detailed properties
            prop = await cleanup_client.request(
                "PROPFIND",
                f"{webdav_base}/",
                auth=(userid, password),
                headers={"Depth": "1", "Content-Type": "application/xml"},
                content="""<?xml version="1.0"?>
                <d:propfind xmlns:d="DAV:">
                  <d:prop>
                    <d:resourcetype/>
                    <d:displayname/>
                  </d:prop>
                </d:propfind>"""
            )
            
            if prop.status_code in (207, 200):
                import xml.etree.ElementTree as ET
                try:
                    root = ET.fromstring(prop.text)
                    items_to_delete = []
                    
                    for resp in root.findall('.//{DAV:}response'):
                        href = resp.find('.//{DAV:}href')
                        if not href or not href.text:
                            continue
                            
                        h = href.text
                        # Skip the root directory itself
                        if h.rstrip('/') == f"/remote.php/dav/files/{userid}":
                            continue
                            
                        # Get display name if available
                        display_name = None
                        display_elem = resp.find('.//{DAV:}displayname')
                        if display_elem is not None and display_elem.text:
                            display_name = display_elem.text
                        
                        items_to_delete.append((h, display_name))
                    
                    # Delete all found items
                    for h, display_name in items_to_delete:
                        del_url = f"{base_url}{h}"
                        try:
                            delete_resp = await cleanup_client.delete(del_url, auth=(userid, password))
                            if delete_resp.status_code in (204, 404):  # 204 = deleted, 404 = already gone
                                deleted_count += 1
                                item_name = display_name or h.split('/')[-1]
                                logger.info(f"[SmartDrive] ✓ Deleted default item: {item_name}")
                            else:
                                logger.warning(f"[SmartDrive] Failed to delete {h}: HTTP {delete_resp.status_code}")
                        except Exception as e:
                            logger.warning(f"[SmartDrive] Exception deleting {h}: {e}")
                    
                    # Also try to delete common Nextcloud skeleton files by name in parallel
                    common_skeleton_items = [
                        "Documents", "Photos", "Templates",
                        "Readme.md", "Nextcloud intro.mp4", "Nextcloud Manual.pdf",
                        "Nextcloud.png", "Reasons to use Nextcloud.pdf", "Templates credits.md"
                    ]
                    
                    import asyncio as _asyncio
                    sem = _asyncio.Semaphore(8)
                    async def _del_item(_name: str) -> int:
                        async with sem:
                            try:
                                _del_url = f"{webdav_base}/{_name}"
                                _resp = await cleanup_client.delete(_del_url, auth=(userid, password))
                                if _resp.status_code == 204:
                                    logger.info(f"[SmartDrive] ✓ Deleted common skeleton item: {_name}")
                                    return 1
                                elif _resp.status_code == 404:
                                    logger.debug(f"[SmartDrive] Skeleton item not found (expected): {_name}")
                                    return 0
                                else:
                                    logger.warning(f"[SmartDrive] Failed to delete skeleton item {_name}: HTTP {_resp.status_code}")
                                    return 0
                            except Exception as _e:
                                logger.warning(f"[SmartDrive] Exception deleting skeleton item {_name}: {_e}")
                                return 0

                    _results = await _asyncio.gather(*(_del_item(n) for n in common_skeleton_items))
                    deleted_count += sum(_results)

                    # Final verification: do another PROPFIND to check what remains
                    try:
                        final_check = await cleanup_client.request(
                            "PROPFIND",
                            f"{webdav_base}/",
                            auth=(userid, password),
                            headers={"Depth": "1", "Content-Type": "application/xml"},
                            content="""<?xml version="1.0"?>
                            <d:propfind xmlns:d="DAV:">
                              <d:prop>
                                <d:displayname/>
                              </d:prop>
                            </d:propfind>"""
                        )
                        
                        if final_check.status_code in (207, 200):
                            import xml.etree.ElementTree as ET_final
                            try:
                                root_final = ET_final.fromstring(final_check.text)
                                remaining_files = []
                                for resp in root_final.findall('.//{DAV:}response'):
                                    href = resp.find('.//{DAV:}href')
                                    if href and href.text:
                                        h = href.text
                                        if h.rstrip('/') != f"/remote.php/dav/files/{userid}":
                                            display_elem = resp.find('.//{DAV:}displayname')
                                            if display_elem is not None and display_elem.text:
                                                remaining_files.append(display_elem.text)
                                            else:
                                                remaining_files.append(h.split('/')[-1])
                                
                                if remaining_files:
                                    logger.warning(f"[SmartDrive] Files still remaining after cleanup: {remaining_files}")
                                    # Try to delete remaining files one more time
                                    for remaining_file in remaining_files:
                                        try:
                                            final_del_url = f"{webdav_base}/{remaining_file}"
                                            final_delete_resp = await cleanup_client.delete(final_del_url, auth=(userid, password))
                                            if final_delete_resp.status_code == 204:
                                                deleted_count += 1
                                                logger.info(f"[SmartDrive] ✓ Final cleanup - deleted: {remaining_file}")
                                            else:
                                                logger.error(f"[SmartDrive] Failed final deletion of {remaining_file}: HTTP {final_delete_resp.status_code}")
                                        except Exception as e:
                                            logger.error(f"[SmartDrive] Exception in final cleanup of {remaining_file}: {e}")
                                else:
                                    logger.info(f"[SmartDrive] ✅ Account is completely clean - no files remaining")
                            except Exception as parse_error:
                                logger.warning(f"[SmartDrive] Error parsing final check response: {parse_error}")
                    except Exception as final_check_error:
                        logger.warning(f"[SmartDrive] Final verification failed: {final_check_error}")
                            
                    logger.info(f"[SmartDrive] Cleanup complete: removed {deleted_count} default items for user {userid}")
                    
                except Exception as parse_error:
                    logger.error(f"[SmartDrive] Error parsing cleanup response: {parse_error}")
            else:
                logger.error(f"[SmartDrive] Failed to list files for cleanup: HTTP {prop.status_code}")
                
    except Exception as cleanup_error:
        logger.error(f"[SmartDrive] Comprehensive file cleanup failed: {cleanup_error}")
        
    return deleted_count

async def aggressive_cleanup_remaining_files(base_url: str, userid: str, password: str) -> int:
    """
    Additional aggressive cleanup to catch any files that might have been missed or recreated.
    This runs after the main cleanup to ensure the account is completely empty.
    """
    logger.info(f"[SmartDrive] Running aggressive cleanup for any remaining files for user: {userid}")
    additional_deleted = 0
    
    try:
        webdav_base = f"{base_url}/remote.php/dav/files/{userid}"
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Do a fresh PROPFIND to see what's currently in the account
            prop = await client.request(
                "PROPFIND",
                f"{webdav_base}/",
                auth=(userid, password),
                headers={"Depth": "1", "Content-Type": "application/xml"},
                content="""<?xml version="1.0"?>
                <d:propfind xmlns:d="DAV:">
                  <d:prop>
                    <d:displayname/>
                    <d:resourcetype/>
                  </d:prop>
                </d:propfind>"""
            )
            
            if prop.status_code in (207, 200):
                import xml.etree.ElementTree as ET
                try:
                    root = ET.fromstring(prop.text)
                    remaining_items = []
                    
                    for resp in root.findall('.//{DAV:}response'):
                        href = resp.find('.//{DAV:}href')
                        if not href or not href.text:
                            continue
                            
                        h = href.text
                        # Skip the root directory itself
                        if h.rstrip('/') == f"/remote.php/dav/files/{userid}":
                            continue
                            
                        # Get display name if available
                        display_elem = resp.find('.//{DAV:}displayname')
                        display_name = display_elem.text if display_elem is not None and display_elem.text else h.split('/')[-1]
                        
                        remaining_items.append((h, display_name))
                    
                    if remaining_items:
                        logger.warning(f"[SmartDrive] Found {len(remaining_items)} files still remaining after initial cleanup")
                        for item_href, item_name in remaining_items:
                            logger.warning(f"[SmartDrive] Remaining: {item_name} ({item_href})")
                        
                        # Delete everything that remains
                        for h, display_name in remaining_items:
                            try:
                                del_url = f"{base_url}{h}"
                                delete_resp = await client.delete(del_url, auth=(userid, password))
                                if delete_resp.status_code in (204, 404):
                                    additional_deleted += 1
                                    logger.info(f"[SmartDrive] 🔥 AGGRESSIVE: Deleted remaining item: {display_name}")
                                else:
                                    logger.error(f"[SmartDrive] Failed aggressive deletion of {display_name}: HTTP {delete_resp.status_code}")
                            except Exception as e:
                                logger.error(f"[SmartDrive] Exception in aggressive cleanup of {display_name}: {e}")
                    else:
                        logger.info(f"[SmartDrive] ✅ No additional files found - account is clean")
                        
                except Exception as parse_error:
                    logger.error(f"[SmartDrive] Error parsing aggressive cleanup response: {parse_error}")
            else:
                logger.error(f"[SmartDrive] Aggressive cleanup PROPFIND failed: HTTP {prop.status_code}")
                
    except Exception as cleanup_error:
        logger.error(f"[SmartDrive] Aggressive cleanup failed: {cleanup_error}")
        
    return additional_deleted

async def get_mock_files_response(path: str) -> Dict:
    """Fallback mock data when Nextcloud is unavailable"""
    mock_files = [
        {
            "name": "Documents",
            "path": f"{path}Documents/",
            "type": "directory",
            "size": None,
            "modified": "2024-01-15T10:30:00Z"
        },
        {
            "name": "Training_Materials.pdf",
            "path": f"{path}Training_Materials.pdf",
            "type": "file",
            "size": 2048576,
            "modified": "2024-01-14T15:45:00Z",
            "mime_type": "application/pdf"
        },
        {
            "name": "Project_Notes.docx",
            "path": f"{path}Project_Notes.docx",
            "type": "file",
            "size": 1024000,
            "modified": "2024-01-13T09:20:00Z",
            "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        }
    ]
    
    return {
        "files": mock_files,
        "path": path,
        "total_count": len(mock_files)
    }

@app.post("/api/custom/smartdrive/mkdir")
async def smartdrive_mkdir(
    request: Request,
    payload: Dict[str, str] = Body(...),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a new directory via WebDAV (MKCOL). Ensures parent directories exist."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        await _check_rate_limit("mkdir", str(onyx_user_id))
        raw_path = (payload.get("path") or "/").strip()
        path = await _normalize_smartdrive_path(raw_path)

        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)

        webdav_user_base = f"{base_url}/remote.php/dav/files/{username}"
        # Ensure parent directories
        await _ensure_folder_tree(webdav_user_base, user_root_prefix + path, auth=(username, password))

        # Create target folder (MKCOL)
        target = _ensure_trailing_slash(_encode_dav_path(user_root_prefix + path))
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.request("MKCOL", f"{webdav_user_base}{target}", auth=(username, password))
        if resp.status_code in (201, 405):  # created or already exists
            return {"success": True}
        raise HTTPException(status_code=_map_webdav_status(resp.status_code), detail=_dav_error(resp))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] mkdir error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to create folder")


@app.post("/api/custom/smartdrive/upload")
async def smartdrive_upload(
    request: Request,
    path: str = Query("/", description="Destination directory path"),
    files: List[UploadFile] = File(...),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Upload one or more files via WebDAV PUT using streaming (no full file buffering)."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        norm_dir = await _normalize_smartdrive_path(path)
        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
        webdav_user_base = f"{base_url}/remote.php/dav/files/{username}"

        # Ensure destination folder exists
        await _ensure_folder_tree(webdav_user_base, _ensure_trailing_slash(_encode_dav_path(user_root_prefix + norm_dir)), auth=(username, password))

        results: List[Dict[str, Any]] = []
        async with httpx.AsyncClient(timeout=None) as client:
            for f in files:
                safe_name = _sanitize_filename(f.filename or "upload.bin")
                # Encode destination path and filename for WebDAV
                from urllib.parse import quote as _quote
                dest_dir = _ensure_trailing_slash(_encode_dav_path(user_root_prefix + norm_dir))
                dest_url = f"{webdav_user_base}{dest_dir}{_quote(safe_name, safe='')}"

                # Track file size during streaming
                file_size = 0
                async def aiter():
                    nonlocal file_size
                    while True:
                        chunk = await f.read(1024 * 64)
                        if not chunk:
                            break
                        file_size += len(chunk)
                        yield chunk

                resp = await client.put(dest_url, auth=(username, password), content=aiter())
                entry: Dict[str, Any] = {"file": safe_name, "success": resp.status_code in (200, 201, 204), "status": resp.status_code, "size": file_size}
                if not entry["success"]:
                    entry["error"] = _dav_error(resp)
                    results.append(entry)
                else:
                    # Post-upload: import into Onyx immediately
                    try:
                        smart_path = f"{_ensure_trailing_slash(norm_dir)}{safe_name}"
                        file_info = {"name": safe_name, "path": smart_path, "type": "file", "mime_type": resp.headers.get("content-type", "application/octet-stream")}
                        session_cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
                        onyx_file_id = await import_file_to_onyx_individual(
                            username, password, base_url, smart_path, file_info, str(onyx_user_id), session_cookies
                        )
                        if onyx_file_id:
                            entry["onyx_file_id"] = int(onyx_file_id) if str(onyx_file_id).isdigit() else onyx_file_id
                            async with pool.acquire() as conn2:
                                await conn2.execute(
                                    """
                                    INSERT INTO smartdrive_imports (onyx_user_id, smartdrive_path, onyx_file_id, etag, checksum, imported_at)
                                    VALUES ($1, $2, $3, $4, $5, $6)
                                    ON CONFLICT (onyx_user_id, smartdrive_path)
                                    DO UPDATE SET onyx_file_id = EXCLUDED.onyx_file_id, etag = EXCLUDED.etag, checksum = EXCLUDED.checksum, imported_at = EXCLUDED.imported_at
                                    """,
                                    str(onyx_user_id),
                                    smart_path,
                                    str(entry["onyx_file_id"]),
                                    resp.headers.get("etag", f"etag_{hash(safe_name)}"),
                                    f"imported_{int(time.time())}",
                                    datetime.now(timezone.utc),
                                )
                    except Exception as import_err:
                        logger.warning(f"Post-upload Onyx import failed for {safe_name}: {import_err}")
                    
                    # Update storage usage tracking
                    try:
                        async with pool.acquire() as conn2:
                            await conn2.execute(
                                """
                                INSERT INTO user_storage_usage (onyx_user_id, used_bytes, updated_at)
                                VALUES ($1, $2, now())
                                ON CONFLICT (onyx_user_id)
                                DO UPDATE SET used_bytes = user_storage_usage.used_bytes + EXCLUDED.used_bytes, updated_at = now()
                                """,
                                str(onyx_user_id),
                                file_size,
                            )
                            logger.info(f"[STORAGE] Updated usage for {onyx_user_id}: +{file_size} bytes")
                    except Exception as storage_err:
                        logger.warning(f"Failed to update storage usage: {storage_err}")
                    
                    results.append(entry)
                await f.close()

        # If any failed, return 207 multi-status style payload
        if any(not r["success"] for r in results):
            return JSONResponse(status_code=207, content={"results": results})
        return {"success": True, "results": results}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] upload error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to upload files")


@app.get("/api/custom/smartdrive/indexing-status")
async def smartdrive_indexing_status(
    request: Request,
    paths: List[str] = Query([]),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"[SmartDrive] IndexingStatus: user_id={onyx_user_id}, raw_paths={paths}")
        
        # Normalize paths
        norm_paths: List[str] = []
        for p in paths:
            try:
                norm_p = await _normalize_smartdrive_path(p)
                norm_paths.append(norm_p)
                logger.info(f"[SmartDrive] IndexingStatus: normalized '{p}' -> '{norm_p}'")
            except Exception as e:
                logger.error(f"[SmartDrive] IndexingStatus: failed to normalize path '{p}': {e}")
                continue
        
        if not norm_paths:
            logger.warning(f"[SmartDrive] IndexingStatus: no valid normalized paths")
            return {"statuses": {}}

        # Lookup Onyx file IDs
        path_to_file_id: Dict[str, str] = {}
        async with pool.acquire() as conn:
            logger.info(f"[SmartDrive] IndexingStatus: querying database for user_id={onyx_user_id}, paths={norm_paths}")
            rows = await conn.fetch(
                """
                SELECT smartdrive_path, onyx_file_id
                FROM smartdrive_imports
                WHERE onyx_user_id = $1 AND smartdrive_path = ANY($2::text[])
                """,
                str(onyx_user_id),
                norm_paths,
            )
            logger.info(f"[SmartDrive] IndexingStatus: found {len(rows)} records in database")
            
        for r in rows:
            path = r["smartdrive_path"]
            file_id = str(r["onyx_file_id"])
            path_to_file_id[path] = file_id
            logger.info(f"[SmartDrive] IndexingStatus: mapping path='{path}' -> onyx_file_id='{file_id}'")

        if not path_to_file_id:
            logger.warning(f"[SmartDrive] IndexingStatus: no file mappings found")
            return {"statuses": {p: None for p in norm_paths}}

        # Query Onyx for indexing status
        file_ids = list(path_to_file_id.values())
        query_params = []
        for fid in file_ids:
            query_params.append(("file_ids", fid))
        
        onyx_url = f"{ONYX_API_SERVER_URL}/user/file/indexing-status"
        logger.info(f"[SmartDrive] IndexingStatus: querying Onyx at {onyx_url} with file_ids={file_ids}")
        
        async with httpx.AsyncClient(timeout=15.0) as client:
            resp = await client.get(
                onyx_url,
                params=query_params,
                cookies={ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)},
            )
        
        logger.info(f"[SmartDrive] IndexingStatus: Onyx response status={resp.status_code}")
        
        if not resp.is_success:
            logger.error(f"[SmartDrive] IndexingStatus: Onyx request failed: {resp.status_code} - {resp.text[:500]}")
            return {"statuses": {p: None for p in norm_paths}}
        
        status_map = resp.json()  # {file_id: bool}
        logger.info(f"[SmartDrive] IndexingStatus: Onyx response data={status_map}")

        # Map back to paths
        out: Dict[str, Any] = {}
        for p, fid in path_to_file_id.items():
            indexed = bool(status_map.get(str(fid)))
            out[p] = indexed
            logger.info(f"[SmartDrive] IndexingStatus: final mapping path='{p}' (file_id={fid}) -> indexed={indexed}")
        
        for p in norm_paths:
            if p not in out:
                out[p] = None
                logger.info(f"[SmartDrive] IndexingStatus: no mapping found for path='{p}' -> None")
        
        logger.info(f"[SmartDrive] IndexingStatus: returning statuses={out}")
        return {"statuses": out}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] indexing-status error: {e}", exc_info=True)
        return {"statuses": {p: None for p in paths}}


def _estimate_tokens_from_file_info(file_path: str, file_size_bytes: int, mime_type: str) -> int:
    """Estimate token count based on file type, size, and MIME type."""
    file_path_lower = file_path.lower()
    mime_type_lower = mime_type.lower()
    
    # Token estimation ratios (tokens per byte) based on file type
    if file_path_lower.endswith('.pdf') or 'pdf' in mime_type_lower:
        # PDFs: ~1 token per 2-3 bytes (2x faster processing)
        # Larger PDFs tend to have more images/formatting, so lower ratio
        if file_size_bytes < 100_000:  # < 100KB
            ratio = 1/2  # 1 token per 2 bytes (2x faster)
        elif file_size_bytes < 1_000_000:  # < 1MB
            ratio = 1/2.5  # 1 token per 2.5 bytes (2x faster)
        else:  # >= 1MB
            ratio = 1/3  # 1 token per 3 bytes (2x faster)
        return max(250, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.doc', '.docx')) or 'word' in mime_type_lower or 'document' in mime_type_lower:
        # Word docs: ~1 token per 1.5-2 bytes (2x faster processing)
        if file_size_bytes < 50_000:  # < 50KB
            ratio = 1/1.5  # 1 token per 1.5 bytes (2x faster)
        elif file_size_bytes < 500_000:  # < 500KB
            ratio = 1/1.75  # 1 token per 1.75 bytes (2x faster)
        else:  # >= 500KB
            ratio = 1/2  # 1 token per 2 bytes (2x faster)
        return max(150, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.txt', '.md', '.rtf')) or 'text' in mime_type_lower:
        # Plain text: ~1 token per 1-1.5 bytes (2x faster processing)
        if file_size_bytes < 10_000:  # < 10KB
            ratio = 1/1  # 1 token per 1 byte (2x faster)
        elif file_size_bytes < 100_000:  # < 100KB
            ratio = 1/1.25  # 1 token per 1.25 bytes (2x faster)
        else:  # >= 100KB
            ratio = 1/1.5  # 1 token per 1.5 bytes (2x faster)
        return max(50, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.ppt', '.pptx')) or 'presentation' in mime_type_lower:
        # PowerPoint: ~1 token per 2.5-3.5 bytes (2x faster processing)
        if file_size_bytes < 100_000:  # < 100KB
            ratio = 1/2.5  # 1 token per 2.5 bytes (2x faster)
        elif file_size_bytes < 1_000_000:  # < 1MB
            ratio = 1/3  # 1 token per 3 bytes (2x faster)
        else:  # >= 1MB
            ratio = 1/3.5  # 1 token per 3.5 bytes (2x faster)
        return max(200, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.xls', '.xlsx')) or 'spreadsheet' in mime_type_lower or 'excel' in mime_type_lower:
        # Excel: ~1 token per 2-3 bytes (2x faster processing)
        if file_size_bytes < 50_000:  # < 50KB
            ratio = 1/2  # 1 token per 2 bytes (2x faster)
        elif file_size_bytes < 500_000:  # < 500KB
            ratio = 1/2.5  # 1 token per 2.5 bytes (2x faster)
        else:  # >= 500KB
            ratio = 1/3  # 1 token per 3 bytes (2x faster)
        return max(100, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.html', '.htm')) or 'html' in mime_type_lower:
        # HTML: ~1 token per 1.75 bytes (2x faster processing)
        ratio = 1/1.75
        return max(100, int(file_size_bytes * ratio))
    
    elif file_path_lower.endswith(('.json', '.xml')) or 'json' in mime_type_lower or 'xml' in mime_type_lower:
        # JSON/XML: ~1 token per 1.25 bytes (2x faster processing)
        ratio = 1/1.25
        return max(50, int(file_size_bytes * ratio))
    
    else:
        # Unknown file type: conservative estimate
        # Assume it's mostly binary with some text content
        ratio = 1/4  # 1 token per 4 bytes (2x faster)
        return max(100, int(file_size_bytes * ratio))


def _estimate_tokens_from_file_type(file_path: str) -> int:
    """Fallback token estimation based on file type only (when size is unknown)."""
    file_path_lower = file_path.lower()
    
    if file_path_lower.endswith('.pdf'):
        return 2500  # Average PDF (2x faster)
    elif file_path_lower.endswith(('.doc', '.docx')):
        return 1500  # Average Word doc (2x faster)
    elif file_path_lower.endswith(('.txt', '.md')):
        return 500   # Average text file (2x faster)
    elif file_path_lower.endswith(('.ppt', '.pptx')):
        return 2000  # Average PowerPoint (2x faster)
    elif file_path_lower.endswith(('.xls', '.xlsx')):
        return 1000  # Average Excel (2x faster)
    elif file_path_lower.endswith(('.html', '.htm')):
        return 750   # Average HTML (2x faster)
    elif file_path_lower.endswith(('.json', '.xml')):
        return 400   # Average JSON/XML (2x faster)
    else:
        return 1000  # Default for unknown types (2x faster)


@app.get("/api/custom/smartdrive/indexing-progress")
async def smartdrive_indexing_progress(
    request: Request,
    paths: List[str] = Query([]),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Get detailed indexing progress for SmartDrive files using IndexAttempt data."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"[SmartDrive] IndexingProgress: user_id={onyx_user_id}, raw_paths={paths}")
        
        # Normalize paths
        norm_paths: List[str] = []
        for p in paths:
            try:
                norm_p = await _normalize_smartdrive_path(p)
                norm_paths.append(norm_p)
                logger.info(f"[SmartDrive] IndexingProgress: normalized '{p}' -> '{norm_p}'")
            except Exception as e:
                logger.error(f"[SmartDrive] IndexingProgress: failed to normalize path '{p}': {e}")
                continue
        
        if not norm_paths:
            logger.warning(f"[SmartDrive] IndexingProgress: no valid normalized paths")
            return {"progress": {}}

        # Lookup Onyx file IDs and get token estimates
        path_to_file_id: Dict[str, str] = {}
        path_to_tokens: Dict[str, int] = {}
        
        async with pool.acquire() as conn:
            logger.info(f"[SmartDrive] IndexingProgress: querying database for user_id={onyx_user_id}, paths={norm_paths}")
            rows = await conn.fetch(
                """
                SELECT smartdrive_path, onyx_file_id
                FROM smartdrive_imports
                WHERE onyx_user_id = $1 AND smartdrive_path = ANY($2::text[])
                """,
                str(onyx_user_id),
                norm_paths,
            )
            logger.info(f"[SmartDrive] IndexingProgress: found {len(rows)} records in database")
            
        for r in rows:
            path = r["smartdrive_path"]
            file_id = str(r["onyx_file_id"])
            path_to_file_id[path] = file_id
            logger.info(f"[SmartDrive] IndexingProgress: mapping path='{path}' -> onyx_file_id='{file_id}'")

        if not path_to_file_id:
            logger.warning(f"[SmartDrive] IndexingProgress: no file mappings found")
            return {"progress": {p: None for p in norm_paths}}

        # Get token estimates for all files
        for path in path_to_file_id.keys():
            try:
                # Use the existing token-estimate endpoint logic
                file_id = path_to_file_id[path]
                async with httpx.AsyncClient(timeout=10.0) as client:
                    resp = await client.get(
                        f"{ONYX_API_SERVER_URL}/user/file/token-estimate",
                        params={"file_ids": file_id},
                        cookies={ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)},
                    )
                if resp.is_success:
                    data = resp.json() or {}
                    total_tokens = int(data.get("total_tokens") or 0)
                    path_to_tokens[path] = total_tokens
                    logger.info(f"[SmartDrive] IndexingProgress: path='{path}' tokens={total_tokens}")
                else:
                    logger.warning(f"[SmartDrive] IndexingProgress: token estimate request failed for path='{path}': {resp.status_code}")
                    path_to_tokens[path] = 0
            except Exception as e:
                logger.warning(f"[SmartDrive] IndexingProgress: failed to get tokens for path='{path}': {e}")
                path_to_tokens[path] = 0
            
            # If we still have 0 tokens, estimate based on file type and size
            if path_to_tokens[path] == 0:
                try:
                    # Get file size from WebDAV HEAD request
                    async with pool.acquire() as conn:
                        username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
                    
                    base = f"{base_url}/remote.php/dav/files/{username}"
                    file_url = f"{base}{user_root_prefix}{path}"
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        head = await client.head(file_url, auth=(username, password))
                    
                    if head.is_success:
                        content_length = head.headers.get("content-length")
                        mime_type = head.headers.get("content-type", "")
                        
                        if content_length and content_length.isdigit():
                            file_size_bytes = int(content_length)
                            estimated_tokens = _estimate_tokens_from_file_info(path, file_size_bytes, mime_type)
                            path_to_tokens[path] = estimated_tokens
                            logger.info(f"[SmartDrive] IndexingProgress: estimated tokens for path='{path}' (size={file_size_bytes} bytes, type={mime_type}): {estimated_tokens}")
                        else:
                            # Fallback to file type only
                            estimated_tokens = _estimate_tokens_from_file_type(path)
                            path_to_tokens[path] = estimated_tokens
                            logger.info(f"[SmartDrive] IndexingProgress: fallback token estimate for path='{path}': {estimated_tokens}")
                    else:
                        # Fallback to file type only
                        estimated_tokens = _estimate_tokens_from_file_type(path)
                        path_to_tokens[path] = estimated_tokens
                        logger.info(f"[SmartDrive] IndexingProgress: fallback token estimate for path='{path}': {estimated_tokens}")
                        
                except Exception as e:
                    logger.warning(f"[SmartDrive] IndexingProgress: failed to get file size for path='{path}': {e}")
                    # Final fallback to file type only
                    estimated_tokens = _estimate_tokens_from_file_type(path)
                    path_to_tokens[path] = estimated_tokens
                    logger.info(f"[SmartDrive] IndexingProgress: final fallback token estimate for path='{path}': {estimated_tokens}")

        # First, get the basic indexing status using the existing endpoint
        file_ids = list(path_to_file_id.values())
        query_params = []
        for fid in file_ids:
            query_params.append(("file_ids", fid))
        
        onyx_status_url = f"{ONYX_API_SERVER_URL}/user/file/indexing-status"
        logger.info(f"[SmartDrive] IndexingProgress: querying Onyx indexing status at {onyx_status_url} with file_ids={file_ids}")
        
        async with httpx.AsyncClient(timeout=15.0) as client:
            resp = await client.get(
                onyx_status_url,
                params=query_params,
                cookies={ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)},
            )
        
        if not resp.is_success:
            logger.error(f"[SmartDrive] IndexingProgress: Onyx indexing status request failed: {resp.status_code}")
            return {"progress": {p: None for p in norm_paths}}
        
        status_map = resp.json()  # {file_id: bool}
        logger.info(f"[SmartDrive] IndexingProgress: Onyx indexing status response: {status_map}")

        # Build progress data based on indexing status
        progress_data = {}
        for path, file_id in path_to_file_id.items():
            is_indexed = bool(status_map.get(str(file_id)))
            estimated_tokens = path_to_tokens.get(path, 0)
            
            if is_indexed:
                # File is fully indexed
                progress_data[path] = {
                    "status": "success",
                    "time_started": None,
                    "time_updated": None,
                    "total_docs_indexed": 1,  # Single file = 1 document
                    "estimated_tokens": estimated_tokens,
                    "is_complete": True
                }
                logger.info(f"[SmartDrive] IndexingProgress: path='{path}' is fully indexed")
            else:
                # File is still being indexed - provide enhanced progress data
                # For now, we'll use a simple status with token-based estimation
                progress_data[path] = {
                    "status": "in_progress",
                    "time_started": None,  # We don't have this from the basic endpoint
                    "time_updated": None,
                    "total_docs_indexed": 0,
                    "estimated_tokens": estimated_tokens,
                    "is_complete": False
                }
                logger.info(f"[SmartDrive] IndexingProgress: path='{path}' is still being indexed")

        # Fill in missing paths
        for p in norm_paths:
            if p not in progress_data:
                progress_data[p] = None
                logger.info(f"[SmartDrive] IndexingProgress: no data for path='{p}' -> None")
        
        logger.info(f"[SmartDrive] IndexingProgress: returning progress data for {len(progress_data)} paths")
        return {"progress": progress_data}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] indexing-progress error: {e}", exc_info=True)
        return {"progress": {p: None for p in paths}}


@app.post("/api/custom/smartdrive/move")
async def smartdrive_move(
    request: Request,
    payload: Dict[str, str] = Body(...),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Move a file or folder via WebDAV MOVE."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        src = await _normalize_smartdrive_path(payload.get("from") or "/")
        dst = await _normalize_smartdrive_path(payload.get("to") or "/")
        
        logger.info(f"[SmartDrive] MOVE request: from={src} to={dst}")
        
        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
        
        base = f"{base_url}/remote.php/dav/files/{username}"
        
        # Ensure destination parent directory exists to avoid 403
        try:
            dst_parent = dst.rsplit("/", 1)[0] or "/"
            # Don't encode here - _ensure_folder_tree will encode internally
            await _ensure_folder_tree(base, _ensure_trailing_slash(user_root_prefix + dst_parent), auth=(username, password))
        except Exception as _e:
            logger.debug(f"[SmartDrive] ensure parent for MOVE failed (non-fatal): {_e}")
        
        # Build source and destination URLs
        source_url = f"{base}{_encode_dav_path(user_root_prefix + src)}"
        dest_url = f"{base}{_encode_dav_path(user_root_prefix + dst)}"
        
        headers = {"Destination": dest_url, "Overwrite": "T"}
        
        logger.info(f"[SmartDrive] MOVE: {source_url} -> Destination: {dest_url}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            resp = await client.request("MOVE", source_url, auth=(username, password), headers=headers)
        
        logger.info(f"[SmartDrive] MOVE response: status={resp.status_code}")
        
        # WebDAV MOVE success codes: 201 (Created), 204 (No Content), 200 (OK)
        if resp.status_code in (200, 201, 204):
            # Update the file path mapping in smartdrive_imports table
            try:
                async with pool.acquire() as conn:
                    await conn.execute(
                        """
                        UPDATE smartdrive_imports 
                        SET smartdrive_path = $1, imported_at = NOW()
                        WHERE onyx_user_id = $2 AND smartdrive_path = $3
                        """,
                        dst, onyx_user_id, src
                    )
                    logger.info(f"[SmartDrive] Updated file mapping: {src} -> {dst}")
            except Exception as map_err:
                logger.warning(f"[SmartDrive] Failed to update file mapping: {map_err}")
            return {"success": True}
        
        # If 403 and error mentions "out of base uri" with "/smartdrive", retry with adjusted Destination
        if resp.status_code == 403 and "/smartdrive" in resp.text and "out of base uri" in resp.text:
            logger.info(f"[SmartDrive] MOVE 403 - trying with /smartdrive prefix in Destination")
            # Extract just the path and add /smartdrive prefix
            from urllib.parse import urlparse
            parsed_dest = urlparse(dest_url)
            adjusted_dest = f"/smartdrive{parsed_dest.path}"
            headers_retry = {"Destination": adjusted_dest, "Overwrite": "T"}
            logger.info(f"[SmartDrive] MOVE retry: {source_url} -> Destination: {adjusted_dest}")
            
            async with httpx.AsyncClient(timeout=60.0) as client2:
                resp2 = await client2.request("MOVE", source_url, auth=(username, password), headers=headers_retry)
            
            logger.info(f"[SmartDrive] MOVE retry response: status={resp2.status_code}")
            if resp2.status_code in (200, 201, 204):
                # Update the file path mapping in smartdrive_imports table
                try:
                    async with pool.acquire() as conn:
                        await conn.execute(
                            """
                            UPDATE smartdrive_imports 
                            SET smartdrive_path = $1, imported_at = NOW()
                            WHERE onyx_user_id = $2 AND smartdrive_path = $3
                            """,
                            dst, onyx_user_id, src
                        )
                        logger.info(f"[SmartDrive] Updated file mapping (retry): {src} -> {dst}")
                except Exception as map_err:
                    logger.warning(f"[SmartDrive] Failed to update file mapping (retry): {map_err}")
                return {"success": True}
            error_detail = _dav_error(resp2)
            logger.error(f"[SmartDrive] MOVE retry failed: status={resp2.status_code}, detail={error_detail}")
            raise HTTPException(status_code=_map_webdav_status(resp2.status_code), detail=error_detail)
        
        error_detail = _dav_error(resp)
        logger.error(f"[SmartDrive] MOVE failed: status={resp.status_code}, detail={error_detail}")
        raise HTTPException(status_code=_map_webdav_status(resp.status_code), detail=error_detail)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] move error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to move item")


@app.post("/api/custom/smartdrive/copy")
async def smartdrive_copy(
    request: Request,
    payload: Dict[str, str] = Body(...),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Copy a file or folder via WebDAV COPY."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        src = await _normalize_smartdrive_path(payload.get("from") or "/")
        dst = await _normalize_smartdrive_path(payload.get("to") or "/")
        
        logger.info(f"[SmartDrive] COPY request: from={src} to={dst}")
        
        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
        
        base = f"{base_url}/remote.php/dav/files/{username}"
        
        # Ensure destination parent directory exists to avoid 403
        try:
            dst_parent = dst.rsplit("/", 1)[0] or "/"
            # Don't encode here - _ensure_folder_tree will encode internally
            await _ensure_folder_tree(base, _ensure_trailing_slash(user_root_prefix + dst_parent), auth=(username, password))
        except Exception as _e:
            logger.debug(f"[SmartDrive] ensure parent for COPY failed (non-fatal): {_e}")
        
        # Build source and destination URLs
        source_url = f"{base}{_encode_dav_path(user_root_prefix + src)}"
        dest_url = f"{base}{_encode_dav_path(user_root_prefix + dst)}"
        
        headers = {"Destination": dest_url, "Overwrite": "T"}
        
        logger.info(f"[SmartDrive] COPY: {source_url} -> Destination: {dest_url}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            resp = await client.request("COPY", source_url, auth=(username, password), headers=headers)
        
        logger.info(f"[SmartDrive] COPY response: status={resp.status_code}")
        
        # WebDAV COPY success codes: 201 (Created), 204 (No Content), 200 (OK)
        if resp.status_code in (200, 201, 204):
            return {"success": True}
        
        # If 403 and error mentions "out of base uri" with "/smartdrive", retry with adjusted Destination
        if resp.status_code == 403 and "/smartdrive" in resp.text and "out of base uri" in resp.text:
            logger.info(f"[SmartDrive] COPY 403 - trying with /smartdrive prefix in Destination")
            # Extract just the path and add /smartdrive prefix
            from urllib.parse import urlparse
            parsed_dest = urlparse(dest_url)
            adjusted_dest = f"/smartdrive{parsed_dest.path}"
            headers_retry = {"Destination": adjusted_dest, "Overwrite": "T"}
            logger.info(f"[SmartDrive] COPY retry: {source_url} -> Destination: {adjusted_dest}")
            
            async with httpx.AsyncClient(timeout=60.0) as client2:
                resp2 = await client2.request("COPY", source_url, auth=(username, password), headers=headers_retry)
            
            logger.info(f"[SmartDrive] COPY retry response: status={resp2.status_code}")
            if resp2.status_code in (200, 201, 204):
                return {"success": True}
            error_detail = _dav_error(resp2)
            logger.error(f"[SmartDrive] COPY retry failed: status={resp2.status_code}, detail={error_detail}")
            raise HTTPException(status_code=_map_webdav_status(resp2.status_code), detail=error_detail)
        
        error_detail = _dav_error(resp)
        logger.error(f"[SmartDrive] COPY failed: status={resp.status_code}, detail={error_detail}")
        raise HTTPException(status_code=_map_webdav_status(resp.status_code), detail=error_detail)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] copy error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to copy item")


@app.delete("/api/custom/smartdrive/delete")
async def smartdrive_delete(
    request: Request,
    payload: Dict[str, List[str]] = Body(...),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Delete one or more files/folders via WebDAV DELETE."""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        paths = payload.get("paths") or []
        if not isinstance(paths, list) or not paths:
            raise HTTPException(status_code=400, detail="paths array is required")
        norm_paths = [await _normalize_smartdrive_path(p) for p in paths]
        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
        base = f"{base_url}/remote.php/dav/files/{username}"
        results: List[Dict[str, Any]] = []
        async with httpx.AsyncClient(timeout=60.0) as client:
            for p in norm_paths:
                resp = await client.delete(f"{base}{_encode_dav_path(user_root_prefix + p)}", auth=(username, password))
                ok = resp.status_code in (200, 204)
                results.append({"path": p, "success": ok, "status": resp.status_code, "error": None if ok else _dav_error(resp)})
        if any(not r["success"] for r in results):
            return JSONResponse(status_code=207, content={"results": results})
        return {"success": True, "results": results}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] delete error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to delete items")


@app.get("/api/custom/smartdrive/token-estimate")
async def smartdrive_token_estimate(
    request: Request,
    path: str = Query(..., description="File path to estimate tokens for"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Estimate token count for a SmartDrive file.
    - Primary: Use stored Onyx token_count if we have a mapping and the file has been processed previously
    - Fallback: Use Content-Length from WebDAV HEAD and approximate tokens ~= bytes / 4
    """
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        norm_path = await _normalize_smartdrive_path(path)
        async with pool.acquire() as conn:
            username, password, base_url, user_root_prefix = await _get_nextcloud_credentials(conn, onyx_user_id)
            # Try to find mapped onyx_file_id
            rec = await conn.fetchrow(
                """
                SELECT onyx_file_id FROM smartdrive_imports
                WHERE onyx_user_id = $1 AND smartdrive_path = $2
                """,
                str(onyx_user_id),
                norm_path,
            )
        # If we have an Onyx file id, try to fetch its token_count quickly
        if rec and rec.get("onyx_file_id"):
            file_id = str(rec["onyx_file_id"]).strip()
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    resp = await client.get(
                        f"{ONYX_API_SERVER_URL}/user/file/token-estimate",
                        params={"file_ids": file_id},
                        cookies={ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)},
                    )
                if resp.is_success:
                    data = resp.json() or {}
                    total_tokens = int(data.get("total_tokens") or 0)
                    if total_tokens > 0:
                        return {"tokens": total_tokens, "source": "onyx"}
            except Exception:
                pass
        # Otherwise use HEAD to get Content-Length and approximate
        base = f"{base_url}/remote.php/dav/files/{username}"
        file_url = f"{base}{_encode_dav_path(user_root_prefix + norm_path)}"
        async with httpx.AsyncClient(timeout=15.0) as client:
            head = await client.head(file_url, auth=(username, password))
        if not head.is_success:
            raise HTTPException(status_code=_map_webdav_status(head.status_code), detail=_dav_error(head))
        content_length = head.headers.get("content-length")
        mime_type = head.headers.get("content-type", "")
        approx_tokens = 0
        if content_length and content_length.isdigit():
            # Heuristic per type: md/txt fastest, others moderate, pdf largest
            lower = mime_type.lower()
            if "markdown" in lower or "md" in lower or "text/plain" in lower:
                divisor = 4
            elif "pdf" in lower:
                divisor = 8
            else:
                divisor = 6
            approx_tokens = max(1, int(int(content_length) / divisor))
        else:
            # Fallback: conservative small estimate
            approx_tokens = 2000
        return {"tokens": approx_tokens, "source": "approx", "mime_type": mime_type}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] token-estimate error for path={path}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to estimate tokens")

@app.get("/api/custom/smartdrive/download")
async def smartdrive_download(
    request: Request,
    path: str = Query(..., description="File path to download"),
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Create a public download link for the file (like LMS export approach)."""
    try:
        from app.services.nextcloud_share import create_public_download_link
        
        onyx_user_id = await get_current_onyx_user_id(request)
        norm_path = await _normalize_smartdrive_path(path)
        
        logger.info(f"[SmartDrive] Creating download link for path={norm_path}")
        
        # Create public download link with short expiry (1 day for direct downloads)
        download_link = await create_public_download_link(onyx_user_id, norm_path, expiry_days=1)
        
        logger.info(f"[SmartDrive] Download link created: {download_link}")
        
        # Return the download link for the frontend to open
        return {"downloadUrl": download_link}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SmartDrive] download error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to create download link")


# --------- Helpers: auth, paths, rate limiting, dav utils ---------
# Simple in-memory rate limiting per user+operation
_RATE_BUCKETS: Dict[Tuple[str, str], list] = {}
_RATE_LIMITS: Dict[str, Tuple[int, int]] = {
    "mkdir": (30, 60),
    "upload": (60, 60),
    "move": (30, 60),
    "copy": (30, 60),
    "delete": (60, 60),
}

async def _check_rate_limit(operation: str, user_id: str) -> None:
    import time
    limit, window = _RATE_LIMITS.get(operation, (60, 60))
    key = (user_id, operation)
    now = time.time()
    bucket = _RATE_BUCKETS.get(key)
    if bucket is None:
        bucket = []
        _RATE_BUCKETS[key] = bucket
    # drop old
    cutoff = now - window
    i = 0
    for ts in bucket:
        if ts >= cutoff:
            break
        i += 1
    if i:
        del bucket[:i]
    if len(bucket) >= limit:
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    bucket.append(now)

async def _get_nextcloud_credentials(conn: asyncpg.Connection, onyx_user_id: str) -> Tuple[str, str, str, str]:
    """
    Returns (username, password, base_url, user_root_prefix)
    - Requires per-user credentials stored in smartdrive_accounts
    """
    account = await conn.fetchrow(
        "SELECT onyx_user_id, nextcloud_username, nextcloud_password_encrypted, nextcloud_base_url FROM smartdrive_accounts WHERE onyx_user_id = $1",
        onyx_user_id,
    )
    if not account or not account.get("nextcloud_username") or not account.get("nextcloud_password_encrypted"):
        raise HTTPException(status_code=401, detail="SmartDrive account not connected")

    base_url = (account.get("nextcloud_base_url") or os.environ.get("NEXTCLOUD_BASE_URL") or "").rstrip("/")
    if not base_url:
        raise HTTPException(status_code=400, detail="Nextcloud base URL not configured")

    try:
        password = decrypt_password(account["nextcloud_password_encrypted"])  # type: ignore
    except Exception:
        raise HTTPException(status_code=500, detail="Failed to decrypt SmartDrive credentials")

    return account["nextcloud_username"], password, base_url, ""


def _ensure_trailing_slash(p: str) -> str:
    return p if p.endswith("/") else p + "/"


def _sanitize_filename(name: str) -> str:
    # Basic filename sanitization
    name = name.strip().replace("\\", "_").replace("/", "_")
    if not name:
        return "file"
    return name


async def _normalize_smartdrive_path(p: str) -> str:
    """Normalize and validate a SmartDrive path. Must be absolute within the user's root."""
    from urllib.parse import unquote
    p = unquote(p or "/")
    if not p.startswith("/"):
        p = "/" + p
    # Collapse multiple slashes
    while "//" in p:
        p = p.replace("//", "/")
    # Reject traversal
    parts = [seg for seg in p.split("/") if seg not in ("", ".")]
    if any(seg == ".." for seg in parts):
        raise HTTPException(status_code=400, detail="Invalid path")
    # Rebuild
    normalized = "/" + "/".join(parts)
    return normalized if normalized != "" else "/"


def _encode_dav_path(path: str) -> str:
    """Percent-encode each path segment for WebDAV URLs, preserving slashes.
    Ensures leading/trailing slash semantics are kept.
    """
    try:
        from urllib.parse import quote
        if path is None:
            return "/"
        is_abs = path.startswith("/")
        is_dir = path.endswith("/")
        parts = [seg for seg in path.split("/") if seg != ""]
        encoded = "/".join(quote(seg, safe="") for seg in parts)
        return ("/" if is_abs else "") + encoded + ("/" if is_dir and encoded else "")
    except Exception:
        return path

async def _ensure_folder_tree(base: str, full_path: str, auth: Tuple[str, str]) -> None:
    """Ensure the full folder tree exists using MKCOL on each segment."""
    async with httpx.AsyncClient(timeout=30.0) as client:
        # Only directories
        path = _ensure_trailing_slash(full_path)
        segments = [s for s in path.strip("/").split("/") if s]
        cumulative = ""
        for seg in segments:
            cumulative += f"/{seg}"
            url = f"{base}{_ensure_trailing_slash(_encode_dav_path(cumulative))}"
            try:
                r = await client.request("MKCOL", url, auth=auth)
                if r.status_code in (201, 405):
                    continue
                elif 200 <= r.status_code < 300:
                    continue
                else:
                    logger.warning(f"MKCOL {url} -> {r.status_code} {r.text[:120]}")
            except Exception as e:
                logger.warning(f"MKCOL failed {url}: {e}")


def _map_webdav_status(status: int) -> int:
    mapping = {
        401: 401,
        403: 403,
        404: 404,
        405: 405,
        409: 409,
        423: 423,
        507: 507,
    }
    if status in mapping:
        return mapping[status]
    # Treat non-2xx as 500 by default
    return 500


def _dav_error(resp: httpx.Response) -> str:
    try:
        txt = resp.text[:400]
    except Exception:
        txt = ""
    return f"WebDAV error {resp.status_code}: {txt}"


def _guess_filename_from_path(path: str) -> str:
    try:
        return (path.rsplit("/", 1)[-1]) or "download"
    except Exception:
        return "download"

@app.post("/api/custom/smartdrive/import")
async def import_smartdrive_files(
    request: Request,
    file_paths: List[str] = None,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Stream files into Onyx; returns fileIds"""
    try:
        if file_paths is None:
            payload = await request.json()
            file_paths = payload.get('paths', [])
            
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Importing SmartDrive files for user: {onyx_user_id}, paths: {file_paths}")

        imported_file_ids = []
        
        # Get SmartDrive account details for the user
        async with pool.acquire() as conn:
            account = await conn.fetchrow(
                "SELECT nextcloud_username, nextcloud_password_encrypted, nextcloud_base_url FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                logger.error(f"No SmartDrive account found for user {onyx_user_id}")
                raise HTTPException(status_code=400, detail="SmartDrive not configured for this user")
            
            # Decrypt password (simplified - you may need proper decryption)
            # For now, assuming password is stored in plain text or you have decryption logic
            nextcloud_username = account['nextcloud_username']
            nextcloud_password = account['nextcloud_password_encrypted']  # TODO: Implement proper decryption
            nextcloud_base_url = account.get('nextcloud_base_url', 'http://nc1.contentbuilder.ai:8080')
            
            logger.info(f"Using SmartDrive account: {nextcloud_username} at {nextcloud_base_url}")

        # Process each file
            for file_path in file_paths:
                try:
                    logger.info(f"Processing SmartDrive file: {file_path}")
                    
                    # Download file from Nextcloud
                    file_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{file_path}"
                    
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        response = await client.get(
                            file_url,
                            auth=(nextcloud_username, nextcloud_password)
                        )
                        response.raise_for_status()
                        
                        file_content = response.content
                        file_name = os.path.basename(file_path)
                        
                        logger.info(f"Downloaded {file_name} ({len(file_content)} bytes)")
                    
                    # Create a temporary UploadFile object for Onyx
                    file_obj = io.BytesIO(file_content)
                    temp_file = UploadFile(
                        file=file_obj,
                        filename=file_name,
                        headers={"content-type": response.headers.get("content-type", "application/octet-stream")}
                    )
                    
                    # Import into Onyx using the standard process
                    # This will create real Onyx file records and return proper file IDs
                    from onyx.server.documents.connector import upload_files
                    from onyx.db.engine import get_session_with_tenant
                    
                    # Get a database session for Onyx operations
                    db_session = next(get_session_with_tenant())
                    
                    try:
                        # Upload file to Onyx file store
                        upload_response = upload_files([temp_file], db_session)
                        real_file_id = upload_response.file_paths[0]  # Get the real Onyx file ID
                        
                        logger.info(f"Uploaded to Onyx with file ID: {real_file_id}")
                        
                        # Store mapping in smartdrive_imports with REAL file ID
                        async with pool.acquire() as conn:
                            import_record_id = await conn.fetchval(
                    """
                    INSERT INTO smartdrive_imports (onyx_user_id, smartdrive_path, onyx_file_id, etag, checksum, imported_at)
                    VALUES ($1, $2, $3, $4, $5, $6)
                                ON CONFLICT (onyx_user_id, smartdrive_path) 
                                DO UPDATE SET 
                                    onyx_file_id = EXCLUDED.onyx_file_id,
                                    etag = EXCLUDED.etag,
                                    checksum = EXCLUDED.checksum,
                                    imported_at = EXCLUDED.imported_at
                    RETURNING id
                    """,
                    onyx_user_id,
                    file_path,
                                real_file_id,  # REAL Onyx file ID!
                                response.headers.get("etag", f"etag_{hash(file_path)}"),
                                f"imported_{int(time.time())}",  # Simple checksum
                    datetime.now(timezone.utc)
                )
                        
                        imported_file_ids.append(import_record_id)
                        logger.info(f"✅ Successfully imported {file_path} -> Onyx file ID: {real_file_id}")
                        
                    finally:
                        db_session.close()
                        
                except Exception as e:
                    logger.error(f"❌ Failed to import {file_path}: {e}", exc_info=True)
                    # Continue with other files even if one fails
                    continue

        return {
            "success": True,
            "fileIds": imported_file_ids,
            "imported_count": len(imported_file_ids)
        }
        
    except Exception as e:
        logger.error(f"Error importing SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to import SmartDrive files")

@app.post("/api/custom/smartdrive/import-new")
async def import_new_smartdrive_files(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Import new/updated files since the last sync"""
    try:
        onyx_user_id = await get_current_onyx_user_id(request)
        logger.info(f"Importing new SmartDrive files for user: {onyx_user_id}")
        
        # Extract session cookies for Onyx authentication
        session_cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}

        async with pool.acquire() as conn:
            # Get user's SmartDrive account
            account = await conn.fetchrow(
                "SELECT * FROM smartdrive_accounts WHERE onyx_user_id = $1",
                onyx_user_id
            )
            
            if not account:
                raise HTTPException(status_code=404, detail="SmartDrive account not found")

            # Check if user has set up their Nextcloud credentials
            if not account['nextcloud_username'] or not account['nextcloud_password_encrypted']:
                raise HTTPException(status_code=400, detail="Please set up your Nextcloud credentials first")

            # Decrypt user's credentials
            nextcloud_username = account['nextcloud_username']
            nextcloud_password = decrypt_password(account['nextcloud_password_encrypted'])
            nextcloud_base_url = account['nextcloud_base_url'] or 'http://nc1.contentbuilder.ai:8080'
            
            # Parse JSON cursor from database
            sync_cursor = json.loads(account['sync_cursor']) if account['sync_cursor'] else {}
            last_sync = sync_cursor.get('last_sync') if sync_cursor else None
            
            # Get list of all files from user's individual Nextcloud account
            all_files = await get_all_nextcloud_files_individual(nextcloud_username, nextcloud_password, nextcloud_base_url, "/")
            
            imported_count = 0
            imported_files = []
            
            logger.info(f"Processing {len(all_files)} items from Nextcloud")
            
            for file_info in all_files:
                logger.debug(f"Processing item: {file_info}")
                
                if file_info['type'] == 'directory':
                    logger.debug(f"Skipping directory: {file_info['path']}")
                    continue  # Skip directories for now
                    
                file_path = file_info['path']
                file_modified = file_info['modified']
                file_etag = file_info.get('etag')
                
                logger.info(f"Processing file: {file_path} (type: {file_info['type']}, etag: {file_etag}, modified: {file_modified})")
                
                # Check if already imported
                existing = await conn.fetchrow(
                    "SELECT id, etag FROM smartdrive_imports WHERE onyx_user_id = $1 AND smartdrive_path = $2",
                    onyx_user_id, file_path
                )
                
                logger.info(f"Existing record for {file_path}: {existing}")
                
                # Skip if already imported with same etag (only if both etags exist and match)
                if existing and existing.get('etag') and file_etag and existing['etag'] == file_etag:
                    logger.info(f"Skipping {file_path} - already imported with same etag: {file_etag}")
                    continue
                
                # For debugging: let's import all files for now and check the etag logic later
                logger.info(f"Will import file: {file_path} (etag: {file_etag}, existing_etag: {existing.get('etag') if existing else None})")
                
                # Try to import the file into Onyx
                try:
                    # Extract session cookies for Onyx authentication from the request object
                    session_cookies = {ONYX_SESSION_COOKIE_NAME: request.cookies.get(ONYX_SESSION_COOKIE_NAME)}
                    
                    onyx_file_id = await import_file_to_onyx_individual(
                        nextcloud_username,
                        nextcloud_password,
                        nextcloud_base_url, 
                        file_path, 
                        file_info, 
                        onyx_user_id,
                        session_cookies
                    )
                    
                    if onyx_file_id:
                        # Update or insert import record
                        if existing:
                            await conn.execute(
                                """
                                UPDATE smartdrive_imports 
                                SET onyx_file_id = $1, etag = $2, checksum = $3, imported_at = $4, last_modified = $5
                                WHERE id = $6
                                """,
                                onyx_file_id,
                                file_info.get('etag', ''),
                                file_info.get('checksum', ''),
                                datetime.now(timezone.utc),
                                parse_http_date(file_modified) if file_modified else None,
                                existing['id']
                            )
                        else:
                            await conn.execute(
                                """
                                INSERT INTO smartdrive_imports 
                                (onyx_user_id, smartdrive_path, onyx_file_id, etag, checksum, file_size, mime_type, imported_at, last_modified)
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                                """,
                                onyx_user_id,
                                file_path,
                                onyx_file_id,
                                file_info.get('etag', ''),
                                file_info.get('checksum', ''),
                                file_info.get('size'),
                                file_info.get('mime_type'),
                                datetime.now(timezone.utc),
                                parse_http_date(file_modified) if file_modified else None
                            )
                        
                        imported_count += 1
                        imported_files.append({
                            "name": file_info['name'],
                            "path": file_path,
                            "onyx_file_id": onyx_file_id
                        })
                        logger.info(f"Successfully imported {file_path} as Onyx file {onyx_file_id}")
                        
                except Exception as import_error:
                    logger.error(f"Failed to import {file_path}: {import_error}")
                    continue

            # Update sync cursor
            await conn.execute(
                "UPDATE smartdrive_accounts SET sync_cursor = $1, updated_at = $2 WHERE onyx_user_id = $3",
                json.dumps({"last_sync": datetime.now(timezone.utc).isoformat()}),
                datetime.now(timezone.utc),
                onyx_user_id
            )
            
            logger.info(f"Import completed: {imported_count} files imported for user {onyx_user_id}")

        return {
            "success": True,
            "imported_count": imported_count,
            "imported_files": imported_files,
            "message": f"Imported {imported_count} new files"
        }
        
    except Exception as e:
        logger.error(f"Error importing new SmartDrive files: {e}")
        raise HTTPException(status_code=500, detail="Failed to import new SmartDrive files")

async def ensure_user_folder_exists(nextcloud_username: str, nextcloud_password: str, user_folder: str):
    """Ensure the user's folder exists in Nextcloud, create if it doesn't"""
    try:
        folder_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{user_folder}/"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient() as client:
            # Check if folder exists
            response = await client.request("PROPFIND", folder_url, auth=auth, headers={"Depth": "0"})
            
            if response.status_code == 404:
                # Folder doesn't exist, create it
                logger.info(f"Creating Nextcloud folder for user: {user_folder}")
                create_response = await client.request("MKCOL", folder_url, auth=auth)
                
                if create_response.status_code in [201, 405]:  # 201 = Created, 405 = Already exists
                    logger.info(f"Successfully created/verified folder: {user_folder}")
                else:
                    logger.error(f"Failed to create folder {user_folder}: {create_response.status_code}")
            elif response.status_code == 207:
                # Folder exists
                logger.debug(f"User folder already exists: {user_folder}")
            else:
                logger.warning(f"Unexpected response checking folder {user_folder}: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Error ensuring user folder exists: {e}")

async def get_all_nextcloud_files_individual(nextcloud_username: str, nextcloud_password: str, nextcloud_base_url: str, base_path: str = "/") -> List[Dict]:
    """Get all files from user's individual Nextcloud account recursively"""
    all_files = []
    visited_paths = set()  # Prevent infinite recursion
    
    async def traverse_directory(path: str, depth: int = 0):
        # Prevent infinite recursion
        if depth > 10:  # Max depth limit
            logger.warning(f"Max recursion depth reached for path: {path}")
            return
            
        if path in visited_paths:
            logger.warning(f"Already visited path, skipping: {path}")
            return
            
        visited_paths.add(path)
        
        try:
            webdav_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{path}"
            auth = (nextcloud_username, nextcloud_password)
            
            logger.debug(f"Traversing directory: {path} (depth: {depth}, URL: {webdav_url})")
            
            async with httpx.AsyncClient() as client:
                response = await client.request(
                    "PROPFIND", 
                    webdav_url, 
                    auth=auth, 
                    headers={"Depth": "1", "Content-Type": "application/xml"},
                    content="""<?xml version="1.0"?>
                    <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                        <d:prop>
                            <d:resourcetype/>
                            <d:getcontentlength/>
                            <d:getlastmodified/>
                            <d:getcontenttype/>
                            <d:getetag/>
                        </d:prop>
                    </d:propfind>"""
                )
                
                if response.status_code == 404:
                    logger.warning(f"Directory not found: {path}")
                    return
                elif response.status_code != 207:
                    logger.error(f"WebDAV PROPFIND failed: {response.status_code} - {response.text}")
                    return
                    
                files = await parse_webdav_response(response.text, nextcloud_base_url)
                logger.debug(f"Found {len(files)} items in {path}")
                
                for file_info in files:
                    if file_info['name'] != '.' and file_info['name'] != '..':  # Skip current and parent directory entries
                        all_files.append(file_info)
                        
                        if file_info['type'] == 'directory':
                            # For subdirectories, traverse recursively
                            subdir_path = file_info['path']
                            logger.debug(f"Found subdirectory: {file_info['name']} -> {subdir_path}")
                            await traverse_directory(subdir_path, depth + 1)
                            
        except Exception as e:
            logger.error(f"Error traversing directory {path}: {e}")
    
    await traverse_directory(base_path)
    logger.info(f"Directory traversal completed. Found {len(all_files)} total items.")
    return all_files


async def get_all_nextcloud_files(nextcloud_user_folder: str, base_path: str = "/") -> List[Dict]:
    """Recursively get all files from Nextcloud using shared account"""
    all_files = []
    
    try:
        # Use shared Nextcloud account
        nextcloud_username = os.getenv("NEXTCLOUD_USERNAME", "smart_drive_user")
        nextcloud_password = os.getenv("NEXTCLOUD_PASSWORD", "nextcloud_password")
        
        webdav_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{nextcloud_user_folder}{base_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient() as client:
            response = await client.request(
                "PROPFIND",
                webdav_url,
                auth=auth,
                headers={
                    "Depth": "infinity",  # Get all files recursively
                    "Content-Type": "application/xml"
                },
                content="""<?xml version="1.0"?>
                <d:propfind xmlns:d="DAV:" xmlns:oc="http://owncloud.org/ns">
                    <d:prop>
                        <d:resourcetype/>
                        <d:getcontentlength/>
                        <d:getlastmodified/>
                        <d:getcontenttype/>
                        <d:getetag/>
                    </d:prop>
                </d:propfind>"""
            )
            
            if response.status_code == 207:
                files = await parse_webdav_response(response.text, base_path)
                all_files.extend(files)
                
    except Exception as e:
        logger.error(f"Error getting files from Nextcloud: {e}")
        
    return all_files

async def import_file_to_onyx(nextcloud_user_folder: str, file_path: str, file_info: Dict, onyx_user_id: str) -> str:
    """Download file from Nextcloud and upload to Onyx"""
    try:
        # Download file from Nextcloud using shared account
        nextcloud_username = os.getenv("NEXTCLOUD_USERNAME", "smart_drive_user")
        nextcloud_password = os.getenv("NEXTCLOUD_PASSWORD", "nextcloud_password")
        
        download_url = f"http://nc1.contentbuilder.ai:8080/remote.php/dav/files/{nextcloud_username}/{nextcloud_user_folder}{file_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            download_response = await client.get(download_url, auth=auth)
            
            if download_response.status_code != 200:
                logger.error(f"Failed to download {file_path}: {download_response.status_code}")
                return None
                
            file_content = download_response.content
            file_name = file_info['name']
            mime_type = file_info.get('mime_type', 'application/octet-stream')
            
            # Upload to Onyx using the user file upload endpoint (same as frontend)
            onyx_upload_url = f"{ONYX_API_SERVER_URL}/user/file/upload"
            
            # Create multipart form data with folder_id parameter
            files = {
                'files': (file_name, file_content, mime_type)
            }
            data = {
                'folder_id': '-1'  # Use RECENT_DOCS_FOLDER_ID (default "Recent Documents" folder)
            }
            
            # Upload to Onyx with session authentication
            upload_response = await client.post(
                onyx_upload_url,
                files=files,
                data=data,
                timeout=60.0
            )
            
            if upload_response.status_code in [200, 201]:
                response_data = upload_response.json()
                # Extract file ID from Onyx response
                if isinstance(response_data, list) and len(response_data) > 0:
                    return str(response_data[0].get('id'))
                elif isinstance(response_data, dict):
                    return str(response_data.get('id'))
                else:
                    logger.error(f"Unexpected Onyx response format: {response_data}")
                    return None
            else:
                logger.error(f"Failed to upload to Onyx: {upload_response.status_code} - {upload_response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error importing file {file_path} to Onyx: {e}")
        return None


async def import_file_to_onyx_individual(
    nextcloud_username: str, 
    nextcloud_password: str, 
    nextcloud_base_url: str, 
    file_path: str, 
    file_info: Dict, 
    onyx_user_id: str,
    session_cookies: Dict[str, str]
) -> str:
    """Download file from individual Nextcloud account and upload to Onyx"""
    try:
        # Build download URL for individual account
        download_url = f"{nextcloud_base_url}/remote.php/dav/files/{nextcloud_username}{file_path}"
        auth = (nextcloud_username, nextcloud_password)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            download_response = await client.get(download_url, auth=auth)
            
            if download_response.status_code != 200:
                logger.error(f"Failed to download {file_path}: {download_response.status_code}")
                return None
                
            file_content = download_response.content
            file_name = file_info['name']
            mime_type = file_info.get('mime_type', 'application/octet-stream')
            
            # Upload to Onyx using the user file upload endpoint (same as frontend)
            onyx_upload_url = f"{ONYX_API_SERVER_URL}/user/file/upload"
            
            # Create multipart form data with folder_id parameter
            files = {
                'files': (file_name, file_content, mime_type)
            }
            data = {
                'folder_id': '-1'  # Use RECENT_DOCS_FOLDER_ID (default "Recent Documents" folder)
            }
            
            # Upload to Onyx with session authentication
            upload_response = await client.post(
                onyx_upload_url,
                files=files,
                data=data,
                cookies=session_cookies,
                timeout=60.0
            )
            
            if upload_response.status_code in [200, 201]:
                response_data = upload_response.json()
                # Extract file ID from Onyx response
                if isinstance(response_data, list) and len(response_data) > 0:
                    return str(response_data[0].get('id'))
                elif isinstance(response_data, dict):
                    return str(response_data.get('id'))
                else:
                    logger.error(f"Unexpected Onyx response format: {response_data}")
                    return None
            else:
                logger.error(f"Failed to upload to Onyx: {upload_response.status_code} - {upload_response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error importing individual file {file_path}: {e}")
        return None


@app.post("/api/custom/smartdrive/webhook")
async def smartdrive_webhook(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Handle Nextcloud webhooks for file changes"""
    try:
        payload = await request.json()
        logger.info(f"Received SmartDrive webhook: {payload}")

        # TODO: Implement webhook processing
        # This would handle notifications from Nextcloud about file changes
        
        return {"success": True, "message": "Webhook processed"}
        
    except Exception as e:
        logger.error(f"Error processing SmartDrive webhook: {e}")
        raise HTTPException(status_code=500, detail="Failed to process webhook")

# ============================
# PER-USER CONNECTORS API (DEPRECATED)
# ============================
# 
# NOTE: We now use Onyx's native connector system with AccessType.PRIVATE
# instead of our custom connector implementation. The frontend redirects users
# to /admin/connectors/{source}?access_type=private to create connectors using
# Onyx's existing OAuth-enabled forms and configuration system.
#
# Users' private connectors are managed through:
# - Creation: /admin/connectors/{source} with access_type=private
# - Listing: /api/manage/admin/connector (filtered for private connectors)
# - Management: /admin/connector/{id} (Onyx's existing connector management UI)
# - Syncing: /api/manage/admin/connector/{id}/index (Onyx's existing sync API)
#
# This gives users the full Onyx connector experience (including OAuth support)
# while keeping connectors private to each user.

# New SmartDrive connector creation endpoint (bypasses admin requirements)
@app.post("/api/custom/smartdrive/connectors/create")
async def create_smartdrive_connector(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """
    Create a private connector for the current user without requiring admin privileges.
    This allows non-admin users to create connectors for Smart Drive.
    """
    try:
        # Get the main app domain (remove /custom-projects-ui path)
        host = request.headers.get('host', 'localhost')
        # Force HTTPS for production domains, use HTTP only for localhost
        if 'localhost' in host or '127.0.0.1' in host:
            protocol = 'http'
        else:
            protocol = 'https'
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies (using the same pattern as other endpoints)
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'x-smart-drive-connector': 'true'  # Smart Drive header to bypass admin checks
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Get connector data from request
        connector_data = await request.json()
        connector_id = connector_data.get('connector_id')  # This is the source type (e.g., 'notion', 'slack')
        credential_id = connector_data.get('credential_id')  # ID of existing credential to use
        name = connector_data.get('name', f'Smart Drive {connector_id}')
        indexing_start = connector_data.get('indexing_start', None)
        
        if not connector_id:
            raise HTTPException(status_code=400, detail="Connector ID is required")
        
        if not credential_id:
            raise HTTPException(status_code=400, detail="Credential ID is required")
        
        # Define which fields are credentials vs connector config
        credential_fields = {
            'notion': ['notion_integration_token'],
            'slack': ['slack_bot_token'],
            'github': ['github_access_token'],
            'zendesk': ['zendesk_subdomain', 'zendesk_email', 'zendesk_token'],
            'asana': ['asana_api_token_secret'],
            'dropbox': ['dropbox_access_token'],
            'confluence': ['confluence_username', 'confluence_access_token'],
            'jira': ['jira_user_email', 'jira_api_token'],
            'linear': ['linear_access_token'],
            'hubspot': ['hubspot_access_token'],
            'clickup': ['clickup_api_token', 'clickup_team_id'],
            'google_drive': ['google_tokens', 'google_primary_admin'],
            'gmail': ['google_tokens', 'google_primary_admin'],
            'salesforce': ['sf_username', 'sf_password', 'sf_security_token', 'is_sandbox'],
            'sharepoint': ['sp_client_id', 'sp_client_secret', 'sp_directory_id'],
            'airtable': ['airtable_access_token'],
            'document360': ['portal_id', 'document360_api_token'],
            'slab': ['slab_bot_token'],
            'guru': ['guru_user', 'guru_user_token'],
            'gong': ['gong_access_key', 'gong_access_key_secret'],
            'loopio': ['loopio_subdomain', 'loopio_client_id', 'loopio_client_token'],
            'productboard': ['productboard_access_token'],
            'zulip': ['zuliprc_content'],
            'gitbook': ['gitbook_api_key'],
            'gitlab': ['gitlab_url', 'gitlab_access_token'],
            'bookstack': ['bookstack_base_url', 'bookstack_api_token_id', 'bookstack_api_token_secret'],
            's3': ['aws_access_key_id', 'aws_secret_access_key', 'aws_role_arn'],
            'r2': ['account_id', 'r2_access_key_id', 'r2_secret_access_key'],
            'google_cloud_storage': ['access_key_id', 'secret_access_key'],
            'oci_storage': ['namespace', 'region', 'access_key_id', 'secret_access_key'],
            'teams': ['teams_client_id', 'teams_client_secret', 'teams_directory_id']
        }

        # Connectors that support only load_state input type
        load_connectors = ['airtable', 'google_sites', 'xenforo', 'web']
        
        # Separate credential fields from connector config fields
        connector_credential_fields = credential_fields.get(connector_id, [])
        credential_json = {}
        connector_specific_config = {}
        
        for key, value in connector_data.items():
            if key not in ['connector_id', 'name', 'access_type', 'smart_drive', 'credential_id']:
                if key in connector_credential_fields:
                    credential_json[key] = value
                else:
                    connector_specific_config[key] = value
        
        # Filter connector config to only include supported parameters
        supported_connector_params = {
            'google_drive': [
                'include_shared_drives', 'include_my_drives', 'include_files_shared_with_me',
                'shared_drive_urls', 'my_drive_emails', 'shared_folder_urls', 
                'specific_user_emails', 'batch_size',
                # Legacy parameters (deprecated but still supported)
                'folder_paths', 'include_shared', 'follow_shortcuts', 
                'only_org_public', 'continue_on_failure'
            ],
            'notion': [
                'root_page_id', 'recursive_search', 'follow_links', 
                'retrieve_blocks', 'include_people', 'include_databases'
            ],
            'slack': [
                'channels', 'channel_regex_enabled'
            ],
            'github': [
                'repo_owner', 'repositories', 'include_prs', 'include_issues',
                'include_code', 'include_releases', 'include_wikis'
            ],
            'confluence': [
                'is_cloud', 'wiki_base', 'space', 'page_id', 'index_recursively', 'cql_query'
            ],
            'web': [
                'base_url', 'web_connector_type', 'scroll_before_scraping',
                'recurse_depth', 'sitemap_url'
            ]
        }
        
        # First, remove common frontend form control parameters that should never be passed to connectors
        frontend_only_params = {
            'indexing_scope', 'everything', 'specific_folders',  # Tab structure parameters
            'tabs', 'fields', 'sections',  # Form structure parameters
            'file_types', 'folder_ids',  # Legacy/unsupported parameters
            'submitEndpoint', 'oauthSupported', 'oauthConfig',  # Form config parameters
            'indexing_start' # Universal indexing start field
        }
        
        # Remove frontend-only parameters
        cleaned_config = {
            key: value for key, value in connector_specific_config.items() 
            if key not in frontend_only_params
        }
        
        # Log if any frontend parameters were filtered out
        frontend_filtered = set(connector_specific_config.keys()) - set(cleaned_config.keys())
        if frontend_filtered:
            logger.info(f"Removed frontend form parameters for {connector_id}: {frontend_filtered}")
        
        connector_specific_config = cleaned_config
        
        if connector_id in supported_connector_params:
            supported_params = supported_connector_params[connector_id]
            filtered_config = {
                key: value for key, value in connector_specific_config.items() 
                if key in supported_params
            }
            # Log if any parameters were filtered out
            filtered_out = set(connector_specific_config.keys()) - set(filtered_config.keys())
            if filtered_out:
                logger.warning(f"Filtered unsupported parameters for {connector_id}: {filtered_out}")
            connector_specific_config = filtered_config
        
        # Create the connector payload
        connector_payload = {
            "name": name,
            "source": connector_id,
            "input_type": "poll" if connector_id not in load_connectors else "load_state",
            "access_type": "private",  # Required field for Smart Drive connectors
            "connector_specific_config": connector_specific_config,
            "refresh_freq": 3600,  # 1 hour
            "prune_freq": 86400,   # 1 day
            "indexing_start": indexing_start if indexing_start else None
        }
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        # Create the connector using httpx
        async with httpx.AsyncClient(timeout=30.0) as client:
            connector_url = ensure_https_url("/api/manage/admin/connector")
            
            connector_response = await client.post(
                connector_url,
                headers=auth_headers,
                json=connector_payload
            )
            
            if not connector_response.is_success:
                logger.error(f"Failed to create connector: {connector_response.text}")
                raise HTTPException(
                    status_code=connector_response.status_code,
                    detail=f"Failed to create connector: {connector_response.text}"
                )
            
            connector_result = connector_response.json()
            connector_id = connector_result.get('id')
            
            if not connector_id:
                raise HTTPException(status_code=500, detail="Failed to get connector ID")
            
            # Use the existing credential instead of creating a new one
            # Verify the credential exists and is accessible
            credential_response = await client.get(
                ensure_https_url(f"/api/manage/credential/{credential_id}"),
                headers=auth_headers
            )
            
            if not credential_response.is_success:
                logger.error(f"Failed to access credential: {credential_response.text}")
                # If credential access fails, try to delete the connector
                try:
                    await client.delete(
                        ensure_https_url(f"/api/manage/admin/connector/{connector_id}"),
                        headers=auth_headers
                    )
                except:
                    pass
                
                raise HTTPException(
                    status_code=credential_response.status_code,
                    detail=f"Failed to access credential: {credential_response.text}"
                )
            
            credential_result = credential_response.json()
            
            # Link the credential to the connector using Onyx's linkCredential approach
            auto_sync_options = {
                "enabled": True,
                "frequency": 3600
            }
            
            # Add Smart Drive header for credential linking
            linking_headers = auth_headers.copy()
            linking_headers['x-smart-drive-credential'] = 'true'
            
            # Sanitize name to prevent JSON issues
            safe_name = str(name).replace('"', '\\"').replace("'", "\\'") if name else f'Smart Drive {connector_id}'
            
            # Create payload with sanitized data
            cc_pair_payload = {
                "name": safe_name,
                "access_type": "private",
                "groups": [],  # Must be an empty list, not None
                "auto_sync_options": auto_sync_options
            }
            
            # Log the payload for debugging (without sensitive data)
            logger.info(f"Creating connector-credential pair with payload: {cc_pair_payload}")
            
            cc_pair_response = await client.put(
                ensure_https_url(f"/api/manage/connector/{connector_id}/credential/{credential_id}"),
                headers=linking_headers,
                json=cc_pair_payload
            )
            
            if not cc_pair_response.is_success:
                logger.error(f"Failed to create connector-credential pair: {cc_pair_response.text}")
                # If CC pair creation fails, try to delete both connector and credential
                try:
                    await client.delete(
                        ensure_https_url(f"/api/manage/admin/connector/{connector_id}"),
                        headers=auth_headers
                    )
                    await client.delete(
                        ensure_https_url(f"/api/manage/credential/{credential_id}"),
                        headers=auth_headers
                    )
                except:
                    pass
                
                raise HTTPException(
                    status_code=cc_pair_response.status_code,
                    detail=f"Failed to create connector-credential pair: {cc_pair_response.text}"
                )
            
            cc_pair_result = cc_pair_response.json()
            
            return {
                "success": True,
                "message": "Connector created successfully",
                "connector": connector_result,
                "credential": credential_result,
                "cc_pair": cc_pair_result
            }
            
    except httpx.RequestError as e:
        logger.error(f"Network error creating connector: {e}")
        raise HTTPException(status_code=500, detail="Network error occurred")
    except Exception as e:
        logger.error(f"Error creating connector: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Legacy endpoint stub (kept for backwards compatibility)
@app.get("/api/custom/smartdrive/connectors/")
async def list_user_connectors(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use Onyx's native connector system instead"""
    return {
        "message": "This endpoint is deprecated. Use Onyx's native connector system with AccessType.PRIVATE",
        "redirect_url": "/admin/connectors/",
        "api_endpoint": "/api/manage/admin/connector",
        "connectors": []
    }

@app.post("/api/custom/smartdrive/connectors/")
async def create_user_connector(
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connectors/{source}?access_type=private instead"""
    payload = await request.json()
    source = payload.get('source', 'unknown')
    
    raise HTTPException(
        status_code=410,  # Gone
        detail={
            "message": "This endpoint is deprecated. Create connectors using Onyx's native system.",
            "redirect_url": f"/admin/connectors/{source}?access_type=private",
            "instructions": "Visit the connector creation page to set up your private connector with OAuth support."
        }
    )

@app.put("/api/custom/smartdrive/connectors/{connector_id}")
async def update_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connector/{connector_id} instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Manage connectors using Onyx's native system.",
            "redirect_url": f"/admin/connector/{connector_id}",
            "instructions": "Visit the connector management page to update your connector configuration."
        }
    )

@app.delete("/api/custom/smartdrive/connectors/{connector_id}")
async def delete_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /admin/connector/{connector_id} instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Manage connectors using Onyx's native system.",
            "redirect_url": f"/admin/connector/{connector_id}",
            "instructions": "Visit the connector management page to delete your connector."
        }
    )

@app.post("/api/custom/smartdrive/connectors/{connector_id}/sync")
async def sync_user_connector(
    connector_id: int,
    request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """DEPRECATED: Use /api/manage/admin/connector/{connector_id}/index instead"""
    raise HTTPException(
        status_code=410,
        detail={
            "message": "This endpoint is deprecated. Sync connectors using Onyx's native API.",
            "api_endpoint": f"/api/manage/admin/connector/{connector_id}/index",
            "instructions": "Use Onyx's connector sync API or the connector management UI."
        }
    )


# Credential proxy endpoints for non-admin users
@app.get("/api/custom/credentials/{source_type}")
async def get_credentials_for_source(source_type: str, request: Request):
    """
    Proxy endpoint to fetch credentials for a specific source type.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Accept': 'application/json',
            'x-smart-drive-credential': 'true'  # Smart Drive header to bypass admin checks
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's credential endpoint
            credentials_url = ensure_https_url(f"/api/manage/admin/similar-credentials/{source_type}")
            
            response = await client.get(
                credentials_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            elif response.status_code == 403:
                # User doesn't have admin access, return empty list
                # This allows the frontend to show "No existing credentials" 
                # and proceed with credential creation
                logger.info(f"Non-admin user accessing credentials for {source_type}, returning empty list")
                return []
            else:
                logger.error(f"Failed to fetch credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to fetch credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error fetching credentials: {str(e)}")
        # For any error, return empty list to allow credential creation
        return []


@app.post("/api/custom/credentials")
async def create_credential(request: Request):
    """
    Proxy endpoint to create credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        # Get credential data from request
        credential_data = await request.json()
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's credential creation endpoint
            credentials_url = ensure_https_url("/api/manage/credential")
            
            response = await client.post(
                credentials_url,
                headers=auth_headers,
                json=credential_data
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to create credential: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to create credential: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error creating credential: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error creating credential: {str(e)}")

@app.get("/api/custom/connector/google-drive/authorize/{credential_id}")
async def google_drive_authorize(credential_id: str, request: Request):
    """
    Proxy endpoint to get Google Drive authorization URL.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Google Drive authorization endpoint
            auth_url = ensure_https_url(f"/api/manage/connector/google-drive/authorize/{credential_id}")
            
            response = await client.get(
                auth_url,
                headers=auth_headers
            )
            
            if response.is_success:
                # Create response with JSON data
                from fastapi.responses import JSONResponse
                json_response = JSONResponse(content=response.json())
                
                # Forward all Set-Cookie headers from the original response
                for cookie_header in response.headers.get_list("set-cookie"):
                    # Parse the cookie header to extract name, value, and attributes
                    cookie_parts = cookie_header.split(";")
                    cookie_name_value = cookie_parts[0].strip()
                    if "=" in cookie_name_value:
                        cookie_name, cookie_value = cookie_name_value.split("=", 1)
                        
                        # Extract cookie attributes
                        cookie_attrs = {}
                        for part in cookie_parts[1:]:
                            part = part.strip()
                            if "=" in part:
                                attr_name, attr_value = part.split("=", 1)
                                cookie_attrs[attr_name.lower()] = attr_value
                            else:
                                cookie_attrs[part.lower()] = True
                        
                        # Set the cookie with proper attributes
                        json_response.set_cookie(
                            key=cookie_name,
                            value=cookie_value,
                            httponly=cookie_attrs.get("httponly", False),
                            secure=cookie_attrs.get("secure", False),
                            samesite=cookie_attrs.get("samesite", "lax"),
                            max_age=int(cookie_attrs.get("max-age", 0)) if cookie_attrs.get("max-age") else None
                        )
                
                return json_response
            else:
                logger.error(f"Failed to get Google Drive authorization URL: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get Google Drive authorization URL: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error getting Google Drive authorization URL: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting Google Drive authorization URL: {str(e)}")


@app.put("/api/custom/connector/google-drive/app-credential")
async def google_drive_put_app_credential(request: Request):
    """
    Proxy endpoint to save Google Drive app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get request body
        body = await request.body()
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Google Drive app credential endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/google-drive/app-credential")
            
            response = await client.put(
                app_cred_url,
                headers=auth_headers,
                content=body
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to save Google Drive app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to save Google Drive app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error saving Google Drive app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error saving Google Drive app credentials: {str(e)}")

@app.get("/api/custom/connector/google-drive/app-credential")
async def google_drive_get_app_credential(request: Request):
    """
    Proxy endpoint to get Google Drive app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Google Drive app credential GET endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/google-drive/app-credential")
            
            response = await client.get(
                app_cred_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to get Google Drive app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get Google Drive app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error getting Google Drive app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting Google Drive app credentials: {str(e)}")

@app.delete("/api/custom/connector/google-drive/app-credential")
async def google_drive_delete_app_credential(request: Request):
    """
    Proxy endpoint to delete Google Drive app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Google Drive app credential DELETE endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/google-drive/app-credential")
            
            response = await client.delete(
                app_cred_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to delete Google Drive app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to delete Google Drive app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error deleting Google Drive app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting Google Drive app credentials: {str(e)}")

@app.get("/api/custom/connector/gmail/authorize/{credential_id}")
async def gmail_authorize(credential_id: str, request: Request):
    """
    Proxy endpoint to get Gmail authorization URL.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get authentication from cookies
        session_cookie = request.cookies.get(ONYX_SESSION_COOKIE_NAME)
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Gmail authorization endpoint
            auth_url = ensure_https_url(f"/api/manage/connector/gmail/authorize/{credential_id}")
            
            response = await client.get(
                auth_url,
                headers=auth_headers
            )
            
            if response.is_success:
                # Create response with JSON data
                from fastapi.responses import JSONResponse
                json_response = JSONResponse(content=response.json())
                
                # Forward all Set-Cookie headers from the original response
                for cookie_header in response.headers.get_list("set-cookie"):
                    # Parse the cookie header to extract name, value, and attributes
                    cookie_parts = cookie_header.split(";")
                    cookie_name_value = cookie_parts[0].strip()
                    if "=" in cookie_name_value:
                        cookie_name, cookie_value = cookie_name_value.split("=", 1)
                        
                        # Extract cookie attributes
                        cookie_attrs = {}
                        for part in cookie_parts[1:]:
                            part = part.strip()
                            if "=" in part:
                                attr_name, attr_value = part.split("=", 1)
                                cookie_attrs[attr_name.lower()] = attr_value
                            else:
                                cookie_attrs[part.lower()] = True
                        
                        # Set the cookie with proper attributes
                        json_response.set_cookie(
                            key=cookie_name,
                            value=cookie_value,
                            httponly=cookie_attrs.get("httponly", False),
                            secure=cookie_attrs.get("secure", False),
                            samesite=cookie_attrs.get("samesite", "lax"),
                            max_age=int(cookie_attrs.get("max-age", 0)) if cookie_attrs.get("max-age") else None
                        )
                
                return json_response
            else:
                logger.error(f"Failed to get Gmail authorization URL: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get Gmail authorization URL: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error getting Gmail authorization URL: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting Gmail authorization URL: {str(e)}")

@app.put("/api/custom/connector/gmail/app-credential")
async def gmail_put_app_credential(request: Request):
    """
    Proxy endpoint to save Gmail app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        # Get request body
        body = await request.body()
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Gmail app credential endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/gmail/app-credential")
            
            response = await client.put(
                app_cred_url,
                headers=auth_headers,
                content=body
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to save Gmail app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to save Gmail app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error saving Gmail app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error saving Gmail app credentials: {str(e)}")

@app.get("/api/custom/connector/gmail/app-credential")
async def gmail_get_app_credential(request: Request):
    """
    Proxy endpoint to get Gmail app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Gmail app credential GET endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/gmail/app-credential")
            
            response = await client.get(
                app_cred_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to get Gmail app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get Gmail app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error getting Gmail app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting Gmail app credentials: {str(e)}")


@app.delete("/api/custom/connector/gmail/app-credential")
async def gmail_delete_app_credential(request: Request):
    """
    Proxy endpoint to delete Gmail app credentials.
    Bypasses admin requirement by using user's session.
    """
    try:
        # Get current host from request
        host = request.headers.get("host", "localhost:8000")
        protocol = "https" if "localhost" not in host else "http"
        main_app_url = f"{protocol}://{host}"
        
        auth_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # Forward all cookies to maintain session state
        if request.cookies:
            cookie_header = '; '.join([f'{name}={value}' for name, value in request.cookies.items()])
            auth_headers['Cookie'] = cookie_header
        
        # Helper function to ensure HTTPS for production domains
        def ensure_https_url(path: str) -> str:
            url = f"{main_app_url}{path}"
            if 'localhost' not in main_app_url and '127.0.0.1' not in main_app_url and url.startswith('http://'):
                url = url.replace('http://', 'https://')
            return url
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Call Onyx's Gmail app credential DELETE endpoint
            app_cred_url = ensure_https_url("/api/manage/admin/connector/gmail/app-credential")
            
            response = await client.delete(
                app_cred_url,
                headers=auth_headers
            )
            
            if response.is_success:
                return response.json()
            else:
                logger.error(f"Failed to delete Gmail app credentials: {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to delete Gmail app credentials: {response.text}"
                )
                
    except Exception as e:
        logger.error(f"Error deleting Gmail app credentials: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting Gmail app credentials: {str(e)}")

# --- Offers API Endpoints ---

@app.get("/api/custom/offers", response_model=List[OfferResponse])
async def get_offers(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id),
    company_id: Optional[int] = Query(None, description="Filter by company ID"),
    status: Optional[str] = Query(None, description="Filter by status"),
    search: Optional[str] = Query(None, description="Search in offer name or manager")
):
    """Get all offers for the current user with optional filtering"""
    try:
        
        # Build the query with optional filters
        query = """
            SELECT o.*, pf.name as company_name
            FROM offers o
            LEFT JOIN project_folders pf ON o.company_id = pf.id
            WHERE o.onyx_user_id = $1
        """
        params = [onyx_user_id]
        param_count = 1
        
        if company_id is not None:
            param_count += 1
            query += f" AND o.company_id = ${param_count}"
            params.append(company_id)
        
        if status is not None:
            param_count += 1
            query += f" AND o.status = ${param_count}"
            params.append(status)
        
        if search is not None:
            param_count += 1
            query += f" AND (o.offer_name ILIKE ${param_count} OR o.manager ILIKE ${param_count})"
            params.append(f"%{search}%")
        
        query += " ORDER BY o.created_on DESC"
        
        async with DB_POOL.acquire() as connection:
            rows = await connection.fetch(query, *params)
            
        offers = []
        for row in rows:
            offers.append(OfferResponse(
                id=row['id'],
                onyx_user_id=row['onyx_user_id'],
                company_id=row['company_id'],
                offer_name=row['offer_name'],
                created_on=row['created_on'],
                manager=row['manager'],
                status=row['status'],
                total_hours=row['total_hours'],
                link=row['link'],
                created_at=row['created_at'],
                updated_at=row['updated_at'],
                company_name=row['company_name']
            ))
        
        return offers
        
    except Exception as e:
        logger.error(f"Error fetching offers: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offers")

@app.get("/api/custom/offers/counts")
async def get_offer_counts(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Return a mapping of company_id to number of offers for the current user."""
    try:
        async with DB_POOL.acquire() as connection:
            rows = await connection.fetch(
                """
                SELECT company_id, COUNT(*) AS cnt
                FROM offers
                WHERE onyx_user_id = $1
                GROUP BY company_id
                """,
                onyx_user_id,
            )
        result = {row["company_id"]: int(row["cnt"]) for row in rows}
        return result
    except Exception as e:
        logger.error(f"Error fetching offer counts: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offer counts")

@app.post("/api/custom/offers", response_model=OfferResponse)
async def create_offer(
    offer_data: OfferCreate,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Create a new offer"""
    try:
        
        # Validate that the company exists and belongs to the user
        async with DB_POOL.acquire() as connection:
            company_exists = await connection.fetchval(
                "SELECT id FROM project_folders WHERE id = $1 AND onyx_user_id = $2",
                offer_data.company_id, onyx_user_id
            )
            
            if not company_exists:
                raise HTTPException(status_code=404, detail="Company not found")
            
            # First insert the offer without link to get the ID
            row = await connection.fetchrow("""
                INSERT INTO offers (onyx_user_id, company_id, offer_name, manager, status, total_hours, link)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                RETURNING *, (SELECT name FROM project_folders WHERE id = $2) as company_name
            """, onyx_user_id, offer_data.company_id, offer_data.offer_name, 
                 offer_data.manager, offer_data.status, offer_data.total_hours, None)
            
            # Generate auto link based on the offer ID
            offer_id = row['id']
            # Get the base URL from the request
            base_url = str(request.base_url).rstrip('/')
            
            # Log the base URL for debugging
            logger.info(f"Original base URL: {base_url}")
            
            # Remove /api/custom-projects-backend from the base URL if present
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            auto_link = f"{base_url}/custom-projects-ui/offer/{offer_id}"
            
            # Log the generated link for debugging
            logger.info(f"Generated auto link for offer {offer_id}: {auto_link}")
            
            # Update the offer with the auto-generated link
            update_result = await connection.execute(
                "UPDATE offers SET link = $1 WHERE id = $2",
                auto_link, offer_id
            )
            
            # Log the update result
            logger.info(f"Update result for offer {offer_id}: {update_result}")
            
            # Update the row dict with the new link
            row_dict = dict(row)
            row_dict['link'] = auto_link
        
        return OfferResponse(
            id=row_dict['id'],
            onyx_user_id=row_dict['onyx_user_id'],
            company_id=row_dict['company_id'],
            offer_name=row_dict['offer_name'],
            created_on=row_dict['created_on'],
            manager=row_dict['manager'],
            status=row_dict['status'],
            total_hours=row_dict['total_hours'],
            link=row_dict['link'],
            created_at=row_dict['created_at'],
            updated_at=row_dict['updated_at'],
            company_name=row_dict['company_name']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create offer")

@app.put("/api/custom/offers/{offer_id}", response_model=OfferResponse)
async def update_offer(
    offer_id: int,
    offer_data: OfferUpdate,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Update an existing offer"""
    try:
        
        # Build dynamic update query
        update_fields = []
        params = [onyx_user_id, offer_id]
        param_count = 2
        
        if offer_data.company_id is not None:
            param_count += 1
            update_fields.append(f"company_id = ${param_count}")
            params.append(offer_data.company_id)
        
        if offer_data.offer_name is not None:
            param_count += 1
            update_fields.append(f"offer_name = ${param_count}")
            params.append(offer_data.offer_name)
        
        if offer_data.manager is not None:
            param_count += 1
            update_fields.append(f"manager = ${param_count}")
            params.append(offer_data.manager)
        
        if offer_data.status is not None:
            param_count += 1
            update_fields.append(f"status = ${param_count}")
            params.append(offer_data.status)
        
        if offer_data.total_hours is not None:
            param_count += 1
            update_fields.append(f"total_hours = ${param_count}")
            params.append(offer_data.total_hours)
        
        if offer_data.created_on is not None:
            param_count += 1
            update_fields.append(f"created_on = ${param_count}")
            params.append(offer_data.created_on)
        
        # Note: link is auto-generated and not editable
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")
        
        # Add updated_at timestamp
        param_count += 1
        update_fields.append(f"updated_at = CURRENT_TIMESTAMP")
        
        query = f"""
            UPDATE offers 
            SET {', '.join(update_fields)}
            WHERE id = $2 AND onyx_user_id = $1
            RETURNING *, (SELECT name FROM project_folders WHERE id = company_id) as company_name
        """
        
        async with DB_POOL.acquire() as connection:
            row = await connection.fetchrow(query, *params)
            
            if not row:
                raise HTTPException(status_code=404, detail="Offer not found")
        
        return OfferResponse(
            id=row['id'],
            onyx_user_id=row['onyx_user_id'],
            company_id=row['company_id'],
            offer_name=row['offer_name'],
            created_on=row['created_on'],
            manager=row['manager'],
            status=row['status'],
            total_hours=row['total_hours'],
            link=row['link'],
            created_at=row['created_at'],
            updated_at=row['updated_at'],
            company_name=row['company_name']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to update offer")

@app.delete("/api/custom/offers/{offer_id}")
async def delete_offer(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Delete an offer"""
    try:
        
        async with DB_POOL.acquire() as connection:
            result = await connection.execute(
                "DELETE FROM offers WHERE id = $1 AND onyx_user_id = $2",
                offer_id, onyx_user_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(status_code=404, detail="Offer not found")
        
        return {"message": "Offer deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting offer: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to delete offer")

@app.get("/api/custom/offers/{offer_id}/details")
async def get_offer_details(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Get detailed offer information for offer detail page"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offer basic info
            offer_row = await connection.fetchrow("""
                SELECT o.*, pf.name as company_name
                FROM offers o
                JOIN project_folders pf ON o.company_id = pf.id
                WHERE o.id = $1 AND o.onyx_user_id = $2
            """, offer_id, onyx_user_id)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Offer not found")
            
            offer = dict(offer_row)
            
            # Get projects in the company folder to build course modules
            projects_rows = await connection.fetch("""
                SELECT p.id, p.project_name, p.microproduct_content
                FROM projects p
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                AND p.microproduct_content IS NOT NULL
                AND p.microproduct_content->>'sections' IS NOT NULL
            """, offer['company_id'], onyx_user_id)
            
            course_modules = []
            total_lessons = 0
            total_learning_duration = 0
            total_production_time = 0
            
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    project_lessons = 0
                    project_modules = 0
                    project_completion_time = 0
                    project_hours = 0
                    
                    for section in content['sections']:
                        if section.get('lessons'):
                            project_modules += 1  # Count modules (sections)
                            for lesson in section['lessons']:
                                project_lessons += 1
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                    project_completion_time += completion_minutes
                                except (ValueError, AttributeError):
                                    project_completion_time += 5
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                project_hours += lesson_hours
                    
                    if project_lessons > 0:
                        learning_duration_hours = round(project_completion_time / 60.0, 1)
                        course_modules.append({
                            'title': project_dict['project_name'],
                            'modules': project_modules,
                            'lessons': project_lessons,
                            'learningDuration': f"{learning_duration_hours}h",
                            'productionTime': f"{project_hours}h"
                        })
                        
                        total_lessons += project_lessons
                        total_learning_duration += learning_duration_hours
                        total_production_time += project_hours
            
            # Calculate quality-level-specific totals by examining each lesson's quality tier
            quality_tier_totals = {
                'basic': {'learning_duration': 0, 'production_time': 0},
                'interactive': {'learning_duration': 0, 'production_time': 0},
                'advanced': {'learning_duration': 0, 'production_time': 0},
                'immersive': {'learning_duration': 0, 'production_time': 0}
            }
            
            # Re-process projects to calculate quality-specific totals
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    for section in content['sections']:
                        if section.get('lessons'):
                            for lesson in section['lessons']:
                                # Determine effective quality tier for this lesson
                                effective_quality_tier = (
                                    lesson.get('quality_tier') or 
                                    section.get('quality_tier') or 
                                    content.get('quality_tier') or
                                    'interactive'  # Default fallback
                                ).lower()
                                
                                # Map alternative names to standard tiers
                                tier_mapping = {
                                    'basic': 'basic',
                                    'interactive': 'interactive',
                                    'advanced': 'advanced',
                                    'immersive': 'immersive',
                                    'medium': 'interactive',  # Map medium to interactive
                                    'premium': 'advanced',    # Map premium to advanced
                                }
                                
                                standard_tier = tier_mapping.get(effective_quality_tier, 'interactive')
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                except (ValueError, AttributeError):
                                    completion_minutes = 5
                                
                                learning_duration_hours = completion_minutes / 60.0
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                
                                # Add to the appropriate quality tier totals
                                quality_tier_totals[standard_tier]['learning_duration'] += learning_duration_hours
                                quality_tier_totals[standard_tier]['production_time'] += lesson_hours
            
            # Generate quality levels data using quality-specific totals
            quality_levels = [
                {
                    'level': 'Level 1 - Basic',
                    'learningDuration': f"{round(quality_tier_totals['basic']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['basic']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 2 - Interactive',
                    'learningDuration': f"{round(quality_tier_totals['interactive']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['interactive']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 3 - Advanced',
                    'learningDuration': f"{round(quality_tier_totals['advanced']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['advanced']['production_time'], 1)}h"
                },
                {
                    'level': 'Level 4 - Immersive',
                    'learningDuration': f"{round(quality_tier_totals['immersive']['learning_duration'], 1)}h",
                    'productionTime': f"{round(quality_tier_totals['immersive']['production_time'], 1)}h"
                }
            ]
            
            return {
                'offer': offer,
                'courseModules': course_modules,
                'qualityLevels': quality_levels
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching offer details: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch offer details")

@app.post("/api/custom/offers/migrate-links")
async def migrate_offer_links(
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Update existing offers with auto-generated links"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offers without links or with empty links
            offers = await connection.fetch("""
                SELECT id FROM offers 
                WHERE onyx_user_id = $1 AND (link IS NULL OR link = '')
            """, onyx_user_id)
            
            updated_count = 0
            base_url = str(request.base_url).rstrip('/')
            
            # Log the base URL for debugging
            logger.info(f"Migration: Original base URL: {base_url}")
            
            # Remove /api/custom-projects-backend from the base URL if present
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            logger.info(f"Migration: Found {len(offers)} offers to update")
            
            for offer in offers:
                offer_id = offer['id']
                auto_link = f"{base_url}/custom-projects-ui/offer/{offer_id}"
                
                logger.info(f"Migration: Updating offer {offer_id} with link: {auto_link}")
                
                await connection.execute(
                    "UPDATE offers SET link = $1 WHERE id = $2",
                    auto_link, offer_id
                )
                updated_count += 1
            
            logger.info(f"Migration: Successfully updated {updated_count} offers")
            return {"message": f"Updated {updated_count} offers with auto-generated links"}
            
    except Exception as e:
        logger.error(f"Error migrating offer links: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to migrate offer links")

@app.post("/api/custom/offers/{offer_id}/generate-share-link")
async def generate_offer_share_link(
    offer_id: int,
    request: Request,
    onyx_user_id: str = Depends(get_current_onyx_user_id)
):
    """Generate a shareable link for an offer that doesn't require authentication"""
    try:
        async with DB_POOL.acquire() as connection:
            # Verify offer exists and belongs to user
            offer_row = await connection.fetchrow("""
                SELECT id, share_token FROM offers 
                WHERE id = $1 AND onyx_user_id = $2
            """, offer_id, onyx_user_id)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Offer not found")
            
            # Generate or use existing share token
            share_token = offer_row['share_token']
            if not share_token:
                share_token = str(uuid.uuid4())
                await connection.execute(
                    "UPDATE offers SET share_token = $1 WHERE id = $2",
                    share_token, offer_id
                )
            
            # Build shareable URL
            base_url = str(request.base_url).rstrip('/')
            if base_url.endswith('/api/custom-projects-backend'):
                base_url = base_url[:-27]
            elif base_url.endswith('/api/custom'):
                base_url = base_url[:-11]
            
            share_url = f"{base_url}/custom-projects-ui/offer/shared/{share_token}"
            
            return {
                "share_token": share_token,
                "share_url": share_url
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating share link: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to generate share link")

@app.get("/api/custom/offers/shared/{share_token}/details")
async def get_shared_offer_details(share_token: str):
    """Get offer details using share token - no authentication required"""
    try:
        async with DB_POOL.acquire() as connection:
            # Get offer basic info using share token
            offer_row = await connection.fetchrow("""
                SELECT o.*, pf.name as company_name
                FROM offers o
                JOIN project_folders pf ON o.company_id = pf.id
                WHERE o.share_token = $1
            """, share_token)
            
            if not offer_row:
                raise HTTPException(status_code=404, detail="Shared offer not found")
            
            offer = dict(offer_row)
            
            # Get projects in the company folder to build course modules
            projects_rows = await connection.fetch("""
                SELECT p.id, p.project_name, p.microproduct_content
                FROM projects p
                WHERE p.folder_id = $1 AND p.onyx_user_id = $2
                AND p.microproduct_content IS NOT NULL
                AND p.microproduct_content->>'sections' IS NOT NULL
            """, offer['company_id'], offer['onyx_user_id'])
            
            course_modules = []
            total_lessons = 0
            total_learning_duration = 0
            total_production_time = 0
            
            for project_row in projects_rows:
                project_dict = dict(project_row)
                content = project_dict.get('microproduct_content', {})
                
                if content and content.get('sections'):
                    project_lessons = 0
                    project_modules = 0
                    project_completion_time = 0
                    project_hours = 0
                    
                    for section in content['sections']:
                        if section.get('lessons'):
                            project_modules += 1  # Count modules (sections)
                            for lesson in section['lessons']:
                                project_lessons += 1
                                
                                # Parse completion time
                                completion_time_str = lesson.get('completionTime', '5m')
                                try:
                                    completion_minutes = int(completion_time_str.replace('m', ''))
                                    project_completion_time += completion_minutes
                                except (ValueError, AttributeError):
                                    project_completion_time += 5
                                
                                # Get lesson hours (creation time)
                                lesson_hours = lesson.get('hours', 0)
                                project_hours += lesson_hours
                    
                    if project_lessons > 0:
                        learning_duration_hours = round(project_completion_time / 60.0, 1)
                        course_modules.append({
                            'title': project_dict['project_name'],
                            'modules': project_modules,
                            'lessons': project_lessons,
                            'learningDuration': f"{learning_duration_hours}h",
                            'productionTime': f"{project_hours}h"
                        })
                        
                        total_lessons += project_lessons
                        total_learning_duration += learning_duration_hours
                        total_production_time += project_hours
            
            # Generate quality levels data - using actual totals from all courses
            quality_levels = [
                {
                    'level': 'Level 1 - Basic',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{total_production_time}h"
                },
                {
                    'level': 'Level 2 - Interactive',
                    'learningDuration': f"{total_learning_duration}h", 
                    'productionTime': f"{int(total_production_time * 1.5)}h"  # 50% more for interactive
                },
                {
                    'level': 'Level 3 - Advanced',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{int(total_production_time * 2)}h"  # 2x for advanced
                },
                {
                    'level': 'Level 4 - Immersive',
                    'learningDuration': f"{total_learning_duration}h",
                    'productionTime': f"{int(total_production_time * 3)}h"  # 3x for immersive
                }
            ]
            
            # Remove sensitive information for shared view
            offer_public = {
                'id': offer['id'],
                'offer_name': offer['offer_name'],
                'company_name': offer['company_name'],
                'manager': offer['manager'],
                'created_on': offer['created_on'],
                'status': offer['status'],
                'total_hours': offer['total_hours']
            }
            
            return {
                'offer': offer_public,
                'courseModules': course_modules,
                'qualityLevels': quality_levels
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching shared offer details: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to fetch shared offer details")

# ============================================================================
# WORKSPACE MANAGEMENT API ENDPOINTS
# ============================================================================

# Dependency to get current user ID (placeholder - integrate with your auth system)
async def get_current_user_id_for_workspaces(request: Request) -> str:
    """Get current user ID for workspace operations - uses same logic as projects"""
    try:
        # Use the same user identification logic as projects
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # For workspace operations, we'll use email as the primary identifier
        # since workspace members are stored with emails
        return user_email if user_email else user_uuid
    except Exception as e:
        logger.error(f"Failed to get user ID for workspace operations: {e}")
        # Fallback for development
        return "current_user_123"

# Workspace Management Endpoints

@app.post("/api/custom/workspaces", response_model=Workspace)
async def create_workspace(workspace_data: WorkspaceCreate, request: Request):
    """Create a new workspace."""
    try:
        # Get user identifiers (same logic as get_workspaces endpoint)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # Use email for workspace operations since members are stored with emails
        current_user_id = user_email if user_email else user_uuid
        
        logger.info(f"🔍 [WORKSPACE CREATE] Creating workspace '{workspace_data.name}' for user: {current_user_id} (UUID: {user_uuid})")
        workspace = await WorkspaceService.create_workspace(workspace_data, current_user_id)
        logger.info(f"✅ [WORKSPACE CREATE] Successfully created workspace {workspace.id} for user {current_user_id}")
        return workspace
    except ValueError as e:
        logger.error(f"❌ [WORKSPACE CREATE] Validation error: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"❌ [WORKSPACE CREATE] Failed to create workspace: {e}")
        raise HTTPException(status_code=500, detail="Failed to create workspace")

@app.get("/api/custom/workspaces", response_model=List[Workspace])
async def get_workspaces(request: Request):
    """Get all workspaces where the current user is a member."""
    try:
        # Get user identifiers (same logic as projects endpoint)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        # Use email for workspace operations since members are stored with emails
        current_user_id = user_email if user_email else user_uuid
        
        logger.info(f"🔍 [WORKSPACE LIST] Getting workspaces for user: {current_user_id} (UUID: {user_uuid})")
        workspaces = await WorkspaceService.get_user_workspaces(current_user_id)
        logger.info(f"🔍 [WORKSPACE LIST] Found {len(workspaces)} workspaces for user {current_user_id}")
        
        return workspaces
    except Exception as e:
        logger.error(f"Failed to retrieve workspaces: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve workspaces")

@app.get("/api/custom/workspaces/{workspace_id}", response_model=Workspace)
async def get_workspace(workspace_id: int, request: Request):
    """Get a specific workspace by ID."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        workspace = await WorkspaceService.get_workspace(workspace_id)
        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace")

@app.get("/api/custom/workspaces/{workspace_id}/full", response_model=WorkspaceWithMembers)
async def get_workspace_with_members(workspace_id: int, request: Request):
    """Get a workspace with all its members and roles."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        workspace_data = await WorkspaceService.get_workspace_with_members(workspace_id)
        if not workspace_data:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace_data
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace data")

@app.put("/api/custom/workspaces/{workspace_id}", response_model=Workspace)
async def update_workspace(workspace_data: WorkspaceUpdate, workspace_id: int, request: Request):
    """Update a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        workspace = await WorkspaceService.update_workspace(workspace_id, workspace_data, current_user_id)
        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return workspace
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update workspace")

@app.delete("/api/custom/workspaces/{workspace_id}")
async def delete_workspace(workspace_id: int, request: Request):
    """Delete a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        success = await WorkspaceService.delete_workspace(workspace_id, current_user_id)
        if not success:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        return {"message": "Workspace deleted successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to delete workspace")

# Role Management Endpoints

@app.post("/api/custom/workspaces/{workspace_id}/roles", response_model=WorkspaceRole)
async def create_role(role_data: WorkspaceRoleCreate, workspace_id: int, request: Request):
    """Create a new custom role in a workspace."""
    try:
        # Ensure workspace_id matches path parameter
        role_data.workspace_id = workspace_id
        
        role = await RoleService.create_custom_role(role_data, "current_user_123")
        return role
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to create role")

@app.get("/api/custom/workspaces/{workspace_id}/roles", response_model=List[WorkspaceRole])
async def get_workspace_roles(workspace_id: int, request: Request):
    """Get all roles for a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        roles = await RoleService.get_workspace_roles(workspace_id)
        logger.info(f"✅ [WORKSPACE ROLES] Retrieved {len(roles)} roles for workspace {workspace_id}")
        return roles
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [WORKSPACE ROLES] Failed to get roles for workspace {workspace_id}: {e}")
        logger.error(f"❌ [WORKSPACE ROLES] Error type: {type(e).__name__}")
        import traceback
        logger.error(f"❌ [WORKSPACE ROLES] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve roles: {str(e)}")

@app.get("/api/custom/workspaces/{workspace_id}/roles/{role_id}", response_model=WorkspaceRole)
async def get_workspace_role(workspace_id: int, role_id: int, request: Request):
    """Get a specific role from a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        role = await RoleService.get_workspace_role(role_id, workspace_id)
        if not role:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return role
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve role")

@app.put("/api/custom/workspaces/{workspace_id}/roles/{role_id}", response_model=WorkspaceRole)
async def update_role(role_data: WorkspaceRoleUpdate, workspace_id: int, role_id: int, request: Request):
    """Update a custom role in a workspace."""
    try:
        role = await RoleService.update_custom_role(role_id, workspace_id, role_data, "current_user_123")
        if not role:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return role
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update role")

@app.delete("/api/custom/workspaces/{workspace_id}/roles/{role_id}")
async def delete_role(workspace_id: int, role_id: int):
    """Delete a custom role from a workspace."""
    try:
        success = await RoleService.delete_custom_role(role_id, workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Role not found")
        
        return {"message": "Role deleted successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to delete role")

# Member Management Endpoints

@app.post("/api/custom/workspaces/{workspace_id}/members", response_model=WorkspaceMember)
async def add_member(member_data: WorkspaceMemberCreate, workspace_id: int, request: Request):
    """Add a new member to a workspace."""
    try:
        # Ensure workspace_id matches path parameter
        member_data.workspace_id = workspace_id
        
        member = await WorkspaceService.add_member(workspace_id, member_data, "current_user_123")
        return member
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to add member")

@app.get("/api/custom/workspaces/{workspace_id}/members", response_model=List[WorkspaceMember])
async def get_workspace_members(workspace_id: int, request: Request):
    """Get all members of a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        logger.info(f"🔍 [WORKSPACE ROLES] Getting roles for workspace {workspace_id}, user: {current_user_id}")
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, current_user_id)
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        members = await WorkspaceService.get_workspace_members(workspace_id)
        logger.info(f"✅ [WORKSPACE MEMBERS] Retrieved {len(members)} members for workspace {workspace_id}")
        return members
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ [WORKSPACE MEMBERS] Failed to get members for workspace {workspace_id}: {e}")
        logger.error(f"❌ [WORKSPACE MEMBERS] Error type: {type(e).__name__}")
        import traceback
        logger.error(f"❌ [WORKSPACE MEMBERS] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve members: {str(e)}")

@app.put("/api/custom/workspaces/{workspace_id}/members/{user_id}", response_model=WorkspaceMember)
async def update_member(member_data: WorkspaceMemberUpdate, workspace_id: int, user_id: str):
    """Update a workspace member."""
    try:
        member = await WorkspaceService.update_member(workspace_id, user_id, member_data, "current_user_123")
        if not member:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return member
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to update member")

@app.delete("/api/custom/workspaces/{workspace_id}/members/{user_id}")
async def remove_member(workspace_id: int, user_id: str):
    """Remove a member from a workspace."""
    try:
        success = await WorkspaceService.remove_member(workspace_id, user_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return {"message": "Member removed successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to remove member")

@app.post("/api/custom/workspaces/{workspace_id}/leave")
async def leave_workspace(workspace_id: int, request: Request):
    """Leave a workspace."""
    try:
        # Get user identifiers
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        current_user_id = user_email if user_email else user_uuid
        success = await WorkspaceService.leave_workspace(workspace_id, current_user_id)
        if not success:
            raise HTTPException(status_code=404, detail="Member not found")
        
        return {"message": "Successfully left workspace"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to leave workspace")

# Product Access Control Endpoints

@app.post("/api/custom/products/{product_id}/access", response_model=ProductAccess)
async def grant_product_access(
    access_data: ProductAccessCreate, 
    product_id: int,
    request: Request
):
    """Grant access to a product for a workspace, role, or individual."""
    try:
        logger.info(f"🔍 [PRODUCT ACCESS DEBUG] Grant access request for product {product_id}")
        # Get user identifiers (both UUID and email)
        user_uuid, user_email = await get_user_identifiers_for_workspace(request)
        logger.info(f"🔍 [PRODUCT ACCESS] Grant access request - User: {user_uuid} (email: {user_email})")
        logger.info(f"   - Product ID: {product_id}, Workspace ID: {access_data.workspace_id}")
        logger.info(f"   - Access type: {access_data.access_type}, Target ID: {access_data.target_id}")
        logger.info(f"🔍 [PRODUCT ACCESS] Grant access request - User: {user_uuid} (email: {user_email})")
        logger.info(f"   - Product ID: {product_id}")
        logger.info(f"   - Workspace ID: {access_data.workspace_id}")
        logger.info(f"   - Access type: {access_data.access_type}")
        logger.info(f"   - Target ID: {access_data.target_id}")
        
        # Ensure product_id matches path parameter
        access_data.product_id = product_id
        
        # Check if user is a member of the workspace (use email for membership check)
        member = await WorkspaceService.get_workspace_member(access_data.workspace_id, user_email)
        if not member:
            logger.error(f"❌ [PRODUCT ACCESS] User {user_email} is not a member of workspace {access_data.workspace_id}")
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        logger.info(f"✅ [PRODUCT ACCESS] User {user_email} is a member of workspace {access_data.workspace_id}")
        
        access = await ProductAccessService.grant_access(access_data, user_email)
        return access
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to grant product access")

@app.get("/api/custom/products/{product_id}/access", response_model=List[ProductAccess])
async def get_product_access_list(product_id: int):
    """Get all access records for a specific product."""
    try:
        # TODO: Check if user has permission to view product access
        # This might require checking if the user owns the product or has admin access
        
        access_list = await ProductAccessService.get_product_access_list(product_id)
        return access_list
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve product access list")

@app.delete("/api/custom/products/{product_id}/access/{access_id}")
async def revoke_product_access(product_id: int, access_id: int):
    """Revoke access to a product."""
    try:
        # Get the access record to find the workspace_id
        access = await ProductAccessService.get_product_access(access_id)
        if not access:
            raise HTTPException(status_code=404, detail="Access record not found")
        
        if access.product_id != product_id:
            raise HTTPException(status_code=400, detail="Access record does not match product")
        
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(access.workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        success = await ProductAccessService.revoke_access(access_id, access.workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Access record not found")
        
        return {"message": "Product access revoked successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to revoke product access")

@app.delete("/api/custom/products/{product_id}/access/remove")
async def remove_product_access(product_id: int, criteria: dict):
    """Remove product access based on criteria (access_type, target_id, workspace_id)."""
    try:
        access_type = criteria.get('access_type')
        target_id = criteria.get('target_id')
        workspace_id = criteria.get('workspace_id')
        
        if not access_type or not workspace_id:
            raise HTTPException(status_code=400, detail="access_type and workspace_id are required")
        
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        # Find and remove the matching access record
        access_list = await ProductAccessService.get_product_access_list(product_id)
        matching_access = None
        
        for access in access_list:
            if (access.access_type == access_type and 
                access.workspace_id == workspace_id and
                access.target_id == target_id):
                matching_access = access
                break
        
        if not matching_access:
            raise HTTPException(status_code=404, detail="No matching access record found")
        
        success = await ProductAccessService.revoke_access(matching_access.id, workspace_id, "current_user_123")
        if not success:
            raise HTTPException(status_code=404, detail="Failed to remove access record")
        
        return {"message": "Product access removed successfully"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to remove product access")

@app.get("/api/custom/products/{product_id}/access/check")
async def check_user_product_access(product_id: int, workspace_id: int):
    """Check if the current user has access to a specific product in a workspace."""
    try:
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        has_access = await ProductAccessService.check_user_product_access(product_id, "current_user_123", workspace_id)
        
        return {
            "product_id": product_id,
            "workspace_id": workspace_id,
            "user_id": "current_user_123",
            "has_access": has_access
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to check product access")

@app.get("/api/custom/products/workspace/{workspace_id}/access")
async def get_workspace_product_access(workspace_id: int):
    """Get all product access records for a specific workspace."""
    try:
        # Check if user is a member of the workspace
        member = await WorkspaceService.get_workspace_member(workspace_id, "current_user_123")
        if not member:
            raise HTTPException(status_code=403, detail="Access denied to workspace")
        
        access_list = await ProductAccessService.get_workspace_product_access(workspace_id)
        
        return {
            "workspace_id": workspace_id,
            "access_records": access_list,
            "count": len(access_list)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to retrieve workspace product access")


class LMSExportRequest(BaseModel):
    productIds: List[int]
    options: dict = {}
    token: Optional[str] = None


def validate_export_request(request: LMSExportRequest):
    if not request.productIds:
        raise HTTPException(status_code=400, detail="No products selected")
    if len(request.productIds) > 10:
        raise HTTPException(status_code=400, detail="Too many products selected for a single export")


@app.post("/api/custom/lms/export")
async def export_to_lms(
    request: LMSExportRequest,
    http_request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Export selected course outlines to LMS format with streaming keep-alive."""
    from app.services.lms_exporter import export_course_outline_to_lms_format

    logger.info(f"[API:LMS] Request start | productIds={request.productIds}")

    validate_export_request(request)

    user_uuid, user_email = await get_user_identifiers_for_workspace(http_request)
    onyx_user_id = user_uuid
    logger.info(f"[API:LMS] User resolved | onyx_user_id={onyx_user_id} email={user_email}")

    async with pool.acquire() as connection:
        accessible_products = await connection.fetch(
            """
            SELECT p.id
            FROM projects p
            LEFT JOIN design_templates dt ON p.design_template_id = dt.id
            WHERE p.id = ANY($1::int[]) AND p.onyx_user_id = $2 AND dt.microproduct_type = 'Training Plan'
            """,
            request.productIds, onyx_user_id
        )
        accessible_ids = [p['id'] for p in accessible_products]
        logger.info(f"[API:LMS] Accessible IDs | {accessible_ids}")
        if not accessible_ids:
            logger.warning("[API:LMS] No accessible course outlines found")
            raise HTTPException(status_code=404, detail="No accessible course outlines found")

    async def streamer():
        last_send = asyncio.get_event_loop().time()
        results = []
        total = len(accessible_ids)
        completed = 0
        # Resolve LMS base URL from user feature flags
        is_dev = True
        is_us = True
        try:
            async with DB_POOL.acquire() as connection:
                rows = await connection.fetch(
                    """
                    SELECT feature_name, is_enabled
                    FROM user_features
                    WHERE user_id = $1 AND feature_name IN ('is_us_lms','is_dev_lms','is_chudomaket')
                    """,
                    onyx_user_id,
                )
                flags = {r['feature_name']: bool(r['is_enabled']) for r in rows}
                is_chudo = flags.get('is_chudomaket', False)
                is_dev = flags.get('is_dev_lms', True)
                is_us = flags.get('is_us_lms', True)
        except Exception:
            pass
        smartexpert_base_url = (
            "https://lms.toliman.com.ua" if is_chudo else (
            "https://dev.smartexpert.net" if is_dev else (
            "https://app.smartexpert.io" if is_us else "https://app.smartexpert.net"))
        )
        yield (json.dumps({"type": "start", "total": total}) + "\n").encode()

        for product_id in accessible_ids:
            try:
                start_ts = asyncio.get_event_loop().time()
                yield (json.dumps({"type": "progress", "message": f"Exporting course {product_id}...", "productId": product_id}) + "\n").encode()
                export_task = asyncio.create_task(
                    export_course_outline_to_lms_format(
                        product_id,
                        onyx_user_id,
                        user_email,
                        request.token,
                        smartexpert_base_url,
                    )
                )
                while not export_task.done():
                    await asyncio.sleep(8)
                    elapsed = int(asyncio.get_event_loop().time() - start_ts)
                    # Heartbeat keep-alive + lightweight progress ping
                    yield b" "
                    yield (json.dumps({"type": "progress", "message": f"Working on course {product_id}... ({elapsed}s)", "productId": product_id}) + "\n").encode()
                course_structure = await export_task
                results.append(course_structure)
                completed += 1
                yield (json.dumps({
                    "type": "progress",
                    "message": f"Course {product_id} exported",
                    "productId": product_id,
                    "downloadLink": course_structure.get("downloadLink")
                }) + "\n").encode()
            except Exception as e:
                logger.error(f"[API:LMS] Course export failed | id={product_id} err={e}")
                results.append({
                    "courseTitle": f"Course {product_id}",
                    "error": str(e),
                    "downloadLink": None,
                    "structure": None
                })
                yield (json.dumps({
                    "type": "progress",
                    "message": f"Course {product_id} failed: {str(e)}",
                    "productId": product_id,
                    "error": True
                }) + "\n").encode()

            now = asyncio.get_event_loop().time()
            if now - last_send > 8:
                yield b" "
                last_send = now

        status = "completed" if all(r.get("downloadLink") for r in results) else "partial"
        final_payload = {
            "success": True,
            "message": "Export completed",
            "results": results,
            "lmsBaseUrl": smartexpert_base_url,
            "status": status
        }
        user_msg = f"Your courses have been exported to {smartexpert_base_url}. You can find them in your SmartExpert account linked to {user_email}."
        yield (json.dumps({"type": "done", "payload": final_payload, "userMessage": user_msg}) + "\n").encode()
        return

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "X-Accel-Buffering": "no"
        }
    )


@app.get("/api/custom/lms/export/{export_id}/status")
async def get_export_status(export_id: str):
    return {"exportId": export_id, "status": "completed", "progress": 100}

@app.get("/api/custom/lms/user-settings")
async def get_lms_user_settings(http_request: Request):
    try:
        user_uuid, _ = await get_user_identifiers_for_workspace(http_request)
        async with DB_POOL.acquire() as connection:
            row = await connection.fetchrow("SELECT lms_account_choice FROM lms_user_settings WHERE onyx_user_id = $1", user_uuid)
            return {"choice": row["lms_account_choice"] if row else None}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to load LMS user settings")

@app.post("/api/custom/lms/user-settings")
async def set_lms_user_settings(http_request: Request):
    try:
        body = await http_request.json()
        choice = (body or {}).get("choice")
        if choice not in ("yes", "no-success", "no-failed"):
            raise HTTPException(status_code=400, detail="Invalid choice")
        user_uuid, _ = await get_user_identifiers_for_workspace(http_request)
        async with DB_POOL.acquire() as connection:
            await connection.execute(
                """
                INSERT INTO lms_user_settings (onyx_user_id, lms_account_choice)
                VALUES ($1, $2)
                ON CONFLICT (onyx_user_id) DO UPDATE SET lms_account_choice = EXCLUDED.lms_account_choice, updated_at = NOW()
                """,
                user_uuid, choice
            )
            return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to save LMS user settings")

@app.post("/api/custom/admin/lms/reset-user-modal")
async def reset_lms_user_modal_admin(http_request: Request):
    """Admin-only: reset the LMS account choice so the modal shows again for a specific user."""
    await verify_admin_user(http_request)
    try:
        body = await http_request.json()
        target_user_id = (body or {}).get("user_id")
        if not target_user_id:
            raise HTTPException(status_code=400, detail="Missing user_id")
        async with DB_POOL.acquire() as connection:
            await connection.execute(
                "DELETE FROM lms_user_settings WHERE onyx_user_id = $1",
                target_user_id,
            )
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="Failed to reset LMS modal state")

@app.post("/api/custom/lms/create-workspace-owner")
async def create_workspace_owner(http_request: Request):
    """Create SmartExpert Workspace Owner using user's email and provided or default token."""
    try:
        user_uuid, user_email = await get_user_identifiers_for_workspace(http_request)
        # Use the same user id resolution as the browser feature-check endpoint
        onyx_user_id = await get_current_onyx_user_id(http_request)
        if not user_email:
            raise HTTPException(status_code=400, detail="Unable to resolve user email from session")
        name_part = user_email.split("@")[0]
        try:
            body = await http_request.json()
        except Exception:
            body = {}
        token = (body or {}).get("token")
        try:
            from app.services.lms_exporter import DEFAULT_SMARTEXPERT_TOKEN
        except Exception:
            DEFAULT_SMARTEXPERT_TOKEN = None
        # Resolve token by base URL choice later; don't set here
        params = {"name": name_part, "email": user_email}
        # Resolve LMS base URL from user feature flags
        is_dev = True
        is_us = True
        try:
            async with DB_POOL.acquire() as connection:
                rows = await connection.fetch(
                    """
                    SELECT feature_name, is_enabled 
                    FROM user_features 
                    WHERE user_id = $1 AND feature_name IN ('is_us_lms','is_dev_lms','is_chudomaket')
                    """,
                    onyx_user_id,
                )
                flags = {r['feature_name']: bool(r['is_enabled']) for r in rows}
                is_chudo = flags.get('is_chudomaket', False)
                is_dev = flags.get('is_dev_lms', True)
                is_us = flags.get('is_us_lms', True)
        except Exception:
            pass
        if is_chudo:
            base_url = "https://lms.toliman.com.ua"
            resolved_token = token or os.environ.get('LMS_CHUDO_TOKEN') or DEFAULT_SMARTEXPERT_TOKEN
        elif is_dev:
            base_url = "https://dev.smartexpert.net"
            resolved_token = token or DEFAULT_SMARTEXPERT_TOKEN
        else:
            if is_us:
                base_url = "https://app.smartexpert.io"
                resolved_token = token or os.environ.get('LMS_IO_TOKEN') or DEFAULT_SMARTEXPERT_TOKEN
            else:
                base_url = "https://app.smartexpert.net"
                resolved_token = token or os.environ.get('LMS_NET_TOKEN') or DEFAULT_SMARTEXPERT_TOKEN
        params["token"] = resolved_token or ""
        target_url = f"{base_url}/store-workspace-owner"
        logger.info(f"[API:LMS] Workspace owner create start | email={user_email} name={name_part} target={target_url}")
        import httpx
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.get(target_url, params=params, headers={"User-Agent": "Custom Extensions Backend"})
            ok = resp.status_code in (200, 201, 202, 204, 302, 303)
            redirect_url = resp.headers.get("location") or resp.headers.get("Location")
            logger.info(f"[API:LMS] Workspace owner create result | status={resp.status_code} redirect={redirect_url}")
            # Persist choice server-side if successful
            if ok:
                try:
                    async with DB_POOL.acquire() as connection:
                        await connection.execute(
                            """
                            INSERT INTO lms_user_settings (onyx_user_id, lms_account_choice, updated_at)
                            VALUES ($1, $2, NOW())
                            ON CONFLICT (onyx_user_id) DO UPDATE SET lms_account_choice = EXCLUDED.lms_account_choice, updated_at = NOW()
                            """,
                            user_uuid, 'no-success'
                        )
                except Exception as _:
                    pass
            return {"success": ok, "status": resp.status_code, "email": user_email, "redirectUrl": redirect_url, "data": resp.text}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API:LMS] Workspace owner create failed: {e}")
        raise HTTPException(status_code=500, detail=f"Workspace owner creation failed: {str(e)}")

@app.on_event("startup")
async def startup_event_lms_exports():
    try:
        async with DB_POOL.acquire() as connection:
            # Ensure lms_user_settings table exists (per-account persistence)
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS lms_user_settings (
                    onyx_user_id VARCHAR(255) PRIMARY KEY,
                    lms_account_choice VARCHAR(32),
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
            )
            await connection.execute(
                """
                CREATE TABLE IF NOT EXISTS lms_exports (
                    id SERIAL PRIMARY KEY,
                    user_id VARCHAR(255) NOT NULL,
                    product_ids JSONB NOT NULL,
                    status VARCHAR(50) DEFAULT 'processing',
                    progress INTEGER DEFAULT 0,
                    result_data JSONB,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    completed_at TIMESTAMP WITH TIME ZONE
                );
                """
            )
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_user_id ON lms_exports(user_id);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_status ON lms_exports(status);")
            await connection.execute("CREATE INDEX IF NOT EXISTS idx_lms_exports_created_at ON lms_exports(created_at);")
            logger.info("'lms_exports' table ensured.")
    except Exception as e:
        logger.error(f"Failed to ensure lms_exports table: {e}")

# 🔍 STATIC FILE LOGGING MIDDLEWARE
@app.middleware("http")
async def log_static_file_requests(request: Request, call_next):
    """Middleware to log all requests to static files, especially AI-generated images"""
    
    # Check if this is a request to static design images
    if request.url.path.startswith(f"/{STATIC_DESIGN_IMAGES_DIR}/"):
        logger.info(f"🔍 [STATIC FILE REQUEST] Incoming request for static file")
        logger.info(f"🔍 [STATIC FILE REQUEST] Path: {request.url.path}")
        logger.info(f"🔍 [STATIC FILE REQUEST] Method: {request.method}")
        logger.info(f"🔍 [STATIC FILE REQUEST] Headers: {dict(request.headers)}")
        
        # Check if file exists on disk
        file_path = os.path.join(STATIC_DESIGN_IMAGES_DIR, os.path.basename(request.url.path))
        logger.info(f"🔍 [STATIC FILE REQUEST] Expected file path: {file_path}")
        logger.info(f"🔍 [STATIC FILE REQUEST] File exists: {os.path.exists(file_path)}")
        
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            logger.info(f"🔍 [STATIC FILE REQUEST] File size on disk: {file_size} bytes")
            
            # Check file content
            try:
                with open(file_path, "rb") as f:
                    first_bytes = f.read(20)
                    logger.info(f"🔍 [STATIC FILE REQUEST] File first 20 bytes: {first_bytes}")
            except Exception as e:
                logger.error(f"❌ [STATIC FILE REQUEST ERROR] Could not read file: {e}")
    
    # Process the request
    response = await call_next(request)
    
    # Log response details for static files
    if request.url.path.startswith(f"/{STATIC_DESIGN_IMAGES_DIR}/"):
        logger.info(f"🔍 [STATIC FILE RESPONSE] Response status: {response.status_code}")
        logger.info(f"🔍 [STATIC FILE RESPONSE] Response headers: {dict(response.headers)}")
        
        # Log content length if available
        content_length = response.headers.get("content-length")
        if content_length:
            logger.info(f"🔍 [STATIC FILE RESPONSE] Content-Length: {content_length} bytes")
        else:
            logger.warning(f"⚠️ [STATIC FILE RESPONSE WARNING] No Content-Length header")
        
        # Log content type
        content_type = response.headers.get("content-type")
        if content_type:
            logger.info(f"🔍 [STATIC FILE RESPONSE] Content-Type: {content_type}")
        else:
            logger.warning(f"⚠️ [STATIC FILE RESPONSE WARNING] No Content-Type header")
        
        # Check if response is suspiciously small
        if content_length and int(content_length) < 1000:
            logger.warning(f"⚠️ [STATIC FILE RESPONSE WARNING] Response is suspiciously small: {content_length} bytes")
    
    return response
    
class SCORMExportRequest(BaseModel):
    courseOutlineId: int

@app.post("/api/custom/lms/export/scorm")
async def export_scorm_package(
    request: SCORMExportRequest,
    http_request: Request,
    pool: asyncpg.Pool = Depends(get_db_pool)
):
    """Build and return a SCORM 2004 (4th Ed) package zip for a course outline."""
    try:
        # Resolve user identity
        user_uuid, _ = await get_user_identifiers_for_workspace(http_request)
        onyx_user_id = user_uuid

        # Validate access: ensure the course is a Training Plan owned by the user
        async with pool.acquire() as connection:
            row = await connection.fetchrow(
                """
                SELECT p.id
                FROM projects p
                LEFT JOIN design_templates dt ON p.design_template_id = dt.id
                WHERE p.id = $1 AND p.onyx_user_id = $2 AND dt.microproduct_type = 'Training Plan'
                """,
                request.courseOutlineId, onyx_user_id
            )
            if not row:
                raise HTTPException(status_code=404, detail="Course outline not found or not accessible")

        # Build SCORM package
        from app.services.scorm_packager import build_scorm_package_zip
        filename, zip_bytes = await build_scorm_package_zip(request.courseOutlineId, onyx_user_id)

        return StreamingResponse(
            content=io.BytesIO(zip_bytes),
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API:SCORM] Export failed: {e}")
        raise HTTPException(status_code=500, detail="Failed to export SCORM package")
