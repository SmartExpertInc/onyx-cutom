# File Context as Primary Knowledge Source Implementation

## Problem Solved

Previously, when generating content from files, the extracted file context was used merely as "additional context" for the AI prompt. The AI would often rely on its general internet knowledge rather than the specific information in the user's uploaded files, resulting in:

- Generic content that didn't reflect the specific information in the files
- Missing specific examples, data, or case studies from the documents
- Content that taught general concepts rather than what was IN the files
- User frustration when file-based content didn't match the uploaded documents

## Solution Implemented

Created **two streaming functions** that handle OpenAI generation, with the hybrid version treating extracted file content as the **PRIMARY knowledge source** rather than just context:

### 1. `stream_openai_response()` - Direct Generation

For requests without file context (general knowledge requests).

### 2. `stream_hybrid_response()` - File-Based Generation

For requests with file context - NOW TREATS FILES AS PRIMARY KNOWLEDGE SOURCE.

## Key Changes

### File: `custom_extensions/backend/main.py`

### Intelligent Token Limit Handling

**Smart Content Selection:**
- Calculates token count before sending to OpenAI
- Uses full content when files are small enough (< 100k tokens)
- Automatically switches to enhanced summaries + excerpts for large files
- Maximizes document content while staying within API limits

**Token Limits:**
- Max context: 100,000 tokens (~400,000 characters)
- Token estimation: 1 token â‰ˆ 4 characters
- Reserves 28k tokens for AI response

### New Functions (Lines 10627-10810)

#### Function 1: `stream_openai_response()` (Lines 10629-10664)

```python
async def stream_openai_response(message: str, temperature: float = 0.7):
    """Stream response from OpenAI directly without file context."""
```

**Purpose**: Handle content generation when no files are provided (general knowledge mode).

**Features**:
- Direct OpenAI streaming
- No file context injection
- Simple pass-through for wizard messages

#### Function 2: `stream_hybrid_response()` (Lines 10666-10810)

```python
async def stream_hybrid_response(message: str, file_context: Any, product_type: str = "Course Outline", temperature: float = 0.7):
    """
    Stream response from OpenAI using extracted file context as PRIMARY knowledge source.
    
    Intelligently handles large files:
    - Counts tokens to stay within limits (100k tokens max for context)
    - Uses full content when possible
    - Falls back to enhanced summaries for large files
    """
```

**Purpose**: Generate content based PRIMARILY on uploaded file content with intelligent token management.

**Critical Enhancements**:

1. **Explicit Primary Source Instructions** (Lines 10706-10716):
   ```python
   "**CRITICAL INSTRUCTION**: The following content is extracted from the user's uploaded documents. "
   "This is your PRIMARY and MOST IMPORTANT source of information. You MUST:\n"
   "1. Base ALL your content generation on this document content\n"
   "2. Use facts, concepts, and information DIRECTLY from these documents\n"
   "3. Reference and teach the specific topics found in these documents\n"
   "4. DO NOT rely on general internet knowledge unless the documents lack specific information\n"
   "5. If documents contain specific examples, data, or case studies, USE THEM in your content\n"
   "6. Your goal is to teach what's IN these documents, not general knowledge about the topic\n"
   ```

2. **Full Document Content Inclusion** (Lines 10722-10728):
   - Includes COMPLETE file contents, not just summaries
   - Each document clearly labeled and separated
   - Preserves all examples, data, and specific information

3. **System Message Reinforcement** (Lines 10753-10756):
   ```python
   "role": "system",
   "content": "You are an educational content creator. When provided with source documents, 
              you MUST use them as your primary knowledge source. Base all your content on 
              what's actually in the documents, not on general knowledge."
   ```

4. **Intelligent Large File Handling** (Lines 10726-10772):
   - **Token Counting**: Calculates estimated tokens before building prompt
   - **Smart Decision**: Uses full content if < 100k tokens, enhanced excerpts if larger
   - **Enhanced Excerpts Mode** (for large files):
     - Includes summary of each document
     - Adds key content excerpt from beginning of each file
     - Cuts at sentence boundaries for natural breaks
     - Shows how much content was truncated
     - Distributes available tokens equally across files
   
   ```python
   # Calculate total size
   total_content_chars = sum(len(content) for content in file_contents if content)
   estimated_tokens = total_content_chars * 0.25  # 1 token â‰ˆ 4 chars
   
   if estimated_tokens < MAX_CONTEXT_TOKENS:
       # Use FULL CONTENT
       for content in file_contents:
           file_content_section += f"### Document {i}:\n\n{content}\n\n"
   else:
       # Use ENHANCED SUMMARIES + EXCERPTS
       max_chars_per_file = int((MAX_CONTEXT_TOKENS / 0.25) / len(file_contents))
       for content in file_contents:
           file_content_section += f"**Summary**: {summary}\n\n"
           excerpt = content[:max_chars_per_file]
           file_content_section += f"**Key Content Excerpt**: {excerpt}\n\n"
   ```

5. **Visual Separation** (Lines 10710-10783):
   - Document content wrapped in clear visual markers
   - Header: "ðŸ“š SOURCE DOCUMENT CONTENT (PRIMARY KNOWLEDGE BASE)"
   - Footer: "END OF SOURCE DOCUMENTS - USE THIS AS YOUR PRIMARY KNOWLEDGE BASE"
   - Makes it impossible for AI to miss the source material

### Enhanced Context Structure

**Decision Tree for Content Inclusion**:

```
Calculate total content size
  â†“
Estimate tokens (chars Ã— 0.25)
  â†“
Is estimated_tokens < 100,000?
  â†“
YES â†’ Use FULL CONTENT          NO â†’ Use ENHANCED EXCERPTS
  â†“                                â†“
Include complete text        Include summary + excerpt
from each file              from each file
  â†“                                â†“
Log: "Using FULL CONTENT"    Log: "Using ENHANCED SUMMARIES"
```

**What Gets Included (Small Files - Full Content Mode)**:

1. **Key Topics** (Line 10723): Top 20 topics from documents for quick reference
2. **Full Document Contents** (Lines 10736-10741): Complete text from each file
3. **Visual Markers**: Clear headers and separators
4. **Document Numbers**: Each file clearly labeled

**What Gets Included (Large Files - Enhanced Excerpts Mode)**:

1. **Key Topics** (Line 10723): Top 20 topics from documents for quick reference
2. **Per-File Summary** (Lines 10756-10758): Existing summary of each document
3. **Key Content Excerpt** (Lines 10760-10768): Beginning of each file (smart truncation)
4. **Truncation Notice**: Shows how much content was excluded
5. **Token Distribution**: Available tokens split equally across all files
6. **Sentence Boundaries**: Cuts at periods when possible for natural breaks

### Integration with Existing Flow

The functions integrate seamlessly with the existing wizard preview flow:

- **Line 16636**: `stream_hybrid_response()` called when file context available
- **Line 16784**: `stream_openai_response()` called for direct generation
- **Both return same format**: `{"type": "delta", "text": "..."}` for streaming compatibility

## Examples

### Example 1: Small Files (Full Content Mode)

**Input**: 3 files, 50k characters each = 150k chars (~37.5k tokens)

**Output**:
```
================================================================================
ðŸ“š SOURCE DOCUMENT CONTENT (PRIMARY KNOWLEDGE BASE)
================================================================================

**CRITICAL INSTRUCTION**: Use as PRIMARY source...
[instructions]

**Key Topics from Documents**: machine learning, neural networks, deep learning, ...

---
**FULL DOCUMENT CONTENT**:
---

### Document 1:
[Complete 50k characters of first file]
---

### Document 2:
[Complete 50k characters of second file]
---

### Document 3:
[Complete 50k characters of third file]
---

END OF SOURCE DOCUMENTS
================================================================================
```

**Log**: `[HYBRID_STREAM] Using FULL CONTENT (within token limit)`

### Example 2: Large Files (Enhanced Excerpts Mode)

**Input**: 10 files, 100k characters each = 1M chars (~250k tokens) - EXCEEDS LIMIT

**Output**:
```
================================================================================
ðŸ“š SOURCE DOCUMENT CONTENT (PRIMARY KNOWLEDGE BASE)
================================================================================

**CRITICAL INSTRUCTION**: Use as PRIMARY source...
[instructions]

**Key Topics from Documents**: machine learning, neural networks, deep learning, ...

---
**DOCUMENT CONTENT** (Enhanced excerpts from large documents):
---

**Note**: Documents were too large to include in full. Below are key excerpts and summaries.

### Document 1:

**Summary**: This document covers machine learning fundamentals including supervised learning, neural networks, and practical applications...

**Key Content Excerpt** (first ~40,000 chars of 100,000):
Machine learning is a subset of artificial intelligence...
[40k characters of actual content]...
concluding the introduction section.

*[Document continues with 60,000 more characters]*

---

### Document 2:
[Similar structure: summary + excerpt]
---

...

END OF SOURCE DOCUMENTS
================================================================================
```

**Log**: `[HYBRID_STREAM] Using ENHANCED SUMMARIES (content exceeds token limit)`

## Before vs. After Comparison

### Before (Context as Secondary)

**Prompt Structure**:
```
WIZARD_REQUEST: Create course outline about Machine Learning
[Some file summary at the end as "context"]
```

**AI Behavior**:
- Generates generic ML course from internet knowledge
- Might mention files but doesn't use their specific content
- Missing specific examples/data from documents

### After (Context as Primary)

**Prompt Structure**:
```
================================================================================
ðŸ“š SOURCE DOCUMENT CONTENT (PRIMARY KNOWLEDGE BASE)
================================================================================

**CRITICAL INSTRUCTION**: This is your PRIMARY source. You MUST:
1. Base ALL content on this document content
2. Use facts DIRECTLY from these documents
[... detailed instructions ...]

---
**FULL DOCUMENT CONTENT**:
---

### Document 1:
[Complete content from user's first file]

### Document 2:
[Complete content from user's second file]
...

================================================================================
END OF SOURCE DOCUMENTS - USE THIS AS YOUR PRIMARY KNOWLEDGE BASE
================================================================================

WIZARD_REQUEST: Create course outline about Machine Learning
```

**AI Behavior**:
- Reads and analyzes the FULL document content first
- Generates course based on what's IN the documents
- Uses specific examples, data, and case studies from files
- Only supplements with general knowledge if documents lack info

## Technical Implementation Details

### Error Handling

- **Missing API Key**: Graceful error with clear message (Lines 10641-10645, 10686-10690)
- **Streaming Errors**: Caught and yielded as error type (Lines 10662-10664, 10768-10770)
- **Context Type Handling**: Supports both dict and string contexts (Lines 10697-10743)

### Logging

Comprehensive logging at multiple levels:
- File context structure: `[HYBRID_STREAM] File context: X contents, Y summaries, Z topics`
- Message lengths: `Enhanced message length: X (original: Y, file context: Z)`
- Streaming start: `Starting streaming for product type: {product_type}`

### Performance Considerations

1. **Full Content vs. Summaries**: Prefers full content for accuracy, falls back to summaries
2. **Topic Limiting**: Only includes top 20 topics to avoid bloat (Line 10720)
3. **Streaming**: Yields chunks as they arrive for responsive UI

## API Configuration

### OpenAI Model

- **Model**: `gpt-4-turbo-preview` (Lines 10652, 10751)
- **Reason**: Larger context window for full document content
- **Future**: Can be configured based on content size

### Temperature

- **Default**: 0.7 (balanced creativity)
- **Configurable**: Parameter available for adjustment
- **Rationale**: Lower than 1.0 for more faithful reproduction of document content

## Testing Recommendations

### Test Scenarios

1. **Single File**:
   - Upload one document with specific data
   - Verify generated content uses that specific data

2. **Multiple Files**:
   - Upload 3-5 documents with related content
   - Verify content synthesizes from all documents

3. **Specific Examples**:
   - Upload document with specific case study/example
   - Verify generated content includes that example

4. **Data-Rich Documents**:
   - Upload document with statistics/data
   - Verify generated content references those specific numbers

### Validation Checklist

âœ… Content references specific information from files  
âœ… Examples from documents appear in generated content  
âœ… Data/statistics from files are used  
âœ… Content teaches what's IN files, not general knowledge  
âœ… Missing information prompts general knowledge supplement  
âœ… Multiple files properly synthesized  

## Benefits

### For Users

1. **Accurate Content**: Generated content matches uploaded documents
2. **Specific Examples**: Real examples from documents included
3. **Data Preservation**: Statistics and data from files used
4. **Trust**: Content demonstrably based on their materials
5. **Large File Support**: Works with files of any size without token limit errors
6. **Cost Effective**: Only sends necessary content, reducing API costs

### For System

1. **Better Quality**: More relevant, specific content generation
2. **Clear Intent**: AI understands to prioritize document content
3. **Fewer Rewrites**: Content matches expectations first time
4. **Scalable**: Works with any number and size of files
5. **No Token Errors**: Intelligent truncation prevents API limit errors
6. **Optimal Use**: Maximizes document content within available tokens
7. **Transparent**: Logs show whether full content or excerpts were used

## Integration with Parallel File Processing

This enhancement works perfectly with the parallel file processing (see `PARALLEL_FILE_PROCESSING_IMPLEMENTATION.md`):

1. **Files processed in parallel** (8 at a time) â†’ Fast extraction
2. **Full content extracted** from each file â†’ Rich context
3. **Context passed to hybrid streaming** â†’ Primary knowledge source
4. **OpenAI generates from documents** â†’ Accurate, specific content

**End-to-End Flow**:
```
User uploads 10 files
  â†“
Parallel batch processing (45 seconds instead of 200 seconds)
  â†“
Full file contents extracted
  â†“
Hybrid streaming with PRIMARY knowledge instructions
  â†“
Content generated from actual file data
```

## Files Modified

- `custom_extensions/backend/main.py`:
  - **Lines 10627-10664**: Added `stream_openai_response()` for direct generation
  - **Lines 10666-10810**: Added `stream_hybrid_response()` with intelligent token management
    - Lines 10694-10696: Token limit configuration (100k tokens max)
    - Lines 10726-10730: Token counting and size calculation
    - Lines 10733-10741: Full content mode (small files)
    - Lines 10743-10772: Enhanced excerpts mode (large files)
    - Lines 10756-10768: Smart excerpt generation with sentence boundaries
  - **Lines 10710-10720**: Explicit PRIMARY source instructions
  - **Lines 10795-10798**: System message reinforcing document-first approach

## Environment Requirements

**Required Environment Variable**:
```bash
OPENAI_API_KEY=sk-...your-key-here...
```

**Model Access**: Requires access to `gpt-4-turbo-preview` model

## Future Enhancements

Potential improvements for future iterations:

1. **Citation Tracking**: Mark which parts of content came from which document
2. **Confidence Scores**: Indicate when AI supplements with general knowledge
3. **Document Relevance**: Pre-filter documents by relevance to request
4. **Adaptive Context**: Adjust context size based on document sizes
5. **Multi-Language**: Handle documents in different languages
6. **Vector Search**: Use embeddings to find most relevant sections of large documents

## Related Documentation

- `PARALLEL_FILE_PROCESSING_IMPLEMENTATION.md` - Fast parallel file extraction
- `TIMEOUT_ISSUE_FIXED.md` - Connection timeout handling
- `content_builder_ai.txt` - AI assistant instructions for file-based content

